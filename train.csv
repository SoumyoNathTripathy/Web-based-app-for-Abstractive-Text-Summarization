FileName,Abstract,RHS,," This paper introduces four classes of rotation invariant orthogonal moments by generalizing four existing moments that use harmonic functions in their radial kernels . Members of these classes share beneficial properties for image representation and pattern recognition like orthogonality and rotation invariance . The kernel sets of these generic harmonic function based moments are complete in the Hilbert space of square integrable continuous complex valued functions . Due to their resemble definition the computation of these kernels maintains the simplicity and numerical stability of existing harmonic function based moments . In addition each member of one of these classes has distinctive properties that depend on the value of a parameter making it more suitable for some particular applications . Comparison with existing orthogonal moments defined based on Jacobi polynomials and eigenfunctions has been carried out and experimental results show the effectiveness of these classes of moments in terms of representation capability and discrimination power . 
"," This paper introduces four classes of rotation invariant orthogonal moments by generalizing four existing moments that use harmonic functions in their radial kernels . Members of these classes share beneficial properties for image representation and pattern recognition like orthogonality and rotation invariance . The kernel sets of these generic harmonic function based moments are complete in the Hilbert space of square integrable continuous complex valued functions . Due to their resemble definition the computation of these kernels maintains the simplicity and numerical stability of existing harmonic function based moments . In addition each member of one of these classes has distinctive properties that depend on the value of a parameter making it more suitable for some particular applications . Comparison with existing orthogonal moments defined based on Jacobi polynomials and eigenfunctions has been carried out and experimental results show the effectiveness of these classes of moments in terms of representation capability and discrimination power . 
"
S0262885614000857,,,,,
S0169260715300419," In this paper a MATLAB based graphical user interface software tool for general biomedical signal processing and analysis of functional neuroimaging data is introduced . Specifically electroencephalography and electrocardiography signals can be processed and analyzed by the developed tool which incorporates commonly used temporal and frequency analysis methods . In addition to common methods the tool also provides non linear chaos analysis with Lyapunov exponents and entropies multivariate analysis with principal and independent component analyses and pattern classification with discriminant analysis . This tool can also be utilized for training in biomedical engineering education . This easy to use and easy to learn intuitive tool is described in detail in this paper . 
",Design of a MATLAB based GUI tool for general biomedical signal processing and analysis. EEG and ECG signals can be processed and analyzed by the designed tool. The easy to use and easy to learn intuitive tool provides non linear chaos analysis. The tool provides entropy multivariate analysis with PCA and ICA pattern classification. The tool can also be utilized for training in biomedical engineering education.,,,
S0169260715003260," Background and objective A markerless low cost prototype has been developed for the determination of some spatio temporal parameters of human gait step length step width and cadence have been considered . Only a smartphone and a high definition webcam have been used . Methods The signals obtained by the accelerometer embedded in the smartphone are used to recognize the heel strike events while the feet positions are calculated through image processing of the webcam stream . Step length and width are computed during gait trials on a treadmill at various speeds . Results Six subjects have been tested for a total of 504 steps . Results were compared with those obtained by a stereo photogrammetric system . The maximum average errors were 3.7cm for the right step length and 1.63cm for the right step width at 5km h. The maximum average error for step duration was 0.02s at 5km h for the right steps . Conclusion The system is characterized by a very high level of automation that allows its use by non expert users in non structured environments . A low cost system able to automatically provide a reliable and repeatable evaluation of some gait events and parameters during treadmill walking is relevant also from a clinical point of view because it allows the analysis of hundreds of steps and consequently an analysis of their variability . 
",A markerless low cost system for the estimation of some spatial temporal parameters of human gait. A system usable on not instrumented treadmill. A system characterized by a very high level of automation. A system can be used to analyze up to one hundred steps. A system with precision comparable with the gold standard.,,,
S0262885613001443," This paper presents an improved multiple instance learning tracker representing target with Distribution Fields and building a weighted geometric mean MIL classifier . Firstly we adopt DF layer as feature instead of traditional Haar like one to model the target thanks to the DF specificity and the landscape smoothness . Secondly we integrate sample importance into the weighted geometric mean MIL model and derive an online approach to maximize the bag likelihood by AnyBoost gradient framework to select the most discriminative layers . Due to the target model consisting of selected discriminative layers our tracker is more robust while needing fewer features than the traditional Haar like one and the original DFs one . The experimental results show higher performances of our tracker than those of five state of the art ones on several challenging video sequences . 
",We adopt Distribution Field DF layer as feature instead of traditional Haar like one to robustly model the target. We derive an online weighted geometric mean MIL classifier to select the most discriminative layers. Our tracker is more robust while needing fewer features than the traditional Haar like one and the original DFs one. The experiments show higher performance of our tracker than five state of the art ones.,,,
S0262885614000511," Text based image retrieval may perform poorly due to the irrelevant and or incomplete text surrounding the images in the web pages . In such situations visual content of the images can be leveraged to improve the image ranking performance . In this paper we look into this problem of image re ranking and propose a system that automatically constructs multiple candidate multi instance bags which are likely to contain relevant images . These automatically constructed bags are then utilized by ensembles of Multiple Instance Learning classifiers and the images are re ranked according to the final classification responses . Our method is unsupervised in the sense that the only input to the system is the text query itself without any user feedback or annotation . The experimental results demonstrate that constructing multiple instance bags based on the retrieval order and utilizing ensembles of MIL classifiers greatly enhance the retrieval performance achieving on par or better results compared to the state of the art . 
",A system that constructs multi instance bags from text based retrieval order. Ensemble of MI classifiers is learned using these multi instance bags. We report image re ranking performance on multiple datasets. Our system receives on par or better results than the state of the art.,,,
S0169260715300262," Background and objective Non compartmental analysis calculates pharmacokinetic metrics related to the systemic exposure to a drug following administration e.g . area under the concentration time curve and peak concentration . We developed a new package in R called ncappc to perform a NCA and simulation based posterior predictive checks for a population PK model using NCA metrics . Methods The nca feature of ncappc package estimates the NCA metrics by NCA . The ppc feature of ncappc estimates the NCA metrics from multiple sets of simulated concentration time data and compares them with those estimated from the observed data . The diagnostic analysis is performed at the population as well as the individual level . The distribution of the simulated population means of each NCA metric is compared with the corresponding observed population mean . The individual level comparison is performed based on the deviation of the mean of any NCA metric based on simulations for an individual from the corresponding NCA metric obtained from the observed data . The ncappc package also reports the normalized prediction distribution error of the simulated NCA metrics for each individual and their distribution within a population . Results The ncappc produces two default outputs depending on the type of analysis performed i.e . NCA and PopPK diagnosis . The PopPK diagnosis feature of ncappc produces 8 sets of graphical outputs to assess the ability of a population model to simulate the concentration time profile of a drug and thereby evaluate model adequacy . In addition tabular outputs are generated showing the values of the NCA metrics estimated from the observed and the simulated data along with the deviation NPDE regression parameters used to estimate the elimination rate constant and the related population statistics . Conclusions The ncappc package is a versatile and flexible tool set written in R that successfully estimates NCA metrics from concentration time data and produces a comprehensive set of graphical and tabular output to summarize the diagnostic results including the model specific outliers . The output is easy to interpret and to use in evaluation of a population PK model . ncappc is freely available on CRAN and GitHub . 
",ncappc performs i NCA and ii simulation based posterior predictive checks using NCA metrics. ncappc package is highly flexible and comprehensive. Can perform both individual and population level diagnostics. Produces output summarizing the diagnostic results including the model specific outliers. The output is easy to interpret and to use in evaluation of a population PK model.,,,
S0169260715003284," The cardiovascular and respiratory autonomic nervous regulation has been studied mainly by hemodynamic responses during different physical stressors . In this study dynamics of autonomic response to an orthostatic challenge was investigated by hemodynamic variables and by diverse linear and nonlinear indices calculated from time series of beat to beat intervals respiratory cycle duration systolic and diastolic blood pressure . This study included 16 young female patients with vasovagal syncope and 12 age matched female controls . The subjects were enrolled in a head up tilt test breathing normally including 5min of baseline and 18min of 70 orthostatic phase . To increase the time resolution of the analysis the time series were segmented in five minute overlapping windows with a shift of 1min . Hemodynamic parameters did not show any statistical differences between SYN and CON . Time domain linear analysis revealed increased respiratory frequency and increased blood pressure variability in patients during OP meaning increased sympathetic activity and vagal withdrawal . Frequency domain analysis confirmed a predominance of sympathetic tone by steadily increased values of low over high frequency power in BBI and of low frequency power in SYS and DIA in patients during OP . The nonlinear analysis by symbolic dynamics seemed to be highly suitable for differentiation of SYN and CON in the early beginning of OP i.e . 5min after tilt up . In particular the index SYS plvar3 showed less patterns of low variability in patients reflecting a steadily increase in both BPV and sympathetic activity . The proposed dynamical analysis could lead to a better understanding of the temporal underlying mechanisms in healthy subjects and patients under orthostatic stress . 
",Nonlinear dynamic analysis shows sympathetic and vagal activities fluctuations. Hemodynamic analysis did not show any statistical differences between subjects. Blood pressure variability showed the highest differences between CON and SYN. In patients systolic blood pressure variability increased immediately after tilt up. Healthy women show a stable sympatho vagal balance during orthostatic phase.,,,
S0198971516300084," Flooding is a widely occurring natural hazard that noticeably damages property people and the environment . In the context of climate change the integration of spatial planning with flood risk management has gained prominence as an approach to mitigating the risks of flooding . The absence of easy access to integrated and high quality information and the technologies and tools to use information are among the factors that impede this integration . Limited research has been conducted to develop a framework and to investigate the role of information and technologies in this integration . This study draws primarily on the European experiences and literature and identifies three dimensions of the integration of spatial planning with flood risk management territorial policy and institutional . To facilitate integration and in accord with these three dimensions a Spatially Integrated Policy Infrastructure is conceptualised that encompasses data and information decision support and analysis tools and access tools and protocols . This study presents the connections between SIPI elements and integration dimensions which is important for a better understanding of roles of geographic information and technologies in integration . The conceptual framework of SIPI will govern further development and evaluation of SIPI . 
",Integration of spatial planning and flood risk management has three dimensions territorial policy and institutional. Developing infrastructures sharing both data and models and systems is a promising way to support integration. This paper conceptualises the Spatially Integrated Policy Infrastructure SIPI which focuses on policy integration.,,,
S0262885614000134," Dense disparity map is required by many great 3D applications. In this paper a novel stereo matching algorithm is presented. The main contributions of this work are three fold. Firstly a new cost volume filtering method is proposed. A novel concept named two level local adaptation is introduced to guide the proposed filtering approach. Secondly a novel post processing method is proposed to handle both occlusions and textureless regions. Thirdly a parallel algorithm is proposed to efficiently calculate an integral image on GPU and it accelerates the whole cost volume filtering process. The overall stereo matching algorithm generates the state of the art results. At the time of submission it ranks the 10th among about 152 algorithms on the Middlebury stereo evaluation benchmark and takes the 1st place in all local methods. By implementing the entire algorithm on the NVIDIA Tesla C2050 GPU it can achieve over 30 million disparity estimates per second MDE s . 
",Hghlights A novel local stereo matching algorithm with linear complexity is proposed. The overall algorithm generates the state of the art results. Two level local adaptation is introduced to guide the adaptive guided filtering. The novel post processing method handles both occlusions and textureless regions. A parallel algorithm is proposed to speed up the algorithm on GPU.,,,
S0262885613001741," Since 2005 human and computer performance has been systematically compared as part of face recognition competitions with results being reported for both still and video imagery. The key results from these competitions are reviewed. To analyze performance across studies the cross modal performance analysis CMPA framework is introduced. The CMPA framework is applied to experiments that were part of face a recognition competition. The analysis shows that for matching frontal faces in still images algorithms are consistently superior to humans. For video and difficult still face pairs humans are superior. Finally based on the CMPA framework and a face performance index we outline a challenge problem for developing algorithms that are superior to humans for the general face recognition problem. 
",We review experiments comparing humans and machines in NIST face recognition tests. We introduce the cross modal performance analysis CMPA framework. We apply the CMPA framework to the human machine experiments in the NIST tests. We propose a challenge problem to develop algorithms with human level performance.,,,
S0169260715300523," Background and objectives The diagnosis of Developmental Dysplasia of the Hip in infants is currently made primarily by ultrasound . However two dimensional ultrasound images capture only an incomplete portion of the acetabular shape and the alpha and beta angles measured on 2DUS for the Graf classification technique show high inter scan and inter observer variability . This variability relates partly to the manual determination of the apex point separating the acetabular roof from the ilium during index measurement . This study proposes a new 2DUS image processing technique for semi automated tracing of the bony surface followed by automatic calculation of two indices a contour based alpha angle and a new modality independent quantitative rounding index . The new index M is independent of the apex point and can be directly extended to 3D surface models . Methods We tested the proposed indices on a dataset of 114 2DUS scans of infant hips aged between 4 and 183 days scanned using a 12MHz linear transducer . We calculated the manual alpha angle coverage contour based alpha angle and rounding index for each of the recordings and statistically evaluated these indices based on regression analysis area under the receiver operating characteristic curve and analysis of variance . Results Processing time for calculating and M was similar to manual alpha angle measurement 30s per image . Reliability of the new indices was high with inter observer intraclass correlation coefficients 0.90 for and 0.89 for M. For a diagnostic test classifying hips as normal or dysplastic AUC was 93.0 for vs. 92.7 for 91.6 for M alone and up to 95.7 for combination of M with or coverage . Conclusions The rounding index provides complimentary information to conventional indices such as alpha angle and coverage . Calculation of the contour based alpha angle and rounding index is rapid shows potential to improve the reliability and accuracy of DDH diagnosis from 2DUS and could be extended to 3D ultrasound in future . 
",New method to calculate alpha angle for DDH classification from ultrasound 2DUS . New modality independent rounding index M to quantify acetabular rounding. Proposed indices tested on 114 scans and gave 95.7 AUC for normal vs dysplastic. Indices are modality independent and extendable to modalities like CT and MRI.,,,
S0169260715003302," Background and objective Progress in biomedical engineering has improved the hardware available for diagnosis and treatment of cardiac arrhythmias . But although huge amounts of intracardiac electrograms can be acquired during electrophysiological examinations there is still a lack of software aiding diagnosis . The development of novel algorithms for the automated analysis of EGMs has proven difficult due to the highly interdisciplinary nature of this task and hampered data access in clinical systems . Thus we developed a software platform which allows rapid implementation of new algorithms verification of their functionality and suitable visualization for discussion in the clinical environment . Methods A software for visualization was developed in Qt5 and C utilizing the class library of VTK . The algorithms for signal analysis were implemented in MATLAB . Clinical data for analysis was exported from electroanatomical mapping systems . Results The visualization software KaPAVIE was implemented and tested on several clinical datasets . Both common and novel algorithms were implemented which address important clinical questions in diagnosis of different arrhythmias . It proved useful in discussions with clinicians due to its interactive and user friendly design . Time after export from the clinical mapping system to visualization is below 5min . Conclusion KaPAVIE See http www.ibt.kit.edu hardundsoftware.php . is a powerful platform for the development of novel algorithms in the clinical environment . Simultaneous and interactive visualization of measured EGM data and the results of analysis will aid diagnosis and help understanding the underlying mechanisms of complex arrhythmias like atrial fibrillation . 
",Need for novel algorithms in diagnosis and treatment of cardiac arrhythmias. Combined development using MATLAB and C VTK library. Allows for rapid implementation of new algorithms. Interactive visualization of cardiac anatomy measured data and analysis results. Promotes interdisciplinary research and discussion.,,,
S0262885614000146,"In this paper we propose an album oriented face recognition model that exploits the album structure for face recognition in online social networks. Albums usually associated with pictures of a small group of people at a certain event or occasion provide vital information that can be used to effectively reduce the possible list of candidate labels. We show how this intuition can be formalized into a model that expresses a prior on how albums tend to have many pictures of a small number of people. We also show how it can be extended to include other information available in a social network. Using two real world datasets independently drawn from Facebook we show that this model is broadly applicable and can significantly improve recognition rates. 
",We propose a face recognition model based on albums in online social networks. Albums tend to be composed of many pictures of a small group of people. Limiting the set of unique labels and using album level features improve recognition. We present two systems to learn how best to use these social features. We validate our model on two datasets independently downloaded from Facebook.,,,
S0169260715301814," Background and objective Methods used in image processing should reflect any multilevel structures inherent in the image dataset or they run the risk of functioning inadequately . We wish to test the feasibility of multilevel principal components analysis to build active shape models for cases relevant to medical and dental imaging . Methods Multilevel PCA was used to carry out model fitting to sets of landmark points and it was compared to the results of standard PCA . Proof of principle was tested by applying mPCA to model basic peri oral expressions approximated to the junction between the mouth lips . Monte Carlo simulations were used to create this data which allowed exploration of practical implementation issues such as the number of landmark points number of images and number of groups . To further test the robustness of the method mPCA was subsequently applied to a dental imaging dataset utilising landmark points along the boundary of mandibular cortical bone in panoramic radiographs of the face . Results Changes of expression that varied between groups were modelled correctly at one level of the model and changes in lip width that varied within groups at another for the Monte Carlo dataset . Extreme cases in the test dataset were modelled adequately by mPCA but not by standard PCA . Similarly variations in the shape of the cortical bone were modelled by one level of mPCA and variations between the experts at another for the panoramic radiographs dataset . Results for mPCA were found to be comparable to those of standard PCA for point to point errors via miss one out testing for this dataset . These errors reduce with increasing number of eigenvectors values retained as expected . Conclusions We have shown that mPCA can be used in shape models for dental and medical image processing . mPCA was found to provide more control and flexibility when compared to standard single level PCA . Specifically mPCA is preferable to standard PCA when multiple levels occur naturally in the dataset . 
",mPCA in ASMs can be used in medical and dental image analysis. Results provided by mPCA in initial studies appear to be sensible. Between and within subject variations are modelled correctly using mPCA. mPCA has more flexibility control and accuracy than standard PCA. mPCA is the correct method of combining sets of landmark points from different experts.,,,
S0169260715002692," Background Accurate segmentation of human head on medical images is an important process in a wide array of applications such as diagnosis facial surgery planning prosthesis design and forensic identification . Objectives In this study a Bayesian method for segmentation of facial tissues is presented . Segmentation classes include muscle bone fat air and skin . Methods The method presented incorporates information fusion from multiple modalities modelling of image resolution image noise two priors helping to reduce noise and partial volume . Image resolution modelling employed facilitates resolution enhancement and superresolution capabilities during image segmentation . Regularization based on isotropic and directional Markov Random Field priors is integrated . The Bayesian model is solved iteratively yielding tissue class labels at every voxel of the image . Sub methods as variations of the main method are generated by using a combination of the models . Results Testing of the sub methods is performed on two patients using single modality three dimensional image as well as registered MR CT images with information fusion . Numerical visual and statistical analyses of the methods are conducted . High segmentation accuracy values are obtained by the use of image resolution and partial volume models as well as information fusion from MR and CT images . The methods are also compared with our Bayesian segmentation method proposed in a previous study . The performance is found to be similar to our previous Bayesian approach but the presented methods here eliminates ad hoc parameter tuning needed by the previous approach which is system and data acquisition setting dependent . Conclusions The Bayesian approach presented provides resolution enhanced segmentation of very thin structures of the human head . Meanwhile free parameters of the algorithm can be adjusted for different imaging systems and data acquisition settings in a more systematic way as compared with our previous study . 
",Bayesian method provides resolution enhanced segmentation of human head. Segmentation classes incllude muscle bone fat air and skin. Tests were performed on 3D MR and CT images as well as registered MR CT images. The most successful results were obtained by the information fusion of MR and CT. Free parameters of the algorithm can be adjusted in a more systematic way.,,,
S0169260715003247," The assessment of microcirculation spatial heterogeneity on the hand skin is the main objective of this work . Near infrared spectroscopy based 2D imaging is a non invasive technique for the assessment of tissue oxygenation . The haemoglobin oxygen saturation images were acquired by a dedicated camera during baseline ischaemia and reperfusion . Acquired images underwent a preliminary restoration process aimed at removing degradations occurring during signal capturing . Then wavelet transform based multiscale analysis was applied to identify edges by detecting local maxima and minima across successive scales . Segmentation of test areas during different conditions was obtained by thresholding based region growing approach . The method identifies the differences in microcirculatory control of blood flow in different regions of the hand skin . The obtained results demonstrate the potential use of NIRS images for the clinical evaluation of skin disease and microcirculatory dysfunction . 
",We study oxygen saturation by vascular occlusion test. NIRS camera captured 2D images of hand during ischaemia and in perfusion. We applied edge detection and segmentation to NIRS images. With the applied methods we can visualize the oxygenation changes. The method allows to quantitate and visualize oxygenation at the same time.,,,
S0169260715301188," Background and objective We live our lives by the calendar and the clock but time is also an abstraction even an illusion . The sense of time can be both domain specific and complex and is often left implicit requiring significant domain knowledge to accurately recognize and harness . In the clinical domain the momentum gained from recent advances in infrastructure and governance practices has enabled the collection of tremendous amount of data at each moment in time . Electronic health records have paved the way to making these data available for practitioners and researchers . However temporal data representation normalization extraction and reasoning are very important in order to mine such massive data and therefore for constructing the clinical timeline . The objective of this work is to provide an overview of the problem of constructing a timeline at the clinical point of care and to summarize the state of the art in processing temporal information of clinical narratives . Methods This review surveys the methods used in three important area modeling and representing of time medical NLP methods for extracting time and methods of time reasoning and processing . The review emphasis on the current existing gap between present methods and the semantic web technologies and catch up with the possible combinations . Results The main findings of this review are revealing the importance of time processing not only in constructing timelines and clinical decision support systems but also as a vital component of EHR data models and operations . Conclusions Extracting temporal information in clinical narratives is a challenging task . The inclusion of ontologies and semantic web will lead to better assessment of the annotation task and together with medical NLP techniques will help resolving granularity and co reference resolution problems . 
",Multifaceted aspects in time and time oriented concepts. Comparison of clinical data models in handling time. Ontologies of representation and reasoning about time in the clinical domain. Constructing the timelines for the medical histories of patients. Temporal concept coreference resolution problem.,,,
S0169260715300328," In crisis situations a seamless ubiquitous communication is necessary to provide emergency medical service to save people s lives . An excellent prehospital emergency medicine provides immediate medical care to increase the survival rate of patients . On their way to the hospital ambulance personnel must transmit real time and uninterrupted patient information to the hospital to apprise the physician of the situation and provide options to the ambulance personnel . In emergency and crisis situations many communication channels can be unserviceable because of damage to equipment or loss of power . Thus data transmission over wireless communication to achieve uninterrupted network services is a major obstacle . This study proposes a mobile middleware for cognitive radio for improving the wireless communication link . CRs can sense their operating environment and optimize the spectrum usage so that the mobile middleware can integrate the existing wireless communication systems with a seamless communication service in heterogeneous network environments . Eventually the proposed seamless mobile communication middleware was ported into an embedded system which is compatible with the actual network environment without the need for changing the original system architecture . 
",In emergency and crisis situations many communication channels can be unserviceable because of damage to equipment or loss of power. Thus data transmission over wireless communication to achieve uninterrupted network services is a major obstacle. The proposed middleware can sense its operating environment and optimize the spectrum usage to integrate the existing heterogeneous wireless communication systems. The proposed middleware was ported into an embedded system which is compatible with the actual network environment without the need for changing the original system architecture.,,,
S0169260715002679," Cataract is defined as a lenticular opacity presenting usually with poor visual acuity . It is one of the most common causes of visual impairment worldwide . Early diagnosis demands the expertise of trained healthcare professionals which may present a barrier to early intervention due to underlying costs . To date studies reported in the literature utilize a single learning model for retinal image classification in grading cataract severity . We present an ensemble learning based approach as a means to improving diagnostic accuracy . Three independent feature sets i.e . wavelet sketch and texture based features are extracted from each fundus image . For each feature set two base learning models i.e . Support Vector Machine and Back Propagation Neural Network are built . Then the ensemble methods majority voting and stacking are investigated to combine the multiple base learning models for final fundus image classification . Empirical experiments are conducted for cataract detection and cataract grading tasks . The best performance of the ensemble classifier is 93.2 and 84.5 in terms of the correct classification rates for cataract detection and grading tasks respectively . The results demonstrate that the ensemble classifier outperforms the single learning model significantly which also illustrates the effectiveness of the proposed approach . 
",Fundus image analysis provides great potentials for automatic cataract diagnosis. Ensemble learning is exploited for combining multiple models to improve the performance. Three independent feature sets i.e. wavelet sketch and texture based features are utilized. Two base learning models i.e. Support Vector Machine and Back Propagation Neural Network are investigated in the ensemble approach. The experiments demonstrate that our approach outperforms single model significantly.,,,
S0169260715300110," Background and objective The automatic classification of breast imaging lesions is currently an unsolved problem . This paper describes an innovative representation learning framework for breast cancer diagnosis in mammography that integrates deep learning techniques to automatically learn discriminative features avoiding the design of specific hand crafted image based feature detectors . Methods A new biopsy proven benchmarking dataset was built from 344 breast cancer patients cases containing a total of 736 film mammography views representative of manually segmented lesions associated with masses 426 benign lesions and 310 malignant lesions . The developed method comprises two main stages preprocessing to enhance image details and supervised training for learning both the features and the breast imaging lesions classifier . In contrast to previous works we adopt a hybrid approach where convolutional neural networks are used to learn the representation in a supervised way instead of designing particular descriptors to explain the content of mammography images . Results Experimental results using the developed benchmarking breast cancer dataset demonstrated that our method exhibits significant improved performance when compared to state of the art image descriptors such as histogram of oriented gradients and histogram of the gradient divergence increasing the performance from 0.787 to 0.822 in terms of the area under the ROC curve . Interestingly this model also outperforms a set of hand crafted features that take advantage of additional information from segmentation by the radiologist . Finally the combination of both representations learned and hand crafted resulted in the best descriptor for mass lesion classification obtaining 0.826 in the AUC score . Conclusions A novel deep learning based framework to automatically address classification of breast mass lesions in mammography was developed . 
",Innovative representation learning framework for breast cancer lesion classification. A hybrid CNN method to learn image based features in a supervised way. New breast cancer benchmarking dataset to support computer aided diagnosis methods.,,,
S0169260715300420," Nowadays the diagnosis and treatment of pelvic sarcoma pose a major surgical challenge for reconstruction in orthopedics . With the development of manufacturing technology the metal 3D printed customized implants have brought revolution for the limb salvage resection and reconstruction surgery . However the tumor resection is not without risk and the precise implant placement is very difficult due to the anatomic intricacies of the pelvis . In this study a surgical navigation system including the implant calibration algorithm has been developed so that the surgical instruments and the 3D printed customized implant can be tracked and rendered on the computer screen in real time minimizing the risks and improving the precision of the surgery . Both the phantom experiment and the pilot clinical case study presented the feasibility of our computer aided surgical navigation system . According to the accuracy evaluation experiment the precision of customized implant installation can be improved three to five times compared with the non navigated implant installation after the guided osteotomy which means it is sufficient to meet the clinical requirements of the pelvic reconstruction . However more clinical trials will be conducted in the future work for the validation of the reliability and efficiency of our navigation system . 
",A navigation system was developed for pelvic tumor resection and reconstruction. A novel method for tracking the 3D printed patient specific implant was proposed. Both the phantom and clinical case study demonstrated the system feasibility. The high and stable accuracy regarding TRE demonstrated the system repeatability.,,,
S0198971513000240," Iterative proportional fitting is a widely used method for spatial microsimulation . The technique results in non integer weights for individual rows of data . This is problematic for certain applications and has led many researchers to favour combinatorial optimisation approaches such as simulated annealing . An alternative to this is integerisation of IPF weights the translation of the continuous weight variable into a discrete number of unique or cloned individuals . We describe four existing methods of integerisation and present a new one . Our method truncate replicate sample recognises that IPF weights consist of both replication weights and conventional weights the effects of which need to be separated . The procedure consists of three steps separate replication and conventional weights by truncation replication of individuals with positive integer weights and probabilistic sampling . The results which are reproducible using supplementary code and data published alongside this paper show that TRS is fast and more accurate than alternative approaches to integerisation . 
",Integerisation of weights obtained from iterative proportional fitting. Evaluation of five methods for integerisation. New method presented based on truncation replication and sampling TRS . The new method outperforms previously published integerisation strategies. Easily reproducible results using publicly available R code and data.,,,
S0169260715301012," Background and objectives Computer aided analysis of mammograms has been employed by radiologists as a vital tool to increase the precision in the diagnosis of breast cancer . The efficiency of such an analysis is dependent on the employed mammogram enhancement approach as its major role is to yield a visually improved image for radiologists . Methods Non linear Polynomial Filtering framework has been explored previously as a robust approach for contrast improvement of mammographic images . This paper presents the extension of NPF framework for sharpening and edge enhancement of mammogram lesions . Proposed NPF serves to provide enhancement of edges and sharpness of the lesion region in mammograms in a manner to minimize the dependencies on pre selected thresholds . In the present work Logarithmic Image Processing model has been employed for the purpose of improvement in visualization of mammograms based on Human Visual System characteristics . Results The proposed NPF filtering framework yields mammograms with significant improvement in contrast edges as well as sharpness of the lesion region . The performance of the proposed approach has been validated using state of art objective evaluation measures like Contrast Improvement Index Peak Signal to Noise Ratio Average Signal to Noise Ratio and Combined Enhancement Measure as well as subjective evaluation by radiologists opinions . Conclusions The proposed NPF provides a robust solution to perform noise controlled contrast as well as edge enhancement using a single filtering model . This leads to a better visualization of the fine lesion details predictive of their severity . The applicability of single filtering methodology for carrying out denoising contrast and edge enhancement improves the worth of the overall framework . 
",NPF framework serves to catalyze the computer aided analysis of mammograms. It provides the radiologists with improved mammograms for precision in diagnosis. An extension of NPF for sharpening and edge enhancement of mammograms is presented. Integration of LIP model provides enhancement response in coherence to HVS. The obtained results are evaluated using CII PSNR CEM as image quality metrics.,,,
S0169260715003211," Background Diabetes mellitus is associated with an increased risk of liver cancer and these two diseases are among the most common and important causes of morbidity and mortality in Taiwan . Purpose To use data mining techniques to develop a model for predicting the development of liver cancer within 6 years of diagnosis with type II diabetes . Methods Data were obtained from the National Health Insurance Research Database of Taiwan which covers approximately 22 million people . In this study we selected patients who were newly diagnosed with type II diabetes during the 2000 2003 periods with no prior cancer diagnosis . We then used encrypted personal ID to perform data linkage with the cancer registry database to identify whether these patients were diagnosed with liver cancer . Finally we identified 2060 cases and assigned them to a case group and a control group . The risk factors were identified from the literature review and physicians suggestion then chi square test was conducted on each independent variable for a comparison between patients with liver cancer and those without those found to be significant were selected as the factors . We subsequently performed data training and testing to construct artificial neural network and logistic regression prediction models . The dataset was randomly divided into 2 groups a training group and a test group . The training group consisted of 1442 cases and the prediction model was developed on the basis of the training group . The remaining 30 were assigned to the test group for model validation . Results The following 10 variables were used to develop the ANN and LR models sex age alcoholic cirrhosis nonalcoholic cirrhosis alcoholic hepatitis viral hepatitis other types of chronic hepatitis alcoholic fatty liver disease other types of fatty liver disease and hyperlipidemia . The performance of the ANN was superior to that of LR according to the sensitivity specificity and the area under the receiver operating characteristic curve . After developing the optimal prediction model we base on this model to construct a web based application system for liver cancer prediction which can provide support to physicians during consults with diabetes patients . Conclusion In the original dataset 33 of diabetes patients were diagnosed with liver cancer . After using 70 of the original data to training the model and other 30 for testing the sensitivity and specificity of our model were 0.757 and 0.755 respectively this means that 75.7 of diabetes patients can be predicted correctly to receive a future liver cancer diagnosis and 75.5 can be predicted correctly to not be diagnosed with liver cancer . These results reveal that this model can be used as effective predictors of liver cancer for diabetes patients after discussion with physicians they also agreed that model can assist physicians to advise potential liver cancer patients and also helpful to decrease the future cost incurred upon cancer treatment . 
",Developed a liver cancer prediction model for type II diabetic patients. Found the risk factors of liver cancer for type II diabetic patients. Built a web based application for liver cancer prediction for diabetic patient. Provided methods and model facilitating clinicians to remind those diabetic patients prevent liver cancer as early as possible.,,,
S0169260715304600," The mathematical modeling of physical and biologic systems represents an interesting alternative to study the behavior of these phenomena . In this context the development of mathematical models to simulate the dynamic behavior of tumors is configured as an important theme in the current days . Among the advantages resulting from using these models is their application to optimization and inverse problem approaches . Traditionally the formulated Optimal Control Problem has the objective of minimizing the size of tumor cells by the end of the treatment . In this case an important aspect is not considered namely the optimal concentrations of drugs may affect the patients health significantly . In this sense the present work has the objective of obtaining an optimal protocol for drug administration to patients with cancer through the minimization of both the cancerous cells concentration and the prescribed drug concentration . The resolution of this multi objective problem is obtained through the Multi objective Optimization Differential Evolution algorithm . The Pareto s Curve obtained supplies a set of optimal protocols from which an optimal strategy for drug administration can be chosen according to a given criterion . 
",To formulate and to solve a multi objective optimal control problem applied to treatment of tumors. To minimize the cancerous cells concentration and the drug concentration. The results demonstrated that an optimal protocol can be obtained considering both objectives.,,,
S0169260715002734," To facilitate the performance comparison of new methods for sleep patterns analysis datasets with quality content publicly available are very important and useful . We introduce an open access comprehensive sleep dataset called ISRUC Sleep . The data were obtained from human adults including healthy subjects subjects with sleep disorders and subjects under the effect of sleep medication . Each recording was randomly selected between PSG recordings that were acquired by the Sleep Medicine Centre of the Hospital of Coimbra University . The dataset comprises three groups of data data concerning 100 subjects with one recording session per subject data gathered from 8 subjects two recording sessions were performed per subject and data collected from one recording session related to 10 healthy subjects . The polysomnography recordings associated with each subject were visually scored by two human experts . Comparing the existing sleep related public datasets ISRUC Sleep provides data of a reasonable number of subjects with different characteristics such as data useful for studies involving changes in the PSG signals over time and data of healthy subjects useful for studies involving comparison of healthy subjects with the patients suffering from sleep disorders . This dataset was created aiming to complement existing datasets by providing easy to apply data collection with some characteristics not covered yet . ISRUC Sleep can be useful for analysis of new contributions in biomedical signal processing in development of ASSC methods and on sleep physiology studies . To evaluate and compare new contributions which use this dataset as a benchmark results of applying a subject independent automatic sleep stage classification method on ISRUC Sleep dataset are presented . 
",A publicly available sleep dataset called ISRUC Sleep is introduced. It contains data of healthy and patient subjects with different characteristics. Two recording sessions per subject are included in subgroup II of the dataset. ISRUC Sleep were evaluated using the ASSC method SSM4S. Effects of sleep disorders diseases and medications are presented.,,,
S0262885614000444," Dictionary learning plays a crucial role in sparse representation based image classification . In this paper we propose a novel approach to learn a discriminative dictionary with low rank regularization on the dictionary . Specifically we apply Fisher discriminant function to the coding coefficients to make the dictionary more discerning that is a small ratio of the within class scatter to between class scatter . In practice noisy information in the training samples will undermine the discriminative ability of the dictionary . Inspired by the recent advances in low rank matrix recovery theory we apply low rank regularization on the dictionary to tackle this problem . The iterative projection method and inexact augmented Lagrange multiplier algorithm are adopted to solve our objective function . The proposed discriminative dictionary learning with low rank regularization approach is evaluated on four face and digit image datasets in comparison with existing representative dictionary learning and classification algorithms . The experimental results demonstrate the superiority of our approach . 
",Learn a discriminative dictionary with low rank regularization Fisher discriminant function is applied to the coding coefficients. IPM and ALM algorithms are adopted to solve our objective function.,,,
S0169260715003223," Background and objective Cosmetic outcome of breast cancer conservative treatment remains without a standard evaluation method . Subjective methods in spite of their low reproducibility continue to be the most frequently used . Objective methods although more reproducible seem unable to translate all the subtleties involved in cosmetic outcome . The breast cancer conservative treatment cosmetic results software was developed in 2007 to try to overcome these pitfalls . The software is a semi automatic objective tool that evaluates asymmetry color differences and scar visibility using patient s digital pictures . The purpose of this work is to review the use of the BCCT.core software since its availability in 2007 and to put forward future developments . Methods All the online requests for BCCT.core use were registered from June 2007 to December 2014 . For each request the department city and country as well as user intention were questioned . A literature search was performed in Medline Google Scholar and ISI Web of Knowledge for all publications using and citing BCCT.core . Results During this period 102 centers have requested the software essentially for clinical use . The BCCT.core software was used in 19 full published papers and in 29 conference abstracts . Conclusions The BCCT.core is a user friendly semi automatic method for the objective evaluation of BCCT . The number of online requests and publications have been steadily increasing turning this computer program into the most frequently used tool for the objective cosmetic evaluation of BCCT . 
",The BCCT.core is a computer program used for the esthetic evaluation of breast cancer conservative treatment. The BCCT.core software has been used by centres worldwide for clinical use and research. Several publications have confirmed its easiness of use and reproduciblity.,,,
S0262885613001649," This paper deals with model based pose estimation . We propose a direct approach that takes into account the image as a whole . For this we consider a similarity measure the mutual information . Mutual information is a measure of the quantity of information shared by two signals . Exploiting this measure allows our method to deal with different image modalities . Furthermore it handles occlusions and illumination changes . Results with synthetic and real image sequences with static or mobile camera demonstrate the robustness of the method and its ability to produce stable and precise pose estimations . 
",We tackle the 3D model based tracking using the entire image and a textured 3D model. We propose a nonlinear optimization of the problem based on mutual information. Mutual information deals with the different modalities of real and virtual images. Our proposed method withdraws feature detection and matching issues. Results show the success of the approach and its precision.,,,
S0169260715301905," To extend the use of wearable sensor networks for stroke patients training and assessment in non clinical settings this paper proposes a novel remote quantitative Fugl Meyer assessment framework in which two accelerometer and seven flex sensors were used to monitoring the movement function of upper limb wrist and fingers . The extreme learning machine based ensemble regression model was established to map the sensor data to clinical FMA scores while the RRelief algorithm was applied to find the optimal features subset . Considering the FMA scale is time consuming and complicated seven training exercises were designed to replace the upper limb related 33 items in FMA scale . 24 stroke inpatients participated in the experiments in clinical settings and 5 of them were involved in the experiments in home settings after they left the hospital . Both the experimental results in clinical and home settings showed that the proposed quantitative FMA model can precisely predict the FMA scores based on wearable sensor data the coefficient of determination can reach as high as 0.917 . It also indicated that the proposed framework can provide a potential approach to the remote quantitative rehabilitation training and evaluation . 
",A novel remote quantitative Fugl Meyer assessment FMA framework was proposed for stroke patients. Seven training exercises were designed to represent the upper limb related 33 items in FMA scale. Ensemble machine learning and RRelief algorithm were applied to establish the quantitative assessment model. The proposed framework has been implemented in both clinical and home settings.,,,
S0262885613001078," This paper addresses the general problem of robust parametric model estimation from data that has both an unknown and possibly majority fraction of outliers as well as an unknown scale of measurement noise. We focus on computer vision applications from image correspondences such as camera resectioning estimation of the fundamental matrix or relative pose for 3D reconstruction and estimation of 2D homographies for image registration and motion segmentation although there are many other applications. In practice these methods typically rely on a predefined inlier thresholds because automatic scale detection is usually too unreliable or too slow. We propose a new method for robust estimation with automatic scale detection that is faster more precise and more robust than previous alternatives and show that it can be practically applied to these problems. 
",robust scale estimation without a breakdown point insensitive to inlier noise distribution efficient and practical application to many computer vision problems,,,
S0169260715300699," Psoriasis is an autoimmune skin disease with red and scaly plaques on skin and affecting about 125 million people worldwide . Currently dermatologist use visual and haptic methods for diagnosis the disease severity . This does not help them in stratification and risk assessment of the lesion stage and grade . Further current methods add complexity during monitoring and follow up phase . The current diagnostic tools lead to subjectivity in decision making and are unreliable and laborious . This paper presents a first comparative performance study of its kind using principal component analysis based CADx system for psoriasis risk stratification and image classification utilizing 11 higher order spectra features 60 texture features and 86 color feature sets and their seven combinations . Aggregate 540 image samples from 30 psoriasis patients of Indian ethnic origin are used in our database . Machine learning using PCA is used for dominant feature selection which is then fed to support vector machine classifier to obtain optimized performance . Three different protocols are implemented using three kinds of feature sets . Reliability index of the CADx is computed . Among all feature combinations the CADx system shows optimal performance of 100 accuracy 100 sensitivity and specificity when all three sets of feature are combined . Further our experimental result with increasing data size shows that all feature combinations yield high reliability index throughout the PCA cutoffs except color feature set and combination of color and texture feature sets . HOS features are powerful in psoriasis disease classification and stratification . Even though independently all three set of features HOS texture and color perform competitively but when combined the machine learning system performs the best . The system is fully automated reliable and accurate . 
",Comparative analysis of systems with different feature sets. HOS features extraction for psoriasis images. Largest set of mathematical features ever computed for psoriasis images. Understanding the reliability analysis of the CADx system. Accurate system with 100 accuracy and 100 sensitivity and specificity.,,,
S0198971514001367," With the exponential growth in the world population and the constant increase in human mobility the possible impact of outbreaks of epidemics on cities is increasing especially in high density urban areas such as public transportation and transfer points . The volume and proximity of people in these areas can lead to an observed dramatic increase in the transmission of airborne viruses and related pathogens . Due to the critical role these areas play in transmission it is vital that we have a comprehensive understanding of the transmission highways in these areas to predict or prevent the spreading of infectious diseases in general . The principled approach of this paper is to combine and utilize as much information as possible from relevant sources and to integrate these data in a simulated environment that allows for scenario testing and decision support . In this paper we describe a novel approach to study the spread of airborne diseases in cities by combining traffic information with geo spatial data infection dynamics and spreading characteristics . The system is currently being used in an attempt to understand the outbreak of influenza in densely populated cities in China . 
",A novel approach to study Urban Airborne Disease spreading. Integrate as much information as possible in a simulation environment. GIS enabled city modeling and travel routing calculation. Synthesizing the population in a city and modeling people s daily behavior in the presence of epidemics.,,,
S0169260715300055," Background and objective In oral and maxillofacial surgery conventional radiographic cephalometry is one of the standard auxiliary tools for diagnosis and surgical planning . While contemporary computer assisted cephalometric systems and methodologies support cephalometric analysis they tend neither to be practical nor intuitive for practitioners . This is particularly the case for 3D methods since the associated landmarking process is difficult and time consuming . In addition to this there are no 3D cephalometry norms or standards defined therefore new landmark selection methods are required which will help facilitate their establishment . This paper presents and evaluates a novel haptic enabled landmarking approach to overcome some of the difficulties and disadvantages of the current landmarking processes used in 2D and 3D cephalometry . Method In order to evaluate this new system s feasibility and performance 21 dental surgeons performed a range of case studies using a haptic enabled 2D 2 D and 3D digital cephalometric analyses . Results The results compared the 2D 2 D and 3D cephalometric values errors and standard deviations for each case study and associated group of participants and revealed that 3D cephalometry significantly reduced landmarking errors and variability compared to 2D methods . Conclusions Through enhancing the process by providing a sense of touch the haptic enabled 3D digital cephalometric approach was found to be feasible and more intuitive than its counterparts as well effective at reducing errors the variability of the measurements taken and associated task completion times . 
",Computer aided cephalometric systems tend not to be practical and intuitive. A new haptic enabled landmarking approach for 3D cephalometry is proposed. Several experimental tests were conducted to evaluate the proposed approach. Haptic technologies facilities the landmark selection process in 3D cephalometry. The haptic user interface allows the user to feel and touch the virtual patient s skull.,,,
S0262885614000031," In this paper how to calibrate a fixed multi camera system and simultaneously achieve a Euclidean reconstruction from a set of segments is addressed . It is well known that only a projective reconstruction could be achieved without any prior information . Here the known segment lengths are exploited to upgrade the projective reconstruction to a Euclidean reconstruction and simultaneously calibrate the intrinsic and extrinsic camera parameters . At first a DLT like algorithm for the Euclidean upgrading from segment lengths is derived in a very simple way . Although the intermediate results in the DLT like algorithm are essentially equivalent to the quadric of segments the DLT like algorithm is of higher accuracy than the existing linear algorithms derived from the QoS because of a more accurate way to extract the plane at infinity from the intermediate results . Then to further improve the accuracy of Euclidean upgrading two weighted DLT like algorithms are presented by weighting the linear constraint equations in the original DLT like algorithm . Finally using the results of these linear algorithms as the initial values a new weighted nonlinear algorithm for Euclidean upgrading is explored to recover the Euclidean structure more accurately . Extensive experimental results on both the synthetic data and the real image data demonstrate the effectiveness of our proposed algorithms in Euclidean upgrading and multi camera calibration . 
",A DLT like algorithm for Euclidean upgrading from segment lengths is proposed. The constraint equation is parameterized in a simple way. The plane at infinity is directly extracted from the intermediate results. This algorithm has higher accuracy than the existing linear algorithms. Weighting and nonlinear refinement further improve the accuracy.,,,
S0262885613001509,"Recent research emphasizes more on analyzing multiple features to improve face recognition FR performance. One popular scheme is to extend the sparse representation based classification framework with various sparse constraints. Although these methods jointly study multiple features through the constraints they just process each feature individually such that they overlook the possible high level relationship among different features. It is reasonable to assume that the low level features of facial images such as edge information and smoothed low frequency image can be fused into a more compact and more discriminative representation based on the latent high level relationship. FR on the fused features is anticipated to produce better performance than that on the original features since they provide more favorable properties. Focusing on this we propose two different strategies which start from fusing multiple features and then exploit the dictionary learning DL framework for better FR performance. The first strategy is a simple and efficient two step model which learns a fusion matrix from training face images to fuse multiple features and then learns class specific dictionaries based on the fused features. The second one is a more effective model requiring more computational time that learns the fusion matrix and the class specific dictionaries simultaneously within an iterative optimization procedure. Besides the second model considers to separate the shared common components from class specified dictionaries to enhance the discrimination power of the dictionaries. The proposed strategies which integrate multi feature fusion process and dictionary learning framework for FR realize the following goals 1 exploiting multiple features of face images for better FR performances 2 learning a fusion matrix to merge the features into a more compact and more discriminative representation 3 learning class specific dictionaries with consideration of the common patterns for better classification performance. We perform a series of experiments on public available databases to evaluate our methods and the experimental results demonstrate the effectiveness of the proposed models. 
",We propose two strategies for face recognition through multiple features. Our methods integrate multi feature fusion and dictionary learning. The fusion process and dictionary learning are learned simultaneously. Extensive experiments validate the merits of our methods.,,,
S0262885613001017," In this article a novel technique for fixation prediction and saccade generation will be introduced . The proposed model simulates saccadic eye movement to incorporate the underlying eye movement mechanism into saliency estimation . To this end a simple salience measure is introduced . Afterwards we derive a system model for saccade generation and apply it in a stochastic filtering framework . The proposed model will dynamically make a saccade toward the next predicted fixation and produces saliency maps . Evaluation of the proposed model is carried out in terms of saccade generation performance and saliency estimation . Saccade generation evaluation reveals that the proposed model outperforms inhibition of return . Also experiments signify integration of eye movement mechanism into saliency estimation boosts the results . Finally comparison with several saliency models shows the proposed model performs aptly . 
",Introducing a stochastic model for simulating eye movements Implementing the proposed model Implementing an algorithm for predicting saccades and fixations,,,
S0262885613001297," Intensity inhomogeneity often appears in medical images such as X ray tomography and magnetic resonance images due to technical limitations or artifacts introduced by the object being imaged . It is difficult to segment such images by traditional level set based segmentation models . In this paper we propose a new level set method integrating local and global intensity information adaptively to segment inhomogeneous images . The local image information is associated with the intensity difference between the average of local intensity distribution and the original image which can significantly increase the contrast between foreground and background . Thus the images with intensity inhomogeneity can be efficiently segmented . What is more to avoid the re initialization of the level set function and shorten the computational time a simple and fast level set evolution formulation is used in the numerical implementation . Experimental results on synthetic images as well as real medical images are shown in the paper to demonstrate the efficiency and robustness of the proposed method . 
",We constructed a new region based pressure force RBPF function. We proposed a novel level set based model for inhomogeneous image segmentation. We combine the local and global intensity information adaptively. The local image information can significantly increase image contrast. We apply a fast and simple level set method to implement the curve evolution.,,,
S0169260715002783," In this paper we present the dfcomb R package for the implementation of a single prospective clinical trial or simulation studies of phase I combination trials in oncology . The aim is to present the features of the package and to illustrate how to use it in practice though different examples . The use of combination clinical trials is growing but the implementation of existing model based methods is complex so this package should promote the use of innovative adaptive designs for early phases combination trials . 
",R package dfcomb available for phase I combination clinical trials according to previous methodological paper. Implement a single prospective clinical trial or simulation studies of phase I combination trials in oncology. Features and illustration of the functions. Promote the use of innovative adaptive design for early phase combination trials.,,,
S0169260715300432," Interventional cardiologists have a deep interest in risk stratification prior to stenting and percutaneous coronary intervention procedures . Intravascular ultrasound is most commonly adapted for screening but current tools lack the ability for risk stratification based on grayscale plaque morphology . Our hypothesis is based on the genetic makeup of the atherosclerosis disease that there is evidence of a link between coronary atherosclerosis disease and carotid plaque built up . This novel idea is explored in this study for coronary risk assessment and its classification of patients between high risk and low risk . This paper presents a strategy for coronary risk assessment by combining the IVUS grayscale plaque morphology and carotid B mode ultrasound carotid intima media thickness a marker of subclinical atherosclerosis . Support vector machine learning paradigm is adapted for risk stratification where both the learning and testing phases use tissue characteristics derived from six feature combinational spaces which are then used by the SVM classifier with five different kernels sets . These six feature combinational spaces are designed using 56 novel feature sets . K fold cross validation protocol with 10 trials per fold is used for optimization of best SVM kernel and best feature combination set . IRB approved coronary IVUS and carotid B mode ultrasound were jointly collected on 15 patients via 40MHz catheter utilizing iMap with 2865 frames per patient and linear probe B mode carotid ultrasound . Using the above protocol the system shows the classification accuracy of 94.95 and AUC of 0.95 using optimized feature combination . This is the first system of its kind for risk stratification as a screening tool to prevent excessive cost burden and better patients cardiovascular disease management while validating our two hypotheses . 
",Coronary risk stratification. Machine learning to link two arteries. 56 grayscale features. Coronary tissue characterization. Classification accuracy 94.95 and AUC 0.95.,,,
S0169260715300894," One of the key elements of e learning platforms is the content provided to the students . Content creation is a time demanding task that requires teachers to prepare material taking into account that it will be accessed on line . Moreover the teacher is restricted by the functionalities provided by the e learning platforms . In contexts such as radiology where images have a key role the required functionalities are still more specific and difficult to be provided by these platforms . Our purpose is to create a framework to make teacher s tasks easier specially when he has to deal with contents where images have a main role . In this paper we present RadEd a new web based teaching framework that integrates a smart editor to create case based exercises that support image interaction such as changing the window width and the grey scale used to render the image taking measurements on the image attaching labels to images and selecting parts of the images amongst others . It also provides functionalities to prepare courses with different topics exercises and theory material and also functionalities to control students work . Different experts have used RadEd and all of them have considered it a very useful and valuable tool to prepare courses where radiological images are the main component . RadEd provides teachers functionalities to prepare more realistic cases and students the ability to make a more specific diagnosis . 
",Content creation is a very time demanding task. RadEd is a web based framework with a smart editor to create image based exercises. RadEd provides teachers functionalities to prepare more realistic cases. RadEd allows learners to make more specific diagnosis.,,,
S0169260716303418," Background and objectives Because skin cancer affects millions of people worldwide computational methods for the segmentation of pigmented skin lesions in images have been developed in order to assist dermatologists in their diagnosis . This paper aims to present a review of the current methods and outline a comparative analysis with regards to several of the fundamental steps of image processing such as image acquisition pre processing and segmentation . Methods Techniques that have been proposed to achieve these tasks were identified and reviewed . As to the image segmentation task the techniques were classified according to their principle . Results The techniques employed in each step are explained and their strengths and weaknesses are identified . In addition several of the reviewed techniques are applied to macroscopic and dermoscopy images in order to exemplify their results . Conclusions The image segmentation of skin lesions has been addressed successfully in many studies however there is a demand for new methodologies in order to improve the efficiency . 
",The clinical requirement for the early diagnosis of malignant skin lesions from images is introduced and justified. An up to date review about the proposed techniques for the image segmentation of pigmented skin lesions is presented. Additionally the tasks related to image acquisition and pre processing are also taken into account. The techniques are introduced classified and some examples of their results are illustrated and discussed. This review is of interest both for researchers and for health professionals.,,,
S0169260715002667," Background and objective This study proposes an infrastructure with a reporting workflow optimization algorithm in order to interconnect facilities reporting units and radiologists on a single access interface to increase the efficiency of the reporting process by decreasing the medical report turnaround time and to increase the quality of medical reports by determining the optimum match between the inspection and radiologist in terms of subspecialty workload and response time . Methods Workflow centric network architecture with an enhanced caching querying and retrieving mechanism is implemented by seamlessly integrating Grid Agent and Grid Manager to conventional digital radiology systems . The inspection and radiologist attributes are modelled using a hierarchical ontology structure . Attribute preferences rated by radiologists and technical experts are formed into reciprocal matrixes and weights for entities are calculated utilizing Analytic Hierarchy Process . The assignment alternatives are processed by relation based semantic matching and Integer Linear Programming . Results The results are evaluated based on both real case applications and simulated process data in terms of subspecialty response time and workload success rates . Results obtained using simulated data are compared with the outcomes obtained by applying Round Robin Shortest Queue and Random distribution policies . The proposed algorithm is also applied to a real case teleradiology application process data where medical reporting workflow was performed based on manual assignments by the chief radiologist for 6225 inspections . Conclusions RBSM gives the highest subspecialty success rate and integrating ILP with RBSM ratings as RWOA provides a better response time and workload distribution success rate . RWOA based image delivery also prevents bandwidth storage or hardware related stuck and latencies . When compared with a real case teleradiology application where inspection assignments were performed manually the proposed solution was found to increase the experience success rate by 13.25 workload success rate by 63.76 and response time success rate by 120 . The total response time in the real case application data was improved by 22.39 . 
",We propose a workflow centric network architecture for teleradiology applications. We optimize inspection assignments to radiologists for medical reporting process. Subspecialty response time and workload parameters are improved for assignments. Solution gives better results for workflow efficiency compared to manual assignment. Architecture was tested in 52 sites and 3.35 million inspections were processed.,,,
S0262885614000067," The human visual system is quite adept at swiftly detecting objects of interest in complex visual scene . Simulating human visual system to detect visually salient regions of an image has been one of the active topics in computer vision . Inspired by random sampling based bagging ensemble learning method an ensemble dictionary learning framework for saliency detection is proposed in this paper . Instead of learning a universal dictionary requiring a large number of training samples to be collected from natural images multiple over complete dictionaries are independently learned with a small portion of randomly selected samples from the input image itself resulting in more flexible multiple sparse representations for each of the image patches . To boost the distinctness of salient patch from background region we present a reconstruction residual based method for dictionary atom reduction . Meanwhile with the obtained multiple probabilistic saliency responses for each of the patches the combination of them is finally carried out from the probabilistic perspective to achieve better predictive performance on saliency region . Experimental results on several open test datasets and some natural images demonstrate that the proposed EDL for saliency detection is much more competitive compared with some existing state of the art algorithms . 
",We propose an ensemble dictionary learning EDL framework for saliency detection. The saliency detection within this framework is treated as a novelty detection problem. A novel dictionary atom reduction is proposed for boosting the distinctness of salient region. A good probabilistic interpretation is with the proposed EDL model.,,,
S0262885614000274," 3D shape descriptor has been used widely in the field of 3D object retrieval . However the performance of object retrieval greatly depends on the shape descriptor used . The aims of this study is to review and compare the common 3D shape descriptors proposed in 3D object retrieval literature for object recognition and classification based on Kinect like depth image obtained from RGB D object dataset . In this paper we introduce inter class and intra class evaluation in order to study the feasibility of such descriptors in object recognition . Based on these evaluations local spin image outperforms the rest in discriminating different classes when several depth images from an instance per class are used in inter class evaluation . This might be due to the slightly consistent local shape property of such images and due to the proposed local similarity measurement that manages to extract the local based descriptor . However shape distribution performs excellent for intra class evaluation may be due to the global shape from different instances per class is slightly unchanged . These results indicate a remarkable feasibility analysis of the 3D shape descriptor in object recognition that can be potentially used for Kinect like sensor . 
",We model the common 3D shape descriptor from Kinect like depth image. We evaluate the 3D shape descriptor in object recognition. We introduce 3D shape descriptor performance frameworks. Shape distribution and local spin image outperformed other 3D shape descriptors.,,,
S0262885613001765," Despite the successes in the last two decades the state of the art face detectors still have problems in dealing with images in the wild due to large appearance variations . Instead of leaving appearance variations directly to statistical learning algorithms we propose a hierarchical part based structural model to explicitly capture them . The model enables part subtype option to handle local appearance variations such as closed and open month and part deformation to capture the global appearance variations such as pose and expression . In detection candidate window is fitted to the structural model to infer the part location and part subtype and detection score is then computed based on the fitted configuration . In this way the influence of appearance variation is reduced . Besides the face model we exploit the co occurrence between face and body which helps to handle large variations such as heavy occlusions to further boost the face detection performance . We present a phrase based representation for body detection and propose a structural context model to jointly encode the outputs of face detector and body detector . Benefit from the rich structural face and body information as well as the discriminative structural learning algorithm our method achieves state of the art performance on FDDB AFW and a self annotated dataset under wide comparisons with commercial and academic methods . 
",We enrich face detection model by hierarchical structure and part subtype. We propose to explore the face body co occurrence to improve face detection. We achieve state of the art performance on FDDB AFW and a self annotated dataset.,,,
S0169260715301280," Background and objectives The lack of benchmark data in computational ophthalmology contributes to the challenging task of applying disease assessment and evaluate performance of machine learning based methods on retinal spectral domain optical coherence tomography scans . Presented here is a general framework for constructing a benchmark dataset for retinal image processing tasks such as cyst vessel and subretinal fluid segmentation and as a result a benchmark dataset for cyst segmentation has been developed . Method First a dataset captured by different SD OCT vendors with different numbers of scans and pathology qualities are selected . Then a robust and intelligent method is used to evaluate performance of readers partitioning the dataset into subsets . Subsets are then assigned to complementary readers for annotation with respect to a novel confidence based annotation protocol . Finally reader annotations are combined based on their performance to generate final annotations . Result The generated benchmark dataset for cyst segmentation comprises 26 SD OCT scans with differing cyst qualities collected from 4 different SD OCT vendors to cover a wide variety of data . The dataset is partitioned into three subsets which are annotated by complementary readers based on a confidence based annotation protocol . Experimental results show annotations of complementary readers are combined efficiently with respect to their performance generating accurate annotations . Conclusion Our results facilitate the process of generating benchmark datasets . Moreover the generated benchmark data set for cyst segmentation can be used reliably to train and test machine learning based methods . 
",A framework including data selection task assignment and annotation combination stages for a confidence based benchmark dataset for retinal image processing is proposed. A novel task assignment is used to remove data and reader biases. The annotation of readers is combined based on their accuracy and performance. The framework is used to build a confidence based benchmark dataset for cyst segmentation. The generated benchmark can be used to reliably evaluate cyst segmentation methods.,,,
S0262885614000183,"This paper focuses on activity recognition when multiple views are available. In the literature this is often performed using two different approaches. In the first one the systems build a 3D reconstruction and match that. However there are practical disadvantages to this methodology since a sufficient number of overlapping views is needed to reconstruct and one must calibrate the cameras. A simpler alternative is to match the frames individually. This offers significant advantages in the system architecture e.g. it is easy to incorporate new features and camera dropouts can be tolerated . In this paper the second approach is employed and a novel fusion method is proposed. Our fusion method collects the activity labels over frames and cameras and then fuses activity judgments as the sequence label. It is shown that there is no performance penalty when a straightforward weighted voting scheme is used. In particular when there are enough overlapping views to generate a volumetric reconstruction our recognition performance is comparable with that produced by volumetric reconstructions. However if the overlapping views are not adequate the performance degrades fairly gracefully even in cases where test and training views do not overlap. 
",Data fusion based method for activity recognition using multiple views Straightforward architecture to incorporate new cameras or new features Performance generally increases when there are more cameras and features. Comparable performance with that produced by reconstruction Detailed experiments to answer different system considerations,,,
S0169260715002989," Objective The analysis of treatment effects in clinical trials usually focus on efficacy and safety in separate descriptive statistical analyses . The Q TWiST method has been proposed by Gelber in the 90s to enable a statistical comparison between two groups with a graphical representation by incorporating benefit and risk into a single analysis . Although the method has been programmed in SAS it is rarely used . The availability of the method in the freely software environment system like R would greatly enhanced the accessibility by researchers . The objective of this paper is to present a program for Q TWiST analyses within R software environment . Methods The qtwist function was developed in order to estimate and compare Q TWiST for two groups . Two individual patient data files are required used for input one for visits and one for follow up . Q TWiST is obtained as a sum of time spent in three health states period in toxicity period without relapse and toxicity and period in relapse weighted by associated utility scores restricted to median overall survival for example . The bootstrap method is used for testing statistical significance . Threshold analysis and gain functions allow a group comparison for different utility values . Results Input data is checked for consistency . Descriptive statistics and mean durations for each health state are provided allowing statistical comparisons . Graphical results are presented in a PDF file . The use of the function is illustrated with data from a simulated data set and a randomized clinical trial . Conclusions qtwist is an easy to use R function allowing a quality adjusted survival analysis with the Q TWiST method . 
",We developed a function written the R software performing Q TWiST analyses. Three health states are defined toxicity relapse and TWiST. Inputs are checked and descriptive comparative and graphical results are provided. Several arguments take the opportunity to add more options to the analyze.,,,
S0262885614000845," A method to obtain accurate hand gesture classification and fingertip localization from depth images is proposed . The Oriented Radial Distribution feature is utilized exploiting its ability to globally describe hand poses but also to locally detect fingertip positions . Hence hand gesture and fingertip locations are characterized with a single feature calculation . We propose to divide the difficult problem of locating fingertips into two more tractable problems by taking advantage of hand gesture as an auxiliary variable . Along with the method we present the ColorTip dataset a dataset for hand gesture recognition and fingertip classification using depth data . ColorTip contains sequences where actors wear a glove with colored fingertips allowing automatic annotation . The proposed method is evaluated against recent works in several datasets achieving promising results in both gesture classification and fingertip localization . 
",A novel use of the ORD feature to both globally and locally characterize hands A dataset for hand gesture classification and fingertip localization is proposed. We reduce the search space of fingertip locations conditioned to a hand gesture. A new cost function to solve the graph matching problem of fingertip localization,,,
S0169260715302996," Background and Objective Iterative reconstruction from Compton scattered data is known to be computationally more challenging than that from conventional line projection based emission data in that the gamma rays that undergo Compton scattering are modeled as conic projections rather than line projections . In conventional tomographic reconstruction to parallelize the projection and backprojection operations using the graphics processing unit approximated methods that use an unmatched pair of ray tracing forward projector and voxel driven backprojector have been widely used . In this work we propose a new GPU accelerated method for Compton camera reconstruction which is more accurate by using exactly matched pair of projector and backprojector . Methods To calculate conic forward projection we first sample the cone surface into conic rays and accumulate the intersecting chord lengths of the conic rays passing through voxels using a fast ray tracing method . For conic backprojection to obtain the true adjoint of the conic forward projection while retaining the computational efficiency of the GPU we use a voxel driven RTM which is essentially the same as the standard RTM used for the conic forward projector . Results Our simulation results show that while the new method is about 3 times slower than the approximated method it is still about 16 times faster than the CPU based method without any loss of accuracy . Conclusions The net conclusion is that our proposed method is guaranteed to retain the reconstruction accuracy regardless of the number of iterations by providing a perfectly matched projector backprojector pair which makes iterative reconstruction methods for Compton imaging faster and more accurate . 
",Acceleration of iterative Compton camera reconstruction using graphics processing unit. Exactly matched pair of conic projector and backprojector. Significantly reduced reconstruction time without loss of accuracy. Improvement of reconstruction accuracy in advanced iterative methods.,,,
S0262885614001024," In this paper initially the impact of mask spoofing on face recognition is analyzed . For this purpose one baseline technique is selected for both 2D and 3D face recognition . Next novel countermeasures which are based on the analysis of different shape texture and reflectance characteristics of real faces and mask faces are proposed to detect mask spoofing . In this paper countermeasures are developed using both 2D data and 3D data available in the mask database . The results show that each of the proposed countermeasures is successful in detecting mask spoofing and the fusion of these countermeasures further improves the results compared to using a single countermeasure . Since there is no publicly available mask database studies on mask spoofing are limited . This paper provides significant results by proposing novel countermeasures to protect face recognition systems against mask spoofing . 
",2D and 3D face recognition systems are vulnerable to mask spoofing. Countermeasures are proposed based on reflectance texture and depth analysis. Reflectance analysis provides best performance to detect 3D mask attacks. Fusion of countermeasures increases the mask spoofing detection performance. Classification accuracy of 99 almost perfect is reached to detect mask attacks.,,,
S0262885614000614," Finding regions of interest is a fundamentally important problem in the area of computer vision and image processing . Previous studies addressing this issue have mainly focused on investigating chromatic cues to characterize visually salient image regions while less attention has been devoted to monochromatic cues . The purpose of this paper is the study of monochromatic cues which have the potential to complement chromatic cues for the detection of ROIs in an image . This paper first presents a taxonomy of existing ROI detection approaches using monochromatic cues ranging from well known algorithms to the most recently published techniques . We then propose a novel monochromatic cue for ROI detection . Finally a comparative evaluation has been conducted on large scale challenging test sets of real world natural scenes . Experimental results demonstrate that the use of our proposed monochromatic cue yields a more accurate identification of ROIs . This paper serves as a benchmark for future research on this particular topic and a steppingstone for developers and practitioners interested in adopting monochromatic cues to ROI detection systems and methodologies . 
",We study monochromatic cues in the modeling of bottom up attention. We propose a novel monochromatic cue for ROI detection. Experimental results demonstrate the effectiveness of our method.,,,
S0262885614000493,"Concurrently obtaining an accurate robust and fast global registration of multiple 3D scans is still an open issue for modern 3D modeling pipelines especially when high metric precision as well as easy usage of high end devices structured light or laser scanners are required. Various solutions have been proposed either heuristic iterative and or closed form solutions which present some compromise concerning the fulfillment of the above contrasting requirements. Our purpose here compared to existing reference solutions is to go a step further in this perspective by presenting a new technique able to provide improved alignment performance even on large datasets both in terms of number of views and or point density of range images. Relying on the Optimization on a Manifold OOM approach originally proposed by Krishnan et al. we propose a set of methodological and computational upgrades that produce an operative impact on both accuracy robustness and computational performance compared to the original solution. In particular always basing on an unconstrained error minimization over the manifold of rotations instead of relying on a static set of point correspondences our algorithm updates the optimization iterations with a dynamically modified set of correspondences in a computationally effective way leading to substantial improvements in terms of registration accuracy and convergence trend. Other proposed improvements are directed to a substantial reduction of the computational load without sacrificing the alignment performance. Stress tests with increasing view misalignment allowed us to appreciate the convergence robustness of the proposed solution. Eventually we demonstrate that for very large datasets a further computational speedup can be reached by the adoption of a hybrid local heuristic followed by global optimization registration approach. 
",Goal new approach to enable global registration of large collections of point sets. We consider an optimization on a manifold for global registration of multiple scans. We evidence computational and convergence issues in the original approach. We propose computationally effective correspondence update and other improvements. Results better accuracy compared to state of the art good computational performance.,,,
S0169260715301140," An extensive in depth study of cardiovascular risk factors seems to be of crucial importance in the research of cardiovascular disease in order to prevent the chance of developing or dying from CVD . The main focus of data analysis is on the use of models able to discover and understand the relationships between different CVRF . In this paper a report on applying Bayesian network modeling to discover the relationships among thirteen relevant epidemiological features of heart age domain in order to analyze cardiovascular lost years cardiovascular risk score and metabolic syndrome is presented . Furthermore the induced BN was used to make inference taking into account three reasoning patterns causal reasoning evidential reasoning and intercausal reasoning . Application of BN tools has led to discovery of several direct and indirect relationships between different CVRF . The BN analysis showed several interesting results among them CVLY was highly influenced by smoking being the group of men the one with highest risk in CVLY MetS was highly influence by physical activity being again the group of men the one with highest risk in MetS and smoking did not show any influence . BNs produce an intuitive transparent graphical representation of the relationships between different CVRF . The ability of BNs to predict new scenarios when hypothetical information is introduced makes BN modeling an Artificial Intelligence tool of special interest in epidemiological studies . As CVD is multifactorial the use of BNs seems to be an adequate modeling tool . 
",An epidemiologic system analysis of cardiovascular risk is presented through a Bayesian network model. The Bayesian network model can serve as a generic tool for application oriented activities explanation prediction monitoring and prevention. Due to cardiovascular disease is multifactorial the application of this kind of model is of special interest both from theoretical and practical point of view. The induced Bayesian network was used to make inferences taking into account three reasoning patterns causal reasoning evidential reasoning and intercausal reasoning.,,,
S0198971516300047," Geographical masking is the conventional solution to protect the privacy of individuals involved in confidential spatial point datasets . The masking process displaces confidential locations to protect individual privacy while maintaining a fine level of spatial resolution . The adaptive form of this process aims to further minimize the displacement error by taking into account the underlying population density . We describe an alternative adaptive geomasking method referred to as Adaptive Areal Elimination . AAE creates areas of a minimum K anonymity and then original points are either randomly perturbed within the areas or aggregated to the median centers of the areas . In addition to the masked points K anonymized areas can be safely disclosed as well without increasing the risk of re identification . Using a burglary dataset from Vienna AAE is compared with an existing adaptive geographical mask the donut mask . The masking methods are evaluated for preserving a predefined K anonymity and the spatial characteristics of the original points . The spatial characteristics are assessed with four measures of spatial error displaced distance correlation coefficient of density surfaces hotspots divergence and clusters specificity . Masked points from point aggregation of AAE have the highest spatial error in all the measures but the displaced distance . In contrast masked points from the donut mask are displaced the least preserve the original spatial clusters better have the highest clusters specificity and correlation coefficient of density surfaces . However when the donut mask is adapted to achieve an actual K anonymity the random perturbation of AAE introduces less spatial error than the donut mask for all the measures of spatial error . 
",An alternative adaptive geographical masking method referred to as Adaptive Areal Elimination AAE is proposed. K anonymized areas can be safely disclosed as well without increasing the risk of re identification. For an actual K anonymity random perturbation of AAE preserves the spatial characteristics better the other masks.,,,
S0169260715300067," To determine initial velocities of enzyme catalyzed reactions without theoretical errors it is necessary to consider the use of the integrated Michaelis Menten equation . When the reaction product is an inhibitor this approach is particularly important . Nevertheless kinetic studies usually involved the evaluation of other inhibitors beyond the reaction product . The occurrence of these situations emphasizes the importance of extending the integrated Michaelis Menten equation assuming the simultaneous presence of more than one inhibitor because reaction product is always present . This methodology is illustrated with the reaction catalyzed by alkaline phosphatase inhibited by phosphate and urea . The approach is explained in a step by step manner using an Excel spreadsheet . Curve fitting by nonlinear regression was performed with the Solver add in . Discrimination of the kinetic models was carried out based on Akaike information criterion . This work presents a methodology that can be used to develop an automated process to discriminate in real time the inhibition type and kinetic constants as data are achieved by the spectrophotometer . 
",A method for the study of enzyme kinetics in the presence of two inhibitors. Integrated Michaelis Menten equations were developed to carry out the analysis. This methodology is useful to discriminate the inhibition mechanisms. This method can be automated and applied in real time. An Excel template has been developed available in Appendix .,,,
S0169260715301231," Background and Objective Viruses are infectious agents that replicate inside organisms and reveal a plethora of distinct characteristics . Viral infections spread in many ways but often have devastating consequences and represent a huge danger for public health . It is important to design statistical and computational techniques capable of handling the available data and highlighting the most important features . Methods This paper reviews the quantitative and qualitative behaviour of 22 infectious diseases caused by viruses . The information is compared and visualized by means of the multidimensional scaling technique . Results The results are robust to uncertainties in the data and revealed to be consistent with clinical practice . Conclusions The paper shows that the proposed methodology may represent a solid mathematical tool to tackle a larger number of virus and additional information about these infectious agents . 
",We review the quantitative and qualitative behaviour of 22 infectious diseases caused by viruses. The information is compared and visualized by means of Multidimensional Scaling technique. The results are consistent with clinical practice. The proposed mathematical tools tackle a large number of virus and information.,,,
S0169260715003314," An electrocardiogram measures the electric activity of the heart and has been widely used for detecting heart diseases due to its simplicity and non invasive nature . By analyzing the electrical signal of each heartbeat i.e . the combination of action impulse waveforms produced by different specialized cardiac tissues found in the heart it is possible to detect some of its abnormalities . In the last decades several works were developed to produce automatic ECG based heartbeat classification methods . In this work we survey the current state of the art methods of ECG based automated abnormalities heartbeat classification by presenting the ECG signal preprocessing the heartbeat segmentation techniques the feature description methods and the learning algorithms used . In addition we describe some of the databases used for evaluation of methods indicated by a well known standard developed by the Association for the Advancement of Medical Instrumentation and described in ANSI AAMI EC57 1998 2008 . Finally we discuss limitations and drawbacks of the methods in the literature presenting concluding remarks and future challenges and also we propose an evaluation process workflow to guide authors in future works . 
",Surveys the feature description methods and the learning algorithms employed. Also surveys the ECG signal preprocessing and the heartbeat segmentation techniques. Description of databases used for methods evaluation indicated by the AAMI standard. Discussion of limitations and drawbacks of the methods in the literature. Concluding remarks and future challenges are also pointed out.,,,
S0169260715301139," Background and objective Injury of knee joint cartilage may result in pathological vibrations between the articular surfaces during extension and flexion motions . The aim of this paper is to analyze and quantify vibroarthrographic signal irregularity associated with articular cartilage degeneration and injury in the patellofemoral joint . Methods The symbolic entropy approximate entropy fuzzy entropy and the mean standard deviation and root mean squared values of the envelope amplitude were utilized to quantify the signal fluctuations associated with articular cartilage pathology of the patellofemoral joint . The quadratic discriminant analysis generalized logistic regression analysis and support vector machine methods were used to perform signal pattern classifications . Results The experimental results showed that the patients with cartilage pathology possess larger SyEn and ApEn but smaller FuzzyEn over the statistical significance level of the Wilcoxon rank sum test than the healthy subjects . The mean standard deviation and RMS values computed from the amplitude difference between the upper and lower signal envelopes are also consistently and significantly larger for the group of CP patients than for the HS group . The SVM based on the entropy and envelope amplitude features can provide superior classification performance as compared with QDA and GLRA with an overall accuracy of 0.8356 sensitivity of 0.9444 specificity of 0.8 Matthews correlation coefficient of 0.6599 and an area of 0.9212 under the receiver operating characteristic curve . Conclusions The SyEn ApEn and FuzzyEn features can provide useful information about pathological VAG signal irregularity based on different entropy metrics . The statistical parameters of signal envelope amplitude can be used to characterize the temporal fluctuations related to the cartilage pathology . 
",Symbolic entropy approximate entropy and fuzzy entropy are used to measure the irregularity of knee vibroarthrographic signals. Mean standard deviation and root mean squared value parameters of the signal envelope amplitude are computed to characterize the signal fluctuations. Gender dependent statistics of entropy and envelope amplitude parameters are discussed.,,,
S0198971515300107," We study the impact of settlement sizes on network connectivity in a spatial setting . First we develop a model of geometric urban networks that posits a positive relationship between connectivity and size . Empirical evidence is then presented validating the model prediction that local links exhibit super linear scaling with the exponent greater than 1 while long range connections scale linearly with the unit exponent . The scaling exponents thus suggest that the impact of population size on connectivity is stronger within cities than between cities . We next combine the geometric framework with a computational model of interacting agents to generate a realistic settlement distribution and urban networks from the bottom up . Calibrated simulation results demonstrate the consistency between hierarchical rank size distribution and scale free connectivity . Finally coupling the spatial network with a tipping diffusion model allows us to consolidate the evolution of network connectivity city sizes and social practices in a unified computational framework . 
",A theoretical model of urban networks is developed linking connectivity to city sizes. The theoretical model predicts inter city links scale linearly while same city connections exhibit super linear scaling. The data for the U.S. New England and South Korea support the theoretical model s predictions. Agent based simulations are conducted to identify the conditions that preserve the diversity of norms in scale free networks.,,,
S0262885613001285," One of the greatest challenges while working on image segmentation algorithms is a comprehensive measure to evaluate their accuracy . Although there are some measures for doing this task but they can consider only one aspect of segmentation in evaluation process . The performance of evaluation measures can be improved using a combination of single measures . However combination of single measures does not always lead to an appropriate criterion . Besides its effectiveness the efficiency of the new measure should be considered . In this paper a new and combined evaluation measure based on genetic programming has been sought . Because of the nature of evolutionary approaches the proposed approach allows nonlinear and linear combinations of other single evaluation measures and can search within many and different combinations of basic operators to find a good enough one . We have also proposed a new fitness function to make GP enable to search within search space effectively and efficiently . To test the method Berkeley and Weizmann datasets besides several different experiments have been used . Experimental results demonstrate that the GP based approach is suitable for effective combination of single evaluation measures . 
",A genetic programming GP approach is proposed to design a combined evaluation measure for image segmentation algorithms. The proposed approach improved the performance of image segmentation evaluation. A new fitness evaluation scheme is proposed for GP. A new evaluation measure based on intra region similarity and inter region disparity is proposed in this paper.,,,
S0262885614000043,"Confronted with the explosive growth of web images the web image annotation has become a critical research issue for image search and index. Sparse feature selection plays an important role in improving the efficiency and performance of web image annotation. Meanwhile it is beneficial to developing an effective mechanism to leverage the unlabeled training data for large scale web image annotation. In this paper we propose a novel sparse feature selection framework for web image annotation namely sparse Feature Selection based on Graph Laplacian FSLG 2. FSLG applies the l2 1 2 matrix norm into the sparse feature selection algorithm to select the most sparse and discriminative features. Additional graph Laplacian based semi supervised learning is used to exploit both labeled and unlabeled data for enhancing the annotation performance. An efficient iterative algorithm is designed to optimize the objective function. Extensive experiments on two web image datasets are performed and the results illustrate that our method is promising for large scale web image annotation. 
",Spare feature selection method based on l 2 1 2 matix norm is proposed. Graph Laplacian based semi supervised learning is exploited. A effective algorithm for optimizing the objective function is introduced. The convergence of the algorithm is proven. Experiments demonstrate that the method is suitable for web image annotation.,,,
S0262885614000481," Hair segmentation is challenging due to the diverse appearance irregular region boundary and the influence of complex background . To deal with this problem we propose a novel data driven method named Isomorphic Manifold Inference . The IMI method assumes the coarse probability map and the binary segmentation map as a couple of isomorphic manifolds and tries to learn hair specific priors from manually labeled training images . For an input image firstly the method calculates a coarse probability map . Then it exploits regression techniques to obtain the relationship between the coarse probability map of the test image and those of training images . Finally this relationship i.e . a coefficient set is transferred to the binary segmentation maps and a soft segmentation of the test image will be achieved by a linear combination of those binary maps . Further we employ this soft segmentation as a shape cue and integrate it with color and texture cues into a unified segmentation framework . A better segmentation is achieved by the Graph Cuts optimization . Extensive experiments are conducted to validate effectiveness of the IMI method compare contributions of different cues and investigate the generalization of IMI method . The results strongly encourage our method . 
",Similar coarse hair probability maps should correspond to similar segmentations. Data driven Isomorphic Manifold Inference is proposed to exploit the shape priors. We integrate the inferred shape color and texture into a unified framework. Besides hair segmentation we also validate our IMI on horse class segmentation. Experiments on hair and horse databases show impressive performances.,,,
S0262885614000456," Automatic pain recognition from videos is a vital clinical application and owing to its spontaneous nature poses interesting challenges to automatic facial expression recognition research . Previous pain vs no pain systems have highlighted two major challenges ground truth is provided for the sequence but the presence or absence of the target expression for a given frame is unknown and the time point and the duration of the pain expression event in each video are unknown . To address these issues we propose a novel framework where each sequence is represented as a bag containing multiple segments and multiple instance learning is employed to handle this weakly labeled data in the form of sequence level ground truth . These segments are generated via multiple clustering of a sequence or running a multi scale temporal scanning window and are represented using a state of the art Bag of Words representation . This work extends the idea of detecting facial expressions through concept frames to concept segments and argues through extensive experiments that algorithms such as MIL are needed to reap the benefits of such representation . The key advantages of our approach are joint detection and localization of painful frames using only sequence level ground truth incorporation of temporal dynamics by representing the data not as individual frames but as segments and extraction of multiple segments which is well suited to signals with uncertain temporal location and duration in the video . Extensive experiments on UNBC McMaster Shoulder Pain dataset highlight the effectiveness of the approach by achieving competitive results on both tasks of pain classification and localization in videos . We also empirically evaluate the contributions of different components of MS MIL . The paper also includes the visualization of discriminative facial patches important for pain detection as discovered by our algorithm and relates them to Action Units that have been associated with pain expression . We conclude the paper by demonstrating that MS MIL yields a significant improvement on another spontaneous facial expression dataset the FEEDTUM dataset . 
",Target problem of pain classification and localization using weakly labeled pain videos Algorithm combines multiple instance learning with multiple segment representation. Rigorous experiments show that our algorithm achieves state of the art performance. Empirically evaluate the contributions of different components of our algorithm Visualize discriminative facial patches as learned by our algorithm,,,
S0262885613001650,"Multipath interference of light is the cause of important errors in Time of Flight ToF depth estimation. This paper proposes an algorithm that removes multipath distortion from a single depth map obtained by a ToF camera. Our approach does not require information about the scene apart from ToF measurements. The method is based on fitting ToF measurements with a radiometric model. Model inputs are depth values free from multipath interference whereas model outputs consist of synthesized ToF measurements. We propose an iterative optimization algorithm that obtains model parameters that best reproduce ToF measurements recovering the depth of the scene without distortion. We show results with both synthetic and real scenes captured by commercial ToF sensors. In all cases our algorithm accurately corrects the multipath distortion obtaining depth maps that are very close to ground truth data. 
",Multipath Interference MpI is a non systematic error in Time of Flight ToF imaging. MpI causes severe distortions in ToF measurements. We propose a ray tracing model that renders realistic ToF images including MpI. We use nonlinear optimization to find the scene that best renders the input image. We present results with real and synthetic datasets. Error is reduced in all cases.,,,
S0169260715002990," Background and objective In clinical examinations and brain computer interface research a short electroencephalogram measurement time is ideal . The use of event related potentials relies on both estimation accuracy and processing time . We tested a particle filter that uses a large number of particles to construct a probability distribution . Methods We constructed a simple model for recording EEG comprising three components ERPs approximated via a trend model background waves constructed via an autoregressive model and noise . We evaluated the performance of the particle filter based on mean squared error P300 peak amplitude and latency . We then compared our filter with the Kalman filter and a conventional simple averaging method . To confirm the efficacy of the filter we used it to estimate ERP elicited by a P300 BCI speller . Results A 400 particle filter produced the best MSE . We found that the merit of the filter increased when the original waveform already had a low signal to noise ratio . We calculated the amount of averaging necessary after applying a particle filter that produced a result equivalent to that associated with conventional averaging and determined that the particle filter yielded a maximum 42.8 reduction in measurement time . The particle filter performed better than both the Kalman filter and conventional averaging for a low SNR in terms of both MSE and P300 peak amplitude and latency . For EEG data produced by the P300 speller we were able to use our filter to obtain ERP waveforms that were stable compared with averages produced by a conventional averaging method irrespective of the amount of averaging . Conclusions We confirmed that particle filters are efficacious in reducing the measurement time required during simulations with a low SNR . Additionally particle filters can perform robust ERP estimation for EEG data produced via a P300 speller . 
",We constructed a model describing event related potential ERP by trend model. A 400 particle filter produced the best mean square error in the ERP estimation. The filter reduced an amount of average by 42.8 compared with simple averaging. The filter could estimate P300 robustly by application to EEG in P300 speller. Real time processing is realized in any computer with an appropriate particle number.,,,
S0198971516300023," Recent advances in public sector open data and online mapping software are opening up new possibilities for interactive mapping in research applications . Increasingly there are opportunities to develop advanced interactive platforms with exploratory and analytical functionality . This paper reviews tools and workflows for the production of online research mapping platforms alongside a classification of the interactive functionality that can be achieved . A series of mapping case studies from government academia and research institutes are reviewed . The conclusions are that online cartography s technical hurdles are falling due to open data releases open source software and cloud services innovations . The data exploration functionality of these new tools is powerful and complements the emerging fields of big data and open GIS . International data perspectives are also increasingly feasible . Analytical functionality for web mapping is currently less developed but promising examples can be seen in areas such as urban analytics . For more presentational research communication applications there has been progress in story driven mapping drawing on data journalism approaches that are capable of connecting with very large audiences . 
",Interactive online mapping tools are reviewed with workflows for site production. The mapping case studies illustrate best practice alongside the methods used. All the case study sites received tens of thousands of visitors. Exploratory interactive mapping functionality is increasingly extensive. Analytical functionality is more limited but there are signs of innovation.,,,
S0262885614000833," Automatically focusing and seeing occluded moving object in cluttered and complex scene is a significant challenging task for many computer vision applications . In this paper we present a novel synthetic aperture imaging approach to solve this problem . The unique characteristics of this work include the following To the best of our knowledge this work is the first to simultaneously solve camera array auto focusing and occluded moving object imaging problem . A unified framework is designed to achieve seamless interaction between the focusing and imaging modules . In the focusing module a local and global constraint based optimization algorithm is presented to dynamically estimate the focus plane of the moving object . In the imaging module a novel visibility analysis based active synthetic aperture imaging approach is proposed to remove the occluder and significantly improve the quality of occluded object imaging . An active camera array system has been set up and evaluated in challenging indoor and outdoor scenes . Extensive experimental results with qualitative and quantitative analyses demonstrate the superiority of the proposed approach compared with state of the art approaches . 
",This is a novel work to solve camera array auto focusing and occluded object imaging simultaneously. A unified framework is proposed to achieve seamless interaction between focusing and imaging. An optimization algorithm is presented to dynamically estimate the focus plane. A visibility analysis is proposed to remove the occluder and improve the imaging quality.,,,
S0262885614000158," This paper proposes a new method for self calibrating a set of stationary non rotating zooming cameras . This is a realistic configuration usually encountered in surveillance systems in which each zooming camera is physically attached to a static structure . In particular a linear yet effective method to recover the affine structure of the observed scene from two or more such stationary zooming cameras is presented . The proposed method solely relies on point correspondences across images and no knowledge about the scene is required . Our method exploits the mostly translational displacement of the so called principal plane of each zooming camera to estimate the location of the plane at infinity . The principal plane of a camera at any given setting of its zoom is encoded in its corresponding perspective projection matrix from which it can be easily extracted . As a displacement of the principal plane of a camera under the effect of zooming allows the identification of a pair of parallel planes each zooming camera can be used to locate a line on the plane at infinity . Hence two or more such zooming cameras in general positions allow the obtainment of an estimate of the plane at infinity making it possible under the assumption of zero skew and or known aspect ratio to linearly calculate the camera s parameters . Finally the parameters of the camera and the coordinates of the plane at infinity are refined through a nonlinear least squares optimization procedure . The results of our extensive experiments using both simulated and real data are also reported in this paper . 
",A simple linear method to self calibrate stationary zooming cameras Neither special camera motion nor scene knowledge required Two zoom images of a stationary camera provide two parallel principle planes. The plane at infinity can be located from two or more such zooming cameras. Simulation laboratory and real scene experiments with 3D measurements validate our method.,,,
S0169260716000067," This paper proposes a novel active learning framework and combines it with semi supervised learning for segmenting Crohns disease tissues from abdominal magnetic resonance images . Robust fully supervised learning based classifiers require lots of labeled data of different disease severities . Obtaining such data is time consuming and requires considerable expertise . SSL methods use a few labeled samples and leverage the information from many unlabeled samples to train an accurate classifier . AL queries labels of most informative samples and maximizes gain from the labeling effort . Our primary contribution is in designing a query strategy that combines novel context information with classification uncertainty and feature similarity . Combining SSL and AL gives a robust segmentation method that optimally uses few labeled samples and many unlabeled samples and requires lower training time . Experimental results show our method achieves higher segmentation accuracy than FSL methods with fewer samples and reduced training effort . 
",We propose an interactive method combining semi supervised learning SSL and active learning AL for segmenting Crohns disease affected regions in MRI. A novel query strategy for AL has been proposed that makes use of context information to identify query samples. Compared to fully supervised methods we obtain high segmentation accuracy with fewer samples and lesser computation time. Our method has the potential to be used in scenarios which pose difficulties in obtaining large numbers of accurately labeled data.,,,
S0169260716301067," Aim Medical data mining processes for extracting patterns from large datasets . In the current study we intend to assess different medical data mining approaches to predict ischemic stroke . Materials and methods The collected dataset from Turgut Ozal Medical Centre Inonu University Malatya Turkey comprised the medical records of 80 patients and 112 healthy individuals with 17 predictors and a target variable . As data mining approaches support vector machine stochastic gradient boosting and penalized logistic regression were employed . 10 fold cross validation resampling method was utilized and model performance evaluation metrics were accuracy area under ROC curve sensitivity specificity positive predictive value and negative predictive value . The grid search method was used for optimizing tuning parameters of the models . Results The accuracy values with 95 CI were 0.9789 for SVM 0.9737 for SGB and 0.8947 for PLR . The AUC values with 95 CI were 0.9783 for SVM 0.9757 for SGB and 0.8953 for PLR . Conclusions The results of the current study demonstrated that the SVM produced the best predictive performance compared to the other models according to the majority of evaluation metrics . SVM and SGB models explained in the current study could yield remarkable predictive performance in the classification of ischemic stroke . 
",We assessed different medical data mining approaches to predict ischemic stroke. Grid search were used for improving classification performance of the models. The accuracy and AUC values were higher than 0.8947 and 0.8953 respectively. SVM and SGB models yielded remarkable performance for classifying ischemic stroke.,,,
S0262885614000651," Human action recognition has lots of real world applications such as natural user interface virtual reality intelligent surveillance and gaming . However it is still a very challenging problem . In action recognition using the visible light videos the spatiotemporal interest point based features are widely used with good performance . Recently with the advance of depth imaging technology a new modality has appeared for human action recognition . It is important to assess the performance and usefulness of the STIP features for action analysis on the new modality of 3D depth map . In this paper we evaluate the spatiotemporal interest point based features for depth based action recognition . Different interest point detectors and descriptors are combined to form various STIP features . The bag of words representation and the SVM classifiers are used for action learning . Our comprehensive evaluation is conducted on four challenging 3D depth databases . Further we use two schemes to refine the STIP features one is to detect the interest points in RGB videos and apply to the aligned depth sequences and the other is to use the human skeleton to remove irrelevant interest points . These refinements can help us have a deeper understanding of the STIP features on 3D depth data . Finally we investigate a fusion of the best STIP features with the prevalent skeleton features to present a complementary use of the STIP features for action recognition on 3D data . The fusion approach gives significantly higher accuracies than many state of the art results . 
",A comprehensive evaluation of STIP based features on depth based action recognition Two schemes to refine STIP features for a deeper understanding of their behaviors A fusion approach is developed which outperforms many state of the art methods.,,,
S0169260715301401," Background and objectives Angle closure disease in the eye can be detected using time domain Anterior Segment Optical Coherence Tomography . The Anterior Chamber characteristics can be quantified from AS OCT image which is dependent on the image quality at the image acquisition stage . To date to the best of our knowledge there are no objective or automated subjective measurements to assess the quality of AS OCT images . Methods To address AS OCT image quality assessment issue we define a method for objective assessment of AS OCT images using complex wavelet based local binary pattern features . These features are pooled using the Na ve Bayes classifier to obtain the final quality parameter . To evaluate the proposed method a subjective assessment has been performed by clinical AS OCT experts who graded the quality of AS OCT images on a scale of good fair and poor . This was done based on the ability to identify the AC structures including the position of the scleral spur . Results We compared the results of the proposed objective assessment with the subjective assessments . From this comparison it is validated that the proposed objective assessment has the ability of differentiating the good and fair quality AS OCT images for glaucoma diagnosis from the poor quality AS OCT images . Conclusions This proposed algorithm is an automated approach to evaluate the AS OCT images with the intention for collecting of high quality data for further medical diagnosis . Our proposed quality index has the ability of automatic objective and quantitative assessment of AS OCT image quality and this quality index is similar to glaucoma specialist . 
",A new quality assessment method for AS OCT images using wavelet based LBP features. It is a first work so far there is no objective assessment of AS OCT image quality. The proposed algorithm does not require any additional information from the AS OCT device. This work aimed at collecting high quality AS OCT images for Glaucoma diagnosis. Our proposed quality index score is similar to the quality assessment of Glaucoma experts.,,,
S0169260715002758," Anatomical cine cardiovascular magnetic resonance imaging is widely used to assess the systolic cardiac function because of its high soft tissue contrast . Assessment of diastolic LV function has not regularly been performed due the complex and time consuming procedures . This study presents a semi automated assessment of the left ventricular diastolic function using anatomical short axis cine CMR images . The proposed method is based on three main steps non rigid registration which yields a sequence of endocardial boundary points over the cardiac cycle based on a user provided contour on the first frame LV volume and filling rate computations over the cardiac cycle and automated detection of the peak values of early and late ventricular filling waves . In 47 patients cine CMR imaging and Doppler echocardiographic imaging were performed . CMR measurements of peak values of the E and A waves as well as the deceleration time were compared with the corresponding values obtained in Doppler Echocardiography . For the E A ratio the proposed algorithm for CMR yielded a Cohen s kappa measure of 0.70 and a Gwet s AC1 coefficient of 0.70 . Conclusion Semi automated assessment of the left ventricular diastolic function using anatomical short axis cine CMR images provides mitral inflow measurements comparable to Doppler Echocardiography . 
",A semi automatic method was proposed to assess the cardiac LV diastolic function. A non rigid registration was applied to track the endocardial boundary points. LV volume and filling rates are computed over the entire cardiac cycle. The peaks of early and late ventricular filling waves were detected automatically. Experimental evaluations were performed over CMR and echo datasets from 47 subjects.,,,
S0262885613001820," We propose a measure of information gained through biometric matching systems . Firstly we discuss how the information about the identity of a person is derived from biometric samples through a biometric system and define the biometric system entropy or BSE based on mutual information . We present several theoretical properties and interpretations of the BSE and show how to design a biometric system which maximizes the BSE . Then we prove that the BSE can be approximated asymptotically by the relative entropy D fI where fG and fI are probability mass functions of matching scores between samples from individuals and among population . We also discuss how to evaluate the BSE of a biometric system and show experimental evaluation of the BSE of face fingerprint and multimodal biometric systems . 
",We propose an information theoretical measure for biometric systems. We present several useful properties and interpretations of the measure. We prove that the measure can be approximated by the relative entropy. We discuss how to evaluate the measure and show experimental evaluations.,,,
S0169260715003235," Brain ageing is followed by changes of the connectivity of white matter and changes of the grey matter concentration . Neurodegenerative disease is more vulnerable to an accelerated brain ageing which is associated with prospective cognitive decline and disease severity . Accurate detection of accelerated ageing based on brain network analysis has a great potential for early interventions designed to hinder atypical brain changes . To capture the brain ageing we proposed a novel computational approach for modeling the 112 normal older subjects brain age by connectivity analyses of networks of the brain . Our proposed method applied principal component analysis to reduce the redundancy in network topological parameters . Back propagation artificial neural network improved by hybrid genetic algorithm and Levenberg Marquardt algorithm is established to model the relation among principal components and brain age . The predicted brain age is strongly correlated with chronological age . The model has mean absolute error of 4.29 years . Therefore we believe the method can provide a possible way to quantitatively describe the typical and atypical network organization of human brain and serve as a biomarker for presymptomatic detection of neurodegenerative diseases in the future . 
",We proposed a novel computational approach for modeling the normal elderly subjects s brain age by connectivity analyses of networks of the brain. Principal component analysis PCA is applied to reduce the redundancy in network topological parameters. BP artificial neural network BPANN is improved by hybrid genetic algorithm GA and Levenberg Marquardt LM algorithm to model the relation among principal components PCs and brain age. The method has shown good performance for old cohort with limited samples.,,,
S0169260715301152," Background and objective Signal segmentation and spike detection are two important biomedical signal processing applications . Often non stationary signals must be segmented into piece wise stationary epochs or spikes need to be found among a background of noise before being further analyzed . Permutation entropy has been proposed to evaluate the irregularity of a time series . PE is conceptually simple structurally robust to artifacts and computationally fast . It has been extensively used in many applications but it has two key shortcomings . First when a signal is symbolized using the Bandt Pompe procedure only the order of the amplitude values is considered and information regarding the amplitudes is discarded . Second in the PE the effect of equal amplitude values in each embedded vector is not addressed . To address these issues we propose a new entropy measure based on PE the amplitude aware permutation entropy . Methods AAPE is sensitive to the changes in the amplitude in addition to the frequency of the signals thanks to it being more flexible than the classical PE in the quantification of the signal motifs . To demonstrate how the AAPE method can enhance the quality of the signal segmentation and spike detection a set of synthetic and realistic synthetic neuronal signals electroencephalograms and neuronal data are processed . We compare the performance of AAPE in these problems against state of the art approaches and evaluate the significance of the differences with a repeated ANOVA with post hoc Tukey s test . Results In signal segmentation the accuracy of AAPE based method is higher than conventional segmentation methods . AAPE also leads to more robust results in the presence of noise . The spike detection results show that AAPE can detect spikes well even when presented with single sample spikes unlike PE . For multi sample spikes the changes in AAPE are larger than in PE . Conclusion We introduce a new entropy metric AAPE that enables us to consider amplitude information in the formulation of PE . The AAPE algorithm can be used in almost every irregularity based application in various signal and image processing fields . We also made freely available the Matlab code of the AAPE . 
",Permutation entropy PE is a fast method to evaluate the irregularity of signals. PE does not consider the average of amplitude values and equal amplitude values. We propose amplitude aware PE AAPE . We evaluate the AAPE method for signal segmentation and spike detection applications.,,,
S0169260715301425," Background and objective Heart failure due to iron overload cardiomyopathy is one of the main causes of mortality . The cardiomyopathy is reversible if intensive iron chelation treatment is done in time but the diagnosis is often delayed because the cardiac iron deposition is unpredictable and the symptoms are lately detected . There are many ways to assess iron overload . However the widely used and approved method is by using MRI which is performed by calculating the T2 . In order to compute the T2 value the region of interest is manually selected by an expert which may require considerable time and skills . The aim of this work is hence to develop the cardiac T2 measurement by using region growing algorithm for automatically segmenting the ROI in cardiac MR images . Mathematical morphologies are also used to reduce some errors . Methods Thirty MR images with free breathing and respiratory trigger technique were used in this work . The segmentation algorithm yields good results when compared with the manual segmentation performed by two experts . Results The averages of positive predictive value the sensitivity the Hausdorff distance and the Dice similarity coefficient are 0.76 0.84 7.78 pixels and 0.80 when compared with the two experts opinions . The T2 values were carried out based on the automatically segmented ROI s. The mean difference of T2 values between the proposed technique and the experts opinion is about 1.40ms . Conclusions The results demonstrate the accuracy of the proposed method in T2 value estimation . Some previous methods were implemented for comparisons . The results show that the proposed method yields better segmentation and T2 value estimation performances . 
",Novel automatic cardiac T2 relaxation time estimation algorithm from MR images. Novel automatic ROI segmentation algorithm in cardiac MR images. Segmentation results from our technique are very close to the experts opinions. Cardiac T2 values from our technique are very close to that calculated by experts. Our method does not need manual processes in ROI segmentation and T2 calculation.,,,
S0169260715300560," Background and objectives Text mining and semantic analysis approaches can be applied to the construction of biomedical domain specific search engines and provide an attractive alternative to create personalized and enhanced search experiences . Therefore this work introduces the new open source BIOMedical Search Engine Framework for the fast and lightweight development of domain specific search engines . The rationale behind this framework is to incorporate core features typically available in search engine frameworks with flexible and extensible technologies to retrieve biomedical documents annotate meaningful domain concepts and develop highly customized Web search interfaces . Methods The BIOMedical Search Engine Framework integrates taggers for major biomedical concepts such as diseases drugs genes proteins compounds and organisms and enables the use of domain specific controlled vocabulary . Technologies from the Typesafe Reactive Platform the AngularJS JavaScript framework and the Bootstrap HTML CSS framework support the customization of the domain oriented search application . Moreover the RESTful API of the BIOMedical Search Engine Framework allows the integration of the search engine into existing systems or a complete web interface personalization . Results The construction of the Smart Drug Search is described as proof of concept of the BIOMedical Search Engine Framework . This public search engine catalogs scientific literature about antimicrobial resistance microbial virulence and topics alike . The keyword based queries of the users are transformed into concepts and search results are presented and ranked accordingly . The semantic graph view portraits all the concepts found in the results and the researcher may look into the relevance of different concepts the strength of direct relations and non trivial indirect relations . The number of occurrences of the concept shows its importance to the query and the frequency of concept co occurrence is indicative of biological relations meaningful to that particular scope of research . Conversely indirect concept associations i.e . concepts related by other intermediary concepts can be useful to integrate information from different studies and look into non trivial relations . Conclusions The BIOMedical Search Engine Framework supports the development of domain specific search engines . The key strengths of the framework are modularity and extensibilityin terms of software design the use of open source consolidated Web technologies and the ability to integrate any number of biomedical text mining tools and information resources . Currently the Smart Drug Search keeps over 1 186 000 documents containing more than 11 854 000 annotations for 77 200 different concepts . The Smart Drug Search is publicly accessible at http sing.ei.uvigo.es sds . The BIOMedical Search Engine Framework is freely available for non commercial use at https github.com agjacome biomsef . 
",The wealth of knowledge laying in biomedical publications is invaluable to make new scientific discoveries. Domain specific search engines aim to improve retrieval performance and to enhance user search experience. The BIOMedical Search Engine Framework aims to speed up the construction of biomedical domain specific search engines. The BIOMedical Search Engine Framework enables the retrieval and annotation of biomedical documents and the customised visualisation of indexed concepts and documents. The BIOMedical Search Engine Framework supported the construction of the Smart Drug Search SDS in assistance of antimicrobial research.,,,
S0169260715003338," Background and objective Hypokinetic dysarthria is a frequent speech disorder associated with idiopathic Parkinson s disease . It affects all dimensions of speech production . One of the most common features of HD is dysprosody that is characterized by alterations of rhythm and speech rate flat speech melody and impairment of speech intensity control . Dysprosody has a detrimental impact on speech naturalness and intelligibility . Methods This paper deals with quantitative prosodic analysis of neutral stress modified and rhymed speech in patients with PD . The analysis of prosody is based on quantification of monopitch monoloudness and speech rate abnormalities . Experimental dataset consists of 98 patients with PD and 51 healthy speakers . For the purpose of HD identification sequential floating feature selection algorithm and random forests classifier is used . In this paper we also introduce a concept of permutation test applied in the field of acoustic analysis of dysarthric speech . Results Prosodic features obtained from stress modified reading task provided higher classification accuracies compared to the ones extracted from reading task with neutral emotion demonstrating the importance of stress in speech prosody . Features calculated from poem recitation task outperformed both reading tasks in the case of gender undifferentiated analysis showing that rhythmical demands can in general lead to more precise identification of HD . Additionally some gender related patterns of dysprosody has been observed . Conclusions This paper confirms reduced variation of fundamental frequency in PD patients with HD . Interestingly increased variability of speech intensity compared to healthy speakers has been detected . Regarding speech rate disturbances our results does not report any particular pattern . We conclude further development of prosodic features quantifying the relationship between monopitch monoloudness and speech rate disruptions in HD can have a great potential in future PD analysis . 
",We analysed neutral stress modified and rhymed speech in Parkinson s disease. We proposed quantitative prosodic analysis of poem recitation task. We showed rhythmical demands improve identification of hypokinetic dysarthria. We introduced a concept of permutation test in dysarthric speech analysis.,,,
S0262885614000936," This paper presents a matching strategy to improve the discriminative power of histogram based keypoint descriptors by constraining the range of allowable dominant orientations according to the context of the scene under observation . This can be done when the descriptor uses a circular grid and quantized orientation steps by computing or providing a global reference orientation based on the feature matches . The proposed matching strategy is compared with the standard approaches used with the SIFT and GLOH descriptors and the recent rotation invariant MROGH and LIOP descriptors . A new evaluation protocol based on an approximated overlap error is presented to provide an effective analysis in the case of non planar scenes thus extending the current state of the art results . 
",Novel matching strategies for histogram based descriptors are presented. Global dominant orientation is used by exploiting the image context. A new 3D extensible framework to evaluate feature descriptors is introduced. 2D 3D comparisons with state of the art rotational invariant descriptors are reported. Results show the effectiveness of the proposed matching approaches.,,,
S0262885613001753," The analysis of regular texture images is cast in a model comparison framework . Texel lattice hypotheses are used to define statistical models which are compared in terms of their ability to explain the images . This approach is used to estimate lattice geometry from patterns that exhibit translational symmetry . It is also used to determine whether images consist of such regular textures . A method based on this approach is described in which lattice hypotheses are generated using analysis of peaks in the image autocorrelation function statistical models are based on Gaussian or Gaussian mixture clusters and model comparison is performed using the marginal likelihood as approximated by the Bayes Information Criterion . Experiments on public domain images and a commercial textile image archive demonstrate substantially improved accuracy compared to several alternative methods . 
",Regular texture patterns exhibiting translational symmetry were modelled. Model comparison was implemented using Bayes Information Criterion. Texel lattice geometry was estimated from e.g. images from a textile archive. Accuracy was better than other methods with which it was compared.,,,
S0169260715002710," Eye blinks are one of the most influential artifact sources in electroencephalogram recorded from frontal channels and thereby detecting and rejecting eye blink artifacts is regarded as an essential procedure for improving the quality of EEG data . In this paper a novel method to detect eye blink artifacts from a single channel frontal EEG signal was proposed by combining digital filters with a rule based decision system and its performance was validated using an EEG dataset recorded from 24 healthy participants . The proposed method has two main advantages over the conventional methods . First it uses single channel EEG data without the need for electrooculogram references . Therefore this method could be particularly useful in brain computer interface applications using headband type wearable EEG devices with a few frontal EEG channels . Second this method could estimate the ranges of eye blink artifacts accurately . Our experimental results demonstrated that the artifact range estimated using our method was more accurate than that from the conventional methods and thus the overall accuracy of detecting epochs contaminated by eye blink artifacts was markedly increased as compared to conventional methods . The MATLAB package of our library source codes and sample data named Eyeblink Master is open for free download . 
",A novel method to detect eye blink artifacts from a single channel frontal EEG signal was proposed. Overall accuracy of detecting epochs contaminated by eye blink artifacts was markedly increased. An online experiment showed that our method is useful for headband type wearable EEG applications. A MATLAB package of our library and sample data is open for free download.,,,
S0262885614000808," We introduce a new computational phonetic modeling framework for sign language recognition . This is based on dynamic static statistical subunits and provides sequentiality in an unsupervised manner without prior linguistic information . Subunit sequentiality refers to the decomposition of signs into two types of parts varying and non varying that are sequentially stacked across time . Our approach is inspired by the Movement Hold SL linguistic model that refers to such sequences . First we segment signs into intra sign primitives and classify each segment as dynamic or static i.e . movements and non movements . These segments are then clustered appropriately to construct a set of dynamic and static subunits . The dynamic static discrimination allows us employing different visual features for clustering the dynamic or static segments . Sequences of the generated subunits are used as sign pronunciations in a data driven lexicon . Based on this lexicon and the corresponding segmentation each subunit is statistically represented and trained on multimodal sign data as a hidden Markov model . In the proposed approach dynamic static sequentiality is incorporated in an unsupervised manner . Further handshape information is integrated in a parallel hidden Markov modeling scheme . The novel sign language modeling scheme is evaluated in recognition experiments on data from three corpora and two sign languages Boston University American SL which is employed pre segmented at the sign level Greek SL Lemmas and American SL Large Vocabulary Dictionary including both signer dependent and unseen signers testing . Results show consistent improvements when compared with other approaches demonstrating the importance of dynamic static structure in sub sign phonetic modeling . 
",A phonetic modeling approach for unsupervised sequentiality of dynamic static SUs. Model based sign segmentation and subunit modeling with HMMs. Construction of a sign to subunits data driven lexicon. Comparison of different signers pronunciations and unseen signer pronunciation compensation. Evaluation on data from three corpora two sign languages and unseen signers.,,,
S0169260715303473," Background and objective Cell migration differentiation proliferation and apoptosis are the main processes in tissue regeneration . Mesenchymal Stem Cells have the potential to differentiate into many cell phenotypes such as tissue or organ specific cells to perform special functions . Experimental observations illustrate that differentiation and proliferation of these cells can be regulated according to internal forces induced within their Extracellular Matrix . The process of how exactly they interpret and transduce these signals is not well understood . Methods A previously developed three dimensional computational model is here extended and employed to study how force free substrates and force induced substrate control cell differentiation and or proliferation during the mechanosensing process . Consistent with experimental observations it is assumed that cell internal deformation in correlation with the cell maturation state directly triggers cell differentiation and or proliferation . The Extracellular Matrix is modeled as Neo Hookean hyperelastic material assuming that cells are cultured within 3D nonlinear hydrogels . Results In agreement with well known experimental observations the findings here indicate that within neurogenic chondrogenic and osteogenic substrates Mesenchymal Stem Cells differentiation and proliferation can be precipitated by inducing the substrate with an internal force . Therefore cells require a longer time to grow and maturate within force free substrates than within force induced substrates . In the instance of Mesenchymal Stem Cells differentiation into a compatible phenotype the magnitude of the net traction force increases within chondrogenic and osteogenic substrates while it reduces within neurogenic substrates . This is consistent with experimental studies and numerical works recently published by the same authors . However in all cases the magnitude of the net traction force considerably increases at the instant of cell proliferation because of cell cell interaction . Conclusions The present model provides new perspectives to delineate the role of force induced substrates in remotely controlling the cell fate during cell matrix interaction which open the door for new tissue regeneration methodologies . 
",A 3D model is developedto study how force induced substrate controls cell differentiation and or proliferation. Cells are cultured within 3D Neo Hookeanhyperelastic hydrogels with encapsulated magnetic nanoparticles. Internal forces can play an outstanding role in remotelycontrolling the lineage specification of MSCs and cell proliferation. A new perspective to delineate the role of force induced substrates in controlling the cell fate.,,,
S0262885613000760," Conventional particle filtering based visual ego motion estimation or visual odometry often suffers from large local linearization errors in the case of abrupt camera motion . The main contribution of this paper is to present a novel particle filtering based visual ego motion estimation algorithm that is especially robust to the abrupt camera motion . The robustness to the abrupt camera motion is achieved by multi layered importance sampling via particle swarm optimization which iteratively moves particles to higher likelihood region without local linearization of the measurement equation . Furthermore we make the proposed visual ego motion estimation algorithm in real time by reformulating the conventional vector space PSO algorithm in consideration of the geometry of the special Euclidean group SE which is a Lie group representing the space of 3 D camera poses . The performance of our proposed algorithm is experimentally evaluated and compared with the local linearization and unscented particle filter based visual ego motion estimation algorithms on both simulated and real data sets . 
",Novel particle filtering based visual ego motion estimation algorithm robust to abrupt camera motion. Multi layered importance sampling via particle swarm optimization PSO Reformulation of the conventional vector space PSO algorithm by geometric special Euclidean group SE 3 Efficient convergence of PSO and real time visual ego motion estimation performance,,,
S0169260715002709," Glaucoma is a disease of the retina which is one of the most common causes of permanent blindness worldwide . This paper presents an automatic image processing based method for glaucoma diagnosis from the digital fundus image . In this paper wavelet feature extraction has been followed by optimized genetic feature selection combined with several learning algorithms and various parameter settings . Unlike the existing research works where the features are considered from the complete fundus or a sub image of the fundus this work is based on feature extraction from the segmented and blood vessel removed optic disc to improve the accuracy of identification . The experimental results presented in this paper indicate that the wavelet features of the segmented optic disc image are clinically more significant in comparison to features of the whole or sub fundus image in the detection of glaucoma from fundus image . Accuracy of glaucoma identification achieved in this work is 94.7 and a comparison with existing methods of glaucoma detection from fundus image indicates that the proposed approach has improved accuracy of classification . 
",In this work glaucoma identification is done using wavelet features of optic disk. Wavelet features are extracted from segmented and blood vessels removed optic disk. Several machine learning algorithms are used for prominent feature selection. Genetic algorithm is used to reduce the dimensionality of feature vector. Accuracy of glaucoma identification achieved in this work is 94.7 .,,,
S0262885613000784,"Automatic facial landmarking is a crucial prerequisite of many applications dedicated to face analysis. In this paper we describe a two step method. In a first step each landmark position in the image is predicted independently. To achieve fast and accurate localizations we implement detectors based on a two stage classifier and we use multiple kernel learning algorithms to combine multi scale features. In a second step to increase the robustness of the system we introduce spatial constraints between landmarks. To this end parameters of a deformable shape model are optimized using the first step outputs through a Gauss Newton algorithm. Extensive experiments have been carried out on different databases PIE LFPW Cohn Kanade Face Pix and BioID assessing the accuracy and the robustness of the proposed approach. They show that the proposed algorithm is not significantly affected by small rotations facial expressions or natural occlusions and can be favorably compared with the current state of the art landmarking systems. 
",Multi Kernel Appearance Model is a new facial point detector. Multi Kernel SVM combines multi resolution features. A SVM cascade combines increasingly complex kernels to reduce the computational time. A shape model fitting introduces constraints between SVM detections.,,,
S0169260716000055," Abnormal values of vital parameters such as hypotension or tachycardia may occur during anesthesia and may be detected by analyzing time series data collected during the procedure by the Anesthesia Information Management System . When crossed with other data from the Hospital Information System abnormal values of vital parameters have been linked with postoperative morbidity and mortality . However methods for the automatic detection of these events are poorly documented in the literature and differ between studies making it difficult to reproduce results . In this paper we propose a methodology for the automatic detection of abnormal values of vital parameters . This methodology uses an algorithm allowing the configuration of threshold values for any vital parameters as well as the management of missing data . Four examples illustrate the application of the algorithm after which it is applied to three vital signs to all 2014 anesthetic records at our institution . 
",Methods for automatic detection of abnormal vital signs occurring during anesthesia are poorly documented in the literature. Existing methods are not reproducible between databases. We have developed a data model and an algorithm that allow automatic detection of abnormal values of vital parameters occurring during anesthesia. Predefined thresholds and various parameters such as time between measurements and time spent outside predefined thresholds provide adaptability to various clinical situations e.g. hypotension occurring after start of anesthesia. The relation between occurrence of abnormal values of vital parameters and mortality and length of stay may then be studied on a large and automated scale.,,,
S0169260715300468," Background The simultaneous acquisition of electroencephalogram and functional magnetic resonance imaging provides both high temporal and spatial resolution when measuring brain activity . A real time analysis during a simultaneous EEG fMRI acquisition is essential when studying neurofeedback and conducting effective brain activity monitoring . However the ballistocardiogram artifacts which are induced by heartbeat related electrode movements in an MRI scanner severely contaminate the EEG signals and hinder a reliable real time analysis . New method The optimal basis sets method is an effective candidate for removing BCG artifacts in a traditional offline EEG fMRI analysis but has yet to be applied to a real time EEG fMRI analysis . Here a novel real time technique based on OBS method is proposed to remove BCG artifacts on a moment to moment basis . Real time electrocardiogram R peak detection procedure and sliding window OBS method were adopted . Results A series of simulated data was constructed to verify the feasibility of the rtOBS technique . Furthermore this method was applied to real EEG fMRI data to remove BCG artifacts . The results of both simulated data and real EEG fMRI data from eight healthy human subjects demonstrate the effectiveness of rtOBS in both the time and frequency domains . Comparison with existing methods A comparison between rtOBS and real time averaged artifact subtraction was conducted . The results suggest the efficacy and advantage of rtOBS in the real time removal of BCG artifacts . Conclusions In this study a novel real time OBS technique was proposed for the real time removal of BCG artifacts . The proposed method was tested using simulated data and applied to real simultaneous EEG fMRI data . The results suggest the effectiveness of this method . 
",A real time OBS was proposed for the real time removal of ballistocardiogram artifacts. The real time OBS performed better than rtAAS in the real time removal of BCG artifacts. A real time analysis during a simultaneous EEG fMRI acquisition is essential to achieving neurofeedback.,,,
S0169260715300663," Transfer function design is a key issue in direct volume rendering . Many sophisticated transfer functions have been proposed to visualize boundaries in volumetric data sets such as computed tomography and magnetic resonance imaging . However it is still conventionally challenging to reliably detect boundaries . Meanwhile the interactive strategy is complicated for new users or even experts . In this paper we first propose the human centric boundary extraction criteria and our boundary model . Based on the model we present a boundary visualization method through a what material you pick is what boundary you see approach . Users can pick out the material of interest to directly convey semantics . In addition the 3 D canny edge detection is utilized to ensure the good localization of boundaries . Furthermore we establish a point to material distance measure to guarantee the accuracy and integrity of boundaries . The proposed boundary visualization is intuitive and flexible for the exploration of volumetric data . 
",Human centric boundary extraction criteria and new boundary model. A novel boundary visualization method though a what material you pick is what boundary you see approach. Point to material distance measure. A complete application.,,,
S0169260715301826," Background and objective Integrative approaches for the study of biological systems have gained popularity in the realm of statistical genomics . For example The Cancer Genome Atlas has applied integrative clustering methodologies to various cancer types to determine molecular subtypes within a given cancer histology . In order to adequately compare integrative or systems biology type methods realistic and related datasets are needed to assess the methods . This involves simulating multiple types of omic data with realistic correlation between features of the same type and across data types . Methods We present the software application tool InterSIM for simulating multiple interrelated data types with realistic intra and inter relationships based on the DNA methylation mRNA gene expression and protein expression from the TCGA ovarian cancer study . Results The resulting simulated datasets can be used to assess and compare the operating characteristics of newly developed integrative bioinformatics methods to existing methods . Application of InterSIM is presented with an example of heatmaps of the simulated datasets . Conclusions InterSIM allows researchers to evaluate and test new integrative methods with realistically simulated interrelated genomic datasets . The software tool InterSIM is implemented in R and is freely available from CRAN . 
",Integrative approaches for the study of biological systems have gained popularity.. There is lack of integrative simulation techniques to assess systems biology methods. InterSIM bridges this gap by simulating complex interrelated genomic data. InterSIM allows researchers to evaluate and test new integrative methods. The software tool InterSIM is implemented in R and is freely available from CRAN.,,,
S0262885613000899," Current image matting methods based on color sampling use color to distinguish between foreground and background pixels . However they fail when the corresponding color distributions overlap . Other methods that define correlation between neighboring pixels based on color aim to propagate the opacity parameter from known pixels to unknown pixels . However strong edges of textured regions may block the propagation of . In this paper a new matting strategy is proposed that delivers an accurate matte by considering texture as a feature that can complement color even if the foreground and background color distributions overlap and the image is a complex one with highly textured regions . The texture feature is extracted in such a way as to increase distinction between foreground and background regions . An objective function containing color and texture components is optimized to find the best foreground and background pair among a set of candidate pairs . The effectiveness of proposed method is compared quantitatively as well as qualitatively with other matting methods by evaluating their results on a benchmark dataset and a set of complex images . The evaluations show that the proposed method presented the best among state of the art matting methods . 
",The texture information is leveraged to complement color in image matting. The proposed method combines color and texture information in matting process. The proposed combined method presents the best among current matting methods. The proposed method has first ranks with respect to MSE and Gradient error. Several experiments are carried out to reveal potential power of texture in matting.,,,
S0169260715301929," Background and objectives The automated analysis of indirect immunofluorescence images for Anti Nuclear Autoantibody testing is a fairly recent field that is receiving ever growing interest from the research community . ANA testing leverages on the categorization of intensity level and fluorescent pattern of IIF images of HEp 2 cells to perform a differential diagnosis of important autoimmune diseases . Nevertheless it suffers from tremendous lack of repeatability due to subjectivity in the visual interpretation of the images . The automatization of the analysis is seen as the only valid solution to this problem . Several works in literature address individual steps of the work flow nonetheless integrating such steps and assessing their effectiveness as a whole is still an open challenge . Methods We present a modular tool ANAlyte able to characterize a IIF image in terms of fluorescent intensity level and fluorescent pattern without any user interactions . For this purpose ANAlyte integrates the following Intensity Classifier module that categorizes the intensity level of the input slide based on multi scale contrast assessment Cell Segmenter module that splits the input slide into individual HEp 2 cells Pattern Classifier module that determines the fluorescent pattern of the slide based on the pattern of the individual cells . Results To demonstrate the accuracy and robustness of our tool we experimentally validated ANAlyte on two different public benchmarks of IIF HEp 2 images with rigorous leave one out cross validation strategy . We obtained overall accuracy of fluorescent intensity and pattern classification respectively around 85 and above 90 . We assessed all results by comparisons with some of the most representative state of the art works . Conclusions Unlike most of the other works in the recent literature ANAlyte aims at the automatization of all the major steps of ANA image analysis . Results on public benchmarks demonstrate that the tool can characterize HEp 2 slides in terms of intensity and fluorescent pattern with accuracy better or comparable with the state of the art techniques even when such techniques are run on manually segmented cells . Hence ANAlyte can be proposed as a valid solution to the problem of ANA testing automatization . 
",We propose ANAlyte an automated tool for HEp 2 image analysis. ANAlyte is a valid solution to the problem of ANA test automatization. We integrate modules for the full characterization of HEp 2 slides. We propose a new technique for HEp 2 intensity characterization. Our tool is validated on two different public benchmarks of HEp 2 images.,,,
S0169260715002795," Although direct volume rendering has become a commodity effective rendering of interesting features is still a challenge . In one of active DVR application fields the medicine radiologists have used DVR for the diagnosis of lesions or diseases that should be visualized distinguishably from other surrounding anatomical structures . One of most frequent and important radiologic tasks is the detection of lesions usually constrictions in complex tubular structures . In this paper we propose a 3D spatial field for the effective visualization of constricted tubular structures called as a stenosis map which stores the degree of constriction at each voxel . Constrictions within tubular structures are quantified by using newly proposed measures based on the localized structure analysis and classified with a proposed transfer function mapping the degree of constriction to color and opacity . We show the application results of our method to the visualization of coronary artery stenoses . We present performance evaluations using twenty eight clinical datasets demonstrating high accuracy and efficacy of our proposed method . The ability of our method to saliently visualize the constrictions within tubular structures and interactively adjust the visual appearance of the constrictions proves to deliver a substantial aid in radiologic practice . 
",A new 3D spatial field for visualizing constricted tubular structures is proposed. Constrictions are quantified by new measures using the localized structure analysis. A new transfer function maps the degree of constriction to color and opacity. Our method provides intuitive adjustment of the visual appearance of constrictions. Our method can be applied to various constrictions with relevant parameters.,,,
S0198971515300077," Forecasting the variability of dwellings and residential land is important for estimating the future potential of environmental technologies . This paper presents an innovative method of converting average residential density into a set of one hectare 3D tiles to represent the dwelling stock . These generic tiles include residential land as well as the dwelling characteristics . The method was based on a detailed analysis of the English House Condition Survey data and density was calculated as the inverse of the plot area per dwelling . This found that when disaggregated by age band urban morphology and area type the frequency distribution of plot density per dwelling type can be represented by the gamma distribution . The shape parameter revealed interesting characteristics about the dwelling stock and how this has changed over time . It showed a consistent trend that older dwellings have greater variability in plot density than newer dwellings and also that apartments and detached dwellings have greater variability in plot density than terraced and semi detached dwellings . Once calibrated the shape parameter of the gamma distribution was used to convert the average density per housing type into a frequency distribution of plot density . These were then approximated by systematically selecting a set of generic tiles . These tiles are particularly useful as a medium for multidisciplinary research on decentralized environmental technologies or climate adaptation which requires this understanding of the variability of dwellings occupancies and urban space . It thereby links the socioeconomic modeling of city regions with the physical modeling of dwellings and associated infrastructure across the spatial scales . The tiles method has been validated by comparing results against English regional housing survey data and dwelling footprint area data . The next step would be to explore the possibility of generating generic residential area types and adapt the method to other countries that have similar housing survey data . 
",A statistical method of forecasting housing stock variability from average density A unique method for studying the variability of dwellings by type and age bands The 3D tile typologies include both dwelling characteristics and the plot sizes. The tiles are useful for estimating the potential for sustainable technologies. The outputs have been validated at against housing survey and footprint data.,,,
S0169260715303321," Background and objective Ankle motion and proprioception in multiple axis movements are crucial for daily activities . However few studies have developed and used a multiple axis system for measuring ankle motion and proprioception . This study was designed to validate a novel ankle haptic interface system that measures the ankle range of motion and joint position sense in multiple plane movements investigating the proprioception deficits during joint position sense tasks for patients with ankle instability . Methods Eleven healthy adults and thirteen patients with ankle instability were recruited in this study . All subjects were asked to perform tests to evaluate the validity of the ankle ROM measurements and underwent tests for validating the joint position sense measurements conducted during multiple axis movements of the ankle joint . Pearson correlation was used for validating the angular position measurements obtained using the developed system the independent t test was used to investigate the differences in joint position sense task performance for people with or without ankle instability . Results The ROM measurements of the device were linearly correlated with the criterion standards . The ankle instability and healthy groups were significantly different in direction absolute and variable errors of plantar flexion dorsiflexion inversion and eversion . Conclusions The results demonstrate that the novel ankle joint motion and position sense measurement system is valid and can be used for measuring the ankle ROM and joint position sense in multiple planes and indicate proprioception deficits for people with ankle instability . 
",A novel measurement system can be used to measure ankle ROM and perception in multiple planes under weight bearing condition. The programmable graphic user interface and data management for this developed system with excellent validity was established. The proprioception measures JPS DE JPS AE and JPS VE for differentiating the ankle perception deficits were demonstrated.,,,
S0262885614000055," Mobile devices namely phones and tablets have long gone smart . Their growing use is both a cause and an effect of their technological advancement . Among the others their increasing ability to store and exchange sensitive information has caused interest in exploiting their vulnerabilities and the opposite need to protect users and their data through secure protocols for access and identification on mobile platforms . Face and iris recognition are especially attractive since they are sufficiently reliable and just require the webcam normally equipping the involved devices . On the contrary the alternative use of fingerprints requires a dedicated sensor . Moreover some kinds of biometrics lend themselves to uses that go beyond security . Ambient intelligence services bound to the recognition of a user as well as social applications such as automatic photo tagging on social networks can especially exploit face recognition . This paper describes FIRME as a biometric application based on a multimodal recognition of face and iris which is designed to be embedded in mobile devices . Both design and implementation of FIRME rely on a modular architecture whose workflow includes separate and replaceable packages . The starting one handles image acquisition . From this point different branches perform detection segmentation feature extraction and matching for face and iris separately . As for face an antispoofing step is also performed after segmentation . Finally results from the two branches are fused . In order to address also security critical applications FIRME can perform continuous reidentification and best sample selection . To further address the possible limited resources of mobile devices all algorithms are optimized to be low demanding and computation light . 
",Face and iris authentication on mobile Fusion strategy driven by response reliability Spoofing detection for mobile face recognition Best sample selection for higher accuracy Optimization for Android,,,
S0169260715303205," Background Dynamic measurements of human muscle fascicle length from sequences of B mode ultrasound images have become increasingly prevalent in biomedical research . Manual digitisation of these images is time consuming and algorithms for automating the process have been developed . Here we present a freely available software implementation of a previously validated algorithm for semi automated tracking of muscle fascicle length in dynamic ultrasound image recordings UltraTrack . Methods UltraTrack implements an affine extension to an optic flow algorithm to track movement of the muscle fascicle end points throughout dynamically recorded sequences of images . The underlying algorithm has been previously described and its reliability tested but here we present the software implementation with features for tracking multiple fascicles in multiple muscles simultaneously correcting temporal drift in measurements manually adjusting tracking results saving and re loading of tracking results and loading a range of file formats . Results Two example runs of the software are presented detailing the tracking of fascicles from several lower limb muscles during a squatting and walking activity . Conclusion We have presented a software implementation of a validated fascicle tracking algorithm and made the source code and standalone versions freely available for download . 
",We have developed a freely available software package for semi automated tracking of muscle fascicles in B mode ultrasound image sequences. Includes features to track multiple fascicles in multiple regions of the image and to correct for measurement drift with time. The software is available for Windows and MacOS as a standalone program or source code.,,,
S0262885613000474," In this paper an efficient method for text independent writer identification using a codebook method is proposed . The method uses the occurrence histogram of the shapes in a codebook to create a feature vector for each specific manuscript . For cursive handwritings a wide variety of different shapes exist in the connected components obtained from the handwriting . Small fragments of connected components are used to avoid complex patterns . Two efficient methods for extracting codes from contours are introduced . One method uses the actual pixel coordinates of contour fragments while the other one uses a linear piece wise approximation using segment angles and lengths . To evaluate the methods writer identification is conducted on two English and three Farsi handwriting databases . Both methods show promising performances with the performance of second method being better than the first one . 
",Two new codebook based methods for writer identification are proposed. The introduced code extraction methods are very efficient. Optimal parameter values for English and Farsi handwritings are determined. The method is compared with the existing methods comprehensively. The proposed method outperforms existing methods for Farsi and English languages.,,,
S0262885614000298," In this paper a statistical approach to static texture description is developed which combines a local pattern coding strategy with a robust global descriptor to achieve highly discriminative power invariance to photometric transformation and strong robustness against geometric changes . Built upon the local binary patterns that are encoded at multiple scales a statistical descriptor called pattern fractal spectrum characterizes the self similar behavior of the local pattern distributions by calculating fractal dimension on each type of pattern . Compared with other fractal based approaches the proposed descriptor is compact highly distinctive and computationally efficient . We applied the descriptor to texture classification . Our method has demonstrated excellent performance in comparison with state of the art approaches on four challenging benchmark datasets . 
",A powerful texture descriptor is developed for texture classification. The descriptor is built via fractal analysis on the local binary patterns. The descriptor enjoys both high discriminative power and robustness. The descriptor is compact and computationally efficient. The descriptor demonstrated excellent performance on four datasets.,,,
S0169260715301383," The Non local means denoising filter has been established as gold standard for image denoising problem in general and particularly in medical imaging due to its efficiency . However its computation time limited its applications in real world application especially in medical imaging . In this paper a distributed version on parallel hybrid architecture is proposed to solve the computation time problem and a new method to compute the filters coefficients is also proposed where we focused on the implementation and the enhancement of filters parameters via taking the neighborhood of the current voxel more accurately into account . In terms of implementation our key contribution consists in reducing the number of shared memory accesses . The different tests of the proposed method were performed on the brain web database for different levels of noise . Performances and the sensitivity were quantified in terms of speedup peak signal to noise ratio execution time the number of floating point operations . The obtained results demonstrate the efficiency of the proposed method . Moreover the implementation is compared to that of other techniques recently published in the literature . 
",The symmetric weight scheme allows reducing the computation time by a factor 2. Our implementation of NL Means reduces computation time by a factor of 510. Self adaptive method is proposed to allow the filter to be more efficient.,,,
S0262885613000590," In this paper we introduce a novel framework for low level image processing and analysis . First we process images with very simple difference based filter functions . Second we fit the 2 parameter Weibull distribution to the filtered output . This maps each image to the 2D Weibull manifold . Third we exploit the information geometry of this manifold and solve low level image processing tasks as minimisation problems on point sets . For a proof of concept example we examine the image autofocusing task . We propose appropriate cost functions together with a simple implicitly constrained manifold optimisation algorithm and show that our framework compares very favourably against common autofocus methods from literature . In particular our approach exhibits the best overall performance in terms of combined speed and accuracy . 
",We model filtered image responses with the Weibull distribution. We represent images as points on the 2D Weibull manifold. We solve image processing problems as optmisation on the manifold. Application to automatic image focusing,,,
S0262885613001637," Methods designed for tracking in dense crowds typically employ prior knowledge to make this difficult problem tractable. In this paper we show that it is possible to handle this problem without any priors by utilizing the visual and contextual information already available in such scenes. We propose a novel tracking method tailored to dense crowds which provides an alternative and complementary approach to methods that require modeling of crowd flow and simultaneously is less likely to fail in the case of dynamic crowd flows and anomalies by minimally relying on previous frames. Our method begins with the automatic identification of prominent individuals from the crowd that are easy to track. Then we use Neighborhood Motion Concurrence to model the behavior of individuals in a dense crowd this predicts the position of an individual based on the motion of its neighbors. When the individual moves with the crowd flow we use Neighborhood Motion Concurrence to predict motion while leveraging five frame instantaneous flow in case of dynamically changing flow and anomalies. All these aspects are then embedded in a framework which imposes hierarchy on the order in which positions of individuals are updated. Experiments on a number of sequences show that the proposed solution can track individuals in dense crowds without requiring any pre processing making it a suitable online tracking algorithm for dense crowds. 
",We propose an online method for tracking dense crowds capable of handling anomalies An alternative to existing methods which require modeling of crowd flows Introduce the notion of prominent individuals for tracking dense crowds Employ a new model Neighborhood Motion Concurrence to predict individual s position Comparison with existing methods using ground truth for eight sequences,,,
S0262885613001303,"There are many machine vision models of the visual saliency mechanism which controls the process of selecting and allocating attention to the most prominent locations in the scene and helps humans interact with the visual environment efficiently Itti and C. Koch 2001 Gao et al. 2000 . It is important to know which models perform the best in mimicking the saliency mechanism of the human visual system. There are several metrics to compare saliency models however results from different metrics vary widely in evaluating models. In this paper a procedure is proposed for evaluating metrics for comparing saliency maps using a database of human fixations on approximately 1000 images. This procedure is then employed to identify the best metric. This best metric is then used to evaluate ten published bottom up saliency models. An optimized level of the blurriness and center bias is found for each visual saliency model. Performance of the models is also analyzed on a dataset of 54 synthetic images. 
",Introduced a set of experiments to judge the biological plausibility of visual saliency models. Introduced a novel method to evaluate saliency map comparison metrics using a database of human fixation maps. Employed the introduced method to identify the best saliency map comparison metric. Examined nine well known models of visual saliency using the best metric to identify the best visual saliency models.,,,
S0262885613001029,"Tracking vehicles using a network of cameras with non overlapping views is a challenging problem of great importance in traffic surveillance. One of the main challenges is accurate vehicle matching across the cameras. Even if the cameras have similar views on vehicles vehicle matching remains a difficult task due to changes of their appearance between observations and inaccurate detections and occlusions which often occur in real scenarios. To be executed on smart cameras the matching has also to be efficient in terms of needed data and computations. To address these challenges we present a low complexity method for vehicle matching robust against appearance changes and inaccuracies in vehicle detection. We efficiently represent vehicle appearances using signature vectors composed of Radon transform like projections of the vehicle images and compare them in a coarse to fine fashion using a simple combination of 1 D correlations. To deal with appearance changes we include multiple observations in each vehicle appearance model. These observations are automatically collected along the vehicle trajectory. The proposed signature vectors can be calculated in low complexity smart cameras by a simple scan line algorithm of the camera software itself and transmitted to the other smart cameras or to the central server. Extensive experiments based on real traffic surveillance videos recorded in a tunnel validate our approach. 
",We propose a novel method for vehicle appearance modeling and matching. A key novelty is automatic collection of good observations for matching. Our method is robust in real world scenarios. Our method outperforms more complex object matching methods. Our method is data and computationally efficient deployable on smart cameras.,,,
S0169260715300973," Background and objective Carpal fusions are useful for treating specific carpal disorders maximizing postoperative wrist motion hand strength reducing pain and instability of the joint . The surgeon selects the appropriate treatment by considering the degree of stability the chronicity of the injury functional demands of the patient and former patient s outcomes as well . However there are not many studies regarding the load distribution provided by the treatment . So the purpose of this study is to analyze the load distribution through the wrist joint with an arthrodesis treatment and compare the results with a normal wrist . Method To this end the rigid body spring model method was used on a three dimensional model of the wrist joint . The cartilage and ligaments were simulated as springs acting under compression and tension respectively while the bones were considered as rigid bodies . To simulate the arthrodesis the fused bones were considered as a single rigid body . Results The changes on the load distribution for each arthrodesis agree with the treatment objective reducing load transmission through a specific articular surface . For example for SLAC SNAC II most of the treatments reduced the load transmitted through the radioscaphoid fossae almost by 8 . However the capitolunate arthrodesis was the treatment that managed to keep the load transmitted through the radiolunate joint closer to normal conditions . Also in treatments where the scaphoid was excised the joint surface between the lunate surface compensates by doubling the transmitted force to the radius . Conclusions The common arthrodesis for treating SLAC SNAC II III reduces in fact the load on the radioscaphoid joint . Alternative treatments that reduce load distribution on the radiocarpal joint should be three corner and capitolunate arthrodesis for treating SLAC SNAC II and for SLAC SNAC III four corners with scaphoid excision . On Kienbock s disease . Scaphocapitate arthrodesis is more effective on reducing the load transmission through the radiolunate and ulnolunate joints . All arthrodesis treatment should consider changes on the load transmission and also bones fusion rates and pain reduction on patient s outcomes . 
",We analyze the load distribution through the wrist joint with several arthrodesis treatments. We used the rigid body spring model method on a 3D model of the wrist joint. We found the load distributions on each carpal articular surface of radius for each treatment. Obtained results allowed comparing the different treatments and their efficacy under static conditions.,,,
S0198971515300090," Urbanisation environmental risks and resource scarcity are but three of many challenges that cities must address if they are to become more sustainable . However the policies and spatial development strategies implemented to achieve individual sustainability objectives frequently interact and conflict presenting decision makers a multi objective spatial optimisation problem . This work presents a developed spatial optimisation framework which optimises the location of future residential development against several sustainability objectives . The framework is applied to a case study over Middlesbrough in the North East of the United Kingdom . In this context the framework optimises five sustainability objectives from our case study site minimising risk from heat waves minimising the risk from flood events minimising travel costs to minimise transport emissions minimising the expansion of urban sprawl and preventing development on green spaces . A series of optimised spatial configurations of future development strategies are presented . The results compare strategies that are optimal against individual pairs and multiple sustainability objectives such that each of these optimal strategies out performs all other development strategies in at least one sustainability objective . Moreover the resulting spatial strategies significantly outperform the current local authority strategy for all objectives with for example a relative improvement of up to 68 in the performance of distance to CBD . Based on these results it suggests that spatial optimisation can provide a powerful decision support tool to help planners to identify spatial development strategies that satisfy multiple sustainability objectives . 
",Present the use of computational spatial optimisation framework to enable planners to identify optimal spatial plans in the presence of competing sustainability objectives Application over case study provides a set of best trade off spatial plans Many optimised spatial plans outperform local authorities development plan Rich set of diagnostic information provides an evidence basis to assist planners to achieve more sustainable patterns of development,,,
S0169260715303035," The peristaltic flow of a copper oxide water fluid investigates the effects of heat generation and magnetic field in permeable tube is studied . The mathematical formulation is presented the resulting equations are solved exactly . The obtained expressions for pressure gradient pressure rise temperature velocity profile are described through graphs for various pertinent parameters . It is found that pressure gradient is reduce with enhancement of particle concentration and velocity profile is upturn beside it is observed that temperature increases as more volume fraction of copper oxide . The streamlines are drawn for some physical quantities to discuss the trapping phenomenon . 
",The peristaltic flow of a copper oxide water fluid investigate the effects of heat generation and magnetic field. The mathematical formulation is presented the resulting equations are solved exactly. The obtained expressions for pressure gradient pressure rise temperature velocity profile are described through graphs for various pertinent parameters. The streamlines are drawn for some physical quantities to discuss the trapping phenomenon.,,,
S0169260715302236," Background and Objective The HIV AIDS related issue has given rise to a priority concern in which potential new therapies are increasingly highlighted to lessen the negative impact of highly active anti retroviral therapy in the healthcare industry . With the motivation of medical applications this study focuses on the main advanced feature selection techniques and classification approaches that reflect a new architecture and a trial to build a hybrid model for interested parties . Methods This study first uses an integrated linear nonlinear feature selection technique to identify the determinants influencing HAART medication and utilizes organizations of different condition attributes to generate a hybrid model based on a rough set classifier to study evolving HIV AIDS research in order to improve classification performance . Results The proposed model makes use of a real data set from Taiwan s specialist medical center . The experimental results show that the proposed model yields a satisfactory result that is superior to the listed methods and the core condition attributes PVL CD4 Code Age Year PLT and Sex were identified in the HIV AIDS data set . In addition the decision rule set created can be referenced as a knowledge based healthcare service system as the best of evidence based practices in the workflow of current clinical diagnosis . Conclusions This study highlights the importance of these key factors and provides the rationale that the proposed model is an effective alternative to analyzing sustained HAART medication in follow up studies of HIV AIDS treatment in practice . 
",To develop an integrated linear nonlinear feature selection technique to identify the determinants of sustained HAART. To generate a hybrid model based on rough set classifiers to verify its performance. To create a relevant decision rule set of an LEM2 algorithm for specialist physicians as a diagnosis reference. To effectively offer the study findings and results from the given data set to relevant medical institutions and patients.,,,
S0262885613001807," Facial expression recognition systems must ultimately work on real data in uncontrolled environments although most research studies have been conducted on lab based data with posed or evoked facial expressions obtained in pre set laboratory environments . It is very difficult to obtain data in real world situations because privacy laws prevent unauthorized capture and use of video from events such as funerals birthday parties marriages etc . It is a challenge to acquire such data on a scale large enough for benchmarking algorithms . Although video obtained from TV or movies or postings on the World Wide Web may also contain acted emotions and facial expressions they may be more realistic than lab based data currently used by most researchers . Or is it One way of testing this is to compare feature distributions and FER performance . This paper describes a database that has been collected from television broadcasts and the World Wide Web containing a range of environmental and facial variations expected in real conditions and uses it to answer this question . A fully automatic system that uses a fusion based approach for FER on such data is introduced for performance evaluation . Performance improvements arising from the fusion of point based texture and geometry features and the robustness to image scale variations are experimentally evaluated on this image and video dataset . Differences in FER performance between lab based and realistic data between different feature sets and between different train test data splits are investigated . 
",A new realistic facial expression recognition FER database Compared feature and performance differences between lab based and realistic data Examined factors that affect FER performance regarding data feature and training,,,
S0262885613000644,"In this paper we tackle the problem of gait recognition based on the model free approach. Numerous methods exist they all lead to high dimensional feature spaces. To address the problem of high dimensional feature space we propose the use of the Random Forest algorithm to rank features importance. In order to efficiently search throughout subspaces we apply a backward feature elimination search strategy. Our first experiments are carried out on unknown covariate conditions. Our first results suggest that the selected features contribute to increase the CCR of different existing classification methods. Secondary experiments are performed on unknown covariate conditions and viewpoints. Inspired by the location of our first experiments features we proposed a simple mask. Experimental results demonstrate that the proposed mask gives satisfactory results for all angles of the probe and consequently is not view specific. We also show that our mask performs well when an uncooperative experimental setup is considered as compared to the state of the art methods. As a consequence we propose a panoramic gait recognition framework on unknown covariate conditions. Our results suggest that panoramic gait recognition can be performed under unknown covariate conditions. Our approach can greatly reduce the complexity of the classification problem while achieving fair correct classification rates when gait is captured with unknown conditions. 
",A feature selection framework is proposed to achieve high performance model free gait recognition. The feature selection mechanism relies on the Random Forest algorithm. Regions selected are more robust to covariates while reducing the computational cost. Panoramic gait recognition is achieved under covariate conditions.,,,
S0262885613001431,"A large number of methods have been published that aim to evaluate various components of multi view geometry systems. Most of these have focused on the feature extraction description and matching stages the visual front end since geometry computation can be evaluated through simulation. Many data sets are constrained to small scale scenes or planar scenes that are not challenging to new algorithms or require special equipment. This paper presents a method for automatically generating geometry ground truth and challenging test cases from high spatio temporal resolution video. The objective of the system is to enable data collection at any physical scale in any location and in various parts of the electromagnetic spectrum. The data generation process consists of collecting high resolution video computing accurate sparse 3D reconstruction video frame culling and down sampling and test case selection. The evaluation process consists of applying a test 2 view geometry method to every test case and comparing the results to the ground truth. This system facilitates the evaluation of the whole geometry computation process or any part thereof against data compatible with a realistic application. A collection of example data sets and evaluations is included to demonstrate the range of applications of the proposed system. 
",Easy data collection in any 3D feature rich scene with any calibrated camera Ground truth acquired automatically by high resolution structure from motion Rigorous 2 view geometry evaluation with many reduced resolution test samples More elaborate rigid geometry tests are trivial to devise given 3D ground truth.,,,
S0262885613001510,"In this paper we propose a visual tracking algorithm by incorporating the appearance information gathered from two collaborative feature sets and exploiting its geometric structures. A structured visual dictionary SVD can be learned from both appearance and geometric structure thereby enhancing its discriminative strength between the foreground object and the background. Experimental results show that the proposed tracking algorithm using SVD SVDTrack performs favorably against the state of the art methods. 
",A tracking algorithm which exploits both appearance and geometric information Two complementary features are adopted in building the appearance dictionary. A shape context approach to capture the stable geometric patterns of keypoints The proposed tracking algorithm performs well in challenging conditions.,,,
S0262885613001121," We present a novel method for on line joint object tracking and segmentation in a monocular video captured by a possibly moving camera . Our goal is to integrate tracking and fine segmentation of a single previously unseen potentially non rigid object of unconstrained appearance given its segmentation in the first frame of an image sequence as the only prior information . To this end we tightly couple an existing kernel based object tracking method with Random Walker based image segmentation . Bayesian inference mediates between tracking and segmentation enabling effective data fusion of pixel wise spatial and color visual cues . The fine segmentation of an object at a certain frame provides tracking with reliable initialization for the next frame closing the loop between the two building blocks of the proposed framework . The effectiveness of the proposed methodology is evaluated experimentally by comparing it to a large collection of state of the art tracking and video based object segmentation methods on the basis of a data set consisting of several challenging image sequences for which ground truth data is available . 
",A novel method for joint tracking and fine object segmentation in videos Efficient integration of EM based tracking and Random Walker based image segmentation No strong constraints are imposed on the target appearance or the camera motion. Experimental evaluation against a large collection of state of the art methods Explicit and efficient fine object segmentation facilitates drift free tracking.,,,
S0169260715301358," Background and objectives Gene splicing is a vital source of protein diversity . Perfectly eradication of introns and joining exons is the prominent task in eukaryotic gene expression as exons are usually interrupted by introns . Identification of splicing sites through experimental techniques is complicated and time consuming task . With the avalanche of genome sequences generated in the post genomic age it remains a complicated and challenging task to develop an automatic robust and reliable computational method for fast and effective identification of splicing sites . Methods In this study a hybrid model iSS Hyb mRMR is proposed for quickly and accurately identification of splicing sites . Two sample representation methods namely pseudo trinucleotide composition and pseudo tetranucleotide composition were used to extract numerical descriptors from DNA sequences . Hybrid model was developed by concatenating PseTNC and PseTetraNC . In order to select high discriminative features minimum redundancy maximum relevance algorithm was applied on the hybrid feature space . The performance of these feature representation methods was tested using various classification algorithms including K nearest neighbor probabilistic neural network general regression neural network and fitting network . Jackknife test was used for evaluation of its performance on two benchmark datasets S 1 and S 2 respectively . Results The predictor proposed in the current study achieved an accuracy of 93.26 sensitivity of 88.77 and specificity of 97.78 for S 1 and the accuracy of 94.12 sensitivity of 87.14 and specificity of 98.64 for S 2 respectively . Conclusion It is observed that the performance of proposed model is higher than the existing methods in the literature so for and will be fruitful in the mechanism of RNA splicing and other research academia . 
",iSS Hyb mRMR model is proposed for identification of splicing sites. Trinucleotide and tetranucleotide composition are used as feature extraction schemes. Hybrid space is formed by using TNC and TetraNC spaces. Various classification algorithms are analyzed. mRMR is utilized to reduce feature space.,,,
S0262885614001012," Facial expression is central to human experience . Its efficiency and valid measurement are challenges that automated facial image analysis seeks to address . Most publically available databases are limited to 2D static images or video of posed facial behavior . Because posed and un posed facial expressions differ along several dimensions including complexity and timing well annotated video of un posed facial behavior is needed . Moreover because the face is a three dimensional deformable object 2D video may be insufficient and therefore 3D video archives are required . We present a newly developed 3D video database of spontaneous facial expressions in a diverse group of young adults . Well validated emotion inductions were used to elicit expressions of emotion and paralinguistic communication . Frame level ground truth for facial actions was obtained using the Facial Action Coding System . Facial features were tracked in both 2D and 3D domains . To the best of our knowledge this new database is the first of its kind for the public . The work promotes the exploration of 3D spatiotemporal features in subtle facial expression better understanding of the relation between pose and motion dynamics in facial action units and deeper understanding of naturally occurring facial action . 
",We present the first 3D dynamic spontaneous facial expression database. Meta data include FACS coding head pose data and 2D 3D landmarks. We present an effective emotion elicitation protocol using eight tasks. The database is analyzed by self report observers rating and AU annotation. The database is validated through expression AU recognition and pain analysis.,,,
S0169260715301693," Background and Objectives Light sharing PET detector configuration coupled with thick light guide and Geiger mode avalanche photodiode with large area microcells was proposed to overcome the energy non linearity problem and to obtain high light collection efficiency . Methods A Monte Carlo simulation was conducted for the three types of LSO block 4 4 array of 3 3 20 mm3 discrete crystals 6 6 array of 2 2 20 mm3 discrete crystals and 12 12 array of 1 1 20 mm3 discrete crystals to investigate the scintillation light distribution after conversion of the rays in LSO . The incident photons were read out by three types of 4 4 array photosensors which were PSPMT of 25 quantum efficiency GAPD1 with 50 50 m2 microcells of 30 photon detection efficiency and GAPD2 with 100 100 m2 of 45 PDE . The number of counted photons in each photosensor was analytically calculated . The LCE linearity and flood histogram were examined for each PET detector module having 99 different configurations as a function of light guide thickness ranging from 0 to 10 mm . Results The performance of PET detector modules based on GAPDs was considerably improved by using the thick light guide . The LCE was increased from 24 to 30 and from 14 to 41 and the linearity was also improved from 0.97 to 0.99 and from 0.75 to 0.99 for GAPD1 and GAPD2 respectively . As expected the performance of PSPMT based detector did not change . The flood histogram of 12 12 array PET detector modules using 3 mm light guide coupled with GAPDs was obtained by simulation and all crystals of 1 1 20 mm3 size were clearly identified . PET detector module coupled with thick light guide and GAPD array with large area microcells was proposed to obtain high QE and high spatial resolution and its feasibility was verified . Conclusions This study demonstrated that the overall PET performance of the proposed design was considerably improved and this approach will provide opportunities to develop GAPD based PET detector with a high LCE . 
",This study proposed and examined a new approach employing the light sharing PET detector configuration with thick light guide and GAPD array having large area microcells for high effective quantum efficiency. The number of counted photons was considerably increased and the LCE of 40 could be obtained with the detector configuration. The flood histogram showed good separation and wide margins between the spots of the scintillators. Moreover a considerable improvement in the energy linearity performance was observed and severe saturation effects could be clearly avoided. This study demonstrated that GAPDs with large area microcells which have not been actively studied as PET photosensors because of their non linearity properties could be utilized for PET applications and even high photon flux regime.,,,
S0262885613001108,"Range imaging sensors such as Kinect and time of flight cameras can produce aligned depth and color images in real time. However the depth maps captured by such sensors contain numerous invalid regions and suffer from heavy noise. These defects more or less influence the use of depth information in practical applications. In order to enhance the depth maps this paper proposes a new inpainting approach based on the fast marching method FMM . We extend the inpainting model and the propagation strategy of FMM to incorporate color information for depth inpainting. An edge preserving guided filter is further applied for noise reduction. To validate our algorithm we perform experiments on both Kinect data and Middlebury dataset which respectively provide qualitative and quantitative results. Meanwhile we also compare it to the original FMM and other two state of the art depth enhancement methods. Experimental results show that our method performs better than the local methods in terms of both visual and metric qualities and it achieves visually comparable results to the time consuming global method. 
",We propose a new depth enhancement method for RGB D sensors. It extends the fast marching method to incorporate color and depth information. It outperforms state of the art local methods in terms of visual and metric qualities. It achieves visually comparable results to time consuming global methods. It provides better inputs to the applications based on RGB D sensors.,,,
S0169260716000043," Motor unit action potential which consists of individual muscle fiber action potentials represents the electrical activity of the motor unit . The values of the MUAP features are changed by denervation and reinnervation in neurogenic involvement as well as muscle fiber loss with increased diameter variability in myopathic diseases . The present study is designed to investigate how increased muscle fiber diameter variability affects MUAP parameters in simulated motor units . In order to detect this variation simulated MUAPs were calculated both at the innervation zone where the MFAPs are more synchronized and near the tendon where they show increased temporal dispersion . Reinnervation in neurogenic state increases MUAP amplitude for the recordings at both the innervation zone and near the tendon . However MUAP duration and the number of peaks significantly increased in a case of myopathy for recordings near the tendon . Furthermore of the new features number of peaks spike duration was found as the strongest indicator of MFAP dispersion in myopathy . MUAPs were also recorded from healthy participants in order to investigate the biological counterpart of the simulation data . MUAPs which were recorded near to tendon revealed significantly prolonged duration and decreased amplitude . Although the number of peaks was increased by moving the needle near to tendon this was not significant . 
",We simulated five motor units for normal neurogenic and myopathic cases. We investigated the effect of recording sites on electromyography signals. Myopathic cases demonstrated the most prolonged phase duration near the tendon. Highest number of peaks are observed near the tendon in myopathic conditions. A new feature referred as number of peaks spike duration was defined.,,,
S0169260715002953," Current telehealth services are dominated by conventional 2D video conferencing systems which are limited in their capabilities in providing a satisfactory communication experience due to the lack of realism . The immersiveness provided by 3D technologies has the potential to promote telehealth services to a wider range of applications . However conventional stereoscopic 3D technologies are deficient in many aspects including low resolution and the requirement for complicated multi camera setup and calibration and special glasses . The advent of light field photography enables us to record light rays in a single shot and provide glasses free 3D display with continuous motion parallax in a wide viewing zone which is ideally suited for 3D telehealth applications . As far as our literature review suggests there have been no reports of 3D telemedicine systems using LF technology . In this paper we propose a cross platform solution for a LF based 3D telemedicine system . Firstly a novel system architecture based on LF technology is established which is able to capture the LF of a patient and provide an immersive 3D display at the doctor site . For 3D modeling we further propose an algorithm which is able to convert the captured LF to a 3D model with a high level of detail . For the software implementation on different platforms a cross platform solution is proposed . Demo applications have been developed for 2D 3D video conferencing 3D model display and edit blood pressure and heart rate monitoring and patient data viewing functions . The demo software can be extended to multi discipline telehealth applications such as tele dentistry tele wound and tele psychiatry . The proposed 3D telemedicine solution has the potential to revolutionize next generation telemedicine technologies by providing a high quality immersive tele consultation experience . 
",We propose a novel framework for a world s first LF based 3D telemedicine system. We develop an algorithm to convert the LF into 3D models with high levels of details. We design a cross platform solution for different systems and platforms. We develop a demo platform with multidiscipline 3D telemedicine applications.,,,
S0169260715301735," An acetabular cup with larger abduction angles is able to affect the normal function of the cup seriously that may cause early failure of the total hip replacement . Complexity of the finite element simulation in the wear analysis of the THR is usually concerned with the contact status the computational effort and the possible divergence of results which become more difficult on THRs with larger cup abduction angles . In the study we propose a FE approach with contact transformation that offers less computational effort . Related procedures such as Lagrangian Multiplier partitioned matrix inversion detection of contact forces continuity of contact surface nodal area estimation etc . are explained in this report . Through the transformed methodology the computer round off error is tremendously reduced and the embedded repetitive procedure can be processed precisely and quickly . Here wear behaviors of THR with various abduction angles are investigated . The most commonly used combination i.e . metal on polyethylene is adopted in the current study where a cobalt chromium femoral head is paired with an Ultra High Molecular Weight Polyethylene cup . In all illustrations wear coefficients are estimated by self averaging strategy with available experimental datum reported elsewhere . The results reveal that the THR with larger abduction angles may produce deeper depth of wear but the volume of wear presents an opposite tendency these results are comparable with clinical and experimental reports . The current approach can be widely applied easily to fields such as the study of the wear behaviors on ante version impingement and time dependent behaviors of prostheses etc . 
",Wear behaviors of the total hip replacement THR with various abduction angles are investigated. The current finite element approach can improve the computational efforts without significant loss of data precision. The THR with larger abduction angles may produce deeper depth of wear but the volume of wear presents an opposite tendency.,,,
S0262885613000656," In the spirit of recent work on contextual recognition and estimation we present a method for estimating the pose of human hands employing information about the shape of the object in the hand . Despite the fact that most applications of human hand tracking involve grasping and manipulation of objects the majority of methods in the literature assume a free hand isolated from the surrounding environment . Occlusion of the hand from grasped objects does in fact often pose a severe challenge to the estimation of hand pose . In the presented method object occlusion is not only compensated for it contributes to the pose estimation in a contextual fashion this without an explicit model of object shape . Our hand tracking method is non parametric performing a nearest neighbor search in a large database of hand poses with and without grasped objects . The system that operates in real time is robust to self occlusions object occlusions and segmentation errors and provides full hand pose reconstruction from monocular video . Temporal consistency in hand pose is taken into account without explicitly tracking the hand in the high dim pose space . Experiments show the non parametric method to outperform other state of the art regression methods while operating at a significantly lower computational cost than comparable model based hand tracking methods . 
",We developed a system that estimates in real time the articulated pose of the hand. Our approach is discriminative with low computational load and fast error recovery. Our system is robust to occlusions implicitly extracting information from them. The system is thoroughly evaluated with quantitative and qualitative experiments.,,,
S0262885613001479," In this paper we present a comparative study of two approaches for road traffic density estimation . The first approach uses the microscopic parameters which are extracted using both motion detection and tracking methods from a video sequence and the second approach uses the macroscopic parameters which are directly estimated by analyzing the global motion in the video scene . The extracted parameters are applied to three classifiers the K Nearest Neighbor classifier the LVQ classifier and the SVM classifier in order to classify the road traffic in three categories light medium and heavy . The methods are compared based on their robustness to the classification of different road traffic states . The goal of this study is to propose an algorithm for road traffic density estimation with a high precision . 
",We propose an algorithm for road traffic congestion estimation from video scenes. We compare between macroscopic and microscopic parameters in terms of accuracy. The method proposed is accurate and it is computationally inexpensive. It does not require segmentation or tracking of vehicles. It is robust towards illumination changes.,,,
S0262885613001091," Shape from focus is a passive technique widely used in image processing for obtaining depth maps . This technique is attractive since it only requires a single monocular camera with focus control thus avoiding correspondence problems typically found in stereo as well as more expensive capturing devices . However one of its main drawbacks is its poor performance when the change in the focus level is difficult to detect . Most research in SFF has focused on improving the accuracy of the depth estimation . Less attention has been paid to the problem of providing quality measures in order to predict the performance of SFF without prior knowledge of the recovered scene . This paper proposes a reliability measure aimed at assessing the quality of the depth map obtained using SFF . The proposed reliability measure analyzes the shape of the focus measure function and estimates the likelihood of obtaining an accurate depth estimation without any previous knowledge of the recovered scene . The proposed R measure is then applied for determining the image regions where SFF will not perform correctly in order to discard them . Experiments with both synthetic and real scenes are presented . 
",The performance of focus measure depends on the optics and imaging conditions. The concept of reliability in focus measure is introduced. A method for computing the reliability of shape from focus is presented. The proposed reliability integrates efficiently to shape from focus. The proposed method is experimentally effective.,,,
S0169260715301656," Background and objectives Mathematical models are suitable to simulate complex biological processes by a set of non linear differential equations . These simulation models can be used as an e learning tool in medical education . However in many cases these mathematical systems have to be treated numerically which is computationally intensive . The aim of the study was to develop a system for numerical simulation to be used in an online e learning environment . Methods In the software system the simulation is located on the server as a CGI application . The user selects the boundary conditions for the simulation on the browser . With these parameters the simulation on the server is started and the simulation result is re transferred to the browser . Results With this system two examples of e learning units were realized . The first one uses a multi compartment model of the glucose insulin control loop for the simulation of the plasma glucose level after a simulated meal or during diabetes . The second one simulates the ion transport leading to the resting and action potential in nerves . The student can vary parameters systematically to explore the biological behavior of the system . Conclusions The described system is able to simulate complex biological processes and offers the possibility to use these models in an online e learning environment . As far as the underlying principles can be described mathematically this type of system can be applied to a broad spectrum of biomedical or natural scientific topics . 
",The system uses complex numerical simulations in an online e learning environment. Complex calculations are performed on the Web server as a CGI application. The user enters simulation parameters on the browser which are sent to the server. The simulation result is re transferred as a graphical representation. Two examples were realized glucose insulin homeostasis and basic neurophysiology.,,,
S0262885613000929,"Motion segmentation refers to the problem of separating the objects in a video sequence according to their motion. It is a fundamental problem of computer vision since various systems focusing on the analysis of dynamic scenes include motion segmentation algorithms. In this paper we present a novel approach where a video shot is temporally divided in successive and overlapping windows and motion segmentation is performed on each window respectively. This attribute renders the algorithm suitable even for long video sequences. In the last stage of the algorithm the segmentation results for every window are aggregated into a final segmentation. The presented algorithm can handle effectively asynchronous trajectories on each window even when they have no temporal intersection. The evaluation of the proposed algorithm on the Berkeley motion segmentation benchmark demonstrates its scalability and accuracy compared to the state of the art. 
",We divide a video sequence on overlapping subsequences. On each subsequence we perform over segmentation handling efficiently asynchronous trajectories. The number of moving objects is automatically estimated. The segmentation results are aggregated into a final segmentation. Our method is tested on the Berkeley motion segmentation benchmark.,,,
S0169260715300213," In this paper the unsteady pulsatile magneto hydrodynamic blood flows through porous arteries concerning the influence of externally imposed periodic body acceleration and a periodic pressure gradient are numerically simulated . Blood is taken into account as the third grade non Newtonian fluid . Besides the numerical solution for small Womersley parameter the analytical perturbation method is used to solve the nonlinear governing equations . Consequently analytical expressions for the velocity profile wall shear stress and blood flow rate are obtained . Excellent agreement between the analytical and numerical predictions is evident . Also the effects of body acceleration magnetic field third grade non Newtonian parameter pressure gradient and porosity on the flow behaviors are examined . Some important conclusions are that when the Womersley parameter is low viscous forces tend to dominate the flow velocity profiles are parabolic in shape and the center line velocity oscillates in phase with the driving pressure gradient . In addition by increasing the pressure gradient the mean value of the velocity profile increases and the amplitude of the velocity remains constant . Also when non Newtonian effect increases the amplitude of the velocity profile . 
",The pulsatile MHD third grade blood flow through arteries is analytically studied. The influence of externally imposed periodic body acceleration is considered. Analytical formulas for velocity profile wall shear stress and flow rate are given.,,,
S0262885614000171," When estimating human gaze directions from captured eye appearances most existing methods assume a fixed head pose because head motion changes eye appearance greatly and makes the estimation inaccurate. To handle this difficult problem in this paper we propose a novel method that performs accurate gaze estimation without restricting the user s head motion. The key idea is to decompose the original free head motion problem into subproblems including an initial fixed head pose problem and subsequent compensations to correct the initial estimation biases. For the initial estimation automatic image rectification and joint alignment with gaze estimation are introduced. Then compensations are done by either learning based regression or geometric based calculation. The merit of using such a compensation strategy is that the training requirement to allow head motion is not significantly increased only capturing a 5 s video clip is required. Experiments are conducted and the results show that our method achieves an average accuracy of around 3 by using only a single camera. 
",Appearance based gaze estimation with head motion is decomposed into subproblems. Subproblems are solved by compensating for two types of estimation biases. The compensation method only requires a 5 second video for training. Eye images for different head poses are aligned via rectification and optimization. We achieve a gaze estimation accuracy of 3 with free head motion.,,,
S0198971514001112," Models that simulate land use patterns often use either inductive data driven approaches or deductive theory based methods to describe the relative strength of the social economic and biophysical forces that drive the various sectors in the land system . An integrated framework is proposed here that incorporates both approaches based on a unified assessment for local land suitability following a monetary utility based logic . The framework is illustrated with a hedonic pricing analysis of urban land values and a net present value assessment for agricultural production system in combination with statistics based assessments of land suitability for other sectors . The results show that limited difference exists between the most commonly applied inductive approaches that use either multinomial or binomial logistic regression specifications of suitability . Land use simulations following the binomial regression based suitability values that were rescaled to bid prices perform better for all individual land use types . Performance improves even further when a land value based description of urban bid prices is added to this approach . Interestingly enough the better fitting description of suitability for urban areas also improves the ability of the model to simulate correct locations for business estates and greenhouses . The simulation alternatives that consider the net present values for agricultural types of land use show the relevance of this approach for understanding the spatial distribution of these types of land use . The combined use of urban land values and net present values for agricultural land use in defining land suitability performs best in our validation exercise . The proposed methodology can also be used to incorporate information from other research frameworks that describe the utility of land for different types of use . 
",Novel multi sector framework for integrated local scale land use modeling. We apply deductive theory based and inductive data driven methods to describe land use change. We incorporate hedonic pricing net present value and statistics based methods. Utility based approaches outperform common statistics based approaches in our Dutch case study. Output from other research methods can be incorporated to describe utility of land for other uses.,,,
S0262885613001480," Discriminative human pose estimation is the problem of inferring the 3D articulated pose of a human directly from an image feature . This is a challenging problem due to the highly non linear and multi modal mapping from the image feature space to the pose space . To address this problem we propose a model employing a mixture of Gaussian processes where each Gaussian process models a local region of the pose space . By employing the models in this way we are able to overcome the limitations of Gaussian processes applied to human pose estimation their O time complexity and their uni modal predictive distribution . Our model is able to give a multi modal predictive distribution where each mode is represented by a different Gaussian process prediction . A logistic regression model is used to give a prior over each expert prediction in a similar fashion to previous mixture of expert models . We show that this technique outperforms existing state of the art regression techniques on human pose estimation data sets for ballet dancing sign language and the HumanEva data set . 
",Novel algorithm for large scale human pose estimation problems. Uses multiple Gaussian processes in a mixture of expert framework. Allows the accurate regression of Gaussian processes to be scaled to large data. Algorithm gives state of the art performance on 3 pose estimation data sets.,,,
S0262885613001066,"Cheap ubiquitous high resolution digital cameras have led to opportunities that demand camera based text understanding such as wearable computing or assistive technology. Perspective distortion is one of the main challenges for text recognition in camera captured images since the camera may often not have a fronto parallel view of the text. We present a method for perspective recovery of text in natural scenes where text can appear as isolated words short sentences or small paragraphs as found on posters billboards shop and street signs etc. . It relies on the geometry of the characters themselves to estimate a rectifying homography for every line of text irrespective of the view of the text over a large range of orientations. The horizontal perspective foreshortening is corrected by fitting two lines to the top and bottom of the text while the vertical perspective foreshortening and shearing are estimated by performing a linear regression on the shear variation of the individual characters within the text line. The proposed method is efficient and fast. We present comparative results with improved recognition accuracy against the current state of the art. 
",We present a method for perspective recovery of text in natural scenes. It relies on the characters geometry to estimate a rectifying homography. The proposed method is efficient and fast. Comparative results show improved recognition accuracy against the state of the art.,,,
S0262885614000626," Avoiding the use of complicated pre processing steps such as accurate face and body part segmentation or image normalization this paper proposes a novel face person image representation which can properly handle background and illumination variations . Denoted as gBiCov this representation relies on the combination of Biologically Inspired Features and Covariance descriptors . More precisely gBiCov is obtained by computing and encoding the difference between BIF features at different scales . The distance between two persons can then be efficiently measured by computing the Euclidean distance of their signatures avoiding some time consuming operations in Riemannian manifold required by the use of Covariance descriptors . In addition the recently proposed KISSME framework is adopted to learn a metric adapted to the representation . To show the effectiveness of gBiCov experiments are conducted on three person re identification tasks and one face verification task on which competitive results are obtained . As an example the matching rate at rank 1 on the VIPeR dataset is of 31.11 improving the best previously published result by more than 10 . 
",This paper proposes a novel person face image representation. The representation avoids the use of body segmentation or image normalization. The representation relies on the combination of BIF and Covariance descriptor. The representation can handle background and illumination variations. The matching rate at rank 1 on VIPeR is 31.11 and the accuracy on LFW is 84.48 .,,,
S0262885613001467," Many recent image retrieval methods are based on the bag of words model with some additional spatial consistency checking . This paper proposes a more accurate similarity measurement that takes into account spatial layout of visual words in an offline manner . The similarity measurement is embedded in the standard pipeline of the BoW model and improves two features of the model i latent visual words are added to a query based on spatial co occurrence to improve query recall and ii weights of reliable visual words are increased to improve the precision . The combination of these methods leads to a more accurate measurement of image similarity . This is similar in concept to the combination of query expansion and spatial verification but does not require query time processing which is too expensive to apply to full list of ranked results . Experimental results demonstrate the effectiveness of our proposed method on three public datasets . 
",We propose a more accurate similarity measurement for object retrieval. Our method improves two features of the BoW model. Spatial expansion can incorporate more latent visual words into a query. Visual word re weighting can increase weights of reliable visual words. The combination of them can improve both precision and recall.,,,
S0169260715302753," Background and objective Percutaneous coronary interventional procedures need advance planning prior to stenting or an endarterectomy . Cardiologists use intravascular ultrasound for screening risk assessment and stratification of coronary artery disease . We hypothesize that plaque components are vulnerable to rupture due to plaque progression . Currently there are no standard grayscale IVUS tools for risk assessment of plaque rupture . This paper presents a novel strategy for risk stratification based on plaque morphology embedded with principal component analysis for plaque feature dimensionality reduction and dominant feature selection technique . The risk assessment utilizes 56 grayscale coronary features in a machine learning framework while linking information from carotid and coronary plaque burdens due to their common genetic makeup . Method This system consists of a machine learning paradigm which uses a support vector machine combined with PCA for optimal and dominant coronary artery morphological feature extraction . Carotid artery proven intima media thickness biomarker is adapted as a gold standard during the training phase of the machine learning system . For the performance evaluation K fold cross validation protocol is adapted with 20 trials per fold . For choosing the dominant features out of the 56 grayscale features a polling strategy of PCA is adapted where the original value of the features is unaltered . Different protocols are designed for establishing the stability and reliability criteria of the coronary risk assessment system . Results Using the PCA based machine learning paradigm and cross validation protocol a classification accuracy of 98.43 with K 10 folds using an SVM radial basis function kernel was achieved . A reliability index of 97.32 and machine learning stability criteria of 5 were met for the cRAS . Conclusions This is the first Computer aided design system of its kind that is able to demonstrate the ability of coronary risk assessment and stratification while demonstrating a successful design of the machine learning system based on our assumptions . 
",Coronary artery disease risk assessment in intravascular ultrasound. A link between carotid and coronary grayscale plaque morphology. Principal component analysis PCA for dominant feature selection. Classification accuracy of 98.43 and reliability index of 97.32 .,,,
S0262885614000924," We introduce a robust framework for learning and fusing of orientation appearance models based on both texture and depth information for rigid object tracking . Our framework fuses data obtained from a standard visual camera and dense depth maps obtained by low cost consumer depth cameras such as the Kinect . To combine these two completely different modalities we propose to use features that do not depend on the data representation angles . More specifically our framework combines image gradient orientations as extracted from intensity images with the directions of surface normals computed from dense depth fields . We propose to capture the correlations between the obtained orientation appearance models using a fusion approach motivated by the original Active Appearance Models . To incorporate these features in a learning framework we use a robust kernel based on the Euler representation of angles which does not require off line training and can be efficiently implemented online . The robustness of learning from orientation appearance models is presented both theoretically and experimentally in this work . This kernel enables us to cope with gross measurement errors missing data as well as other typical problems such as illumination changes and occlusions . By combining the proposed models with a particle filter the proposed framework was used for performing 2D plus 3D rigid object tracking achieving robust performance in very difficult tracking scenarios including extreme pose variations . 
",Robust learning and fusing of orientation appearance models. Combination of image gradient orientations with the directions of surface normals. Use of a robust kernel based on the Euler representation of angles. Performing 2D plus 3D rigid object tracking achieving robust performance.,,,
S0169260715300390," Objectives The present work has the goal of developing a secure medical imaging information system based on a combined steganography and cryptography technique . It attempts to securely embed patient s confidential information into his her medical images . Methods The proposed information security scheme conceals coded Electronic Patient Records into medical images in order to protect the EPRs confidentiality without affecting the image quality and particularly the Region of Interest which is essential for diagnosis . The secret EPR data is converted into ciphertext using private symmetric encryption method . Since the Human Visual System is less sensitive to alterations in sharp regions compared to uniform regions a simple edge detection method has been introduced to identify and embed in edge pixels which will lead to an improved stego image quality . In order to increase the embedding capacity the algorithm embeds variable number of bits in edge pixels based on the strength of edges . Moreover to increase the efficiency two message coding mechanisms have been utilized to enhance the 1 steganography . The first one which is based on Hamming code is simple and fast while the other which is known as the Syndrome Trellis Code is more sophisticated as it attempts to find a stego image that is close to the cover image through minimizing the embedding impact . The proposed steganography algorithm embeds the secret data bits into the Region of Non Interest where due to its importance the ROI is preserved from modifications . Results The experimental results demonstrate that the proposed method can embed large amount of secret data without leaving a noticeable distortion in the output image . The effectiveness of the proposed algorithm is also proven using one of the efficient steganalysis techniques . Conclusion The proposed medical imaging information system proved to be capable of concealing EPR data and producing imperceptible stego images with minimal embedding distortions compared to other existing methods . In order to refrain from introducing any modifications to the ROI the proposed system only utilizes the Region of Non Interest in embedding the EPR data . 
",A method for embedding patient s information into medical image is proposed. Two coding methods have been utilized to embed the EPR and improve imperceptibility. Cost optimization function is contributed to enhance the quality of the stego image. The proposed system is robust against textural feature steganalysis.,,,
S0262885613001662," This paper presents a thorough study of gender classification methodologies performing on neutral expressive and partially occluded faces when they are used in all possible arrangements of training and testing roles . A comprehensive comparison of two representation approaches three types of features three classifiers and two performance measures is provided over single and cross database experiments . Experiments revealed some interesting findings which were supported by three non parametric statistical tests when training and test sets contain different types of faces local models using the 1 NN rule outperform global approaches even those using SVM classifiers however with the same type of faces even if the acquisition conditions are diverse the statistical tests could not reject the null hypothesis of equal performance of global SVMs and local 1 NNs . 
",Study of gender recognition from neutral expressive and occluded faces Comparison of global local approaches grey level PCA LBP features and three classifiers Three statistical tests over two performance measures are employed to support the conclusions. Local models surpass global ones with different types of training and test faces. Global and local models perform equally with the same type of training and test faces.,,,
S0169260715300146," The development of adequate mathematical models for blood glucose dynamics may improve early diagnosis and control of diabetes mellitus . We have developed a stochastic nonlinear second order differential equation to describe the response of blood glucose concentration to food intake using continuous glucose monitoring data . A variational Bayesian learning scheme was applied to define the number and values of the system s parameters by iterative optimisation of free energy . The model has the minimal order and number of parameters to successfully describe blood glucose dynamics in people with and without DM . The model accounts for the nonlinearity and stochasticity of the underlying glucose insulin dynamic process . Being data driven it takes full advantage of available CGM data and at the same time reflects the intrinsic characteristics of the glucose insulin system without detailed knowledge of the physiological mechanisms . We have shown that the dynamics of some postprandial blood glucose excursions can be described by a reduced model previously seen in the literature . A comprehensive analysis demonstrates that deterministic system parameters belong to different ranges for diabetes and controls . Implications for clinical practice are discussed . This is the first study introducing a continuous data driven nonlinear stochastic model capable of describing both DM and non DM profiles . 
",Data driven model is presented for the response of glucose to food intake. Model describes blood glucose dynamics in people with and without diabetes. First study introducing a continuous data driven nonlinear stochastic model. Model s parameters belong to different ranges for diabetes and controls. Variational Bayesian learning approach was employed for model identification.,,,
S0262885614000523,"Robust high dimensional data processing has witnessed an exciting development in recent years. Theoretical results have shown that it is possible using convex programming to optimize data fit to a low rank component plus a sparse outlier component. This problem is also known as robust PCA and it has found application in many areas of computer vision. In image and video processing and face recognition the opportunity to process massive image databases is emerging as people upload photo and video data online in unprecedented volumes. However data quality and consistency is not controlled in any way and the massiveness of the data poses a serious computational challenge. In this paper we present t GRASTA or Transformed GRASTA Grassmannian robust adaptive subspace tracking algorithm . t GRASTA iteratively performs incremental gradient descent constrained to the Grassmann manifold of subspaces in order to simultaneously estimate three components of a decomposition of a collection of images a low rank subspace a sparse part of occlusions and foreground objects and a transformation such as rotation or translation of the image. We show that t GRASTA is 4 faster than state of the art algorithms has half the memory requirement and can achieve alignment for face images as well as jittered camera surveillance images. 
",Propose an online algorithm t GRASTA for the transformed robust PCA problem. Use a union of subspaces to approximate the nonlinear subspace learning process. Demonstrate the fully online mode of t GRASTA with videos with camera jitter.,,,
S0169260715301218," This work provides a performance comparison of four different machine learning classifiers multinomial logistic regression with ridge estimators classifier k nearest neighbours support vector machine and na ve Bayes as applied to terahertz transient time domain sequences associated with pixelated images of different powder samples . The six substances considered although have similar optical properties their complex insertion loss at the THz part of the spectrum is significantly different because of differences in both their frequency dependent THz extinction coefficient as well as differences in their refractive index and scattering properties . As scattering can be unquantifiable in many spectroscopic experiments classification solely on differences in complex insertion loss can be inconclusive . The problem is addressed using two dimensional cross correlations between background and sample interferograms these ensure good noise suppression of the datasets and provide a range of statistical features that are subsequently used as inputs to the above classifiers . A cross validation procedure is adopted to assess the performance of the classifiers . Firstly the measurements related to samples that had thicknesses of 2mm were classified then samples at thicknesses of 4mm and after that 3mm were classified and the success rate and consistency of each classifier was recorded . In addition mixtures having thicknesses of 2 and 4mm as well as mixtures of 2 3 and 4mm were presented simultaneously to all classifiers . This approach provided further cross validation of the classification consistency of each algorithm . The results confirm the superiority in classification accuracy and robustness of the MLR and KNN algorithms which consistently outperformed the SVM and NB classifiers for the same number of feature vectors across all studies . The work establishes a general methodology for assessing the performance of other hyperspectral dataset classifiers on the basis of 2 D cross correlations in far infrared spectroscopy or other parts of the electromagnetic spectrum . It also advances the wider proliferation of automated THz imaging systems across new application areas e.g . biomedical imaging industrial processing and quality control where interpretation of hyperspectral images is still under development . 
",A 2 D cross correlation based scheme is introduced for classification of THz signals. This work establishes a general way for assessing performance of other THz datasets. The proposed approach yields better performance than the existing method. The results confirm the superiority in classification accuracy of the MLR and KNN. It advances the wider proliferation of automated THz signals across new applications.,,,
S0262885614000262," Graphical abstract Figure illustrates the role of person re identification in a typical surveillance scenario . An area monitored by multiple cameras is depicted by top view of a building floor plan and the relative placement of the cameras with respect to the building . Colored dots depict different people and numbers besides the dots are the IDs assigned to the people . As a person moves from one camera s FOV into another camera s FOV re identification is required to establish correspondence between disconnected tracks to accomplish multiple camera tracking . This paper explores the problem of person re identification and discusses the current solutions . Open issues and challenges of the problem are highlighted with a discussion on potential directions for further research . 
",We present the problem of person re identification Re ID and associated challenges. A methodology based taxonomy and survey of current approaches is presented. We identify open and closed set re identification scenarios. Public datasets and current evaluation techniques for Re ID are discussed. Unaddressed issues like open set Re ID and long period Re ID are also discussed.,,,
S0262885613000358," We propose a novel symmetry driven Bayesian framework to incorporate structural shape into conventional geometrical shape descriptor of an image indexing and retrieval . We use rotation and reflection symmetries for structural shape description . Symmetry detection on each shape image provides a qualitative and a quantitative categorization of the types and the degrees of symmetry level . The posterior shape similarity enhances the shape matching performance based on the symmetry structural discrimination . Experimental results show statistically significant improvement on retrieval accuracy over the state of the art methods on MPEG 7 data set . 
",We use rotation and reflection symmetries for structural shape description. The posterior shape similarity enhances the shape matching performance. Test results on public dataset show increased retrieval accuracy. Any other previous or future shape representation can be combined with our method.,,,
S0262885614000778,"Recently Universum data that does not belong to any class of the training data has been applied for training better classifiers. In this paper we address a novel boosting algorithm called AdaBoost that can improve the classification performance of AdaBoost with Universum data. AdaBoost chooses a function by minimizing the loss for labeled data and Universum data. The cost function is minimized by a greedy stagewise functional gradient procedure. Each training stage of AdaBoost is fast and efficient. The standard AdaBoost weights labeled samples during training iterations while AdaBoost gives an explicit weighting scheme for Universum samples as well. In addition this paper describes the practical conditions for the effectiveness of Universum learning. These conditions are based on the analysis of the distribution of ensemble predictions over training samples. Experiments on handwritten digits classification and gender classification problems are presented. As exhibited by our experimental results the proposed method can obtain superior performances over the standard AdaBoost by selecting proper Universum data. 
",We address a novel boosting algorithm by taking advantage of Universum data. A greedy stagewise functional gradient procedure is taken to derive the method. Explicit weighting schemes for labeled and Universum samples are provided. Practical conditions to verify effectiveness of Universum learning are described. This algorithm obtains superior performances over AdaBoost with Universum data.,,,
S0169260716301420," The purpose of this study was to evaluate the use of fractional order modeling in asthma . To this end three FrOr models were compared with traditional parameters and an integer order model . We investigated which model would best fit the data the correlation with traditional lung function tests and the contribution to the diagnostic of airway obstruction . The data consisted of forced oscillation measurements obtained from healthy and asthmatic volunteers with mild moderate and severe obstructions . The first part of this study showed that a FrOr was the model that best fit the data . The correlation analysis resulted in reasonable to very good associations between FrOr parameters and spirometry . The closest associations were observed between parameters related to peripheral airway obstruction showing a clear relationship between the FrOr models and lung mechanics . Receiver operator analysis showed that FrOr parameters presented a high potential to contribute to the detection of the mild obstruction in a clinical setting . The accuracy observed in these parameters was higher than that observed in traditional FO parameters and that obtained from the InOr model . Patients with moderate and severe obstruction were identified with high accuracy . In conclusion the results obtained are in close agreement with asthma pathology and provide evidence that FO measurement associated with FrOr models is a non invasive simple and radiation free method for the detection of biomechanical abnormalities in asthma . 
",Our aim was to evaluate the use of fractional order FrOr modeling in asthma. We compared FrOr models with traditional parameters and an integer order model FrOr parameters best fit the data showing good associations with spirometry. They also showed a high accuracy in the detection of the mild obstruction in asthma FrOr models provide meaningful information in asthmatic patients.,,,
S0169260715002965," Today smart mobile devices are very commonly used due to their powerful hardware and useful features . According to an eMarketer report in 2014 there were 1.76 billion smartphone users in the world it is predicted that this number will rise by 15.9 to 2.04 billion in 2015 . It is thought that these devices can be used successfully in biomedical applications . A wireless blood pressure measuring device used together with a smart mobile device was developed in this study . By means of an interface developed for smart mobile devices with Android and iOS operating systems a smart mobile device was used both as an indicator and as a control device . The cuff communicating with this device through Bluetooth was designed to measure blood pressure via the arm . A digital filter was used on the cuff instead of the traditional analog signal processing and filtering circuit . The newly developed blood pressure measuring device was tested on 18 patients and 20 healthy individuals of different ages under a physician s supervision . When the test results were compared with the measurements made using a sphygmomanometer it was shown that an average 93.52 accuracy in sick individuals and 94.53 accuracy in healthy individuals could be achieved with the new device . 
",A wireless blood pressure measuring device used together with a smart mobile device was developed. The smart mobile device was used as an indicator and a control device. The cuff communicating with this device through Bluetooth was designed to measure blood pressure on the arm. Digital filter was used on the cuff instead of classical analog signal processing and filtering circuit. The test results showed that higher accuracy could be achieved with the device developed.,,,
S0262885613001030,"This paper proposes a weighted scheme for elastic graph matching hand posture recognition. Visual features scattered on the elastic graph are assigned corresponding weights according to their relative ability to discriminate between gestures. The weights values are determined using adaptive boosting. A dictionary representing the variability of each gesture class is expressed in the form of a bunch graph. The positions of the nodes in the bunch graph are determined using three techniques manually semi automatically and automatically. Experimental results also show that the semi automatic annotation method is efficient and accurate in terms of three performance measures assignment cost accuracy and transformation error. In terms of the recognition accuracy our results show that the hierarchical weighting on features has more significant discriminative power than the classic method uniform weighting . The hierarchical elastic graph matching WEGM approach was used to classify a lexicon of ten hand postures and it was found that the poses were recognized with a recognition accuracy of 97.08 on average. Using the weighted scheme computing cycles can be decreased by only computing the features for those nodes whose weight is relatively high and ignoring the remaining nodes. It was found that only 30 of the nodes need to be computed to obtain a recognition accuracy of over 90 . 
",The HEGM method for classifying hand postures with a hit rate of 97.08 on average over uniform and complex backgrounds. This method allows computing only features corresponding to highly discriminative nodes thus decreasing computing time. A semi automatic technique to annotate bunch graphs is described which is efficient and leads to faster graph creation.,,,
S0262885613001789," This paper proposes an unsupervised variational segmentation approach of color texture images . To improve the description ability the compact multi scale structure tensor total variation flow and color information are integrated to extract color texture information . Since heterogeneous image object and nonlinear variation exist in color texture image it is not appropriate to use one single multiple constant in the Chan and Vese model to describe each phase . Therefore a multiphase successive active contour model based on the multivariable Gaussian distribution is presented to describe each phase . As geodesic active contour has a stronger ability in capturing boundary . To inherit the advantages of edge based model and region based model we incorporate the GAC into the MSACM to enhance the detection ability for concave edge . Although multiphase optimization of our proposed MSACM is a NP hard problem we can discretely and approximately solve it by a multilayer graph method . In addition to segment the color texture image automatically an adaptive iteration convergence criterion is designed by incorporating the local Kullback Leibler distance and global phase label so that we can control the segmentation process converges . Comparing to state of the art unsupervised segmentation methods on a substantial of color texture images our approach achieves a significantly better performance on capture ability of homogeneous region smooth boundary and accuracy . 
",A new color texture feature is constructed. Multiphase successive active contour model MSACM is proposed. MSACM can be discretely optimized by multilayer graph. The combined energy of local and global is iteratively optimized. Performances are assessed through the qualitative and quantitative comparisons.,,,
S0262885614000821," The present work attempts to build a bio cryptographic system that combines transformed minutiae pairwise feature and user generated password fuzzy vault . The fingerprint fuzzy vault is based on a new minutiae pairwise structure which overcomes the fingerprint feature publication while the secret binary vault code is generated according to the fingerprint fuzzy vault result . The authentication process involves two stages fuzzy vault matching and secret vault code validation . Our minutiae pairwise transformation produces different templates thus resolving the problem of cross matching attacks in fingerprint fuzzy vault . So the original fingerprint template can not be recreated because it is protected by the key generated from the user password . In addition the proposed bio cryptographic system ensures an acceptable security level for user authentication . 
",A new fingerprint feature representation. The proposed fingerprint fuzzy vault scheme has three layers of security. The hardened fuzzy vault scheme produces different templates. The hardened fuzzy vault scheme can resolve the problem of cross matching attacks. The results show better user authentication performance with FRR 6.31 at FAR 0 .,,,
S0169260715300158," This paper presents a new heuristic algorithm for reduct selection based on credible index in the rough set theory applications . This algorithm is efficient and effective in selecting the decision rules particularly the problem to be solved in a large scale . This algorithm is capable to derive the rules with multi outcomes and identify the most significant features simultaneously which is unique and useful in solving predictive medical problems . The end results of the proposed approach are a set of decision rules that illustrates the causes for solitary pulmonary nodule and results of the long term treatment . 
",Discover certain regularities at the primitive concept level. Improve the efficiency of the discovery process and express the user s preference. Develop high quality attribute oriented and rule based models. Find the causes for solitary pulmonary nodule and results of the long term treatment.,,,
S0262885613000607," Human Nonverbal Communication Computing aims to investigate how people exploit nonverbal aspects of their communication to coordinate their activities and social relationships . Nonverbal behavior plays important roles in message production and processing relational communication social interaction and networks deception and impression management and emotional expression . This is a fundamental yet challenging research topic . To effectively analyze Nonverbal Communication Computing motion analysis methods have been widely investigated and employed . In this paper we introduce the concept and applications of Nonverbal Communication Computing and also review some of the motion analysis methods employed in this area . They include face tracking expression recognition body reconstruction and group activity analysis . In addition we also discuss some open problems and the future directions of this area . 
",Introduce the concept of nonverbal communication computing and it use cases Review motion analysis techniques used for nonverbal communication computing Discuss future directions of this area,,,
S0262885614000316," In this paper we present a robust and lightweight method for the automatic fitting of deformable 3D face models on facial images . Popular fitting techniques such as those based on statistical models of shape and appearance require a training stage based on a set of facial images and their corresponding facial landmarks which have to be manually labeled . Therefore new images in which to fit the model can not differ too much in shape and appearance from those used for training . By contrast our approach can fit a generic face model in two steps the detection of facial features based on local image gradient analysis and the backprojection of a deformable 3D face model through the optimization of its deformation parameters . The proposed approach can retain the advantages of both learning free and learning based approaches . Thus we can estimate the position orientation shape and actions of faces and initialize user specific face tracking approaches such as Online Appearance Models which have shown to be more robust than generic user tracking approaches . Experimental results show that our method outperforms other fitting alternatives under challenging illumination conditions and with a computational cost that allows its implementation in devices with low hardware specifications such as smartphones and tablets . Our proposed approach lends itself nicely to many frameworks addressing semantic inference in face images and videos . 
",We robustly fit deformable 3D face models on facial images. We estimate the 3D position 3D orientation shape and actions of faces. We quickly and robustly initialize user specific face tracking approaches. Our approach outperforms others under challenging illumination conditions. Our approach runs in real time in smartphones and tablets.,,,
S0262885613001327,"Building facade detection is an important problem in computer vision with applications in mobile robotics and semantic scene understanding. In particular mobile platform localization and guidance in urban environments can be enabled with accurate models of the various building facades in a scene. Toward that end we present a system for detection segmentation and parameter estimation of building facades in stereo imagery. The proposed method incorporates multilevel appearance and disparity features in a binary discriminative model and generates a set of candidate planes by sampling and clustering points from the image with Random Sample Consensus RANSAC using local normal estimates derived from Principal Component Analysis PCA to inform the planar models. These two models are incorporated into a two layer Markov Random Field MRF an appearance and disparity based discriminative classifier at the mid level and a geometric model to segment the building pixels into facades at the high level. By using object specific stereo features our discriminative classifier is able to achieve substantially higher accuracy than standard boosting or modeling with only appearance based features. Furthermore the results of our MRF classification indicate a strong improvement in accuracy for the binary building detection problem and the labeled planar surface models provide a good approximation to the ground truth planes. 
",We perform automatic modeling of building facades from single view stereo. Goal is to provide semantic landmarks for mobile platform localization. Use of novel appearance model with stereo features and plane fitting to disparity. Facade segmentation by graphical model hierarchical Markov Random Field. Good performance in segmenting and modeling major facades in the scene.,,,
S0198971513000835," Segregation models often focus on private racial preference but overlook the institutional context . This paper represents an effort to move beyond the preference centricity . In this paper an ideal Pigovian regulatory intervention is emulated and added into Schelling s classic spatial proximity model of racial segregation with an aim to preserve collective welfare against the negative externalities induced by the changing local racial compositions after individual relocations . A key discovery from a large number of cellular automata is that the Pigovian regulation tends to result in less segregated but also less efficient residential patterns than laissez faire . This finding albeit from a highly stylized model bears intellectual relations to an important practical question What are the potential racial effects of Pigovian local planning interventions such as financially motivated anti density zoning or the collection of a development impact fee On top of its modest policy implications this paper demonstrates a bottom up computational modelling approach to reconcile the preference based and institution orientated academic perspectives regarding racial residential segregation . 
",A stylized Pigovian intervention into Schelling s classic segregation model. A bottom up computational approach to modelling interactions between private preferences and public institutions. A technical attempt to address the relations between race space and planning.,,,
S0198971514001355," Over the last few years much online volunteered geographic information has emerged and has been increasingly analyzed to understand places and cities as well as human mobility and activity . However there are concerns about the quality and usability of such VGI . In this study we demonstrate a complete process that comprises the collection unification classification and validation of a type of VGI online point of interest data and develop methods to utilize such POI data to estimate disaggregated land use at a very high spatial resolution using part of the Boston metropolitan area as an example . With recent advances in activity based land use transportation and environment models such disaggregated land use data become important to allow LUTE models to analyze and simulate a person s choices of work location and activity destinations and to understand policy impacts on future cities . These data can also be used as alternatives to explore economic activities at the local level especially as government published census based disaggregated employment data have become less available in the recent decade . Our new approach provides opportunities for cities to estimate land use at high resolution with low cost by utilizing VGI while ensuring its quality with a certain accuracy threshold . The automatic classification of POI can also be utilized for other types of analyses on cities . 
",We demonstrate the unification classification and validation of online POI data. The classified POI data is used to disaggregate land use at a high spatial resolution. The disaggregated land use data is useful for LUTE models and for economic analyses.,,,
S0169260715302583," Background and objective Retinal blood vessel segmentation is a prominent task for the diagnosis of various retinal pathology such as hypertension diabetes glaucoma etc . In this paper a novel matched filter approach with the Gumbel probability distribution function as its kernel is introduced to improve the performance of retinal blood vessel segmentation . Methods Before applying the proposed matched filter the input retinal images are pre processed . During pre processing stage principal component analysis based gray scale conversion followed by contrast limited adaptive histogram equalization are applied for better enhancement of retinal image . After that an exhaustive experiments have been conducted for selecting the appropriate value of parameters to design a new matched filter . The post processing steps after applying the proposed matched filter include the entropy based optimal thresholding and length filtering to obtain the segmented image . Results For evaluating the performance of proposed approach the quantitative performance measures an average accuracy average true positive rate and average false positive rate are calculated . The respective values of the quantitative performance measures are 0.9522 0.7594 0.0292 for DRIVE data set and 0.9270 0.7939 0.0624 for STARE data set . To justify the effectiveness of proposed approach receiver operating characteristic curve is plotted and the average area under the curve is calculated . The average AUC for DRIVE and STARE data sets are 0.9287 and 0.9140 respectively . Conclusions The obtained experimental results confirm that the proposed approach performance better with respect to other prominent Gaussian distribution function and Cauchy PDF based matched filter approaches . 
",A novel matched filter approach with the Gumbel PDF as its kernel is proposed. Pre processing includes PCA based gray scale conversion and contrast enhancement. Post processing includes the entropy based optimal thresholding and length filtering. On the basis of exhaustive experiment select the appropriate value of parameters.,,,
S0262885614000304," We present a method for the recognition of complex actions . Our method combines automatic learning of simple actions and manual definition of complex actions in a single grammar . Contrary to the general trend in complex action recognition that consists in dividing recognition into two stages our method performs recognition of simple and complex actions in a unified way . This is performed by encoding simple action HMMs within the stochastic grammar that models complex actions . This unified approach enables a more effective influence of the higher activity layers into the recognition of simple actions which leads to a substantial improvement in the classification of complex actions . We consider the recognition of complex actions based on person transits between areas in the scene . As input our method receives crossings of tracks along a set of zones which are derived using unsupervised learning of the movement patterns of the objects in the scene . We evaluate our method on a large dataset showing normal suspicious and threat behaviour on a parking lot . Experiments show an improvement of 30 in the recognition of both high level scenarios and their composing simple actions with respect to a two stage approach . Experiments with synthetic noise simulating the most common tracking failures show that our method only experiences a limited decrease in performance when moderate amounts of noise are added . 
",We present a method for the recognition of complex actions in a unified way. We encode simple action HMMs within the stochastic grammar that models complex actions. As input our method receives a sequence of crossings of tracks through a set of zones. We evaluate our method in a threat recognition setting.,,,
S0262885613001534," The estimation of camera orientation from image lines using the anthropic environment restriction is a well known problem but traditional methods to solve it depend on line extraction a relatively complex procedure that is also incompatible with distorted images . We propose Corisco a monocular orientation estimation method based on edgels instead of lines . Edgels are points sampled from image edges with their tangential directions extracted in Corisco using a grid mask . The estimation aligns the measured edgel directions with the predicted directions calculated from the orientation using a known camera model . Corisco uses the M estimation technique to define an objective function that is optimized by two algorithms in sequence RANSAC which gives robustness and flexibility to Corisco and FilterSQP which performs a continuous optimization to refine the initial estimate using closed formulas for the function derivatives . Corisco is the first edgel based method able to analyze images with any camera model and it also allows for a compromise between speed and accuracy so that its performance can be tuned according to the application requirements . Our experiments demonstrate the effectiveness of Corisco with various camera models and its performance surpasses similar edgel based methods . The accuracy displayed a mean error below 2 for execution times above 8s in a conventional computer and above 3 for less than 2s . 
",We propose an orientation estimation method based on edgels and M estimation. Edgels are extracted with a grid mask that can compromise between speed and accuracy. Any camera model can be used and errors are calculated on the original image space. The estimation starts with a random search and ends with a continuous optimization. The method uses quaternions and all derivative calculations use closed formulas.,,,
S0169260716000079," Background and objective Neuroimaging studies have demonstrated dysfunction in the brain reward circuit in individuals with online gaming addiction . We hypothesized that virtual reality therapy for OGA would improve the functional connectivity of the cortico striatal limbic circuit by stimulating the limbic system . Methods Twenty four adults with OGA were randomly assigned to a cognitive behavior therapy group or VRT group . Before and after the four week treatment period the severity of OGA was evaluated with Young s Internet Addiction Scale . Using functional magnetic resonance imaging the amplitude of low frequency fluctuation and FC from the posterior cingulate cortex seed to other brain areas were evaluated . Twelve casual game users were also recruited and underwent only baseline assessment . Results After treatment both CBT and VRT groups showed reductions in YIAS scores . At baseline the OGA group showed a smaller ALFF within the right middle frontal gyrus and reduced FC in the cortico striatal limbic circuit . In the VRT group connectivity from the PCC seed to the left middle frontal and bilateral temporal lobe increased after VRT . Conclusion VRT seemed to reduce the severity of OGA showing effects similar to CBT and enhanced the balance of the cortico striatal limbic circuit . 
",We developed a novel virtual reality therapy VRT program for online gaming addiction OGA . We compared the treatment effect of VRT to cognitive behavior therapy for OGA. VRT reduced the severity of OGA showing effects similar to cognitive behavior therapy. VRT improved the functional connectivity of the cortico limbic circuit.,,,
S0169260715300614," Background and objective Classification of gene expression data is the common denominator of various biomedical recognition tasks . However obtaining class labels for large training samples may be difficult or even impossible in many cases . Therefore semi supervised classification techniques are required as semi supervised classifiers take advantage of unlabeled data . Methods Gene expression data is high dimensional which gives rise to the phenomena known under the umbrella of the curse of dimensionality one of its recently explored aspects being the presence of hubs or hubness for short . Therefore hubness aware classifiers have been developed recently such as Naive Hubness Bayesian k Nearest Neighbor . In this paper we propose a semi supervised extension of NHBNN which follows the self training schema . As one of the core components of self training is the certainty score we propose a new hubness aware certainty score . Results We performed experiments on publicly available gene expression data . These experiments show that the proposed classifier outperforms its competitors . We investigated the impact of each of the components separately and showed that each of these components are relevant to the performance of the proposed approach . Conclusions Our results imply that our approach may increase classification accuracy and reduce computational costs . Based on the promising results presented in the paper we envision that hubness aware techniques will be used in various other biomedical machine learning tasks . In order to accelerate this process we made an implementation of hubness aware machine learning techniques publicly available in the PyHubs software package implemented in Python one of the most popular programming languages of data science . the set of k nearest neighbors of x. probability that x belongs to class C given its nearest neighbors the probability of the event that x appears as one of the k nearest neighbors of any labeled training instance belonging to class C the prior probability of the event that an instance belongs to class C how many times x occurs as one of the k nearest neighbors of labeled training instances belonging to class C how many times x occurs as one of the k nearest neighbors of other instances when considering lab 
",A semi supervised hubness aware classifier is proposed. The classifier is evaluated on publicly available real gene expression data. We made the implementation of hubness aware machine learning techniques available in the PyHubs software package.,,,
S0262885613001315," This paper presents a novel approach for action recognition localization and video matching based on a hierarchical codebook model of local spatio temporal video volumes. Given a single example of an activity as a query video the proposed method finds similar videos to the query in a target video dataset. The method is based on the bag of video words BOV representation and does not require prior knowledge about actions background subtraction motion estimation or tracking. It is also robust to spatial and temporal scale changes as well as some deformations. The hierarchical algorithm codes a video as a compact set of spatio temporal volumes while considering their spatio temporal compositions in order to account for spatial and temporal contextual information. This hierarchy is achieved by first constructing a codebook of spatio temporal video volumes. Then a large contextual volume containing many spatio temporal volumes ensemble of volumes is considered. These ensembles are used to construct a probabilistic model of video volumes and their spatio temporal compositions. The algorithm was applied to three available video datasets for action recognition with different complexities KTH Weizmann and MSR II and the results were superior to other approaches especially in the case of a single training example and cross dataset1 action recognition. 
",A hierarchical structure for accurate video to video matching and event recognition Incorporating contextual information to the bag of video words framework Coding spatio temporal compositions of video volumes by a probabilistic framework,,,
S0262885613001790,"The relationship between nonverbal behavior and severity of depression was investigated by following depressed participants over the course of treatment and video recording a series of clinical interviews. Facial expressions and head pose were analyzed from video using manual and automatic systems. Both systems were highly consistent for FACS action units AUs and showed similar effects for change over time in depression severity. When symptom severity was high participants made fewer affiliative facial expressions AUs 12 and 15 and more non affiliative facial expressions AU 14 . Participants also exhibited diminished head motion i.e. amplitude and velocity when symptom severity was high. These results are consistent with the Social Withdrawal hypothesis that depressed individuals use nonverbal behavior to maintain or increase interpersonal distance. As individuals recover they send more signals indicating a willingness to affiliate. The finding that automatic facial expression analysis was both consistent with manual coding and revealed the same pattern of findings suggests that automatic facial expression analysis may be ready to relieve the burden of manual coding in behavioral and clinical science. 
",We investigated the relation between nonverbal behavior and severity of depression. When symptoms were severe participants showed less AU 12 and AU 15 and more AU 14. When symptoms were severe participants head motion was reduced in size and speed. The pattern of findings was highly consistent for automated and manual measurements. The findings support the hypothesis of nonverbal social withdrawal in depression.,,,
S0262885613000346," Engineers have proposed many watermark mechanisms for protecting the content of digital media from unauthorized use . The visible watermark scheme indicates the copyright of digital media posted over the Internet by embedding an inconspicuous but recognizable pattern into media . However the embedding process often results in serious distortion of the protected image . Since the strength of the watermark in conventional methods mainly depends on the feature of protected media this may lead to unsatisfactory transparency of watermarked images . This paper proposes a removable solution for visible watermark mechanism . By adopting the subsampling technique the method proposes a contrast adaptive strategy to solve this problem . This method can also guarantee the essentials of general visible watermark schemes . Experimental results show that the proposed method outperforms related works in terms of preserving the quality of the restored image . 
",The transparency of the watermark is applicable to the various host images. The browsers can recognize the watermark and the host image is not seriously obscured. The new scheme can remove the embedded watermark from the watermarked image. The PSNR values of the various restored images are around 56dB after the visible watermark has been removed.,,,
S0169260715003272," Equivalence testing is recommended as a better alternative to the traditional difference based methods for demonstrating the comparability of two or more treatment effects . Although equivalent tests of two groups are widely discussed the natural extensions for assessing equivalence between several groups have not been well examined . This article provides a detailed and schematic comparison of the ANOVA F and the studentized range tests for evaluating the comparability of several standardized effects . Power and sample size appraisals of the two grossly distinct approaches are conducted in terms of a constraint on the range of the standardized means when the standard deviation of the standardized means is fixed . Although neither method is uniformly more powerful the studentized range test has a clear advantage in sample size requirements necessary to achieve a given power when the underlying effect configurations are close to the priori minimum difference for determining equivalence . For actual application of equivalence tests and advance planning of equivalence studies both SAS and R computer codes are available as supplementary files to implement the calculations of critical values p values power levels and sample sizes . 
",This article explicates and evaluates the features of the ANOVA F test and the studentized range test for determining the equivalence of multiple standardized effects. The primary emphasis is to reveal the underlying properties of the two methods with regard to power behavior and sample size requirement across a variety of design configurations. To enhance the practical usefulness complete sets of SAS and R computer algorithms for calculating the critical values p values power levels and sample sizes are also developed.,,,
S0198971515300223,"Increasingly realistic virtual three dimensional 3D models have been created that demonstrate a variety of landscape designs. They have supported a more collaborative and participative approach in planning and design. However these 3D landscape models are often developed for use in bespoke virtual reality labs that tie the models to expensive graphics hardware or complex arrays of screens with the viewer spatially detached from the actual site. Given the increase in prevalence of advanced smartphone and tablet technology with GPS and compass functionality this paper demonstrates two methods for on demand dissemination of existing virtual 3D landscape models using 1 a touch based interface with integrated mapping 2 a standard web browser interface on mobile phones. The latter method demonstrates the potential to reduce the complexity of accessing an existing 3D landscape model on site to simply pointing a smartphone in a particular direction loading a web page and seeing the relevant view of the model as an image. A prototype system was developed to demonstrate both methods successfully but it was also ascertained that the accuracy of GPS positional data can have a negative effect on the browser based method. Finally potential developments are presented exploring the future of the technology underpinning the method and possible extensions to the prototype as a technique for increasing public participation in planning and design. 
",A theoretical discussion on the capabilities of differing types of landscape presentation methods Proposed and implemented landscape visualisation on demand and on site from existing virtual 3D landscape models Use of smartphones and standard web browsing technologies to access the landscape visualisations.,,,
S0198971516300394," Social media data are increasingly perceived as alternative sources to public attitude surveys because of the volume of available data that are time stamped and precisely located . Such data can be mined to provide planners marketers and researchers with useful information about activities and opinions across time and space . However in their raw form textual data are still difficult to analyse coherently and Twitter streams pose particular interpretive challenges because they are restricted to just 140 characters . This paper explores the use of an unsupervised learning algorithm to classify geo tagged Tweets from Inner London recorded during typical weekdays throughout 2013 into a small number of groups following extensive text cleaning techniques . Our classification identifies 20 distinctive and interpretive topic groupings which represent key types of Tweets from describing activities or informal conversations between users to the use of check in applets . Our motivation is to use the classification to demonstrate how the nature of the content posted on Twitter varies according to the characteristics of places and users . Topics and attitudes expressed through Tweets are found to vary substantially across Inner London and by time of day . Some observed variations in behaviour on Twitter can be attributed to the inferred demographic and socio economic characteristics of users but place and local activities can also exert a considerable influence . Overall the classification was found to provide a valuable framework for investigating the content and coverage of Twitter usage across Inner London . 
",links social media usage with different urban land uses and locations. provides geo temporal Twitter profiles of behaviour and social attitudes. infers demographic and socio economic characteristics of Twitter users. profiles Twitter messages with respect to inferred age and gender.,,,
S0262885613001522," This paper proposes a method for keyword spotting in off line Chinese handwritten documents using a contextual word model which measures the similarity between the query word and every candidate word in the document by combining a character classifier and the geometric context as well as linguistic context . The geometric context model characterizes the single character likeliness and between character relationship . The linguistic model utilizes the dependency of the word with the external adjacent characters . The combining weights are optimized on training documents . Experiments on a large handwriting database CASIA HWDB demonstrate the effectiveness of the proposed method and justify the benefits of geometric and linguistic contexts . Compared to transcription based text search the proposed method can provide higher recall rate and for spotting words of four characters the proposed method provides both higher precision and recall rate . 
",We propose a contextual word model for keyword spotting from handwritten Chinese documents. The contextual word model combines character classifier geometric and linguistic contexts. Promising results were obtained on a large handwriting database CASIA HWDB. The geometric and linguistic contexts improve the spotting performance significantly.,,,
S0169260715301802," Background and objectives Mammography analysis is an effective technology for early detection of breast cancer . Micro calcification clusters are a vital indicator of breast cancer so detection of MCs plays an important role in computer aided detection system this paper proposes a new hybrid method to improve MCs detection rate in mammograms . Methods The proposed method comprises three main steps firstly remove label and pectoral muscle adopting the largest connected region marking and region growing method and enhance MCs using the combination of double top hat transform and grayscale adjustment function secondly remove noise and other interference information and retain the significant information by modifying the contourlet coefficients using nonlinear function thirdly we use the non linking simplified pulse coupled neural network to detect MCs . Results In our work we choose 118 mammograms including 38 mammograms with micro calcification clusters and 80 mammograms without micro calcification to demonstrate our algorithm separately from two open and common database including the MIAS and JSMIT and we achieve the higher specificity of 94.7 sensitivity of 96.3 AUC of 97.0 accuracy of 95.8 MCC of 90.4 MCC PS of 61.3 and CEI of 53.5 these promising results clearly demonstrate that the proposed approach outperforms the current state of the art algorithms . In addition this method is verified on the 20 mammograms from the People s Hospital of Gansu Province the detection results reveal that our method can accurately detect the calcifications in clinical application . Conclusions This proposed method is simple and fast furthermore it can achieve high detection rate it could be considered used in CAD systems to assist the physicians for breast cancer diagnosis in the future . 
",We propose a new MCs detection method using contourlet transform and non linking simplified PCNN in mammograms. We first introduce the non linking simplified PCNN to detect MCs. We first come up with the evaluate indicators MCC PS CEI which take the samples proportion into account. The results tested on two open and common databases including the MIAS and the database from JSMIT. This method is verified on the mammograms from the People s Hospital of Gansu Province to show our method can be used in clinical application.,,,
S0262885613000772," This paper presents a novel skeleton pruning approach based on a 2D empirical mode like decomposition . The EMD algorithm can decompose any nonlinear and non stationary data into a number of intrinsic mode functions . When the object contour is decomposed by empirical mode like decomposition the IMFs of the object provide a workspace with very good properties for obtaining the object s skeleton . The theoretical properties and the performed experiments demonstrate that the obtained skeletons match to hand labeled skeletons provided by human subjects . Even in the presence of significant noise and shape variations cuts and tears the resulted skeletons have the same topology as the original skeletons . In particular the proposed approach produces no spurious branches as many existing skeleton pruning methods and moreover does not displace the skeleton points which are all centers of maximal disks . 
",Usage of Ensemble Empirical Mode Decomposition EEMD on object skeletonization Modeling of object contour with a 2D EEMD like procedure The produced IMFs provide a very good and novel workspace for image skeletonization. The proposed method is fully automated and unsupervised.,,,
S0262885613000462," Human faces encode plenty of useful information . Recent studies in psychology and human perception have found that facial features have relations to human weight or body mass index . These studies focus on finding the correlations between facial features and the BMI . Motivated by the recent psychology studies we develop a computational method to predict the BMI from face images automatically . We formulate the BMI prediction from facial features as a machine vision problem and evaluate our approach on a large database with more than 14 500 face images . A promising result has been obtained which demonstrates the feasibility of developing a computational system for BMI prediction from face images at a large scale . 
",A computational approach is developed for BMI prediction in face images for the first time. Our work can validate the psychology study results on a large scale database. Our computational approach can be useful for smart health.,,,
S0262885613001546,"Using image hierarchies for visual categorization has been shown to have a number of important benefits. Doing so enables a significant gain in efficiency e.g. logarithmic with the number of categories 16 12 or the construction of a more meaningful distance metric for image classification 17 . A critical question however still remains controversial would structuring data in a hierarchical sense also help classification accuracy In this paper we address this question and show that the hierarchical structure of a database can be indeed successfully used to enhance classification accuracy using a sparse approximation framework. We propose a new formulation for sparse approximation where the goal is to discover the sparsest path within the hierarchical data structure that best represents the query object. Extensive quantitative and qualitative experimental evaluation on a number of branches of the Imagenet database 7 as well as on the Caltech 256 12 demonstrate our theoretical claims and show that our approach produces better hierarchical categorization results than competing techniques. 
",A new hierarchical classification scheme by sparse approximation is proposed. Leverage large scale structured data for the accurate hierarchical classification. Distance function taking into account the hierarchical structure is introduced. Defined two images to be similar if they shared a similar path in the hierarchy Achieved better performances than flat 1 vs N classification methods,,,
S0169260715002722," This paper presents a tool for automatic assessment of skeletal bone age according to a modified version of the Tanner and Whitehouse clinical method . The tool is able to provide an accurate bone age assessment in the range 0 6 years by processing epiphysial metaphysial ROIs with image processing techniques and assigning TW2 stage to each ROI by means of hidden Markov models . The system was evaluated on a set of 360 X rays achieving a high success rate in bone age evaluation as well as outperforming other effective methods . The paper also describes the graphical user interface of the tool which is also released thus to support and speed up clinicians practices when dealing with bone age assessment . 
",Automated segmentation of X ray images for skeletal bone age assessment. Hidden Markov models for bone development modelling. A software tool for supporting clinicians in the X Ray investigation according to the TW2 clinical method.,,,
S0262885614000638,"This article discusses the motion analysis based on dense optical flow fields and for a new generation of robotic moving systems with real time constraints. It focuses on a surveillance scenario where an especially designed autonomous mobile robot uses a monocular camera for perceiving motion in the environment. The computational resources and the processing time are two of the most critical aspects in robotics and therefore two non parametric techniques are proposed namely the Hybrid Hierarchical Optical Flow Segmentation and the Hybrid Density Based Optical Flow Segmentation. Both methods are able to extract the moving objects by performing two consecutive operations refining and collecting. During the refining phase the flow field is decomposed in a set of clusters and based on descriptive motion properties. These properties are used in the collecting stage by a hierarchical or density based scheme to merge the set of clusters that represent different motion models. In addition a model selection method is introduced. This novel method analyzes the flow field and estimates the number of distinct moving objects using a Bayesian formulation. The research evaluates the performance achieved by the methods in a realistic surveillance situation. The experiments conducted proved that the proposed methods extract reliable motion information in real time and without using specialized computers. Moreover the resulting segmentation is less computationally demanding compared to other recent methods and therefore they are suitable for most of the robotic or surveillance applications. 
",This paper focuses on segmenting the motion from dense optical flow fields. Two unsupervised clustering methods are presented and a model selection is proposed. A comparison between the proposed techniques with the K means and EM is made. Experiments are conducted in a surveillance scenario with an autonomous mobile. The proposed techniques are superior in terms of robustness and computational demands,,,
S0169260715303072," Background Structural changes of the brain s third ventricle have been acknowledged as an indicative measure of the brain atrophy progression in neurodegenerative and endocrinal diseases . To investigate the ventricular enlargement in relation to the atrophy of the surrounding structures shape analysis is a promising approach . However there are hurdles in modeling the third ventricle shape . First it has topological variations across individuals due to the inter thalamic adhesion . In addition as an interhemispheric structure it needs to be aligned to the midsagittal plane to assess its asymmetric and regional deformation . Method To address these issues we propose a model based shape assessment . Our template model of the third ventricle consists of a midplane and a symmetric mesh of generic shape . By mapping the template s midplane to the individuals brain midsagittal plane we align the symmetric mesh on the midline of the brain before quantifying the third ventricle shape . To build the vertex wise correspondence between the individual third ventricle and the template mesh we employ a minimal distortion surface deformation framework . In addition to account for topological variations we implement geometric constraints guiding the template mesh to have zero width where the inter thalamic adhesion passes through preventing vertices crossing between left and right walls of the third ventricle . The individual shapes are compared using a vertex wise deformity from the symmetric template . Results Experiments on imaging and demographic data from a study of aging showed that our model was sensitive in assessing morphological differences between individuals in relation to brain volume gender and the fluid intelligence at age 72 . It also revealed that the proposed method can detect the regional and asymmetrical deformation unlike the conventional measures volume and width of the third ventricle . Similarity measures between binary masks and the shape model showed that the latter reconstructed shape details with high accuracy . Conclusions We have demonstrated that our approach is suitable to morphometrical analyses of the third ventricle providing high accuracy and inter subject consistency in the shape quantification . This shape modeling method with geometric constraints based on anatomical landmarks could be extended to other brain structures which require a consistent measurement basis in the morphometry . 
",Present a model based approach to investigate the morphology of the third ventricle. Assess the regional deformations in relation to the atrophy of surrounding structures. Use a symmetric template model with the midplane definition for unbiased analysis. Achieve a robust surface modeling using a progressive surface deformation. Validate the method on a healthy aging sample with different clinical variables.,,,
S0169260715003296," A major difficulty with chest radiographic analysis is the invisibility of abnormalities caused by the superimposition of normal anatomical structures such as ribs over the main tissue to be examined . Suppressing the ribs with no information loss about the original tissue would therefore be helpful during manual identification or computer aided detection of nodules on a chest radiographic image . In this study we introduce a two step algorithm for eliminating rib shadows in chest radiographic images . The algorithm first delineates the ribs using a novel hybrid self template approach and then suppresses these delineated ribs using an unsupervised regression model that takes into account the change in proximal thickness of bone in the vertical axis . The performance of the system is evaluated using a benchmark set of real chest radiographic images . The experimental results determine that proposed method for rib delineation can provide higher accuracy than existing methods . The knowledge of rib delineation can remarkably improve the nodule detection performance of a current computer aided diagnosis system . It is also shown that the rib suppression algorithm can increase the nodule visibility by eliminating rib shadows while mostly preserving the nodule intensity . 
",We introduce a two step algorithm for eliminating rib shadows in chest radiographic images. The algorithm delineates the ribs using a novel hybrid self template approach. It suppresses the delineated ribs using an unsupervised regression model. The rib delineations can improve the performance of CAD systems.,,,
S0169260715302819," Background In the last few years the use of social media in medicine has grown exponentially providing a new area of research based on the analysis and use of Web 2.0 capabilities . In addition the use of social media in medical education is a subject of particular interest which has been addressed in several studies . One example of this application is the medical quizzes of The New England Journal of Medicine that regularly publishes a set of questions through their Facebook timeline . Objective We present an approach for the automatic extraction of medical quizzes and their associated answers on a Facebook platform by means of a set of computer based methods and algorithms . Methods We have developed a tool for the extraction and analysis of medical quizzes stored on Facebook timeline at the NEJM Facebook page based on a set of computer based methods and algorithms using Java . The system is divided into two main modules Crawler and Data retrieval . Results The system was launched on December 31 2014 and crawled through a total of 3004 valid posts and 200 081 valid comments . The first post was dated on July 23 2009 and the last one on December 30 2014 . 285 quizzes were analyzed with 32 780 different users providing answers to the aforementioned quizzes . Of the 285 quizzes patterns were found in 261 . From these 261 quizzes where trends were found we saw that users follow trends of incorrect answers in 13 quizzes and trends of correct answers in 248 . Conclusions This tool is capable of automatically identifying the correct and wrong answers to a quiz provided on Facebook posts in a text format to a quiz with a small rate of false negative cases and this approach could be applicable to the extraction and analysis of other sources after including some adaptations of the information on the Internet . 
",New England Journal of Medicine NEJM is a very prestigious medical journal. NEJM Facebook page currently has more than 1.25 million of users. Medical quizzes are one of the methods to test the knowledge of future physicians. Our approach allows extracting medical quizzes published in NEJM Facebook page. This is the first study done about the content of medical quizzes in social networks.,,,
S0169260715002746," Background and objective Probabilistic topic models provide an unsupervised method for analyzing unstructured text . These models discover semantically coherent combinations of words that could be integrated in a clinical automatic summarization system for primary care physicians performing chart review . However the human interpretability of topics discovered from clinical reports is unknown . Our objective is to assess the coherence of topics and their ability to represent the contents of clinical reports from a primary care physician s point of view . Methods Three latent Dirichlet allocation models were fit to a large collection of clinical reports . Topics were manually evaluated by primary care physicians and graduate students . Wilcoxon Signed Rank Tests for Paired Samples were used to evaluate differences between different topic models while differences in performance between students and primary care physicians were tested using Mann Whitney U tests for each of the tasks . Results While the 150 topic model produced the best log likelihood participants were most accurate at identifying words that did not belong in topics learned by the 100 topic model suggesting that 100 topics provides better relative granularity of discovered semantic themes for the data set used in this study . Models were comparable in their ability to represent the contents of documents . Primary care physicians significantly outperformed students in both tasks . Conclusion This work establishes a baseline of interpretability for topic models trained with clinical reports and provides insights on the appropriateness of using topic models for informatics applications . Our results indicate that PCPs find discovered topics more coherent and representative of clinical reports relative to students warranting further research into their use for automatic summarization . 
",A topic model with three different parameter settings is fit to a large collection of clinical reports. The interpretability of discovered topics is evaluated by clinicians and laypersons. Clinicians are significantly more capable of interpreting topics than laypersons. Topics hold potential for applications in automatic summarization.,,,
S0169260715300742," Recently various non invasive tools such as the magnetic resonance image ultrasound imaging computed tomography and the computational fluid dynamics have been widely utilized to enhance our current understanding of the physiological parameters that affect the initiation and the progression of the cardiovascular diseases associated with heart failure . In particular the hemodynamics of left ventricle has attracted the attention of the researchers due to its significant role in the heart functionality . In this study CFD owing its capability of predicting detailed flow field was adopted to model the blood flow in images based patient specific LV over cardiac cycle . In most published studies the blood is modeled as Newtonian that is not entirely accurate as the blood viscosity varies with the shear rate in non linear manner . In this paper we studied the effect of Newtonian assumption on the degree of accuracy of intraventricular hemodynamics . In doing so various non Newtonian models and Newtonian model are used in the analysis of the intraventricular flow and the viscosity of the blood . Initially we used the cardiac MRI images to reconstruct the time resolved geometry of the patient specific LV . After the unstructured mesh generation the simulations were conducted in the CFD commercial solver FLUENT to analyze the intraventricular hemodynamic parameters . The findings indicate that the Newtonian assumption can not adequately simulate the flow dynamic within the LV over the cardiac cycle which can be attributed to the pulsatile and recirculation nature of the flow and the low blood shear rate . 
",Aim investigate the effect of different rheological models of blood within the LV. Model the MRI images are used to reconstruct the time resolved geometry of LV. Finding the non Newtonian assumption is significant on the LV flow dynamics.,,,
S0169260715300857," Background and objective In this paper we have tested the suitability of using different artificial intelligence based algorithms for decision support when classifying the risk of congenital heart surgery . In this sense classification of those surgical risks provides enormous benefits as the a priori estimation of surgical outcomes depending on either the type of disease or the type of repair and other elements that influence the final result . This preventive estimation may help to avoid future complications or even death . Methods We have evaluated four machine learning algorithms to achieve our objective multilayer perceptron self organizing map radial basis function networks and decision trees . The architectures implemented have the aim of classifying among three types of surgical risk low complexity medium complexity and high complexity . Results Accuracy outcomes achieved range between 80 and 99 being the multilayer perceptron method the one that offered a higher hit ratio . Conclusions According to the results it is feasible to develop a clinical decision support system using the evaluated algorithms . Such system would help cardiology specialists paediatricians and surgeons to forecast the level of risk related to a congenital heart disease surgery . 
",We propose an alternative system for classifying the risk in paediatric congenital heart surgery. Four methods are tested a perceptron multilayer self organising maps a radial basis function neural network and decision trees. We obtain an accuracy of 99.87 using pre and post surgical data and 83 using just pre surgical data .,,,
S0169260715301309," Background and objective The adoption of computerized physician order entry is an important cornerstone of using health information technology in health care . The transition from paper to computer forms presents a change in physicians practices . The main objective of this study was to investigate the impact of implementing a computer based order entry system without clinical decision support on the number of radiographs ordered for patients admitted in the emergency department . Methods This single center pre post intervention study was conducted in January 2013 and January 2014 at the emergency department at N mes University Hospital . All patients admitted in the emergency department who had undergone medical imaging were included in the study . Results Emergency department admissions have increased since the implementation of CPOE . In the period before CPOE implementation 2345 patients had undergone medical imaging in the period after CPOE implementation 2306 patients had undergone medical imaging . In the period before CPOE 2916 medical imaging procedures were ordered in the period after CPOE 2876 medical imaging procedures were ordered . In the period before CPOE 1885 radiographs were ordered in the period after CPOE 1776 radiographs were ordered . The time between emergency department admission and medical imaging did not vary between the two periods . Conclusions Our results show a decrease in the number of radiograph requests after a CPOE system without clinical decision support was implemented in our emergency department . 
",The main objective of this study was to investigate the impact of implementing a computer based order entry system without clinical decision support on the number of radiographs ordered for patients seen in the emergency department. Our results show a decrease in the number of radiographs ordered after computer based order entry system implementation despite an increase in the number of emergency department admissions. Our study also shows that the time interval between emergency department admission and medical imaging was not affected by this new workflow.,,,
S0262885614000286," Object tracking quality usually depends on video scene conditions . In order to overcome this limitation this article presents a new control approach to adapt the object tracking process to the scene condition variations . More precisely this approach learns how to tune the tracker parameters to cope with the tracking context variations . The tracking context or context of a video sequence is defined as a set of six features density of mobile objects their occlusion level their contrast with regard to the surrounding background their contrast variance their 2D area and their 2D area variance . In an offline phase training video sequences are classified by clustering their contextual features . Each context cluster is then associated to satisfactory tracking parameters . In the online control phase once a context change is detected the tracking parameters are tuned using the learned values . The approach has been experimented with three different tracking algorithms and on long complex video datasets . This article brings two significant contributions a classification method of video sequences to learn offline tracking parameters and a new method to tune online tracking parameters using tracking context . 
",We present a new control approach to adapt trackers to scene condition variations. Tracking context is defined as six features describing scene condition. Best tracker parameters are learned offline for tracking contexts. Trackers are then controlled by tuning online their parameters. Experimental results are compared with several recent state of the art trackers.,,,
S0169260715300961," Glomerulus diameter and Bowman s space width in renal microscopic images indicate various diseases . Therefore the detection of the renal corpuscle and related objects is a key step in histopathological evaluation of renal microscopic images . However the task of automatic glomeruli detection is challenging due to their wide intensity variation besides the inconsistency in terms of shape and size of the glomeruli in the renal corpuscle . Here a novel solution is proposed which includes the Particles Analyzer technique based on median filter for morphological image processing to detect the renal corpuscle objects . Afterwards the glomerulus diameter and Bowman s space width are measured . The solution was tested with a dataset of 21 rats renal corpuscle images acquired using light microscope . The experimental results proved that the proposed solution can detect the renal corpuscle and its objects efficiently . As well as the proposed solution has the ability to manage any input images assuring its robustness to the deformations of the glomeruli even with the glomerular hypertrophy cases . Also the results reported significant difference between the control and affected of fructose groups in terms of glomerulus diameter . 
",Glomerulus diameter and Bowman s capsule thickness in renal microscopic images indicate various diseases. Detection of the renal corpuscle and related objects detection is a key step in histopathological evaluation of renal microscopic images. This work proposed the Analysis Particles algorithm based on median filter for morphological image processing to detect the renal corpuscle objects. Afterwards the glomerulus diameter and Bowman s capsule thickness are measured. The proposed system was tested with a dataset of 21 rats renal corpuscle images acquired using light microscope. The experimental results proven that the proposed solution can detect the renal corpuscle and its objects efficiently with robustness to the deformations of the glomeruli even in the case of glomerular hypertrophy that leads to split the bowman space. The results also reported significant difference between the controlled and affected groups in terms of glomerulus diameter 97.40 19.02 m and 177.03 54.48 m respectively .,,,
S0169260716300876," Background and objective Optimal experimental design approaches are seldom used in preclinical drug discovery . The objective is to develop an optimal design software tool specifically designed for preclinical applications in order to increase the efficiency of drug discovery in vivo studies . Methods Several realistic experimental design case studies were collected and many preclinical experimental teams were consulted to determine the design goal of the software tool . The tool obtains an optimized experimental design by solving a constrained optimization problem where each experimental design is evaluated using some function of the Fisher Information Matrix . The software was implemented in C using the Qt framework to assure a responsive user software interaction through a rich graphical user interface and at the same time achieving the desired computational speed . In addition a discrete global optimization algorithm was developed and implemented . Results The software design goals were simplicity speed and intuition . Based on these design goals we have developed the publicly available software PopED lite . Optimization computation was on average over 14 test problems 30 times faster in PopED lite compared to an already existing optimal design software tool . PopED lite is now used in real drug discovery projects and a few of these case studies are presented in this paper . Conclusions PopED lite is designed to be simple fast and intuitive . Simple to give many users access to basic optimal design calculations . Fast to fit a short design execution cycle and allow interactive experimental design . Intuitive so that the input to and output from the software tool can easily be understood by users without knowledge of the theory of optimal design . In this way PopED lite is highly useful in practice and complements existing tools . 
",We have created a software PopED lite in order to increase the use of optimal design in preclinical drug discovery. PopED lite is designed to be simple fast and intuitive so that it is highly useful in practice and complements existing tools. Key functionality of PopED lite is demonstrated by three case studies from real drug discovery projects.,,,
S0262885613001273," Wildfire smoke detection is particularly important for early warning systems because smoke usually rises before flames arise . Therefore this paper presents an automatic wildfire smoke detection method using computer vision and pattern recognition techniques . First candidate blocks are identified using key frame differences and nonparametric smoke color models to detect smoke colored moving objects . Subsequently three dimensional spatiotemporal volumes are built by combining the candidate blocks in the current key frame with the corresponding blocks in previous frames . A histogram of oriented gradient is extracted and a histogram of oriented optical flow is extracted as a temporal feature based on the fact that the direction of smoke diffusion is upward owing to thermal convection . From spatiotemporal features of training data a visual codebook and a bag of features histogram are generated using our proposed weighting scheme . For smoke verification a random forest classifier is built during the training phase using the BoF histogram . The random forest with the BoF histogram can increase the detection accuracy performance when compared with related methods and allow smoke detection to be carried out in near real time . 
",We select key frames from a video and detect candidate blocks only in key frames. We prepare 3D spatiotemporal volumes by combining the candidate blocks. We introduce a new weighting scheme for generating a more reasonable BoF. The random forest classifier is built during the training phase by using the BoF.,,,
S0262885613001492," This paper examines the issue of face speaker and bi modal authentication in mobile environments when there is significant condition mismatch . We introduce this mismatch by enrolling client models on high quality biometric samples obtained on a laptop computer and authenticating them on lower quality biometric samples acquired with a mobile phone . To perform these experiments we develop three novel authentication protocols for the large publicly available MOBIO database . We evaluate state of the art face speaker and bi modal authentication techniques and show that inter session variability modelling using Gaussian mixture models provides a consistently robust system for face speaker and bi modal authentication . It is also shown that multi algorithm fusion provides a consistent performance improvement for face speaker and bi modal authentication . Using this bi modal multi algorithm system we derive a state of the art authentication system that obtains a half total error rate of 6.3 and 1.9 for Female and Male trials respectively . 
",We examine bi modal face speaker authentication in challenging mobile environment. We release new protocols and data with significant mismatch conditions MOBIO . We study bi modal and multi algorithm fusion using generative modelling techniques. Multi algorithm and multi modal fusion provides a consistent performance improvement. The proposed bi modal system significantly outperforms the state of the art.,,,
S0169260715303369," Background and objectives Automatic electrocardiogram heartbeat classification is substantial for diagnosing heart failure . The aim of this paper is to evaluate the effect of machine learning methods in creating the model which classifies normal and congestive heart failure on the long term ECG time series . Methods The study was performed in two phases feature extraction and classification phase . In feature extraction phase autoregressive Burg method is applied for extracting features . In classification phase five different classifiers are examined namely C4.5 decision tree k nearest neighbor support vector machine artificial neural networks and random forest classifier . The ECG signals were acquired from BIDMC Congestive Heart Failure and PTB Diagnostic ECG databases and classified by applying various experiments . Results The experimental results are evaluated in several statistical measures and showed that the random forest method gives 100 classification accuracy . Conclusions Impressive performance of random forest method proves that it plays significant role in detecting congestive heart failure and can be valuable in expressing knowledge useful in medicine . 
",Heartbeat classification is substantial for diagnosing heart failure. Machine learning methods classify normal and congestive heart failure CHF . The random forest method gives 100 classification accuracy in detecting CHF.,,,
S0262885613001339,"Text contained in scene images provides the semantic context of the images. For that reason robust extraction of text regions is essential for successful scene text understanding. However separating text pixels from scene images still remains as a challenging issue because of uncontrolled lighting conditions and complex backgrounds. In this paper we propose a two stage conditional random field TCRF approach to robustly extract text regions from the scene images. The proposed approach models the spatial and hierarchical structures of the scene text and it finds text regions based on the scene text model. In the first stage the system generates multiple character proposals for the given image by using multiple image segmentations and a local CRF model. In the second stage the system selectively integrates the generated character proposals to determine proper character regions by using a holistic CRF model. Through the TCRF approach we cast the scene text separation problem as a probabilistic labeling problem which yields the optimal label configuration of pixels that maximizes the conditional probability of the given image. Experimental results indicate that our framework exhibits good performance in the case of the public databases. 
",Proposed system separates text regions from images under unconstrained environment. Generalized clustering utilizes properties of scene text to detect text boundaries. Multiple image segmentations provide various interpretations on text regions. Two step CRF approach models properties and relationship of text in graph structure. Character proposals are generated and integrated to find proper character regions.,,,
S0169260716000092," Vibroarthographic signals emitted from the knee joint disorder provides an early diagnostic tool . The nonstationary and nonlinear nature of VAG signal makes an important aspect for feature extraction . In this work we investigate VAG signals by proposing a wavelet based decomposition . The VAG signals are decomposed into sub band signals of different frequencies . Nonlinear features such as recurrence quantification analysis approximate entropy and sample entropy are extracted as features of VAG signal . A total of twenty four features form a vector to characterize a VAG signal . Two feature selection techniques apriori algorithm and genetic algorithm selects six and four features as the most significant features . Least square support vector machines and random forest are proposed as classifiers to evaluate the performance of FS techniques . Results indicate that the classification accuracy was more prominent with features selected from FS algorithms . Results convey that LS SVM using the apriori algorithm gives the highest accuracy of 94.31 with false discovery rate of 0.0892 . The proposed work also provided better classification accuracy than those reported in the previous studies which gave an accuracy of 88 . This work can enhance the performance of existing technology for accurately distinguishing normal and abnormal VAG signals . And the proposed methodology could provide an effective non invasive diagnostic tool for knee joint disorders . 
",We proposed RQA ApEn SampEn and wavelet based energy as feature extraction techniques. We have proposed feature selection algorithm to extract the most significant and relevant features. We have used LS SVM and random forest as classifiers. Performance among feature selection algorithms are compared.,,,
S0262885613000917," We present a new method for multi agent activity analysis and recognition that uses low level motion features and exploits the inherent structure and recurrence of motion present in multi agent activity scenarios. Our representation is inspired by the need to circumvent the difficult problem of tracking in multi agent scenarios and the observation that for many visual multi agent recognition tasks the spatiotemporal description of events irrespective of agent identity is sufficient for activity classification. We begin by learning generative models describing motion induced by individual actors or groups which are considered to be agents. These models are Gaussian mixture distributions learned by linking clusters of optical flow to obtain contiguous regions of locally coherent motion. These possibly overlapping regions or segments known as motion patterns are then used to analyze a scene by estimating their spatial and temporal relationships. The geometric transformations between two patterns are obtained by iteratively warping one pattern onto another whereas the temporal relationships are obtained from their relative times of occurrence within videos. These motion segments and their spatio temporal relationships are represented as a graph where the nodes are the statistical distributions and the edges have geometric transformations between motion patterns transformed to Lie space as their attributes. Two activity instances are then compared by estimating the cost of attributed inexact graph matching. We demonstrate the application of our framework in the analysis of American football plays a typical multi agent activity. The performance analysis of our method shows that it is feasible and easily generalizable. 
",Modeling recognition of multi agent activities American football plays . Activities modeled as graphs inexact graph matching used for comparison. Single agent activity represented as motion patterns modeled as graph nodes. Spatio temporal relationships between single agent behaviors modeled as graph edges. We present our own dataset of football plays called UCF Football .,,,
S0169260715303771," Objective Cancer is the primary disease responsible for death and disability worldwide . Currently prevention and early detection represents the best hope for cure . Knowing the expected diseases that occur with a particular cancer in advance could lead to physicians being able to better tailor their treatment for cancer . The aim of this study was to build an animated visualization tool called as Cancer Associations Map Animation to chart the association of cancers with other disease over time . Methods The study population was collected from the Taiwan National Health Insurance Database during the period January 2000 to December 2002 782 million outpatient visits were used to compute the associations of nine major cancers with other diseases . A motion chart was used to quantify and visualize the associations between diseases and cancers . Results The CAMA motion chart that was built successfully facilitated the observation of cancer disease associations across ages and genders . The CAMA system can be accessed online at http 203.71.86.98 web runq16.html . Conclusion The CAMA animation system is an animated medical data visualization tool which provides a dynamic time lapse animated view of cancer disease associations across different age groups and gender . Derived from a large nationwide healthcare dataset this exploratory data analysis tool can detect cancer comorbidities earlier than is possible by manual inspection . Taking into account the trajectory of cancer specific comorbidity development may facilitate clinicians and healthcare researchers to more efficiently explore early stage hypotheses develop new cancer treatment approaches and identify potential effect modifiers or new risk factors associated with specific cancers . Motion chart parameters Mapping variables in this study Time Presents age of patients X axis Presents the scale of association s strength Y axis Presents the scale of count number of relative disease Size of circle Presents the number of co occurrence of both diseases A and B Color Presents the category of disease 
",A novel approach for visualization of temporal patterns focused on the association of cancers with other diseases. A dynamic animation of cancer disease association across different age groups and gender. Identifying comorbidity relationships and providing more information for medical researchers.,,,
S0169260715302224," A toolkit has been developed for calculating the 3 dimensional biological effective dose distributions in multi phase external beam radiotherapy treatments such as those applied in liver stereotactic body radiation therapy and in multi prescription treatments . This toolkit also provides a wide range of statistical results related to dose and BED distributions . MATLAB 2010a version 7.10 was used to create this GUI toolkit . The input data consist of the dose distribution matrices organ contour coordinates and treatment planning parameters from the treatment planning system . The toolkit has the capability of calculating the multi phase BED distributions using different formulas . Following the calculations of the BED distributions the dose and BED distributions can be viewed in different projections . The different elements of this toolkit are presented and the important steps for the execution of its calculations are illustrated . The toolkit is applied on brain head neck and prostate cancer patients who received primary and boost phases in order to demonstrate its capability in calculating BED distributions as well as measuring the inaccuracy and imprecision of the approximate BED distributions . Finally the clinical situations in which the use of the present toolkit would have a significant clinical impact are indicated . 
",A GUI was created to calculate the multi phase BED distributions. The GUI is effective at determining the BED distributions and the dose statistics of the ROIs for multi phase treatment plans. Using this GUI has shown the inaccuracies of using the approximate BED calculation method for multi phase cases. This GUI can be used to optimize multi phase treatment plans.,,,
S0169260715301036," Background and objectives In computed tomography statistical iterative reconstruction approaches can produce images of higher quality compared to the conventional analytical methods such as filtered backprojection algorithm . Effective noise modeling and possibilities to incorporate priors in the image reconstruction problem are the main advantages that lead to continuous development of SIR methods . Oriented by low dose CT requirements several methods are recently developed to obtain a high quality image reconstruction from down sampled or noisy projection data . In this paper a new prior information obtained from probabilistic atlas is proposed for low dose CT image reconstruction . Methods The proposed approach consists of two main phases . In learning phase a dataset of images obtained from different patients is used to construct a 3D atlas with Laplacian mixture model . The expectation maximization algorithm is used to estimate the mixture parameters . In reconstruction phase prior information obtained from the probabilistic atlas is used to construct the cost function for image reconstruction . Results We investigate the low dose imaging by considering the reduction of X ray beam intensity and by acquiring the projection data through a small number of views or limited view angles . Experimental studies using simulated data and chest screening CT data demonstrate that the probabilistic atlas prior is a practically promising approach for the low dose CT imaging . Conclusions The prior information obtained from probabilistic atlas constructed from earlier scans of different patients is useful in low dose CT imaging . 
",A powerful statistical image reconstruction algorithm for CT is proposed. Data obtained from earlier scans are used to construct a probabilistic atlas with Laplacian mixture model. Prior information obtained from a probabilistic atlas is modeled for the CT image reconstruction. We consider low dose CT imaging setups using proposed method and alternative approaches. The proposed method outperforms other alternative methods in terms of image quality.,,,
S0169260715302959," Background M2M communications represent one of the main pillars of the new paradigm of the Internet of Things and is making possible new opportunities for the eHealth business . Nevertheless the large number of M2M protocols currently available hinders the election of a suitable solution that satisfies the requirements that can demand eHealth applications . Objectives In the first place to develop a tool that provides a benchmarking analysis in order to objectively select among the most relevant M2M protocols for eHealth solutions . In the second place to validate the tool with a particular use case the respiratory rehabilitation . Methods A software tool called Distributed Computing Framework has been designed and developed to execute the benchmarking tests and facilitate the deployment in environments with a large number of machines with independence of the protocol and performance metrics selected . Results DDS MQTT CoAP JMS AMQP and XMPP protocols were evaluated considering different specific performance metrics including CPU usage memory usage bandwidth consumption latency and jitter . The results obtained allowed to validate a case of use respiratory rehabilitation of chronic obstructive pulmonary disease patients in two scenarios with different types of requirement Home Based and Ambulatory . Conclusions The results of the benchmark comparison can guide eHealth developers in the choice of M2M technologies . In this regard the framework presented is a simple and powerful tool for the deployment of benchmark tests under specific environments and conditions . 
",The software tool presented allows the deployment of different benchmarking tests for M2M protocols. The most relevant M2M protocols were evaluated considering different specific performance metrics. Benchmark results allowed to select the most suitable M2M protocol a clinical case of use respiratory rehabilitation.,,,
S0169260715301589," Background and objective Transfer function is an important parameter for the analysis and understanding of hemodynamics when arterial stenosis exists in human arterial tree . Aimed to validate the feasibility of using TF to diagnose arterial stenosis the forward problem and inverse problem were simulated and discussed . Methods A calculation method of TF between ascending aorta and any other artery was proposed based on a 55 segment transmission line model of human artery tree . The effects of artery stenosis on TF were studied in two aspects stenosis degree and position . The degree of arterial stenosis was specified to be 10 90 in three representative arteries carotid aorta and iliac artery respectively . In order to validate the feasibility of diagnosis of artery stenosis using TF and support vector machine a database of TF was established to simulate the real conditions of artery stenosis based on the TLM model . And a diagnosis model of artery stenosis was built by using SVM and the database . Results The simulating results showed the modulus and phase of TF were decreasing sharply from frequency 2 to 10Hz with the stenosis degree increasing and displayed their unique and nonlinear characteristics when frequency is higher than 10Hz . The diagnosis results showed the average accuracy was above 76 for the stenosis from 10 to 90 degree and the diagnosis accuracies of moderate and serious stenosis were 87 and 99 respectively . When the stenosis degree increased to 90 the accuracy of stenosis localization reached up to 94 for most of arteries . Conclusions The proposed method of combining TF and SVM is a theoretically feasible method for diagnosis of artery stenosis . 
",A calculation method of transfer function TF was proposed by a TLM model of human artery tree. The effects of artery stenosis on the TF were simulated and discussed by a series of simulation. A novel method of artery stenosis diagnosis was proposed and validated by TF and SVM. The accuracies of the method for moderate and serious stenosis were 87 and 99 respectively. The proposed method is a theoretically feasible method for diagnosis of artery stenosis.,,,
S0169260715302339," In observational studies without random assignment of the treatment the unadjusted comparison between treatment groups may be misleading due to confounding . One method to adjust for measured confounders is inverse probability of treatment weighting . This method can also be used in the analysis of time to event data with competing risks . Competing risks arise if for some individuals the event of interest is precluded by a different type of event occurring before or if only the earliest of several times to event corresponding to different event types is observed or is of interest . In the presence of competing risks time to event data are often characterized by cumulative incidence functions one for each event type of interest . We describe the use of inverse probability of treatment weighting to create adjusted cumulative incidence functions . This method is equivalent to direct standardization when the weight model is saturated . No assumptions about the form of the cumulative incidence functions are required . The method allows studying associations between treatment and the different types of event under study while focusing on the earliest event only . We present a SAS macro implementing this method and we provide a worked example . 
",We use inverse probability of treatment weighting a propensity score based technique for covariate adjustment of the cumulative incidence functions in competing risk analysis. This method requires no assumption about the form of the cumulative incidence functions and the interpretation of the adjusted cumulative incidence functions is intuitively appealing. We developed a SAS macro to make the method readily usable.,,,
S0262885613001042,"We propose a scheme for comparing local neighborhoods window of image points to estimate optical flow using discrete optimization. The proposed approach is based on using large correlation windows with adaptive support weights. We present three new types of weighting constraints derived from image gradient color statistics and occlusion information. The first type provides gradient structure constraints that favor flow consistency across strong image gradients. The second type imposes perceptual color constraints that reinforce relationship among pixels in a window according to their color statistics. The third type yields occlusion constraints that reject pixels that are seen in one window but not seen in the other. All these constraints contribute to suppress the effect of cluttered background which is unavoidably included in the large correlation windows. Experimental results demonstrate that each of the proposed constraints appreciably elevates the quality of estimations and that they jointly yield results that compare favorably to current techniques especially on object boundaries. 
",A method based on using large correlation windows with adaptive support weights Three new weighting constraints from image gradient color statistics and occlusion Contributes to suppress the effect of cluttered background in the windows Elevates the quality of estimations especially on object boundaries,,,
S0262885613000887," In this paper the problem of human ear recognition in the Mid wave infrared spectrum is studied in order to illustrate the advantages and limitations of the ear based biometrics that can operate in day and night time environments . The main contributions of this work are two fold First a dual band database is assembled that consists of visible and mid wave IR left and right profile face images . Profile face images were collected using a high definition mid wave IR camera that is capable of acquiring thermal imprints of human skin . Second a fully automated thermal imaging based ear recognition system is proposed that is designed and developed to perform real time human identification . The proposed system tests several feature extraction methods namely intensity based such as independent component analysis principal component analysis and linear discriminant analysis shape based such as scale invariant feature transform as well as texture based such as local binary patterns and local ternary patterns . Experimental results suggest that LTP yields the best performance on manually segmented ears and on ear images that are automatically detected and segmented . By fusing the matching scores obtained by LBP and LTP the identification performance increases by about 5 . Although these results are promising the outcomes of our study suggest that the design and development of automated ear based recognition systems that can operate efficiently in the lower part of the passive IR spectrum are very challenging tasks . 
",Collected thermal profile face database using a middle wave infrared 3 5 microns camera . Developed a fully automated thermal ear recognition system for real time human identification works in day or night . Local Ternary Pattern yields Rank 1 80.68 and 68.18 using manually and automatically segmented ears respectively. Score Level fusion of the Local Ternary Pattern LTP and Local Binary Pattern LBP enhanced the performance by 5 .,,,
S0306457313000976," On the Semantic Web the types of resources and the semantic relationships between resources are defined in an ontology . By using that information the accuracy of information retrieval can be improved . In this paper we present effective ranking and search techniques considering the semantic relationships in an ontology . Our technique retrieves top k resources which are the most relevant to query keywords through the semantic relationships . To do this we propose a weighting measure for the semantic relationship . Based on this measure we propose a novel ranking method which considers the number of meaningful semantic relationships between a resource and keywords as well as the coverage and discriminating power of keywords . In order to improve the efficiency of the search we prune the unnecessary search space using the length and weight thresholds of the semantic relationship path . In addition we exploit Threshold Algorithm based on an extended inverted index to answer top k results efficiently . The experimental results using real data sets demonstrate that our retrieval method using the semantic information generates accurate results efficiently compared to the traditional methods . 
",Semantic search using ontology overcomes the limitation of the current keyword based search. The ranking method considering semantic relationships improves the search accuracy. The ranking method is based on the weighting measure of semantic relationships. Pruning based on the weight for the semantic relationship reduces the search space. Top k Answering based on the keyword index improves the search efficiency.,,,
S0306457313000514," Transfer learning utilizes labeled data available from some related domain for achieving effective knowledge transformation to the target domain . However most state of the art cross domain classification methods treat documents as plain text and ignore the hyperlink relationship existing among the documents . In this paper we propose a novel cross domain document classification approach called Link Bridged Topic model . LBT consists of two key steps . Firstly LBT utilizes an auxiliary link network to discover the direct or indirect co citation relationship among documents by embedding the background knowledge into a graph kernel . The mined co citation relationship is leveraged to bridge the gap across different domains . Secondly LBT simultaneously combines the content information and link structures into a unified latent topic model . The model is based on an assumption that the documents of source and target domains share some common topics from the point of view of both content information and link structure . By mapping both domains data into the latent topic spaces LBT encodes the knowledge about domain commonality and difference as the shared topics with associated differential probabilities . The learned latent topics must be consistent with the source and target data as well as content and link statistics . Then the shared topics act as the bridge to facilitate knowledge transfer from the source to the target domains . Experiments on different types of datasets show that our algorithm significantly improves the generalization performance of cross domain document classification . 
",We propose a Link Bridged Topic model for cross domain document classification. LBT utilizes an auxiliary link network to discover the co citation relationship. LBT combines the content information and link structures into a graphical model. LBT outperforms both multi view learning and single view transfer baselines.,,,
S0262885615001365,"Background modeling is widely used in visual surveillance systems aiming to facilitate analysis of real world video scenes. The goal is to discriminate between pixels from foreground objects and those ones from the background. However real world scenarios tend to have time and spatial non stationary variations being difficult to reveal the foreground and background entities from video data. Here we propose a novel adaptive background modeling termed Object based Selective Updating with Correntropy OSUC to support video based surveillance systems. Our approach that is developed within an adaptive learning framework unveils existing spatio temporal pixel relationships making use of a single Gaussian for the model representation stage. Moreover we introduce a background updating scheme composed of an updating rule that is based on the stochastic gradient algorithm and Correntropy cost function. As a result this scheme can extract the temporal statistical pixel distribution at the same time dealing with non stationary pixel value fluctuations that affect the background model. Here an automatic tuning strategy of the cost function bandwidth parameter is carried out that can handle both Gaussian and non Gaussian noise environments. Besides to include pixel spatial relationships in the background modeling processing we introduce an object based selective learning rate strategy for enhancing the background modeling accuracy. Particularly an object motion analysis stage is presented to detect and track foreground entities based on pixel intensities and motion direction attained via optical flow computation. Testing is provided on well known datasets for discriminating between foreground and background that include stationary and non stationary behaviors. Achieved results show that the OSUC outperforms in most of the considered cases the state of the art approaches with an affordable computational cost. Therefore the proposed approach is suitable for supporting real world video based surveillance systems. 
",We propose a background modeling method to deal with non stationary conditions. The object based updating strategy allows to work with SFO and RFO. Non Gaussian pixel dynamics are modeled using a Correntropy cost function. Analyzing the regions movement direction avoids tracking background objects. Object based updating strategies improve the performance for indoor scenarios.,,,
S0262885615000128,"Video segmentation is a fundamental problem in computer vision and aims to extract meaningful entities from a video. One of the most useful cues in this quest is motion as is described by the trajectories of tracked points. In this paper we present a motion segmentation method attempting to address some of the major issues in the area. Namely we propose an efficient framework where more complex motion models can be seamlessly integrated both maintaining computational tractability and not penalizing non translational motion. Moreover we expose in depth the problem of object leakage due to occlusion and highlight that motion segmentation could be treated as a graph coloring problem. Our algorithm uses an approach based on graph theory and resolves occlusion cases in a robust manner. To endow our method with scalability we follow the previously presented subsequence architecture and test it in a streaming setup. Extensive experiments demonstrate the flexibility and robustness of the method. The segmentation results are competitive compared to the state of the art. 
",Each video is divided in sub sequences. Motion models are extracted for each video sub sequence. The model error distribution and the model ranking for each trajectory are used to correlate different trajectories. Occlusions cause segmentation leakages when sub sequences are merged. To avoid this effect we model segmentation as a graph coloring problem.,,,
S0306457314000193," Both general and domain specific search engines have adopted query suggestion techniques to help users formulate effective queries . In the specific domain of literature search the initial queries are usually based on a draft paper or abstract rather than short lists of keywords . In this paper we investigate phrasal concept query suggestions for literature search . These suggestions explicitly specify important phrasal concepts related to an initial detailed query . The merits of phrasal concept query suggestions for this domain are their readability and retrieval effectiveness phrasal concepts are natural for academic authors because of their frequent use of terminology and subject specific phrases and academic papers describe their key ideas via these subject specific phrases and thus phrasal concepts can be used effectively to find those papers . We propose a novel phrasal concept query suggestion technique that generates queries by identifying key phrasal concepts from pseudo labeled documents and combines them with related phrases . Our proposed technique is evaluated in terms of both user preference and retrieval effectiveness . We conduct user experiments to verify a preference for our approach in comparison to baseline query suggestion methods and demonstrate the effectiveness of the technique with retrieval experiments . 
",A phrasal concept query suggestion method for literature searches is proposed. Key phrasal concepts are suggested with related concepts. Evaluation was performed using two test collections. User experiments verify preferences to use phrasal concept queries. Phrasal concepts can lead significant improvements over state of the art baselines.,,,
S0262885616300464," This paper presents an orthonormal dictionary learning method for low rank representation . The orthonormal property encourages the dictionary atoms to be as dissimilar as possible which is beneficial for reducing the ambiguities of representations and computation cost . To make the dictionary more discriminative we enhance the ability of the class specific dictionary to well represent samples from the associated class and suppress the ability of representing samples from other classes and also enforce the representations that have small within class scatter and big between class scatter . The learned orthonormal dictionary is used to obtain low rank representations with fast computation . The performances of face recognition demonstrate the effectiveness and efficiency of the method . 
",A discriminative orthonormal dictionary learning method is proposed for low rank representation with fast computation. Two kinds of discriminative information is used for learning the dictionary. Experiments on face recognition from both image and video demonstrate the effectiveness of the proposed method.,,,
S0306457313000381," Most existing search engines focus on document retrieval . However information needs are certainly not limited to finding relevant documents . Instead a user may want to find relevant entities such as persons and organizations . In this paper we study the problem of related entity finding . Our goal is to rank entities based on their relevance to a structured query which specifies an input entity the type of related entities and the relation between the input and related entities . We first discuss a general probabilistic framework derive six possible retrieval models to rank the related entities and then compare these models both analytically and empirically . To further improve performance we study the problem of feedback in the context of related entity finding . Specifically we propose a mixture model based feedback method that can utilize the pseudo feedback entities to estimate an enriched model for the relation between the input and related entities . Experimental results over two standard TREC collections show that the derived relation generation model combined with a relation feedback method performs better than other models . 
",We derive six generative models for related entity finding. We conduct both analytical and empirical studies to compare these six models. We propose a novel entity relation based feedback method to improve performance.,,,
S0262885615000785," Recently a video representation based on dense trajectories has been shown to outperform other human action recognition methods on several benchmark datasets . The trajectories capture the motion characteristics of different moving objects in space and temporal dimensions . In dense trajectories points are sampled at uniform intervals in space and time and then tracked using a dense optical flow field over a fixed length of L frames spread overlapping over the entire video . However among these base trajectories a few may continue for longer than duration L capturing motion characteristics of objects that may be more valuable than the information from the base trajectories . Thus we propose a technique that searches for trajectories with a longer duration and refer to these as ordered trajectories . Experimental results show that ordered trajectories perform much better than the base trajectories both standalone and when combined . Moreover the uniform sampling of dense trajectories does not discriminate objects of interest from the background or other objects . Consequently a lot of information is accumulated which actually may not be useful . This can especially escalate when there is more data due to an increase in the number of action classes . We observe that our proposed trajectories remove some background clutter too . We use a Bag of Words framework to conduct experiments on the benchmark HMDB51 UCF50 and UCF101 datasets containing the largest number of action classes to date . Further we also evaluate three state of the art feature encoding techniques to study their performance on a common platform . 
",A technique that captures information of objects with longer duration. A feature selection like approach that delivers better performance than several trajectory variants. Removal of a large number of trajectories related to background noise. We apply our technique on action datasets HMDB51 UCF50 and UCF101 containing largest number of classes till date.,,,
S0262885614001474," In this paper a novel and effective lip based biometric identification approach with the Discrete Hidden Markov Model Kernel is developed . Lips are described by shape features on two different grid layouts rectangular and polar . These features are then specifically modeled by a DHMMK and learnt by a support vector machine classifier . Our experiments are carried out in a ten fold cross validation fashion on three different datasets GPDS ULPGC Face Dataset PIE Face Dataset and RaFD Face Dataset . Results show that our approach has achieved an average classification accuracy of 99.8 97.13 and 98.10 using only two training images per class on these three datasets respectively . Our comparative studies further show that the DHMMK achieved a 53 improvement against the baseline HMM approach . The comparative ROC curves also confirm the efficacy of the proposed lip contour based biometrics learned by DHMMK . We also show that the performance of linear and RBF SVM is comparable under the frame work of DHMMK . 
",We model a lip biometric approach based on shape information. This system is working with static lip on three public datasets. We develop a kernel based Hidden Markov Model DHMMK . The use of DHMMK obtains discriminative information. The use of DHMMK on lip gets a robust approach for identification.,,,
S0262885616000020," The emergence of large scale human action datasets poses a challenge to efficient action labeling. Hand labeling large scale datasets is tedious and time consuming thus a more efficient labeling method would be beneficial. One possible solution is to make use of the knowledge of a known dataset to aid the labeling of a new dataset. To this end we propose a new transfer learning method for cross dataset human action recognition. Our method aims at learning generalized feature representation for effective cross dataset classification. We propose a novel dual many to one encoder architecture to extract generalized features by mapping raw features from source and target datasets to the same feature space. Benefiting from the favorable property of the proposed many to one encoder cross dataset action data are encouraged to possess identical encoded features if the actions share the same class labels. Experiments on pairs of benchmark human action datasets achieved state of the art accuracy proving the efficacy of the proposed method. 
",Proposed a new transfer learning method for cross dataset action recognition. A new dual many to one encoder method for feature extraction across action datasets. Achieved over 10 increase in recognition accuracy over recent work.,,,
S0306457313000800," Textual entailment is a task for which the application of supervised learning mechanisms has received considerable attention as driven by successive Recognizing Data Entailment data challenges . We developed a linguistic analysis framework in which a number of similarity dissimilarity features are extracted for each entailment pair in a data set and various classifier methods are evaluated based on the instance data derived from the extracted features . The focus of the paper is to compare and contrast the performance of single and ensemble based learning algorithms for a number of data sets . We showed that there is some benefit to the use of ensemble approaches but based on the extracted features Na ve Bayes proved to be the strongest learning mechanism . Only one ensemble approach demonstrated a slight improvement over the technique of Na ve Bayes . 
",Feature extraction for the processing of supervised entailment classification. Comparison of various ensemble methods to single learning approaches. Consideration of a novel heterogeneous homogeneous ensemble combination for the problem area.,,,
S0306457314000387," Nowadays using increasingly granular data from real time location information and detailed demographics to consumers generated content on the social networking sites businesses are starting to offer precise location based product recommendation services through mobile devices . Based on the technology acceptance model this paper develops a theoretical model to examine the adoption intention of active SNS users toward location based recommendation agents . The research model was tested by using the Partial Least Squares technique.The results show that perceived usefulness perceived control and perceived institutional assurance are important in developing adoption intention . Perceived effort saving special treatment and social benefit have influences on the adoption intention through the mediating effect of perceived usefulness . Perceived accuracy has direct influence on adoption intention . 
",We develop a model to examine the adoption of location based recommendation agents. The theoretical model is developed based on the technology acceptance model. Perceived usefulness control and institutional assurance are driving factors. Perceived effort special treatment social benefit and accuracy are antecedents. Perceived usefulness is a significant mediator.,,,
S0306457313000988," Arabic is a widely spoken language but few mining tools have been developed to process Arabic text . This paper examines the crime domain in the Arabic language using text mining techniques . The development and application of a Crime Profiling System is presented . The system is able to extract meaningful information in this case the type of crime location and nationality from Arabic language crime news reports . The system has two unique attributes firstly information extraction that depends on local grammar and secondly dictionaries that can be automatically generated . It is shown that the CPS improves the quality of the data through reduction where only meaningful information is retained . Moreover the Self Organising Map approach is adopted in order to perform the clustering of the crime reports based on crime type . This clustering technique is improved because only refined data containing meaningful keywords extracted through the information extraction process are inputted into it i.e . the data are cleansed by removing noise . The proposed system is validated through experiments using a corpus collated from different sources it was not used during system development . Precision recall and F measure are used to evaluate the performance of the proposed information extraction approach . Also comparisons are conducted with other systems . In order to evaluate the clustering performance three parameters are used data size loading time and quantization error . 
",Text mining system for extraction of information related to crime from Arabic texts. Local grammar used to extract information and build dictionaries automatically. Visualisation of clustering enhances ability to analyse crime information in corpora.,,,
S0262885615000311,"Wide field of view panoramic videos have recently become popular due to the availability of high resolution displays. These panoramic videos are generated by stitching video frames captured from a panoramic video acquisition system typically comprising of multiple video cameras arranged on a static or mobile platform. A mobile panoramic video acquisition system may suffer from global mechanical vibrations as well as independent inter camera vibrations resulting in a jittery panoramic video. While existing stabilization schemes generally tackle single camera vibrations they do not account for these inter camera vibrations. In this paper we propose a video stabilization technique for multi camera panoramic videos under the consideration that independent jitter may be exhibited by content of each camera. The proposed method comprises of three steps the first step removes the global jitter in the video by estimating collective motion and subsequently removing the high frequency component from it. The second step removes the independent i.e. local jitter of each camera by estimating motion of each camera content separately. Pixels that are located in the overlapping regions of panoramic video are contributed by neighboring cameras therefore the estimated camera motion for these pixels is weighted using the blend masks generated by the stitching process. The final step applies local geometric warping to the stitched frames and removes any residual jitter induced due to parallax. Experimental results prove that proposed scheme performs better than existing panoramic stabilization schemes. 
",Stabilization of panoramic and stabilization of single camera videos are separate problems. Panoramic videos suffer from global and inter camera vibrations. Blend masks are useful for dealing with inter camera vibrations. Our survey suggests that viewers prefer this scheme over prior works.,,,
S0306457314000338," Much of the valuable information in supporting decision making processes originates in text based documents . Although these documents can be effectively searched and ranked by modern search engines actionable knowledge need to be extracted and transformed in a structured form before being used in a decision process . In this paper we describe how the discovery of semantic information embedded in natural language documents can be viewed as an optimization problem aimed at assigning a sequence of labels to a set of interdependent variables . Dependencies among variables are efficiently modeled through Conditional Random Fields an indirected graphical model able to represent the distribution of labels given a set of observations . The Markov property of these models prevent them to take into account long range dependencies among variables which are indeed relevant in Natural Language Processing . In order to overcome this limitation we propose an inference method based on Integer Programming formulation of the problem where long distance dependencies are included through non deterministic soft constraints . 
",Named Entity Recognition is addressed by constraining inference in CRF. An two phases integer linear programming approach is proposed. Complex relationships among labels are automatically extracted from data. Extracted relationships are introduced as soft constraints in the ILP formulation. The proposed method significantly outperforms the state of the art approach.,,,
S0262885615000098," IDR QR which is an incremental dimension reduction algorithm based on linear discriminant analysis and QR decomposition has been successfully employed for feature extraction and incremental learning . IDR QR can update the discriminant vectors with light computation when new training samples are inserted into the training data set . However IDR QR has two limitations 1 IDR QR can only process new samples one instance after another even if a chunk of training samples is available at a time and 2 the approximate trick is used in IDR QR . Then there exists a gap in performance between incremental and batch IDR QR solutions . To address the problems of IDR QR in this paper we propose a new chunk IDR method which is capable of processing multiple data instances at a time and can accurately update the discriminant vectors when new data items are added dynamically . Experiments on some real databases demonstrate the effectiveness of the proposed algorithm over the original one . 
",We propose a new implementation of the batch IDR QR method. Our new implementation is theoretically equivalent to the original one but is more efficient. Based on our new implementation of batch IDR QR we propose the chunk IDR method. Chunk IDR is capable of processing multiple data instances at a time. Chunk IDR can accurately update the discriminant vectors when new data items are added dynamically.,,,
S0306457313000794," This paper aims at identifying the factors influencing the implementation of Web accessibility by European banks . We studied a database made up of 49 European banks whose shares are included in the Dow Jones EURO STOXX TMI Banks Index . Regarding the factors for the implementation we considered three feasible reasons . Firstly WA adoption can be motivated by operational factors as WA can aid in increasing operational efficiency . Secondly we expect large banks to have higher WA levels as small firms face competitive disadvantages with regard to technology adoption . Lastly WA can also be understood as a part of the Corporate Social Responsibility strategy so the more committed a bank is to CSR the more prone it will be to implement WA . Our results indicate that neither the operational factors nor the firm size seem to have exerted a significant influence on WA adoption . Regarding CSR commitment results indicate a significant influence on WA adoption . However the effect of the influence is contrary to that hypothesized since more CSR committed banks have less accessible Web sites . A possible reason for this result is that banks not included in the CSR indexes try to overcome this drawback by engaging in alternative CSR activities such as WA . Nowadays Internet banking is a must have service for financial entities . Several authors conclude that it contributes to increase profitability and is a necessary investment to keep the custom of younger members . So Web sites are a key instrument for improving the competitive edge of banks as well as nonfinancial firms . Banks have an additional incentive to have a high quality Web site as Internet is a means of providing information to investors and other parties interacting with the bank . This factor is especially important in the case of banks whose shares are publicly traded . To assess the quality of a Web design many frameworks have been proposed by researchers . Ho proposed a framework that included criteria such as timeliness custom logistics and seasonal factors . Elliot Morup Petersen and Bjon Andersen also proposed a model which considers company information product service promotion transaction processing and customer service . Olsina Godoy Lafuente and Rossi proposed a Web site Quality Evaluation Model based on functionality usability efficiency and site reliability . Other similar proposals are those by DeLone and McLean Katerattanakul and Siau Kim Kishore and Sanders and Miranda and Ba egil . Regarding the specific case of banks Miranda Cort s and Barriuso elaborated an index for the quantitative evaluation of e banking Web sites and Bose and Leung proposed a framework to assess anti phishing preparedness of banks Web sites . However Web accessibility was not explicitly included in any of the proposed frameworks . Web accessibility entails overcoming all disabilities that prejudice Internet access it means that people with disabilities can use it and perceive understand navigate and interact with the Web and they can contribute to the Web. 
",The adoption of Web accessibility by banks can be motivated by several factors. Among these we test firm size operational factors and CSR commitment. We test our hypotheses using PLS methodology and a sample of European banks. Our results indicate a significant influence of CSR commitment on WA adoption. The other two factors do not seem to exert a significant influence.,,,
S0306457313001118," Disaster Management is a diffused area of knowledge . It has many complex features interconnecting the physical and the social views of the world . Many international and national bodies create knowledge models to allow knowledge sharing and effective DM activities . But these are often narrow in focus and deal with specified disaster types . We analyze thirty such models to uncover that many DM activities are actually common even when the events vary . We then create a unified view of DM in the form of a metamodel . We apply a metamodelling process to ensure that this metamodel is complete and consistent . We validate it and present a representational layer to unify and share knowledge as well as combine and match different DM activities according to different disaster situations . 
",Disaster management knowledge is dispersed and requires a centralised and better access. We present metamodelling a way of unifying DM knowledge. We develop our DM Metamodel DMM. We validate theoretically and illustrate how it would apply in representing DM knowledge used in two recent disasters.,,,
S0262885615001353," Automatic face alignment is a fundamental step in facial image analysis . However this problem continues to be challenging due to the large variability of expression illumination occlusion pose and detection drift in the real world face images . In this paper we present a multi view multi scale and multi component cascade shape regression model for robust face alignment . Firstly face view is estimated according to the deformable facial parts for learning view specified CSR which can decrease the shape variance alleviate the drift of face detection and accelerate shape convergence . Secondly multi scale HoG features are used as the shape index features to incorporate local structure information implicitly and a multi scale optimization strategy is adopted to avoid trapping in local optimum . Finally a component based shape refinement process is developed to further improve the performance of face alignment . Extensive experiments on the IBUG dataset and the 300 W challenge dataset demonstrate the superiority of the proposed method over the state of the art methods . 
",We investigate how face detection affects face alignment. We improve the CSR model by multi view multi scale and multi component strategies. We obtain impressive results on the IBUG and 300 W challenge datasets.,,,
S0306457313001155," Multi document discourse parsing aims to automatically identify the relations among textual spans from different texts on the same topic . Recently with the growing amount of information and the emergence of new technologies that deal with many sources of information more precise and efficient parsing techniques are required . The most relevant theory to multi document relationship Cross document Structure Theory has been used for parsing purposes before though the results had not been satisfactory . CST has received many critics because of its subjectivity which may lead to low annotation agreement and consequently to poor parsing performance . In this work we propose a refinement of the original CST which consists in formalizing the relationship definitions pruning and combining some relations based on their meaning and organizing the relations in a hierarchical structure . The hypothesis for this refinement is that it will lead to better agreement in the annotation and consequently to better parsing results . For this aim it was built an annotated corpus according to this refinement and it was observed an improvement in the annotation agreement . Based on this corpus a parser was developed using machine learning techniques and hand crafted rules . Specifically hierarchical techniques were used to capture the hierarchical organization of the relations according to the proposed refinement of CST . These two approaches were used to identify the relations among texts spans and to generate multi document annotation structure . Results outperformed other CST parsers showing the adequacy of the proposed refinement in the theory . 
",CST was refined by formalizing pruning and organizing relations. Refinements improved the annotation agreement in CSTNews corpus. A parser was built based on the refined version of CST. The results obtained by the parser outperformed previous CST based parsers.,,,
S0262885614001644," Feature correspondence lays the foundation for many tasks in computer vision and pattern recognition . In this paper the directed structural model is utilized to represent the feature set and the correspondence problem is then formulated as the structural model matching . Compared with the undirected structural model the proposed directed model provides more discriminating ability and invariance against rotation and scale transformations . Finally the recently proposed convex concave relaxation procedure is generalized to approximately solve the problem . Extensive experiments on synthetic and real data witness the effectiveness of the proposed method . 
",A directed structural model is proposed for feature correspondence. It provides more discriminating ability than commonly used undirected model. The feature correspondence is casted as a directed structural matching problem. The convex concave relaxation procedure CCRP is generalized to solve the problem.,,,
S0306457313000770," A user study of aNobii was conducted with an aim to exploring possible criteria for evaluating social navigational tools . A set of measures designed to capture various aspects of the benefits provided by the tools was proposed . To test the applicability of these measures a within subject experimental design was adopted where fifty regular aNobii users searched alternately with three book finding tools browsing friends bookshelves similar bookshelves and books by known authors . Other than the self report user experience and search result measures the choice set model was used as a novel framework for navigational effectiveness . Further analyses were conducted to explore whether three aspects of reader preference preference insight preference diversity and reading involvement might influence the performance of the tools . Some major findings are as follows . While the author browsing function was shown to be most efficient browsing friends bookshelves was shown to generate more interesting and informative browsing experiences . Three evaluative dimensions were derived from our study search experience search efficiency and result quality . The disagreement of these measures shows a need for a multi faceted evaluative framework for these exploration based navigational tools . Furthermore interaction effects on performance were found between users preference characteristics and tools . While users with high preference insight relied more heavily on author browsing to obtain more accurate results highly involved readers tended percentage wise to examine and select more titles when browsing friends bookshelves . 
",A user study of the book finding tools on aNobii was conducted. A set of performance measures were tested on these exploratory tools. Browsing friends bookshelves was more conducive to novelty and serendipity. Users preference structure shown to impact on the performance of the tools.,,,
S0262885614001607," Visual tracking is an important task in various computer vision applications including visual surveillance human computer interaction event detection video indexing and retrieval . Recent state of the art sparse representation based trackers show better robustness than many of the other existing trackers . One of the issues with these SR trackers is low execution speed . The particle filter framework is one of the major aspects responsible for slow execution and is common to most of the existing SR trackers . In this paper An earlier brief version of the paper has appeared in ICIP 13 Melbourne Australia 2013 . we propose a robust interest point based tracker in l 1 minimization framework that runs at real time with performance comparable to the state of the art trackers . In the proposed tracker the target dictionary is obtained from the patches around target interest points . Next the interest points from the candidate window of the current frame are obtained . The correspondence between target and candidate points is obtained via solving the proposed l 1 minimization problem . In order to prune the noisy matches a robust matching criterion is proposed where only the reliable candidate points that mutually match with target and candidate dictionary elements are considered for tracking . The object is localized by measuring the displacement of these interest points . The reliable candidate patches are used for updating the target dictionary . The performance and accuracy of the proposed tracker is benchmarked with several complex video sequences . The tracker is found to be considerably fast as compared to the reported state of the art trackers . The proposed tracker is further evaluated for various local patch sizes number of interest points and regularization parameters . The performance of the tracker for various challenges including illumination change occlusion and background clutter has been quantified with a benchmark dataset containing 50 videos . 
",The proposed tracker combines the flexibility of interest points and robustness of sparse representation. Proposed a robust matching criteria for reliable tracking via L1 minimization The proposed tracker is computationally efficient and provides real time performance. The tracker is benchmarked with many publicly available complex video sequences. Performance is compared with many recent state of the art trackers using 50 benchmark video dataset.,,,
S0262885615000724,"The goals of this paper are 1 to enhance the quality of images of faces 2 to enable 3D Morphable Models 3DMMs to cope with severely degraded images and 3 to reconstruct textured 3D faces with details that are not in the input images. Details that are lost in the input images due to blur low resolution or occlusions are filled in by the 3DMM and an additional texture enhancement algorithm that adds high resolution details from example faces. By leveraging class specific knowledge this restoration process goes beyond what general image operations such as deblurring or inpainting can achieve. The benefit of the 3DMM for image restoration is that it can be applied to any pose and illumination unlike image based methods. However it is only with the new fitting algorithm that 3DMMs can produce realistic faces from severely degraded images. The new method includes the blurring or downsampling operator explicitly into the analysis by synthesis algorithm. 
",A 3D model based algorithm for face hallucination at any pose and illumination A method for including non local effects e.g. blur in 3D analysis by synthesis The algorithm combines low spatial frequency information with details of a 3D model. Transfer of high spatial frequency details for hallucination on the level of pores Occlusion handling and seamless texture reconstruction,,,
S0262885616300099," This paper investigates the problem of cross domain action recognition . Specifically we present a cross domain action recognition framework by utilizing some labeled data from other data sets as the auxiliary source domain . It is a challenging task as data from different domains may have different feature distribution . To map data from different domains into the same abstract space and boost the action recognition performance we propose a method named collective matrix factorization with graph Laplacian regularization . Our approach is built upon the technique of collective matrix factorization which simultaneously learns a common latent space linear projection matrices for obtaining semantic representations and an optimal linear classifier . Moreover we explore the label consistency across different domain and the local geometric consistency in each domain and obtain a graph Laplacian regularization term to enhance the discrimination of learned features . Experimental results verify that CMFGLR significantly outperforms several state of the art methods . 
",We apply collective matrix factorization to cross domain action recognition. We build a novel graph Laplacian regularization term. The framework jointly learns semantic representations and a linear classifier.,,,
S0306457313000496," The authors of this paper investigate terms of consumers diabetes based on a log from the Yahoo Answers social question and answers forum ascertain characteristics and relationships among terms related to diabetes from the consumers perspective and reveal users diabetes information seeking patterns . In this study the log analysis method data coding method and visualization multiple dimensional scaling analysis method were used for analysis . The visual analyses were conducted at two levels terms analysis within a category and category analysis among the categories in the schema . The findings show that the average number of words per question was 128.63 the average number of sentences per question was 8.23 the average number of words per response was 254.83 and the average number of sentences per response was 16.01 . There were 12 categories in the diabetes related schema which emerged from the data coding analysis . The analyses at the two levels show that terms and categories were clustered and patterns were revealed . Future research directions are also included . 
",A user based diabetes subject directory was discovered in this study. Terms and their relationships in each of the 12 categories were visually displayed and analyzed. The relationships among the 12 categories were analyzed in a visual context. Descriptive statistic data on diabetes in the Q A were revealed.,,,
S0262885615001158," An example based face hallucination system is proposed in which given a low resolution facial image a corresponding high resolution image is automatically obtained . In practice such a problem is extremely challenging since it is often the case that two discriminative high resolution images may have similar low resolution inputs . To address this issue this study proposes an ensemble of image feature representations including various local patch or block based representations a one dimensional vector image representation a two dimensional matrix image representation and a global matrix image representation . Notably some of these representations are designed to preserve the global facial geometry of the low resolution input while others are designed to preserve the local detailed texture . For each feature representation a regression function is constructed to synthesize a high resolution image from the low resolution input image . The synthesis process is conducted in a layer by layer fashion with the output from one layer serving as the input to the following layer . Importantly each regression function is associated with a classifier in order to determine which regression functions are required in the synthesis procedure in accordance with the particular characteristics of the input image . Furthermore these classifiers can also help to deal with the individual ambiguity of system low resolution inputs . The experimental results show that the proposed framework is capable of synthesizing high resolution images from low resolution input images with a wide variety of facial poses geometry misalignments and facial expressions even when such images are not included within the original training dataset . 
",Propose an example based face hallucination framework. Proposed ensemble learning for face hallucination. Reconstruct both global and local facial features. Individual ambiguity of system low resolution inputs is considered in this framework.,,,
S0306457314000119," Automatic text summarization has been an active field of research for many years . Several approaches have been proposed ranging from simple position and word frequency methods to learning and graph based algorithms . The advent of human generated knowledge bases like Wikipedia offer a further possibility in text summarization they can be used to understand the input text in terms of salient concepts from the knowledge base . In this paper we study a novel approach that leverages Wikipedia in conjunction with graph based ranking . Our approach is to first construct a bipartite sentence concept graph and then rank the input sentences using iterative updates on this graph . We consider several models for the bipartite graph and derive convergence properties under each model . Then we take up personalized and query focused summarization where the sentence ranks additionally depend on user interests and queries respectively . Finally we present a Wikipedia based multi document summarization algorithm . An important feature of the proposed algorithms is that they enable real time incremental summarization users can first view an initial summary and then request additional content if interested . We evaluate the performance of our proposed summarizer using the ROUGE metric and the results show that leveraging Wikipedia can significantly improve summary quality . We also present results from a user study which suggests that using incremental summarization can help in better understanding news articles . 
",Summarization using Wikipedia and graph based ranking. Theoretical analysis for various sentence concept models. Real time incremental summarization. Performance analysis using the ROUGE metric and user evaluations. Personalized and query focused summarization multi document summarization.,,,
S0305054816300867," As a result of the growing demand for health services China s large city hospitals have become markedly overstretched resulting in delicate and complex operating room scheduling problems . While the operating rooms are struggling to meet demand they face idle times because of resources being pulled away for other urgent demands and cancellations for economic and health reasons . In this research we analyze the resulting stochastic operating room scheduling problems and the improvements attainable by scheduled cancellations to accommodate the large demand while avoiding the negative consequences of excessive overtime work . We present a three stage recourse model which formalizes the scheduled cancellations and is anticipative to further uncertainty . We develop a solution method for this three stage model which relies on the sample average approximation and the L shaped method . The method exploits the structure of optimal solutions to speed up the optimization . Scheduled cancellations can significantly and substantially improve the operating room schedule when the costs of cancellations are close to the costs of overtime work . Moreover the proposed methods illustrate how the adverse impact of cancellations for economic and health reasons can be largely controlled . The resource unavailability however is shown to cause a more than proportional loss of solution value for the surgery scheduling problems occurring in China s large city hospitals even when applying the proposed solution techniques and requires different management measures . 
",Presents multi stage stochastic programming models the complex stochastic nature of operating room scheduling in overcrowded Chinese big city hospitals. Develops mathematical results and techniques to solve the formulated models to almost optimality. Attains substantial and significant schedule improvements for single operating room instances derived from real life data.,,,
S0304397516000888," In this paper we consider the two stage scheduling problem in which n jobs are first processed on m identical machines at a manufacturing facility and then delivered to their customers by one vehicle which can deliver one job at each shipment . In the problem a set of n delivery times is given in advance and in a schedule the n delivery times should be assigned to the n jobs respectively . The objective is to minimize the maximum delivery completion time i.e . the time when all jobs are delivered to their respective customers and the vehicle returns to the facility . For this problem we present a approximation algorithm and a polynomial time approximation scheme . 
",Scheduling with delivery coordination. Identical machines and assignable delivery times. Minimize the maximum delivery completion time. A 3 2 approximation algorithm and a PTAS.,,,
S0262885615000736," Bipartite graph matching has been demonstrated to be one of the most efficient algorithms to solve error tolerant graph matching . This algorithm is based on defining a cost matrix between the whole nodes of both graphs and solving the nodes correspondence through a linear assignment method . Recently two versions of this algorithm have been published called Fast Bipartite and Square Fast Bipartite . They compute the same distance value than Bipartite but with a reduced runtime if some restrictions on the edit costs are considered . In this paper we do not present a new algorithm but we compare the three versions of Bipartite algorithm and show how the violation of the theoretically imposed restrictions in Fast Bipartite and Square Fast Bipartite do not affect the algorithm s performance . That is in practice we show that these restrictions do not affect the optimality of the algorithm and so the three algorithms obtain similar distances and recognition ratios in classification applications although the restrictions do not hold . Moreover we conclude that the Square Fast Bipartite with the Jonker Volgenant solver is the fastest algorithm . 
",Graph Edit Distance GED is the most used error tolerant graph matching method. Bipartite graph matching algorithm BP is the most used algorithm to solve GED. 2 new versions of BP published that reduce the runtime but restrictions are imposed. We study how much extend these restrictions limits their applicability. Empirical validation shows new versions reduce runtime but keep recognition ratio.,,,
S0262885615000967,"We present a multi view face detector based on Cascade Deformable Part Models CDPM . Over the last decade there have been several attempts to extend the well established Viola Jones face detector algorithm to solve the problem of multi view face detection. Recently a tree structure model for multi view face detection was proposed. This method is primarily designed for facial landmark detection and consequently a face detection is provided. However the effort to model inner facial structures by using a detailed facial landmark labelling resulted on a complex and suboptimal system for face detection. Instead we adopt CDPMs where the models are learned from partially labelled images using Latent Support Vector Machines LSVM . Furthermore LSVM is enhanced with data mining and bootstrapping procedures to enrich models during the training. Furthermore a post optimization procedure is derived to improve the performance. This semi supervised methodology allows us to build models based on weakly labelled data while incrementally learning latent positive and negative samples. Our results show that the proposed model can deal with highly expressive and partially occluded faces while outperforming the state of the art face detectors by a large margin on challenging benchmarks such as the Face Detection Data Set and Benchmark FDDB 1 and the Annotated Facial Landmarks in the Wild AFLW 2 databases. In addition we validate the accuracy of our models under large head pose variation and facial occlusions in the Head Pose Image Database HPID 3 and Caltech Occluded Faces in the Wild COFW datasets 4 respectively. We also outline the suitability of our models to support facial landmark detection algorithms. 
",We present a state of the art multi view face detector based on Cascade Deformable Part Models CDPM . We propose to combine data mining and bootstrapping to learn CDPM models from weakly labelled data. We report extensive validation of our models in the FDDB AFLW HDDB and COFW databases. We show the suitability of our models for face alignment initialization and face detection under partial occlusions.,,,
S0262885616300026,"We present a deeply integrated method of exploiting low cost gyroscopes to improve general purpose feature tracking. Most previous methods use gyroscopes to initialize and bound the search for features. In contrast we use them to regularize the tracking energy function so that they can directly assist in the tracking of ambiguous and poor quality features. We demonstrate that our simple technique offers significant improvements in performance over conventional template based tracking methods and is in fact competitive with more complex and computationally expensive state of the art trackers but at a fraction of the computational cost. Additionally we show that the practice of initializing template based feature trackers like KLT Kanade Lucas Tomasi using gyro predicted optical flow offers no advantage over using a careful optical only initialization method suggesting that some deeper level of integration like the method we propose is needed in order to realize a genuine improvement in tracking performance from these inertial sensors. 
",A 3 axis gyro mounted to a video camera can frequently predict the main component of optical flow. The gyro predicted flow can be used to regularize feature tracking to help track ambiguous features through bad imagery. Gyro regularization does not require all features to belong to a rigid scene. Gyro regularization adds very little computational cost to feature tracking. The common practice of using gyros to initialize trackers offers no advantage over careful optical only initialization.,,,
S0262885615001146,"In recent years much effort has been put into the development of novel algorithms to solve the person re identification problem. The goal is to match a given person s image against a gallery of people. In this paper we propose a single shot supervised method to compute a scoring function that when applied to a pair of images provides a score expressing the likelihood that they depict the same individual. The method is characterized by i the usage of a set of local image descriptors based on Fisher vectors ii the training of a pool of scoring functions based on the local descriptors and iii the construction of a strong scoring function by means of an adaptive boosting procedure. The method has been tested on four data sets and results have been compared with state of the art methods clearly showing superior performance. 
",We propose BFiVe a new supervised algorithm for single shot person re identification. The descriptors are a set of compressed local Fisher vectors extracted from a coarse to fine image subdivision. In the training step each region gives rise to a learnt weak ranking function. The ranking function of the image gallery is obtained by a boosted selection of a weak learner subset. The matching rate at rank 1 on VIPeR is 38.9 on 3DPes 41.7 on PRID 2011 19.6 and on i LIDS 119 48.1 .,,,
S0306437913000768," Modeling collaboration processes is a challenging task . Existing modeling approaches are not capable of expressing the unpredictable non routine nature of human collaboration which is influenced by the social context of involved collaborators . We propose a modeling approach which considers collaboration processes as the evolution of a network of collaborative documents along with a social network of collaborators . Our modeling approach accompanied by a graphical notation and formalization allows to capture the influence of complex social structures formed by collaborators and therefore facilitates such activities as the discovery of socially coherent teams social hubs or unbiased experts . We demonstrate the applicability and expressiveness of our approach and notation and discuss their strengths and weaknesses . 
",We propose a modeling approach and a visual modeling notation for social collaboration processes. We consider collaboration processes as the evolution of a network of documents and people. The visual modeling notation is a fusion of statecharts and graph query languages. The approach is supported by a formal definition to enable automatic reasoning and verification. We present sensitive use cases to demonstrate expressiveness and usefulness of our approach.,,,
S0262885615001067," Time of flight depth cameras have widely been used in many applications such as 3D imaging 3D reconstruction human interaction and robot navigation . However conventional depth cameras are incapable of imaging a translucent object which occupies a substantial portion of a real world scene . Such a limitation prohibits realistic imaging using depth cameras . In this work we propose a new skewed stereo ToF camera for detecting and imaging translucent objects under minimal prior of environment . We find that the depth calculation of a ToF camera with a translucent object presents a systematic distortion due to the superposed reflected light ray observation from multiple surfaces . We propose to use a stereo ToF camera setup and derive a generalized depth imaging formulation for translucent objects . Distorted depth value is refined using an iterative optimization . Experimental evaluation shows that our proposed method reasonably recovers the depth image of translucent objects . 
",A new skewed stereo ToF camera for detecting and imaging translucent objects under minimal prior of environment Translucent region detection method Translucent region recovery method Extensive analysis and evaluations,,,
S0262885615000360," In this paper we address the document image binarization problem with a three stage procedure . First possible stains and general document background information are removed from the image through a background removal stage . The remaining misclassified background and character pixels are then separated using a Local Co occurrence Mapping local contrast and a two state Gaussian Mixture Model . Finally some isolated misclassified components are removed by a morphology operator . The proposed scheme offers robust and fast performance especially for both handwritten and printed documents which compares favorably with other binarization methods . 
",Background removal technique based on adaptive median filtering and thresholding A Local Co occurrence Map with local contrast can distinguish between document text and document stains and background. Low complexity approach with fast and accurate binarization results,,,
S0262885614001693," The Bag of Words framework is well known in image classification . In the framework there are two essential steps 1 coding which encodes local features by a visual vocabulary and 2 pooling which pools over the response of all features into image representation . Many coding and pooling methods are proposed and how to apply them better in different conditions has become a practical problem . In this paper to better use BoW in different applications we study the relation between many typical coding methods and two popular pooling methods . Specifically complete combinations of coding and pooling are evaluated based on an extremely large range of vocabulary sizes on five primary and popular datasets . Three typical ones are 15 Scenes Caltech 101 and PASCAL VOC 2007 while the other two large scale ones are Caltech 256 and ImageNet . Based on the systematic evaluation some interesting conclusions are drawn . Some conclusions are the extensions of previous viewpoints while some are different but important to understand BoW model . Based on these conclusions we provide detailed application criterions by evaluating coding and pooling based on precision efficiency and memory requirements in different applications . We hope that this study can be helpful to evaluate different coding and pooling methods the conclusions can be beneficial to better understand BoW and the application criterions can be valuable to use BoW better in different applications . 
",We empirically study coding and pooling under a large range of vocabulary sizes. We provide detailed application guidelines to use BoW in practical applications. Combined with average pooling most coding methods are comparable. The best performance of max pooling is better than the one of average pooling. A saturant point exists in maximum pooling.,,,
S0262885615000797," Modern appearance based object recognition systems typically involve feature descriptor extraction and matching stages . The extracted descriptors are expected to be robust to illumination changes and to reasonable image object transformations . Some descriptors work well for general object matching but gray scale key point based methods may be suboptimal for matching line rich color scenes objects such as cars buildings and faces . We present a Rotation and Scale Invariant Line based Color aware descriptor which allows matching of objects scenes in terms of their key lines line region properties and line spatial arrangements . An important special application is face matching since face characteristics are best captured by lines curves . We tested RSILC performance on publicly available datasets and compared it with other descriptors . Our experiments show that RSILC is more accurate in line rich object description than other well known generic object descriptors . 
",A new rotation and scale invariant line based color aware descriptor is introduced. The descriptor captures both local texture color and global inter line spatial information. Experiments show that proposed descriptor is robust to rotation scale and illumination. The descriptor is compared to the well known descriptors. The proposed descriptor is more accurate on matching line rich objects such as faces.,,,
S0262885614001073," Improper camera orientation produces convergent vertical lines and skewed horizon lines in digital pictures an a posteriori processing is then necessary to obtain appealing pictures . We show here that after accurate calibration the camera on board accelerometer can be used to automatically generate an alternative perspective view from a virtual camera leading to images with residual keystone and horizon distortions that are essentially imperceptible at visual inspection . Furthermore we describe the uncertainty on the position of each pixel in the corrected image with respect to the accelerometer noise . Experimental results show a similar accuracy for a smartphone and for a digital reflex camera . The method can find application in customer imaging devices as well as in the computer vision field especially when reference vertical and horizontal features are not easily detectable in the image . 
",On board accelerometers make feasible perspective correction in digital photography. Perspective correction is obtained with no human intervention. Perspective correction can be applied also to zoom lenses. Accelerometer calibration is necessary to get high accuracy. Accelerometer pose has to be estimated carefully to get high accuracy.,,,
S0306457313000769," Several studies of Web server workloads have hypothesized that these workloads are self similar . The explanation commonly advanced for this phenomenon is that the distribution of Web server requests may be heavy tailed . However there is another possible explanation self similarity can also arise from deterministic chaotic processes . To our knowledge this possibility has not previously been investigated and so existing studies on Web workloads lack an adequate comparison against this alternative . We conduct an empirical study of workloads from two different Web sites one public university and one private company using the largest datasets that have been described in the literature . Our study employs methods from nonlinear time series analysis to search for chaotic behavior in the web logs of these two sites . While we do find that the deterministic components are significant components in these time series we do not find evidence of chaotic behavior . Predictive modeling experiments contrasting heavy tailed with deterministic models showed that both approaches were equally effective in modeling our datasets . 
",We investigate the models used in Web traffic generators. We examine deterministic chaos as an alternative explanation of self similarity. Deterministic components are found to be significant in Web session data. Deterministic and stochastic predictive models of the data are equally accurate.,,,
S0262885615000116,"We present a novel method that tackles the problem of facial landmarking in unconstrained conditions within the part based framework. Part based methods alternate the evaluation of local appearance models to produce a per point response map and a shape fitting step which finds a valid face shape that maximises the sum of the per point responses. Our approach focuses on obtaining better appearance models for the creation of the response maps and it can be used in combination with any shape fitting strategy. Local appearance models need to tackle very heterogeneous data when dealing with in the wild imagery due to factors as varying head poses facial expressions identity lighting conditions or image quality among others. Pose wise experts are typically used in this scenario so that each expert deals with more homogeneous data. However the computation cost at test time is significantly increased. Furthermore choosing the right expert is not straightforward which can lead to gross errors. We propose to dynamically select at test time the training examples used for inference. We use a global similarity measure to select the most adequate training examples for inference and create a single test sample specific expert using a localised inference technique. To illustrate the validity of these ideas we capitalise on the recently proposed use of regression to generate local appearance models. In particular we use Gaussian processes as their non parametric nature easily allows for localised regression. This novel way of constructing the response maps is combined with two state of the art standard shape fitting algorithms the popular Constrained Local Models framework and the Consensus of Exemplars method. We validate our method on two publicly available datasets as well as on a cross dataset experiment showing a considerable performance improvement of the proposed approach. 
",The problem of facial landmark detection in unconstrained conditions is tackled. A new way of constructing per landmark response maps is proposed. The method is based on the selection at test time of globally similar images. We use local Gaussian process regression for inference.,,,
S0306457314000144," In sponsored search many advertisers have not achieved their expected performances while the search engine also has a large room to improve their revenue . Specifically due to the improper keyword bidding many advertisers can not survive the competitive ad auctions to get their desired ad impressions meanwhile a significant portion of search queries have no ads displayed in their search result pages even if many of them have commercial values . We propose recommending a group of relevant yet less competitive keywords to an advertiser . Hence the advertiser can get the chance to win some ad slots and accumulate a number of impressions . At the same time the revenue of the search engine can also be boosted since many empty ad shots are filled . Mathematically we model the problem as a mixed integer programming problem which maximizes the advertiser revenue and the relevance of the recommended keywords while minimizing the keyword competitiveness subject to the bid and budget constraints . By solving the problem we can offer an optimal group of keywords and their optimal bid prices to an advertiser . Simulation results have shown the proposed method is highly effective in increasing ad impressions expected clicks advertiser revenue and search engine revenue . 
",We perform a comprehensive study on keyword bidding in sponsored search. We propose a competitiveness and relevance based bid keyword suggestion method. We model bid keyword suggestion as a mixed integer optimization problem. Experiments demonstrate the effectiveness of our proposed method.,,,
S0262885616000123," People re identification has been a very active research topic recently in computer vision . It is an important application in surveillance systems with disjoint cameras . In this paper a framework is proposed to extract descriptors of people in videos which are based on soft biometric traits and can be further used for people re identification or other applications . Soft biometric based description is more invariant to changing factors than directly using low level features such as color and texture . The ensemble of a set of soft biometric traits can achieve good performance in people re identification . In the proposed method the body of detected people is divided into three parts and the selected soft biometric traits are extracted from each part . All traits are then combined to form the final descriptor and people re identification is performed based on the descriptor and Nearest Neighbor matching strategy . The experiments are carried out on SAIVT SoftBio database which consists of videos from disjoint surveillance cameras as well as some static image based datasets . An open ID recognition problem is also evaluated for the proposed method . Comparisons with some state of the art methods are provided as well . The experiment results show the good performance of the proposed framework . 
",No training or camera calibration is needed and achieve real time processing. Extract descriptors from targets with rotations and scale change. Descriptors of partially visible people can also be extracted. No high resolution cameras are needed.,,,
S0306457313000320," Privacy preserving collaborative filtering is an emerging web adaptation tool to cope with information overload problem without jeopardizing individuals privacy . However collaborative filtering with privacy schemes commonly suffer from scalability and sparseness as the content in the domain proliferates . Moreover applying privacy measures causes a distortion in collected data which in turn defects accuracy of such systems . In this work we propose a novel privacy preserving collaborative filtering scheme based on bisecting k means clustering in which we apply two preprocessing methods . The first preprocessing scheme deals with scalability problem by constructing a binary decision tree through a bisecting k means clustering approach while the second produces clones of users by inserting pseudo self predictions into original user profiles to boost accuracy of scalability enhanced structure . Sparse nature of collections are handled by transforming ratings into item features based profiles . After analyzing our scheme with respect to privacy and supplementary costs we perform experiments on benchmark data sets to evaluate it in terms of accuracy and online performance . Our empirical outcomes verify that combined effects of the proposed preprocessing schemes relieve scalability and augment accuracy significantly . 
",A novel bisecting k means clustering based privacy preserving CF scheme is proposed. A two level preprocessing scheme is suggested to enhance scalability and accuracy. Effects of scalability and sparseness challenges are alleviated considerably. Accuracy of the solution is significantly better than knn based CF and PPCF methods.,,,
S0262885614001620," Accurate reconstruction of 3D geometrical shape from a set of calibrated 2D multiview images is an active yet challenging task in computer vision . The existing multiview stereo methods usually perform poorly in recovering deeply concave and thinly protruding structures and suffer from several common problems like slow convergence sensitivity to initial conditions and high memory requirements . To address these issues we propose a two phase optimization method for generalized reprojection error minimization where a generalized framework of reprojection error is proposed to integrate stereo and silhouette cues into a unified energy function . For the minimization of the function we first introduce a convex relaxation on 3D volumetric grids which can be efficiently solved using variable splitting and Chambolle projection . Then the resulting surface is parameterized as a triangle mesh and refined using surface evolution to obtain a high quality 3D reconstruction . Our comparative experiments with several state of the art methods show that the performance of TwGREM based 3D reconstruction is among the highest with respect to accuracy and efficiency especially for data with smooth texture and sparsely sampled viewpoints . 
",A generalized reprojection error is proposed to fuse stereo and silhouette cues. The method composes of convex optimization and mesh based surface refinement. Insensitive to initialization and scalable for high resolution reconstruction. Good performance for data with smooth texture and sparsely sampled viewpoints.,,,
S0262885615001134,"This paper proposes to model an action as the output of a sequence of atomic Linear Time Invariant LTI systems. The sequence of LTI systems generating the action is modeled as a Markov chain where a Hidden Markov Model HMM is used to model the transition from one atomic LTI system to another. In turn the LTI systems are represented in terms of their Hankel matrices. For classification purposes the parameters of a set of HMMs one for each action class are learned via a discriminative approach. This work proposes a novel method to learn the atomic LTI systems from training data and analyzes in detail the action representation in terms of a sequence of Hankel matrices. Extensive evaluation of the proposed approach on two publicly available datasets demonstrates that the proposed method attains state of the art accuracy in action classification from the 3D locations of body joints skeleton . 
",We model an action as sequence of outputs of linear time invariant LTI systems. We represent the outputs of LTI systems by means of Hankelets. We adopt an HMM to model the transitions from one LTI system to another. We formulate an inference and supervised learning formulation for our model. We also present a deep analysis of the parameter settings for our action representation.,,,
S0262885614001450,"One of the leading time of flight imaging technologies for depth sensing is based on Photonic Mixer Devices PMD . In PMD sensors each pixel samples the correlation between emitted and received light signals. Current PMD cameras compute eight correlation samples per pixel in four sequential stages to obtain depth with invariance to signal amplitude and offset variations. With motion PMD pixels capture different depths at each stage. As a result correlation samples are not coherent with a single depth producing artifacts. We propose to detect and remove motion artifacts from a single frame taken by a PMD camera. The algorithm we propose is very fast simple and can be easily included in camera hardware. We recover depth of each pixel by exploiting consistency of the correlation samples and local neighbors of the pixel. In addition our method obtains the motion flow of occluding contours in the image from a single frame. The system has been validated in real scenes using a commercial low cost PMD camera and high speed dynamics. In all cases our method produces accurate results and it highly reduces motion artifacts. 
",Motion Artifacts MoArt are a non systematic error in Time of Flight ToF imaging. MoArt emerge when motion appears during the integration time. MoArt cause distorted ToF measurements depth and amplitude . We propose a single frame correction of ToF measurements with motion flow estimation. We present quantitative and qualitative results in challenging scenarios.,,,
S0306457314000156," Downloading software via Web is a major solution for publishers to deliver their software products . In this context user interfaces for software downloading play a key role . Actually they have to allow usable interactions as well as support users in taking conscious and coherent decisions about whether to accept to download a software product or not . This paper presents different design alternatives for software download interfaces i.e . the interface that prompts the user if he wishes to actually complete its download and evaluates their ability to improve the quality of user interactions while reducing errors in user decisions . More precisely we compare Authenticode the leading software download interface for Internet Explorer to Question Answer a software download interface previously proposed by the authors Dini Foglia Prete Zanda . Furthermore we evaluate the effect of extending both interfaces by means of a reputation system similar to the eBay Feedback Forum . The results of the usability studies show that the pure Question Answer interface is the most effective in minimizing users incoherent behaviors and the differences in reputation rankings significantly influence users . Overall results suggest guidelines to design the best interface depending on the context . 
",The paper deals with interfaces that prompt users to download or not a product. We compare 3 different designs of such software download interfaces. Question Answer Q A interfaces result the most effective in minimizing users incoherent behaviors. Differences in reputation rankings significantly influence users. Results suggest guidelines to design the best interface depending on the context.,,,
S0306457313000964," Preprocessing is one of the key components in a typical text classification framework . This paper aims to extensively examine the impact of preprocessing on text classification in terms of various aspects such as classification accuracy text domain text language and dimension reduction . For this purpose all possible combinations of widely used preprocessing tasks are comparatively evaluated on two different domains namely e mail and news and in two different languages namely Turkish and English . In this way contribution of the preprocessing tasks to classification success at various feature dimensions possible interactions among these tasks and also dependency of these tasks to the respective languages and domains are comprehensively assessed . Experimental analysis on benchmark datasets reveals that choosing appropriate combinations of preprocessing tasks rather than enabling or disabling them all may provide significant improvement on classification accuracy depending on the domain and language studied on . 
",The impact of preprocessing on text classification in terms of various aspects is extensively examined. Experiments are conducted on two different domains and in two different languages. Choosing appropriate preprocessing tasks may improve classification accuracy significantly.,,,
S0262885616000135," This paper introduces an action recognition system based on a multiscale local part model . This model includes both a coarse primitive level root patch covering local global information and higher resolution overlapping part patches incorporating local structure and temporal relations . Descriptors are then computed over the local part models by applying fast random sampling at very high density . We also improve the recognition performance using a discontinuity preserving optical flow algorithm . The evaluation shows that the feature dimensions can be reduced by 7 8 through PCA while preserving high accuracy . Our system achieves state of the art results on large challenging realistic datasets namely 61.0 on HMDB51 92.0 on UCF50 86.6 on UCF101 and 65.3 on Hollywood2 . 
",We propose a new local part model for action recognition. A feature sampling strategy with high feature density is used. We explore and prove the benefits of using accurate optical flow algorithm for action recognition. High performance and fast action recognition are achieved.,,,
S0262885615001419," The process of creating a photo product such as a photobook calendar or collage from a large personal image collection requires intensive user effort . The primary goal of the current research was to develop an end to end solution to the problem of photo product generation that enables the user to complete the process with minimal edits where the system intelligently selects assets and groups them before presenting the output to the user . The automation is driven by metadata extracted both from individual images as well as from sets of assets in a collection . In particular we use an automatically detected event hierarchy to establish meaningful groupings in the assets and to determine an appropriate grouping and pagination for the final product . We propose a novel intermediate construct called a storyboard which can be translated to different product types without recomputing the underlying metadata . In addition to chronological storyboards we also describe a novel hybrid storyboard that joins chronological image presentation with groups of images of a common theme . A pagination algorithm uses the information in the storyboard and the product constraints to generate a product . Finally the user is provided with a metadata driven editing mechanism that makes it easy to change the auto populated product . Given that the proposed system envisions user interaction in creating the final product user studies are conducted to judge the usefulness of the system where consumers use the system to generate a photobook with their own images . 
",An end to end metadata driven intelligent system for automatic photobook creation A flexible intermediate story representation for creating multiple product types Automatic generation of representations for event based and theme based groupings A new pagination algorithm for mapping selected images onto photobook pages Metadata aided UI enabling users to view groupings and find alternative images,,,
S0262885615000712,"Advanced segmentation techniques in the surveillance domain deal with shadows to avoid distortions when detecting moving objects. Most approaches for shadow detection are still typically restricted to penumbra shadows and cannot cope well with umbra shadows. Consequently umbra shadow regions are usually detected as part of moving objects thus affecting the performance of the final detection. In this paper we address the detection of both penumbra and umbra shadow regions. First a novel bottom up approach is presented based on gradient and colour models which successfully discriminates between chromatic moving cast shadow regions and those regions detected as moving objects. In essence those regions corresponding to potential shadows are detected based on edge partitioning and colour statistics. Subsequently i temporal similarities between textures and ii spatial similarities between chrominance angle and brightness distortions are analysed for each potential shadow region for detecting the umbra shadow regions. Our second contribution refines even further the segmentation results a tracking based top down approach increases the performance of our bottom up chromatic shadow detection algorithm by properly correcting non detected shadows. To do so a combination of motion filters in a data association framework exploits the temporal consistency between objects and shadows to increase the shadow detection rate. Experimental results exceed current state of the art in shadow accuracy for multiple well known surveillance image databases which contain different shadowed materials and illumination conditions. 
",We address the detection of chromatic moving shadows. Gradient and colour information is exploited for separating shadow regions. Temporal gradients and spatial angle and brightness distortions are used to detect shadows. Shadows and objects are tracked using motion filters. Mutual information and data association between objects and shadows are used to recover misdetected shadows.,,,
S0306457314000296," Sentiment analysis from data streams is aimed at detecting authors attitude emotions and opinions from texts in real time . To reduce the labeling effort needed in the data collection phase active learning is often applied in streaming scenarios where a learning algorithm is allowed to select new examples to be manually labeled in order to improve the learner s performance . Even though there are many on line platforms which perform sentiment analysis there is no publicly available interactive on line platform for dynamic adaptive sentiment analysis which would be able to handle changes in data streams and adapt its behavior over time . This paper describes ClowdFlows a cloud based scientific workflow platform and its extensions enabling the analysis of data streams and active learning . Moreover by utilizing the data and workflow sharing in ClowdFlows the labeling of examples can be distributed through crowdsourcing . The advanced features of ClowdFlows are demonstrated on a sentiment analysis use case using active learning with a linear Support Vector Machine for learning sentiment classification models to be applied to microblogging data streams . 
",We present a cloud based platform for data stream processing with workflows. The ClowdFlows platform enables processing of multiple concurrent data streams. We implement an active learning scenario for sentiment analysis on data streams. Machine learning methods are shown to be suitable for sentiment analysis. Active learning improves the accuracy of sentiment classification.,,,
S0262885616300105," We propose a real time multi view landmark detector based on Deformable Part Models . The detector is composed of a mixture of tree based DPMs each component describing landmark configurations in a specific range of viewing angles . The usage of view specific DPMs allows to capture a large range of poses and to deal with the problem of self occlusions . Parameters of the detector are learned from annotated examples by the Structured Output Support Vector Machines algorithm . The learning objective is directly related to the performance measure used for detector evaluation . The tree based DPM allows to find a globally optimal landmark configuration by the dynamic programming . We propose a coarse to fine search strategy which allows real time processing by the dynamic programming also on high resolution images . Empirical evaluation on in the wild images shows that the proposed detector is competitive with the state of the art methods in terms of speed and accuracy yet it keeps the guarantee of finding a globally optimal estimate in contrast to other methods . 
",Real time multi view landmark detector independent on initialization Detector composed of a mixture of tree based Deformable Part Models Landmark detector learned by the Structured Output Support Vector Machines Coarse to fine strategy to speed up inference based on dynamic programming,,,
S0262885616300324," Efficient feature description and classification of dynamic texture is an important problem in computer vision and pattern recognition . Recently the local binary pattern based dynamic texture descriptor has been proposed to classify DTs by extending the LBP operator used in static texture analysis to the temporal domain . However the extended LBP operator can not characterize the intrinsic motion of dynamic texture well . In this paper we propose a novel video set based collaborative representation dynamic texture classification method . First we divide the dynamic texture sequence into subsequences along the temporal axis to form the video set . For each DT we extract the video set based LBP histogram to describe it . We then propose a regularized collaborative representation model to code the LBP histograms of the query video sets over the LBP histograms of the training video sets . Finally with the coding coefficients the distance between the query video set and the training video sets can be calculated for classification . Experimental results on the benchmark dynamic texture datasets demonstrate that the proposed method can yield good performance in terms of both classification accuracy and efficiency . 
",The weighted volume local binary pattern descriptor is formed to represent dynamic texture. The weights are learned with a regularized collaborative representation model. The representation residual is used for dynamic texture classification.,,,
S0262885615000979," The detection of vanishing points in a monoscopic image is a first step to the extraction of 3D data . This article shows a partition of the image space in order to determine the type of perspective which is present thereby allowing the determination of the vanishing point associated with each of the axes of the special reference system . Additionally the Thales second theorem allows us to determine the position of the vanishing points of the image and to automatize the process . An algorithm has been used with the data provided by the selected edge detector which provides the location of the vanishing points contained on the image plane . The comparative study includes two variables the number of vanishing points and the image resolution . The results show that in general the Prewitt s edge detector provides the best results both positional and statistical . Increasing the image resolution improves the results although the results obtained for a resolution of 640 480 pixels and another of 1024 768 pixels are very similar . 
",An algorithm has been developed for automatic detection of vanishing points. The behavior of gradient based edge detectors has been analyzed. The results have been analyzed according to the number of vanishing points and image resolution.,,,
S0262885615000694," Automatic optical inspection plays an important role to control the appearance quality of wide range of products in the product process . Recently the high popularity of smartphones and information appliances drives significant demand of touch panels . However the traditional frequency based method which exploits the line structure feature of texture images is not effective for the defect detection of touch panels . The paper presents a novel spatial domain algorithm to inspect the defects on touch panel . By utilizing the characteristics of periodic patterns of the sensing circuits an adaptive model for each pattern is learned online to effectively extract defects . The experimental results indicate that our proposed method achieves accurate detection with efficient computation . In addition the users pay very little effort for the testing of different panel products . 
",We develop a novel algorithm to inspect the defects on touch panel. Based on periodic patterns of touch panel an adaptive model is learned online to extract defects. The experimental results indicate that our proposed method achieves accurate detection with efficient computation. The users pay very little effort for the testing of different panel products.,,,
S0262885614001097,"The difficulty of face recognition FR systems to operate efficiently in diverse operational environments e.g. day and night time is aided by employing sensors covering different spectral bands i.e. visible and infrared . Biometric practitioners have identified a framework of band specific algorithms which can contribute to both assessment and intervention. While these motions are proven to achieve improvement of identification performance they traditionally result in solutions that typically fail to work efficiently across multiple spectrums. In this work we designed and developed an efficient fully automated direct matching based FR approach that is designed to operate efficiently when face data is captured using either visible or passive infrared IR sensors. Thus it can be applied in both daytime and nighttime environments. First input face images are geometrically normalized using our pre processing pipeline prior to feature extraction. Then face based features including wrinkles veins as well as edges of facial characteristics are detected and extracted for each operational band visible MWIR and LWIR . Finally global and local face based matching is applied before fusion is performed at the score level. Our approach achieves a rank 1 identification rate of at least 99.43 regardless of the spectrum of operation. This suggests that our approach results in better performance than other tested standard commercial and academic face based matchers on all spectral bands used. 
",Propose new automated tri spectral visible MWIR and LWIR FR approach Design experiments to quantitatively measure benefits of global vs. local matchers Evaluation of global vs. local based matchers when fused at the score level Achieve rank 1 identification rate of at least 99.43 per spectrum of operation,,,
S0306457314000326," A trie is one of the data structures for keyword matching . It is used in natural language processing IP address routing and so on . It is represented by the matrix form the link form the double array and LOUDS . The double array representation combines retrieval speed of the matrix form with compactness of the list form . LOUDS is a succinct data structure using bit string . Retrieval speed of LOUDS is not faster than that of the double array but its space usage is smaller . This paper proposes a compressed version of the double array by dividing the trie into multiple levels and removing the BASE array from the double array . Moreover a retrieval algorithm and a construction algorithm are proposed . According to the presented experimental results for pseudo and real data sets the retrieval speed of the presented method is almost the same as the double array and its space usage is compressed to 66 comparing with LOUDS for a large set of keywords with fixed length . 
",A new compression method of the double array is proposed. The BASE array is removed from the double array. The retrieval and construction algorithms are proposed. The space usage of our method is more compact than that of the double array. The retrieval speed of our method is almost the same as the double array.,,,
S0262885615000700," Pose estimation is a key concern in 3D urban surveying mapping and navigation . Although Global Positioning System technologies can be used to estimate a robot s or vehicle s pose there are many urban environments in which GPS functions poorly or not at all . For these situations we offer a novel approach based on a careful fusion of panoramic camera data and 2D laser scanner input . First a Constrained Bundle Adjustment is introduced to handle scale and loop closure constraints . The fusion of a panoramic image series and laser data then enables an accurate scale to be estimated and loop closures detected . Finally the two geometric constraints are enforced on the global CBA solution which in turn produces a robust pose estimate . Experiments show that the proposed method is practicable and more accurate than vision only methods with an average error of just 0.2m in the horizontal plane over a 580m trajectory . 
",Accurate scale is estimated with laser data and features extracted from camera. Many more loop closures are detected by sensor than that of camera only methods. Scale and loop closure geometric constraints are enforced on Constrained Bundle Adjustment which produces a robust pose estimate. Experiments show that the proposed method is practicable and more accurate than vision only methods.,,,
S0262885614001048,"Locally affine transformation with globally elastic interpolation is a common strategy for non rigid registration. Current techniques improve the registration accuracy by only processing the sub images that contain well defined structures quantified by Moran s spatial correlation. As an indicator Moran s metric successfully excludes noisy structures that result in misleading global optimum in terms of similarity. However some well defined structures with intensity only varying in one direction may also cause mis registration. In this paper we propose a new metric based on the response of a similarity function to quantify the ability of being correctly registered for each sub image. Using receiver operating characteristic analysis we show that the proposed metric more accurately reflects such ability than Moran s metric. Incorporating the proposed metric into a hierarchical non rigid registration scheme we show that registration accuracy is improved relative to Moran s metric. 
",For non rigid registration a hierarchical piece wise affine transform is examined. We propose a metric to quantify the ability of being correctly registered. We examine ROC analysis on the proposed metric and Moran s spatial correlation. Proposed metric reflects the ability of being registered better than Moran s. Registration error is reduced by using the proposed metric as a sub image indicator.,,,
S0262885615000232," In this paper we propose a method for face recognition by using the two dimensional discrete wavelet transform and a new patch strategy . Based on the average image of all training samples by using integral projection technique for two top level s high frequency sub bands of 2D DWT we propose a non uniform patch strategy for the top level s low frequency sub band . This patch strategy is more suitable to reflect the structure feature of face image and it is better for retaining the integrity of local information . By applying the obtained patch strategy to all samples we obtain patches of training samples and testing samples and then give the final decision by using the nearest neighbor classifier and the majority voting . Experiments are run on the AR FERET Extended Yale B and LFW face databases . The obtained numerical results show that the new face recognition method outperforms the traditional 2D DWT method and some state of the art patch based methods . 
",A new strategy of overlapped non uniform patch is proposed. 2D DWT and integral projection technique are used to obtain the patch strategy. The overlapped patches are elected to keep the patches robust. The proposed face recognition method processes good performance.,,,
S0306457314000132," The norm of practice in estimating graph properties is to use uniform random node samples whenever possible . Many graphs are large and scale free inducing large degree variance and estimator variance . This paper shows that random edge sampling and the corresponding harmonic mean estimator for average degree can reduce the estimation variance significantly . First we demonstrate that the degree variance and consequently the variance of the RN estimator can grow almost linearly with data size for typical scale free graphs . Then we prove that the RE estimator has a variance bounded from above . Therefore the variance ratio between RN and RE samplings can be very large for big data . The analytical result is supported by both simulation studies and 18 real networks . We observe that the variance reduction ratio can be more than a hundred for some real networks such as Twitter . Furthermore we show that random walk sampling is always worse than RE sampling and it can reduce the variance of RN method only when its performance is close to that of RE sampling . 
",We derive an upper bound for the variance of RE sampling. We show that variance of RN sampling grows with data size for scale free networks. Thereby RE sampling excels when data size is large. We support the analytical results with simulation studies and 18 real networks.,,,
S0306457314000181," Several Web 2.0 applications allow users to assign keywords to provide better organization and description of the shared content . Tag recommendation methods may assist users in this task improving the quality of the available information and thus the effectiveness of various tag based information retrieval services such as searching content recommendation and classification . This work addresses the tag recommendation problem from two perspectives . The first perspective centered at the object aims at suggesting relevant tags to a target object jointly exploiting the following three dimensions tag co occurrences terms extracted from multiple textual features and various metrics to estimate tag relevance . The second perspective centered at both object and user aims at performing personalized tag recommendation to a target object user pair exploiting in addition to the three aforementioned dimensions a metric that captures user interests . In particular we propose new heuristic methods that extend state of the art strategies by including new metrics that estimate how accurately a candidate tag describes the target object . We also exploit three learning to rank based techniques namely RankSVM Genetic Programming and Random Forest for generating ranking functions that exploit multiple metrics as attributes to estimate the relevance of a tag to a given object or object user pair . We evaluate the proposed methods using data from four popular Web 2.0 applications namely Bibsonomy LastFM YouTube and YahooVideo . Our new heuristics for object centered tag recommendation provide improvements in precision over the best state of the art alternative of 12 on average while our new heuristics for personalized tag recommendation produce average gains in precision of 121 over the baseline . Similar performance gains are also achieved in terms of other metrics notably recall Normalized Discounted Cumulative Gain and Mean Reciprocal Rank . Further improvements for both object centered and personalized tag recommendation can also be achieved with our new L2R based strategies which are flexible and can be easily extended to exploit other aspects of the tag recommendation problem . Finally we also quantify the benefits of personalized tag recommendation to provide better descriptions of the target object when compared to object centered recommendation by focusing only on the relevance of the suggested tags to the object . We find that our best personalized method outperforms the best object centered strategy with average gains in precision of 10 . 
",We propose new heuristics for object centered and personalized tag recommendation. We also propose new learning to rank L2R based strategies for the same tasks. They exploit tag co occurrences textual features relevance metrics and user history. Our solutions greatly outperform state of the art methods on real datasets. Tag personalization produces better descriptions of the objects.,,,
S0262885615000487,"In this paper a tracking method based on sequential Bayesian inference is proposed. The proposed method focuses on solving both the problem of tracking under partial occlusions and the problem of non rigid object tracking in real time on a desktop personal computer PC . The proposed method is mainly composed of two parts 1 modeling the target object using elastic structure of local patches for robust performance and 2 efficient hierarchical diffusion method to perform the tracking procedure in real time. The elastic structure of local patches allows the proposed method to handle partial occlusions and non rigid deformations through the relationship among neighboring patches. The proposed hierarchical diffusion method generates samples from the region where the posterior is concentrated to reduce computation time. The method is extensively tested on a number of challenging image sequences with occlusion and non rigid deformation. The experimental results show the real time capability and the robustness of the proposed method under various situations. 
",We propose a tracking method to solve both the problem of partial occlusions and non rigid deformations in real time. The target object is modeled through an elastic structure of local patches for robust performance. Hierarchical diffusion method is proposed to obtain an acceptable solution in real time. Extensive evaluation shows that the proposed method outperforms state of the art.,,,
S0262885615000347,"In robot localization particle filtering can estimate the position of a robot in a known environment with the help of sensor data. In this paper we present an approach based on particle filtering for accurate stereo matching. The proposed method consists of three parts. First we utilize multiple disparity maps in order to acquire a very distinctive set of features called landmarks and then we use segmentation as a grouping technique. Secondly we apply scan line particle filtering using the corresponding landmarks as a virtual sensor data to estimate the best disparity value. Lastly we reduce the computational redundancy of particle filtering in our stereo correspondence with a Markov chain model given the previous scan line values. More precisely we assist particle filtering convergence by adding a proportional weight in the predicted disparity value estimated by Markov chains. In addition to this we optimize our results by applying a plane fitting algorithm along with a histogram technique to refine any outliers. This work provides new insights into stereo matching methodologies by taking advantage of global geometrical and spatial information from distinctive landmarks. Experimental results show that our approach is capable of providing high quality disparity maps comparable to other well known contemporary techniques. 
",A stereo matching approach motivated by the particle filter framework in robot localization. Highly accurate GCPs acquired by the computation of multiple cost efficient disparity maps. A Markov chain model has been introduced in the process to reduce the computational complexity of particle filtering. Application of RANSAC algorithm along with a histogram technique to refine any outliers.,,,
S0262885614001462," In this paper a local approach for 3D object recognition is presented . It is based on the topological invariants provided by the critical points of the 3D object . The critical points and the links between them are represented by a set of size functions obtained after splitting the 3D object into portions . A suitable similarity measure is used to compare the sets of size functions associated with the 3D objects . In order to validate our approach s recognition performance we used different collections of 3D objects . The obtained scores are favourably comparable to the related work . 
",The 3D object is split into 18 portions according to its principal axes. Each portion is represented by a size function. A well suited similarity measure is used to compare between the 3D objects.,,,
S0262885614001061,"This paper introduces a novel topic model for learning a robust object model. In this hierarchical model the layout topic is used to capture the local relationships among a limited number of parts when the part topic is used to locate the potential part regions. Naturally an object model is represented as a probability distribution over a set of parts with certain layouts. Rather than a monolithic model our object model is composed of multiple sub category models designed to capture the significant variations in appearance and shape of an object category. Given a set of object instances with a bounding box an iterative learning process is proposed to divide them into several sub categories and learn the corresponding sub category models without any supervision. Through an experiment in object detection the learned object model is examined and the results highlight the advantages of our present method compared with others. 
",The ideas of sub category and part based are used to learn a robust object model. A novel object representation is proposed based on the topic model. An iterative learning process is presented under the semi supervision way.,,,
S0262885614001589,"This paper proposes a new method to extract a gait feature from a raw gait video directly. The Space Time Interest Points STIPs are detected where there are significant movements of human body along both spatial and temporal directions in local spatio temporal volumes of a raw gait video. Then a histogram of STIP descriptors HSD is constructed as a gait feature. In the classification stage the support vector machine SVM is applied to recognize gaits based on HSDs. In this study the standard multi class i.e. multiple subjects classification can often be computationally infeasible at test phase when gait recognition is performed by using every possible classifiers i.e. SVM models trained for all individual subjects. In this paper the attribute based classification is applied to reduce the number of SVM models needed for recognizing each probe gait. This process will significantly reduce the test time computational complexity and also retain or even improve the recognition accuracy. When compared with other existing methods in the literature the proposed method is shown to have the promising performance for the case of normal walking and the outstanding performance for the cases of walking with variations such as walking with carrying a bag and walking with varying a type of clothes. 
",A new gait feature is extracted from a raw gait video directly. The proposed gait feature extraction is performed in the spatio temporal domain. The attribute based learning is used to enhance the SVM based gait classification. The proposed method has outstanding performances under changes of clothing types. The proposed method has outstanding performances under changes of carry conditions.,,,
S0262885616300014," Many studies have confirmed gait as a robust biometric feature for identification of individuals . However direction changes cause difficulties for most of the gait recognition systems due to appearance changes . This study presents an efficient multi view gait recognition method that allows curved trajectories on unconstrained paths in indoor environments . The recognition is based on volumetric analysis of the human gait to exploit most of the 3D information enclosed in it . Appearance based gait descriptors are extracted from 3D gait volumes and temporal patterns of them are classified using a Support Vector Machine with a sliding temporal window for majority voting . The proposed approach is experimentally validated on the AVA Multi View Dataset and on the Kyushu University 4D Gait Database . The results show that this new approach is able to identify people walking on curved paths . 
",We propose a new model free approach for gait recognition. The recognition is achieved independently of the trajectory. Our method is based on 3D morphological analysis of gait sequences. Our approach is able to identify people walking on curved paths.,,,
S0262885615000153," This paper proposes a novel method to address the registration of images with affine transformation . Firstly the Maximally Stable Extremal Region detection method is performed on the reference image and the image to be registered respectively . And the coarse affine transformation matrix between the two images is estimated by the matched MSER pairs . Two circular regions containing roughly the same image content are also obtained by fitting and normalizing the centroids of the matched MSERs from the two images . Secondly a scale invariant and approximate affine transformation invariant feature point detection algorithm based on the Gabor filter decomposition and phase congruency is performed on the two coarsely aligned regions and two feature point sets are achieved respectively . Finally the affine transformation matrix between the two feature point sets is obtained by using a probabilistic point set registration algorithm and the final affine transformation matrix between the reference image and the image to be registered is achieved according to the coarse affine transformation matrix and the affine transformation matrix between the two feature point sets . Several sets of experiments demonstrate that our proposed method performs competitively with the classical scale invariant feature transform method for images with scale changes and performs better than the traditional MSER and Affine SIFT methods for images with affine distortions . Moreover the proposed method shows higher computation efficiency and robustness to illumination change than some existing area based or feature based methods do . 
",A combination of coarse alignment and refinement is employed in the method. Input images are coarsely aligned by fitting and normalizing the matched MSERs. A phase congruency and point set registration based refinement step is used. The method accurately and efficiently aligns images with affine distortions. The method is robust to illumination changes.,,,
S0306457314000491," Influence theories constitute formal models that identify those individuals that are able to affect and guide their peers through their activity . There is a large body of work on developing such theories as they have important applications in viral marketing recommendations as well as information retrieval . Influence theories are typically evaluated through a manual process that can not scale to data voluminous enough to draw safe representative conclusions . To overcome this issue we introduce in this paper a formalized framework for large scale automatic evaluation of topic specific influence theories that are specialized in Twitter . Basically it consists of five conjunctive conditions that are indicative of real influence exertion the first three determine which influence theories are compatible with our framework while the other two estimate their relative effectiveness . At the core of these two conditions lies a novel metric that assesses the aggregate sentiment of a group of users and allows for estimating how close the behavior of influencers is to that of the entire community . We put our framework into practice using a large scale test bed with real data from 75 Twitter communities . In order to select the theories that can be employed in our analysis we introduce a generic two dimensional taxonomy that elucidates their functionality . With its help we ended up with five established topic specific theories that are applicable to our settings . The outcomes of our analysis reveal significant differences in their performance . To explain them we introduce a novel methodology for delving into the internal dynamics of the groups of influencers they define . We use it to analyze the implications of the selected theories and based on the resulting evidence we propose a novel partition of influence theories in three major categories with divergent performance . 
",We propose a formalized evaluation framework for topic specific influence theories specialized in Twitter. We introduce a novel evaluation metric that assesses the aggregate sentiment of a group of user. We introduce a two dimensional taxonomy for classifying influence theories. We use the framework to evaluate five existing theories from literature and assess their performance.,,,
S0306457313001143," Despite the fact that both the Efficient Market Hypothesis and Random Walk Theory postulate that it is impossible to predict future stock prices based on currently available information recent advances in empirical research have been proving the opposite by achieving what seems to be better than random prediction performance . We discuss some of the advantages of the most widely used performance metrics and conclude that is difficult to assess the external validity of performance using some of these measures . Moreover there remain many questions as to the real world applicability of these empirical models . In the first part of this study we design novel stock price prediction models based on state of the art text mining techniques to assert whether we can predict the movement of stock prices more accurately by including indicators of irrationality . Along with this we discuss which metrics are most appropriate for which scenarios in order to evaluate the models . Finally we discuss how to gain insight into text mining based stock price prediction models in order to evaluate validate and refine the models . 
",Several state of the art stock prediction models are constructed. Different metrics for evaluating prediction models are discussed. Explanatory techniques are applied to gain insights into the model s decisions.,,,
S0306457313000356," Relevance Based Language Models commonly known as Relevance Models are successful approaches to explicitly introduce the concept of relevance in the statistical Language Modelling framework of Information Retrieval . These models achieve state of the art retrieval performance in the pseudo relevance feedback task . On the other hand the field of recommender systems is a fertile research area where users are provided with personalised recommendations in several applications . In this paper we propose an adaptation of the Relevance Modelling framework to effectively suggest recommendations to a user . We also propose a probabilistic clustering technique to perform the neighbour selection process as a way to achieve a better approximation of the set of relevant items in the pseudo relevance feedback process . These techniques although well known in the Information Retrieval field have not been applied yet to recommender systems and as the empirical evaluation results show both proposals outperform individually several baseline methods . Furthermore by combining both approaches even larger effectiveness improvements are achieved . 
",A new recommendation approach based on the Relevance Modelling RM of the problem is proposed. The neighbour selection problem is improved by Posterior Probabilistic Clustering PPC . Background information such as item popularity is successfully integrated by using RM models. Performance of the recommendation improves when more clusters are considered in the PPC technique. Combination of both contributions leads to an even better performance than their separate application.,,,
S0306457313000721," In this paper we propose improved variants of the sentence retrieval method TF ISF . The improvement is achieved by using context consisting of neighboring sentences and at the same time promoting the retrieval of longer sentences . We thoroughly compare new modified TF ISF methods to the TF ISF baseline to an earlier attempt to include context into TF ISF named tfmix and to a language modeling based method that uses context and promoting retrieval of long sentences named 3MMPDS . Experimental results show that the TF ISF method can be improved using local context . Results also show that the TF ISF method can be improved by promoting the retrieval of longer sentences . Finally we show that the best results are achieved when combining both modifications . All new methods also show statistically significant better results than the other tested methods . 
",We extend the TF ISF method to use local context. We extend the TF ISF method to promote retrieval of long sentences. Context and promoting retrieval of long sentences both improves sentence retrieval. We also combine using context and promoting retrieval of long sentences. It is useful to use at the same time context and promoting retrieval of long sentences.,,,
S0262885614001425," The evaluation of the scale of an object in a cluttered background is a serious problem in computer vision . The most existing contour based approaches relevant to object detection address this problem by normalizing descriptor or multi scale searching such as sliding window searching spatial pyramid model etc . Besides Hough voting framework can predict the scale of an object according to some meaning fragments . However utilizing scale variant descriptor or complicated structure in these measures reduces the efficiency of detection . In the present paper we propose a novel shape feature called scale invariant contour segment context . This feature is based on the angle between contour line segments . It remains unchanged as scale varies . Most importantly it evaluates the scale of objects located in cluttered images and facilitates localization of the boundary of the object in unseen images simultaneously . In this way we need to focus on just the shape matching algorithm without considering the variant scale of the object in an image . This is a procedure which absolutely differs from voting and sliding window searching . We do experiments on ETHZ shape dataset Weizmann horses dataset and the bottle subset from PASCAL datasets . The results confirm that the present model of object detection based on CSC outperforms state of the art of shape based detection methods . 
",We design a scale invariant shape descriptor for shape matching and object detection. A graph based segment matching algorithm is introduced. Accurate boundary of object can be detected in natural image. Compute the similarity between descriptors properly in clutter background,,,
S0262885616300038," An analysis of the relative motion and point feature model configurations leading to solution degeneracy is presented for the case of a Simultaneous Localization and Mapping system using multicamera clusters with non overlapping fields of view . The SLAM optimization system seeks to minimize image space reprojection error and is formulated for a cluster containing any number of component cameras observing any number of point features over two keyframes . The measurement Jacobian is transformed to expose a reduced dimension representation such that the degeneracy of the system can be determined by the rank of a dense submatrix . A set of relative motions sufficient for degeneracy are identified for certain cluster configurations independent of target model geometry . Furthermore it is shown that increasing the number of cameras within the cluster and observing features across different cameras over the two keyframes reduces the size of the degenerate motion sets significantly . 
",Analysis of solution degeneracy in general multicamera cluster SLAM. Decomposition of system determines conditions for degeneracy. Dense sub matrix reveals geometric structure for futher analysis. Identification of degenerate motions independent of feature constellation. Experiments demonstrate that adding features and cameras decreases degenerate set.,,,
S0262885615001390," The article describes a reconstruction pipeline that generates piecewise planar models of man made environments using two calibrated views . The 3D space is sampled by a set of virtual cut planes that intersect the baseline of the stereo rig and implicitly define possible pixel correspondences across views . The likelihood of these correspondences being true matches is measured using signal symmetry analysis which enables to obtain profile contours of the 3D scene that become lines whenever the virtual cut planes intersect planar surfaces . The detection and estimation of these lines cuts is formulated as a global optimization problem over the symmetry matching cost and pairs of reconstructed lines are used to generate plane hypotheses that serve as input to PEARL clustering . The PEARL algorithm alternates between a discrete optimization step which merges planar surface hypotheses and discards detections with poor support and a continuous optimization step which refines the plane poses taking into account surface slant . The pipeline outputs an accurate semi dense Piecewise Planar Reconstruction of the 3D scene . In addition the input images can be segmented into piecewise planar regions using a standard labeling formulation for assigning pixels to plane detections . Extensive experiments with both indoor and outdoor stereo pairs show significant improvements over state of the art methods with respect to accuracy and robustness . 
",A piecewise planar reconstruction pipeline is proposed. Line cuts are detected based on signal symmetry analysis. SymStereo and PEARL are combined for carrying the 3D reconstruction and plane fitting simultaneously. Experiments show significant improvements over state of the art methods.,,,
S0262885614001309," Fully automatic annotation of tennis game using broadcast video is a task with a great potential but with enormous challenges . In this paper we describe our approach to this task which integrates computer vision machine listening and machine learning . At the low level processing we improve upon our previously proposed state of the art tennis ball tracking algorithm and employ audio signal processing techniques to detect key events and construct features for classifying the events . At high level analysis we model event classification as a sequence labelling problem and investigate four machine learning techniques using simulated event sequences . Finally we evaluate our proposed approach on three real world tennis games and discuss the interplay between audio vision and learning . To the best of our knowledge our system is the only one that can annotate tennis game at such a detailed level . 
",Fully automatic annotation of real world tennis video State of the art tennis ball tracking algorithm An integration of computer vision machine listening and machine learning The only system that can annotate tennis game at such a detailed level,,,
S0262885616300579," Feature pooling is a key component in modern visual classification system . However the conventional two prevailing pooling techniques namely average and max poolings are not theoretically optimal due to the unrecoverable loss of the spatial information during the statistical summarization and the underlying over simplified assumption about the feature distribution . Addressing these issues this paper proposes to generalize previous pooling methods toward a weighted norm spatial pooling function tailored for class specific feature spatial distribution . Optimizing such a pooling function toward discriminative class separability that is subject to a spatial smoothness constraint yields a so called geometric norm pooling method . Furthermore to handle the variation of object scale position which would affect not only the learning of discriminative pooling weights but also the applicability of the learned weights we propose a simple yet effective self alignment step during both learning and testing to adaptively adjust the pooling weights for individual images . Image segmentation and visual saliency map are utilized to construct a directed pixel adjacency graph . The discriminative pooling weights are diffused using random walk on the constructed graph and therefore the discriminative pooling weights are propagated onto the salient and foreground region . This leads to a robust version of GLP which can cope with the misalignment of object position and scale in images . Comprehensive experiments validate the effectiveness of the proposed GLP feature pooling framework . The proposed random walk based self alignment step can effectively alleviate the image misalignment issue and further boost classification accuracy . State of the art image classification and action recognition performances are attained on several benchmarks . 
",Consider the spatial distribution information in feature pooling Handle the misalignment problems in image classification and action recognition Improve the discrimination of feature pooling in visual recognition tasks Conduct extensive experiments on visual recognition tasks,,,
S0262885614001449,"Representation of facial expressions using continuous dimensions has shown to be inherently more expressive and psychologically meaningful than using categorized emotions and thus has gained increasing attention over recent years. Many sub problems have arisen in this new field that remain only partially understood. A comparison of the regression performance of different texture and geometric features and the investigation of the correlations between continuous dimensional axes and basic categorized emotions are two of these. This paper presents empirical studies addressing these problems and it reports results from an evaluation of different methods for detecting spontaneous facial expressions within the arousal valence AV dimensional space. The evaluation compares the performance of texture features SIFT Gabor LBP against geometric features FAP based distances and the fusion of the two. It also compares the prediction of arousal and valence obtained using the best fusion method to the corresponding ground truths. Spatial distribution shift similarity and correlation are considered for the six basic categorized emotions i.e. anger disgust fear happiness sadness surprise . Using the NVIE database results show that the fusion of LBP and FAP features performs the best. The results from the NVIE and FEEDTUM databases reveal novel findings about the correlations of arousal and valence dimensions to each of six basic emotion categories. 
",Compared performance of texture SIFT Gabor LBP geometry and their fusions Investigated correlation of arousal and valence dimensions to six basic emotions Novel insights into categorized emotion recognition using dimensional axes,,,
S0306457313000952," This paper reports on an approach to the analysis of form during genre recognition recorded using eye tracking . The researchers focused on eight different types of e mail such as calls for papers newsletters and spam which were chosen to represent different genres . The study involved the collection of oculographic behavior data based on the scanpath duration and scanpath length based metric to highlight the ways in which people view the features of genres . We found that genre analysis based on purpose and form was an effective means of identifying the characteristics of these e mails . The research carried out on a group of 24 participants highlighted their interaction and interpretation of the e mail texts and the visual cues or features perceived . In addition the ocular strategies of scanning and skimming they employed for the processing of the texts by block genre and representation were evaluated . 
",We testify the significance of text structure during identification categorization task. We examine scanpaths of textual genres using temporal and distance based measures. We show how the form features of a genre aid in text interpretation and use. Format and layout plays an important role in human text categorization. We are looking for evidence of skimming and scanning.,,,
S0306457313000368," Named entity recognition is mostly formalized as a sequence labeling problem in which segments of named entities are represented by label sequences . Although a considerable effort has been made to investigate sophisticated features that encode textual characteristics of named entities little attention has been paid to segment representations for multi token named entities . In this paper we investigate the effects of different SRs on NER tasks and propose a feature generation method using multiple SRs . The proposed method allows a model to exploit not only highly discriminative features of complex SRs but also robust features of simple SRs against the data sparseness problem . Since it incorporates different SRs as feature functions of Conditional Random Fields we can use the well established procedure for training . In addition the tagging speed of a model integrating multiple SRs can be accelerated equivalent to that of a model using only the most complex SR of the integrated model . Experimental results demonstrate that incorporating multiple SRs into a single model improves the performance and the stability of NER . We also provide the detailed analysis of the results . 
",Different segmentation representations SRs cause little difference in performance. Different SRs result in quite different outputs. Incorporation of different SRs is beneficial to NER task. We proposed a new feature generation method that uses multiple SRs. The proposed method improves the performance and the stability of NER.,,,
S0262885615000086," Surface defects are important factors of surface quality of industrial products . Most of the traditional machine vision based methods for surface defect recognition have some shortcomings such as low detection rate of defects and high rate of false alarms . Different types of defects have special information at some directions and scales of their images while the traditional methods of feature extraction such as Wavelet transform are unable to get the information at all directions . In this study Shearlet transform is introduced to provide efficient multi scale directional representation and a general framework has been developed to analyze and represent surface defect images with anisotropic information . The metal surface images captured from production lines are decomposed into multiple directional subbands with Shearlet transform and features are extracted from all subbands and combined into a high dimensional feature vector . Kernel Locality Preserving Projection is applied to the dimension reduction of the feature vector . The proposed method is tested with the surface images captured from different production lines and the results show that the classification rates of surface defects of continuous casting slabs hot rolled steels and aluminum sheets are 94.35 95.75 and 92.5 respectively . 
",We developed a method called DST KLPP which is effective in classification of surface defects of different metals. DST KLPP was tested with samples of three typical metals including slabs hot rolled steels and aluminum sheets. DST KLPP provides higher classification rates than other methods including Wavelet Curvelet and Contourlet transform. DST KLPP can recognize defects from complex backgrounds efficiently. DST KLPP can recognize tiny defects from low contrast images availably.,,,
S0306457313000733," Knowledge organization and bibliometrics have traditionally been seen as separate subfields of library and information science but bibliometric techniques make it possible to identify candidate terms for thesauri and to organize knowledge by relating scientific papers and authors to each other and thereby indicating kinds of relatedness and semantic distance . It is therefore important to view bibliometric techniques as a family of approaches to KO in order to illustrate their relative strengths and weaknesses . The subfield of bibliometrics concerned with citation analysis forms a distinct approach to KO which is characterized by its social historical and dynamic nature its close dependence on scholarly literature and its explicit kind of literary warrant . The two main methods co citation analysis and bibliographic coupling represent different things and thus neither can be considered superior for all purposes . The main difference between traditional knowledge organization systems and maps based on citation analysis is that the first group represents intellectual KOSs whereas the second represents social KOSs . For this reason bibliometric maps can not be expected ever to be fully equivalent to scholarly taxonomies but they are along with other forms of KOSs valuable tools for assisting users to orient themselves to the information ecology . Like other KOSs citation based maps can not be neutral but will always be based on researchers decisions which tend to favor certain interests and views at the expense of others . 
",Citation analytic methods as approaches to knowledge organization. Semantic relations between citing and cited documents. The distinction between intellectual and social organizations of knowledge. Citation based maps as social organizations. Positivistic versus pragmatic approaches in bibliometrics.,,,
S0262885615001341," In this paper we present our solution to the 300 Faces in the Wild Facial Landmark Localization Challenge . We demonstrate how to achieve very competitive localization performance with a simple deep learning based system . Human study is conducted to show that the accuracy of our system has been very close to human performance . We discuss how this finding would affect our future direction to improve our system . 
",We show how to achieve state of the art facial landmark localization by CNN. The system s performance is improved by deeper network. We show our system s performance is close to human.,,,
S0306457313000940," The estimation of query model is an important task in language modeling approaches to information retrieval . The ideal estimation is expected to be not only effective in terms of high mean retrieval performance over all queries but also stable in terms of low variance of retrieval performance across different queries . In practice however improving effectiveness can sacrifice stability and vice versa . In this paper we propose to study this tradeoff from a new perspective i.e . the bias variance tradeoff which is a fundamental theory in statistics . We formulate the notion of bias variance regarding retrieval performance and estimation quality of query models . We then investigate several estimated query models by analyzing when and why the bias variance tradeoff will occur and how the bias and variance can be reduced simultaneously . A series of experiments on four TREC collections have been conducted to systematically evaluate our bias variance analysis . Our approach and results will potentially form an analysis framework and a novel evaluation strategy for query language modeling . 
",We study the retrieval effectiveness stability tradeoff in query model estimation. This tradeoff is investigated through a novel angle i.e. bias variance tradeoff. We formulate the performance bias variance and estimation bias variance. We investigate various query estimation methods using bias variance analysis. Experiments have been conducted to verify hypotheses on bias variance analysis.,,,
S0262885615001110," In this work we deal with the problem of high level event detection in video . Specifically we study the challenging problems of i learning to detect video events from solely a textual description of the event without using any positive video examples and ii additionally exploiting very few positive training samples together with a small number of related videos . For learning only from an event s textual description we first identify a general learning framework and then study the impact of different design choices for various stages of this framework . For additionally learning from example videos when true positive training samples are scarce we employ an extension of the Support Vector Machine that allows us to exploit related event videos by automatically introducing different weights for subsets of the videos in the overall training set . Experimental evaluations performed on the large scale TRECVID MED 2014 video dataset provide insight on the effectiveness of the proposed methods . 
",We deal with the challenging problem of high level event detection in video. We build event detectors based solely on textual descriptions of the event classes. We also learn event detectors from very few positive and related training samples. We present results and comparisons on a large scale TRECVID MED video dataset.,,,
S0306457314000120," Social networking sites enable user to personalize their contents and functions . This feature has been assumed as causing positive effects on the use of online information services through enhancing user satisfaction . However unlike other online information services due to the results of personalization in a certain situation SNS users can not help using the SNS even though they feel dissatisfaction on using it . SNSs are different from other information services in the sense that they create and sustain their own value based on the number of participating members . In SNSs personalization reflected by updates and maintenance of profile pages results in such participation . This study hypothesizes that personalization influences on the continued use of SNSs through two factors switching cost and satisfaction . Web based survey was conducted with the samples of 677 SNS users from six universities in the US . In person interviews were conducted with 25 university students to elicit their thoughts on the SNSs . Quantitative analysis employed by testing the proposed model with five hypotheses through a structural equation modeling technique . The transcribed interview data was analyzed following the constant comparative technique . The main findings indicate that as expected the personalization increases its switching cost as well as satisfaction which results in further use of SNSs . These findings suggest that it is necessary to consider both extrinsic and intrinsic factors of user perceptions when adding personalization features on SNSs . 
",Personalization in SNS increases its switching cost and satisfaction. Switching cost and satisfaction affect user continuance of SNSs. SNS users cannot easily switch to other SNS because of time and efforts invested. Social exchange theory provides a useful framework and proves its feasibility. A new insight on research concerning participatory networks such as SNSs.,,,
S0262885614001139," Given a surveillance video of a moving person we present a novel method of estimating layout of a cluttered indoor scene . We propose an idea that trajectories of a moving person can be used to generate features to segment an indoor scene into different areas of interest . We assume a static uncalibrated camera . Using pixel level color and perspective cues of the scene each pixel is assigned to a particular class either a sitting place the ground floor or the static background areas like walls and ceiling . The pixel level cues are locally integrated along global topological order of classes such as sitting objects and background areas are above ground floor into a conditional random field by an ordering constraint . The proposed method yields very accurate segmentation results on challenging real world scenes . We focus on videos with people walking in the scene and show the effectiveness of our approach through quantitative and qualitative results . The proposed estimation method shows better estimation results as compared to the state of the art scene layout estimation methods . We are able to correctly segment 90.3 of background 89.4 of sitting areas and 74.7 of the ground floor . 
",Scene layout estimation using both trajectory data and image features No assumptions are considered about the structure of the scene. Efficient estimation of the scene structure in the presence of scene clutter We show that using line segments we obtain better surface normal. Our data and segmentation results will be publicly available for comparison.,,,
S0262885615000682," Some human detection or tracking algorithms output a low dimensional representation of the human body such as a bounding box . Even though this representation is enough for some tasks a more accurate and detailed point wise representation of the human body is more useful for pose estimation and action recognition . The refinement process can produce a point wise mask of the human body from its low dimensional representation . In this paper we tackle the problem of refining low dimensional human shapes using RGB D data with a novel and accurate method for this purpose . This algorithm combines low level cues such as shape and color and high level observations such as the estimated ground plane in a multi layer graph cut framework . In our algorithm shape prior information is learned by training a classifier . Unlike some existing work our method does not utilize or carry features from the internal steps of the methods which provide the bounding box so our method can work on the outputs of any similar shape providers . Extensive experiments demonstrate that the proposed technique significantly outperforms other suitable methods . Moreover a previously published refinement method is extended by incorporating more generic cues to serve this purpose . 
",A novel and accurate method to refine low dimensional human shape using RGB D data is proposed. Uses of multiple modalities do not carry any features from the shape provider. Combines low and high level observations jointly in multi layer graph structure Extensive experiments showed that it outperforms compared suitable algorithms. Also an existing method is extended by fusing more generic cues for this purpose.,,,
S0306457314000375," Research in natural language processing has increasingly focused on normalizing Twitter messages . Currently while different well defined approaches have been proposed for the English language the problem remains far from being solved for other languages such as Malay . Thus in this paper we propose an approach to normalize the Malay Twitter messages based on corpus driven analysis . An architecture for Malay Tweet normalization is presented which comprises seven main modules enhanced tokenization In Vocabulary detection specialized dictionary query repeated letter elimination abbreviation adjusting English word translation and de tokenization . A parallel Tweet dataset consisting of 9000 Malay Tweets is used in the development and testing stages . To measure the performance of the system an evaluation is carried out . The result is promising whereby we score 0.83 in BLEU against the baseline BLEU which scores 0.46 . To compare the accuracy of the architecture with other statistical approaches an SMT like normalization system is implemented trained and evaluated with an identical parallel dataset . The experimental results demonstrate that we achieve higher accuracy by the normalization system which is designed based on the features of Malay Tweets compared to the SMT like system . 
",To observe features of Malay Tweets three distinct corpus based analyses are done. A rule based architecture is developed based on results of the analyses. The architecture consists of seven distinct modules in a pipeline structure. Experimental results indicate high accuracy in term of BLEU score. The architecture outperforms SMT like normalization approach.,,,
S0306457314000351," This paper describes the use of Wikipedia as a rich knowledge source for a question answering system . We suggest multiple answer matching modules based on different types of semi structured knowledge sources of Wikipedia including article content infoboxes article structure category structure and definitions . These semi structured knowledge sources each have their unique strengths in finding answers for specific question types such as infoboxes for factoid questions category structure for list questions and definitions for descriptive questions . The answers extracted from multiple modules are merged using an answer merging strategy that reflects the specialized nature of the answer matching modules . Through an experiment our system showed promising results with a precision of 87.1 a recall of 52.7 and an F measure of 65.6 all of which are much higher than the results of a simple text analysis based system . 
",The use of Wikipedia as a knowledge source for question answering system. Wikipedia article content structure infobox category and definition are used. Each knowledge source has its unique strength for certain question types. Answer merging strategy for multiple answer matching modules.,,,
S0262885616300063,"Nowadays with so many surveillance cameras having been installed the market demand for intelligent violence detection is continuously growing while it is still a challenging topic in research area. Therefore we attempt to make some improvements of existing violence detectors. The primary contributions of this paper are two fold. Firstly a novel feature extraction method named Oriented VIolent Flows OViF which takes full advantage of the motion magnitude change information in statistical motion orientations is proposed for practical violence detection in videos. The comparison of OViF and baseline approaches on two public databases demonstrates the efficiency of the proposed method. Secondly feature combination and multi classifier combination strategies are adopted and excellent results are obtained. Experimental results show that using combined features with AdaBoost Linear SVM achieves improved performance over the state of the art on the Violent Flows benchmark. 
",A novel feature named OViF for violence detection. OViF makes full use of the orientation information of optical flows compared to ViF. We use AdaBoost as a feature selector and Linear SVM as a classifier. Experiments show that ViF OViF obtain the state of the art violence detection performance when using AdaBoost with SVM.,,,
S0306457313000939," Extensible Markup Language documents are associated with time in two ways XML documents evolve over time and XML documents contain temporal information . The efficient management of the temporal and multi versioned XML documents requires optimized use of storage and efficient processing of complex historical queries . This paper provides a comparative analysis of the various schemes available to efficiently store and query the temporal and multi versioned XML documents based on temporal change management versioning and querying support . Firstly the paper studies the multi versioning control schemes to detect manage and query change in dynamic XML documents . Secondly it describes the storage structures used to efficiently store and retrieve XML documents . Thirdly it provides a comparative analysis of the various commercial tools based on change management versioning collaborative editing and validation support . Finally the paper presents some future research and development directions for the multi versioned XML documents . 
",XML documents contain temporal information and evolve over time. Our work explores change detection versioning and querying support in XML documents. It describes the storage structures to efficiently store and retrieve XML documents. It explains the change management features in some commercial tools. It highlights several open research issues for multi versioned XML documents.,,,
S0306457314000302," Collaborative information retrieval involves retrieval settings in which a group of users collaborates to satisfy the same underlying need . One core issue of collaborative IR models involves either supporting collaboration with adapted tools or developing IR models for a multiple user context and providing a ranked list of documents adapted for each collaborator . In this paper we introduce the first document ranking model supporting collaboration between two users characterized by roles relying on different domain expertise levels . Specifically we propose a two step ranking model we first compute a document relevance score taking into consideration domain expertise based roles . We introduce specificity and novelty factors into language model smoothing and then we assign via an Expectation Maximization algorithm documents to the best suited collaborator . Our experiments employ a simulation based framework of collaborative information retrieval and show the significant effectiveness of our model at different search levels . 
",A collaborative ranking model involving users with different domain expertise levels. EM based learning method for document allocation. Simulation based evaluation with effective results at session and user roles levels.,,,
S0262885614001358,"This paper introduces an adaptive visual tracking method that combines the adaptive appearance model and the optimization capability of the Markov decision process. Most tracking algorithms are limited due to variations in object appearance from changes in illumination viewing angle object scale and object shape. This paper is motivated by the fact that tracking performance degradation is caused not only by changes in object appearance but also by the inflexible controls of tracker parameters. To the best of our knowledge optimization of tracker parameters has not been thoroughly investigated even though it critically influences tracking performance. The challenge is to equip an adaptive tracking algorithm with an optimization capability for a more flexible and robust appearance model. In this paper the Markov decision process which has been applied successfully in many dynamic systems is employed to optimize an adaptive appearance model based tracking algorithm. The adaptive visual tracking is formulated as a Markov decision process based dynamic parameter optimization problem with uncertain and incomplete information. The high computation requirements of the Markov decision process formulation are solved by the proposed prioritized Q learning approach. We carried out extensive experiments using realistic video sets and achieved very encouraging and competitive results. 
",We use an MDP formulation for optimal adaptation of tracking algorithms. We optimize the tracker control parameters using prioritized Q learning. The proposed prioritized Q learning approach is based on sensitivity analysis. The performance of our method is superior to other approaches. The proposed method can balance tracking accuracy and speed.,,,
S0262885616300312," Image classification is to assign a category of an image and image annotation is to describe individual components of an image by using some annotation terms . These two learning tasks are strongly related . The main contribution of this paper is to propose a new discriminative and sparse topic model for image classification and annotation by combining visual annotation and label information from a set of training images . The essential features of DSTM different from existing approaches are that the label information is enforced in the generation of both visual words and annotation terms such that each generative latent topic corresponds to a category the zero mean Laplace distribution is employed to give a sparse representation of images in visual words and annotation terms such that relevant words and terms are associated with latent topics . Experimental results demonstrate that the proposed method provides the discrimination ability in classification and annotation and its performance is better than the other testing methods for LabelMe UIUC NUS WIDE and PascalVOC07 images . 
",The label information is enforced in the generation of visual and annotation terms. The zero mean Laplace distribution is added to a topic generative process. The sparse image representation is helpful to learn a training model. A series of experiments on four data sets demonstrate the performance of DSTM.,,,
S0306457313001106," With ever increasing information being available to the end users search engines have become the most powerful tools for obtaining useful information scattered on the Web . However it is very common that even most renowned search engines return result sets with not so useful pages to the user . Research on semantic search aims to improve traditional information search and retrieval methods where the basic relevance criteria rely primarily on the presence of query keywords within the returned pages . This work is an attempt to explore different relevancy ranking approaches based on semantics which are considered appropriate for the retrieval of relevant information . In this paper various pilot projects and their corresponding outcomes have been investigated based on methodologies adopted and their most distinctive characteristics towards ranking . An overview of selected approaches and their comparison by means of the classification criteria has been presented . With the help of this comparison some common concepts and outstanding features have been identified . 
",An exhaustive review of ranking approaches for semantic search on Web. We identified three stages of ranking in semantic search process. We identified criteria for the comparison of semantic search approaches. We examined some open issues relevant to efficient semantic search.,,,
S0306457314000399," This article describes in depth research on machine learning methods for sentiment analysis of Czech social media . Whereas in English Chinese or Spanish this field has a long history and evaluation datasets for various domains are widely available in the case of the Czech language no systematic research has yet been conducted . We tackle this issue and establish a common ground for further research by providing a large human annotated Czech social media corpus . Furthermore we evaluate state of the art supervised machine learning methods for sentiment analysis . We explore different pre processing techniques and employ various features and classifiers . We also experiment with five different feature selection algorithms and investigate the influence of named entity recognition and preprocessing on sentiment classification performance . Moreover in addition to our newly created social media dataset we also report results for other popular domains such as movie and product reviews . We believe that this article will not only extend the current sentiment analysis research to another family of languages but will also encourage competition potentially leading to the production of high end commercial solutions . 
",We explore state of the art supervised machine learning methods for sentiment analysis of Czech social media. We provide a large human annotated Czech social media corpus. We explore different pre processing techniques and employ various features and classifiers. We experiment with five different feature selection algorithms. Results are also reported on other widely popular domains such as movie and product reviews.,,,
S0262885614001413," In this paper we present an accelerated system for segmenting flower images based on graph cut technique which formulates the segmentation problem as an energy function minimization. The contribution of this paper consists to propose an improvement of the classical used energy function which is composed of a data consistent term and a boundary term. For this we integrate an additional data consistent term based on the spatial prior and we add gradient information in the boundary term. Then we propose an automated coarse to fine segmentation method composed mainly of two levels coarse segmentation and fine segmentation. First the coarse segmentation level is based on minimizing the proposed energy function. Then the fine segmentation is done by optimizing the energy function through the standard graph cut technique. Experiments were performed on a subset of Oxford flower database and the obtained results are compared to the reimplemented method of Nilsback et al. 1 . The evaluation shows that our method consumes less CPU time and it has a satisfactory accuracy compared with the mentioned method above 1 . 
",An improvement of the classical energy function graph cut is proposed. Integration of spatial priori and gradient information improves segmentation result. A new coarse to fine flower segmentation method is presented and implemented.,,,
S0262885615000748,"Semi supervised sparse feature selection which can exploit the large number unlabeled data and small number labeled data simultaneously has placed an important role in web image annotation. However most of the semi supervised sparse feature selection methods are developed for single view data and these methods cannot naturally deal with the multi view data though it has shown that leveraging information contained in multiple views can dramatically improve the feature selection performance. Recently multi view learning has obtained much research attention because it can reveal and leverage the correlated and complementary information between different views. So in this paper we apply multi view learning into semi supervised sparse feature selection and propose a semi supervised sparse feature selection method based on multi view Laplacian regularization namely multi view Laplacian sparse feature selection MLSFS .1 MLSFS utilizes multi view Laplacian regularization to boost semi supervised sparse feature selection performance. A simple iterative method is proposed to solve the objective function of MLSFS. We apply MLSFS algorithm into image annotation task and conduct experiments on two web image datasets. The experimental results show that the proposed MLSFS outperforms the state of art single view sparse feature selection methods. 
",Multi view Laplacian sparse feature selection MLSFS algorithm is proposed. Multi view learning is utilized to exploit the complementation of different views features. A effective iterative algorithm is introduced to optimize the objective function. The convergence of the algorithm is proven. Experiments demonstrate MLSFS has good performance of feature selection.,,,
S0306457313000745," We present PubSearch a hybrid heuristic scheme for re ranking academic papers retrieved from standard digital libraries such as the ACM Portal . The scheme is based on the hierarchical combination of a custom implementation of the term frequency heuristic a time depreciated citation score and a graph theoretic computed score that relates the paper s index terms with each other . We designed and developed a meta search engine that submits user queries to standard digital repositories of academic publications and re ranks the repository results using the hierarchical heuristic scheme . We evaluate our proposed re ranking scheme via user feedback against the results of ACM Portal on a total of 58 different user queries specified from 15 different users . The results show that our proposed scheme significantly outperforms ACM Portal in terms of retrieval precision as measured by most common metrics in Information Retrieval including Normalized Discounted Cumulative Gain Expected Reciprocal Rank as well as a newly introduced lexicographic rule of ranking search results . In particular PubSearch outperforms ACM Portal by more than 77 in terms of ERR by more than 11 in terms of NDCG and by more than 907.5 in terms of LEX . We also re rank the top 10 results of a subset of the original 58 user queries produced by Google Scholar Microsoft Academic Search and ArnetMiner the results show that PubSearch compares very well against these search engines as well . The proposed scheme can be easily plugged in any existing search engine for retrieval of academic publications . 
",We present PubSearch a new system for academic search and retrieval. PubSearch is based on a cascade hierarchy of three heuristics. The heuristics include Term Frequency citation distribution and topics inter relations. We compare PubSearch performance against ACM Portal on 58 user queries. The system outperforms ACM Portal in terms of ERR NDCG LEX metric by large margin.,,,
S0262885615000323,"Sudden illumination changes are a fundamental problem in background modeling applications. Most strategies to solve it are based on determining the particular form of the color transformation which the pixels undergo when an illumination change occurs. Here we present an approach which does not assume any specific form of the color transformation. It is based on a quantitative assessment of the smoothness of the local color transformation from one frame to the background model. In addition to this an assessment of the obtained illumination states of the pixels is carried out with the help of fuzzy logic. Experimental results are presented which demonstrate the performance of our approach in a range of situations. 
",Illumination change detection is carried out without assuming a specific color transformation. The method is designed to improve existing standard foreground detection algorithms. Backup procedures are designed to recover from detection failures. The method is applicable to a wide range of varying illumination conditions.,,,
S0306457314000363," Practical classification problems often involve some kind of trade off between the decisions a classifier may take . Indeed it may be the case that decisions are not equally good or costly therefore it is important for the classifier to be able to predict the risk associated with each classification decision . Bayesian decision theory is a fundamental statistical approach to the problem of pattern classification . The objective is to quantify the trade off between various classification decisions using probability and the costs that accompany such decisions . Within this framework a loss function measures the rates of the costs and the risk in taking one decision over another . In this paper we give a formal justification for a decision function under the Bayesian decision framework that comprises the minimisation of Bayesian risk and an empirical decision function found by Domingos and Pazzani . This new decision function has a very intuitive geometrical interpretation that can be explored on a Cartesian plane . We use this graphical interpretation to analyse different approaches to find the best decision on four different Na ve Bayes classifiers Gaussian Bernoulli Multinomial and Poisson on different standard collections . We show that the graphical interpretation significantly improves the understanding of the models and opens new perspectives for new research studies . 
",A study of a conditional risk based on both variable and constant losses. A geometrical interpretation of the decision function of a classifier. An analysis of performance of different Na ve Bayes classifiers compared to SVM.,,,
S0262885615000141," In this paper a novel method is proposed for real world pose invariant face recognition from only a single image in a gallery . A 3D Facial Expression Generic Elastic Model is proposed to reconstruct a 3D model of each human face using only a single 2D frontal image . Then for each person in the database a Sparse Dictionary Matrix is created from all face poses by rotating the 3D reconstructed models and extracting features in the rotated face . Each SDM is subsequently rendered based on triplet angles of face poses . Before matching to SDM an initial estimate of triplet angles of face poses is obtained in the probe face image using an automatic head pose estimation approach . Then an array of the SDM is selected based on the estimated triplet angles for each subject . Finally the selected arrays from SDMs are compared with the probe image by sparse representation classification . Convincing results were acquired to handle pose changes on the FERET CMU PIE LFW and video face databases based on the proposed method compared to several state of the art in pose invariant face recognition . 
",We have presented a method for facial expression invariant face recognition. We propose a novel method for real world pose invariant face recognition. Proposed method use a single image in gallery with any facial expressions. We generate a sparse dictionary matrix for each people based on triplet pose angles. Convincing results were acquired to handle pose on the FERET CMU PIE and LFW databases.,,,
S0306457313000526," In this work we elaborate on the meaning of metadata quality by surveying efforts and experiences matured in the digital library domain . In particular an overview of the frameworks developed to characterize such a multi faceted concept is presented . Moreover the most common quality related problems affecting metadata both during the creation and the aggregation phase are discussed together with the approaches technologies and tools developed to mitigate them . This survey on digital library developments is expected to contribute to the ongoing discussion on data and metadata quality occurring in the emerging yet more general framework of data infrastructures . 
",A characterization of metadata quality. A discussion on the role of metadata quality in data infrastructures. A survey of metadata quality frameworks to assess metadata quality. An array of solutions to mitigate the effects of metadata quality issues. Directions for future works related to metadata quality in data infrastructures.,,,
S0262885614001395," The basic goal of scene understanding is to organize the video into sets of events and to find the associated temporal dependencies . Such systems aim to automatically interpret activities in the scene as well as detect unusual events that could be of particular interest such as traffic violations and unauthorized entry . The objective of this work therefore is to learn behaviors of multi agent actions and interactions in a semi supervised manner . Using tracked object trajectories we organize similar motion trajectories into clusters using the spectral clustering technique . This set of clusters depicts the different paths routes i.e . the distinct events taking place at various locations in the scene . A temporal mining algorithm is used to mine interval based frequent temporal patterns occurring in the scene . A temporal pattern indicates a set of events that are linked based on their relationship with other events in the set and we use Allen s interval based temporal logic to describe these relations . The resulting frequent patterns are used to generate temporal association rules which convey the semantic information contained in the scene . Our overall aim is to generate rules that govern the dynamics of the scene and perform anomaly detection . We apply the proposed approach on two publicly available complex traffic datasets and demonstrate considerable improvements over the existing techniques . 
",Uses temporal mining technique event recognition in dynamic scenes Temporal association rules are then generated from frequent patterns. These association rules help model the sequence cycle. Spatio temporal anomalies are identified and detected in a hierarchical manner.,,,
S0262885614001085," Gender and ethnicity are both key demographic attributes of human beings and they play a very fundamental and important role in automatic machine based face analysis therefore there has been increasing attention for face based gender and ethnicity classification in recent years . In this paper we present an effective and efficient approach on this issue by combining both boosted local texture and shape features extracted from 3D face models in contrast to the existing ones that only depend on either 2D texture or 3D shape of faces . In order to comprehensively represent the difference between different genders or ethnicities we propose a novel local descriptor namely local circular patterns . LCP improves the widely utilized local binary patterns and its variants by replacing the binary quantization with a clustering based one resulting in higher discriminative power as well as better robustness to noise . Meanwhile the following Adaboost based feature selection finds the most discriminative gender and race related features and assigns them with different weights to highlight their importance in classification which not only further raises the performance but reduces the time and memory cost as well . Experimental results achieved on the FRGC v2.0 and BU 3DFE datasets clearly demonstrate the advantages of the proposed method . 
",We investigate multi modal 2D 3D facial gender and ethnicity classification. We propose a novel local descriptor LCP to capture both 2D and 3D facial clues. LCP is more discriminative and more robust to noise than LBP like features. Combing both modalities improves results in gender and ethnicity classification. We achieve competitive performance compared to state of the art in such tasks.,,,
S0306457313000538," Multimedia objects can be retrieved using their context that can be for instance the text surrounding them in documents . This text may be either near or far from the searched objects . Our goal in this paper is to study the impact in term of effectiveness of text position relatively to searched objects . The multimedia objects we consider are described in structured documents such as XML ones . The document structure is therefore exploited to provide this text position in documents . Although structural information has been shown to be an effective source of evidence in textual information retrieval only a few works investigated its interest in multimedia retrieval . More precisely the task we are interested in this paper is to retrieve multimedia fragments . Our general approach is built on two steps we first retrieve XML elements containing multimedia objects and we then explore the surrounding information to retrieve relevant multimedia fragments . In both cases we study the impact of the surrounding information using the documents structure . Our work is carried out on images but it can be extended to any other media since the physical content of multimedia objects is not used . We conducted several experiments in the context of the Multimedia track of the INEX evaluation campaign . Results showed that structural evidences are of high interest to tune the importance of textual context for multimedia retrieval . Moreover the proposed approach outperforms state of the art approaches . 
",Multimedia objects retrieved by their textual context. Structural factors help to identify relevant textual parts. Multimedia elements are useful to retrieve multimedia fragments. Specific multimedia retrieval system outperforms classical XML retrieval systems.,,,
S0262885614001334,"This paper proposes a novel approach to recognize object and scene categories in depth images. We introduce a Bag of Words BoW representation in 3D the Selective 3D Spatial Pyramid Matching Kernel 3DSPMK . It starts quantizing 3D local descriptors computed from point clouds to build a vocabulary of 3D visual words. This codebook is used to build the 3DSPMK which starts partitioning a working volume into fine sub volumes and computing a hierarchical weighted sum of histogram intersections of visual words at each level of the 3D pyramid structure. With the aim of increasing both the classification accuracy and the computational efficiency of the kernel we propose two selective hierarchical volume decomposition strategies based on representative and discriminative sub volume selection processes which drastically reduce the pyramid to consider. Results on different RGBD datasets show that our approaches obtain state of the art results for both object recognition and scene categorization. 
",We introduce the 3DSPMK for object and scene recognition in depth images. Our model repeatedly subdivides a cube inscribed in the point cloud. Then a weighted sum of histogram of visual word occurrences is computed. Results on publicly available benchmarks have been reported.,,,
S0262885614001863," Researchers have recently been performing region of interest detection in such applications as object recognition object segmentation and adaptive coding . In this paper a novel region of interest detection model that is based on visually salient regions is introduced by utilizing the frequency and space domain features in very high resolution remote sensing images . First the frequency domain features that are based on a multi scale spectrum residual algorithm are extracted to yield intensity features . Next we extract the color and orientation features by generating space dynamic pyramids . Then spectral features are obtained by analyzing spectral information content . In addition a multi scale feature fusion method is proposed to generate a saliency map . Finally the detected visual saliency regions are described using adaptive threshold segmentation . Compared with existing models our model eliminates the background information effectively and highlights the salient detected results with well defined boundaries and shapes . Moreover an experimental evaluation indicates promising results from our model with respect to the accuracy of detection results . 
",Novel multi scale frequency analysis is proposed for intensity feature analysis. Multi scale analysis is introduced to extract color and orientation features. Spectral information content is proposed for spectral feature analysis. Propose a novel weight multi scale feature fusion method Synthesize frequency and space domain features in proposed algorithm frame.,,,
S0262885615000165," We propose a method to produce near laser scan quality 3 D face models of a freely moving user with a low cost low resolution range sensor in real time . Our approach does not require any prior knowledge about the geometry of a face and can produce faithful geometric models of any star shaped object . We use a cylindrical representation which enables us to efficiently process the 3 D mesh by applying 2 D filters . We use the first frame as a reference and incrementally build the model by registering each subsequent cloud of 3 D points to the reference using the ICP algorithm implemented on a GPU . The registered point clouds are merged into a single image through a cylindrical representation . The noise from the sensor and from the pose estimation error is removed with a temporal integration and a spatial smoothing of the successively incremented model . To validate our approach we quantitatively compare our model to laser scans and show comparable accuracy . This paper extends the method presented in . 
",We infer a very accurate 3 D face model for a freely moving user from a single depth camera. Using unwrapped cylindrical 2 D images enables us to use simple 2 D image processing algorithms to process the 3 D information. We use a combination of spatial smoothing and temporal integration for noise removal. A robust rejection method produces reliable results in the presence of facial expression changes and partial occlusions. Our system runs online and in real time.,,,
S0306457314000272," Recently sentiment classification has received considerable attention within the natural language processing research community . However since most recent works regarding sentiment classification have been done in the English language there are accordingly not enough sentiment resources in other languages . Manual construction of reliable sentiment resources is a very difficult and time consuming task . Cross lingual sentiment classification aims to utilize annotated sentiment resources in one language for sentiment classification of text documents in another language . Most existing research works rely on automatic machine translation services to directly project information from one language to another . However different term distribution between original and translated text documents and translation errors are two main problems faced in the case of using only machine translation . To overcome these problems we propose a novel learning model based on active learning and semi supervised co training to incorporate unlabelled data from the target language into the learning process in a bi view framework . This model attempts to enrich training data by adding the most confident automatically labelled examples as well as a few of the most informative manually labelled examples from unlabelled data in an iterative process . Further in this model we consider the density of unlabelled data so as to select more representative unlabelled examples in order to avoid outlier selection in active learning . The proposed model was applied to book review datasets in three different languages . Experiments showed that our model can effectively improve the cross lingual sentiment classification performance and reduce labelling efforts in comparison with some baseline methods . 
",We combine active and semi supervised learning for cross lingual sentiment classification. Density analysis of unlabeled data is used in active learning. We test our proposed model on three different languages. This model reduce manual labelling efforts in cross lingual sentiment classification. Results show that incorporating density analysis can speed up learning process.,,,
S0306457313001131," The volume of entity centric structured data grows rapidly on the Web . The description of an entity composed of property value pairs has become very large in many applications . To avoid information overload efforts have been made to automatically select a limited number of features to be shown to the user based on certain criteria which is called automatic entity summarization . However to the best of our knowledge there is a lack of extensive studies on how humans rank and select features in practice which can provide empirical support and inspire future research . In this article we present a large scale statistical analysis of the descriptions of entities provided by DBpedia and the abstracts of their corresponding Wikipedia articles to empirically study along several different dimensions which kinds of features are preferable when humans summarize . Implications for automatic entity summarization are drawn from the findings . 
",We empirically study how Wikipedians summarize entity descriptions in practice. We compare entity descriptions in DBpedia with their Wikipedia abstracts. We analyze the length of a summary and the priorities of property values. We analyze the priorities of diversity of and correlation between properties. Implications for automatic entity summarization are drawn from the findings.,,,
S0262885615000761,"In this paper we propose a novel contactless palmprint authentication system where the system uses a CCD camera to capture the user s hand at a distance without any restrictions and touching the device. Furthermore a novel and high performance region of interest ROI extraction method which makes use of nonlinear regression and palm model to extract the ROIs with high success is proposed. Comparative results indicate that the proposed ROI extraction method gives superior performance as compared to the previously proposed point based approaches. To show the performance of the proposed system a novel contactless database has also been created. This database includes images captured from the users who present their hands with various hand positions and orientations in cluttered backgrounds. Furthermore experiments show that the proposed system has achieved a recognition rate of 99.488 and equal error rate of 0.277 on the contactless database of 145 people containing 1752 hand images. 
",A novel unrestricted contactless palmprint image acquisition device is developed. A novel model based ROI extraction method robust to the small errors is proposed. A contactless palmprint DB is constituted by collecting 1752 images from 145 subjects. The images in DB have pose rotation position variations and cluttered backgrounds. The results achieved by the proposed system are encouraging 0.277 EER and 99.488 RR.,,,
S0306457313000708," Uncertainty is an important idea in information retrieval research but the concept has yet to be fully elaborated and explored . Common assumptions about uncertainty are that it is a negative state and that it will be reduced through information search and retrieval . Research in the domain of uncertainty in illness however has demonstrated that uncertainty is a complex phenomenon that shares a complicated relationship with information . Past research on people living with HIV and individuals who have tested positive for genetic risk for different illnesses has revealed that information and the reduction of uncertainty can in fact produce anxiety and that maintaining uncertainty can be associated with optimism and hope . We review the theory of communication and uncertainty management and offer nine principles based on that theoretical work that can be used to influence IR system design . The principles reflect a view of uncertainty as a multi faceted and dynamic experience one subject to ongoing appraisal and management efforts that include interaction with and use of information in a variety of forms . 
",We review the prevailing discourse around the concept of uncertainty. We describe a communication based theory of uncertainty management. We explore how this theory can inform the design of information retrieval systems.,,,
S0262885616300075," In many wide area surveillance applications tracking objects is usually accomplished by using network of cameras . A common approach to any multi objects tracking algorithm in a network of cameras comprises of two main steps . First the movement trajectory of each object within the field of view of a camera is extracted and is called object tracklet . Then the set of tracklets are used to determine the persistent trace of each object . In this paper we assume that the tracklets are extracted by a conventional tracking algorithm . The occurrence of occlusion between objects within the viewing scene leads to various types of errors on the extracted tracklets . If these erroneous tracklets are used in a multi object tracking algorithm and ignoring the correction phase then the errors are propagated and affect the results of tracking algorithm . Therefore the true tracklets have to be estimated from the erroneous tracklets . In this paper we propose a variational model for estimating the true tracklets . The variational principle proposed in this model is established by first introducing a variational energy function . Then the erroneous tracklets are used to estimate the true tracklets through optimizing the energy function . The proposed method is evaluated on two well known datasets and a synthetic dataset which is particularly developed to demonstrate the performance of our algorithm under challenging scenarios . The 10 common metrics which are used in other multi objects tracking applications are used for quantitative evaluations . Our experimental results illustrate that our proposed model estimates the true tracklets which improves the overall association performances . 
",Preprocessing of the extracted tracklets before extracting the persistent track An intermediate step on multi object multi camera tracking problem Variational data association model for estimating true tracklets Extracting the least possible true tracklets from erroneous tracklets,,,
S0262885614001784," Finger vein identification is a new biometric identification technology . While many existing works approach the problem by using shape matching which is the generative method in this paper we introduce a joint discriminative and generative algorithm for the task . Our method considers both the discriminative appearance of local image patches as well as their generative spatial layout . The method is based on the popular vocabulary tree model where we utilize the hidden leaf node layer to calculate a generative confidence to weight the discriminative vote from the leaf node . The training process remains the same as building a conventional vocabulary tree while the prediction process utilizes a proposed point set matching method to support non parametric patch layout matching . In this way the entire model retains the efficiency of the vocabulary tree model which is much lighter than other similar models such as the constellation model . The overall estimation follows the Bayesian theory . Experimental results show that our proposed joint model outperformed the purely generative or discriminative counterpart and can offer competitive performance than existing methods for both the vein authentication and recognition tasks . 
",A joint discriminative and generative vocabulary tree model A non parametric image patch layout model and matching method Evaluation in vein authentication and recognition tasks with good performance Comprehensive in depth discussion of the technique,,,
S0306457313001015," In this paper we propose an optimization framework to retrieve an optimal group of experts to perform a multi aspect task . While a diverse set of skills are needed to perform a multi aspect task the group of assigned experts should be able to collectively cover all these required skills . We consider three types of multi aspect expert group formation problems and propose a unified framework to solve these problems accurately and efficiently . The first problem is concerned with finding the top k experts for a given task while the required skills of the task are implicitly described . In the second problem the required skills of the tasks are explicitly described using some keywords but each expert has a limited capacity to perform these tasks and therefore should be assigned to a limited number of them . Finally the third problem is the combination of the first and the second problems . Our proposed optimization framework is based on the Facility Location Analysis which is a well known branch of the Operation Research . In our experiments we compare the accuracy and efficiency of the proposed framework with the state of the art approaches for the group formation problems . The experiment results show the effectiveness of our proposed methods in comparison with state of the art approaches . 
",Using facility location analysis as a well known branch of Operation Research to solve an applied problem in information retrieval. Proposing a unified framework to solve three types of problems for expert group formation. Testing our framework on a real test collection. Proof of optimality of the solution.,,,
S0306457313000782," The study explores the relationship between value attribution and information source use of 17 Chinese business managers during their knowledge management strategic decision making . During semi structured interviews the Chinese business managers half in the telecommunications sector and half in the manufacturing sector were asked to rate 16 information sources on five point Likert Scales . The 16 information sources were grouped into internal external and personal impersonal types . The participants rated the information sources according to five value criteria relevancy comprehensiveness reliability time effort and accessibility . Open ended questions were also asked to get at how and why value attribution affected the participants use of one information source over another during decision making . Findings show that the participants preferred internal personal type of information sources over external impersonal information sources . The differences in value ratings per information source were striking Telecommunications managers rated customers newspapers magazines and conferences trips much lower than the manufacturing managers but they rated corporate library intranet and databases much higher than manufacturing managers . The type of industrial sector therefore highly influenced information source use for decision making by the study s Chinese business managers . Based on this conclusion we added organizational and environmental categories to revise the De Alwis Majid and Chaudhry s typology of factors affecting Chinese managers information source preferences during decision making . 
",We examine information source use during KM strategic decision making. We revised De Alwis et al. s typology of information source preferences. Organizational context is highly determinative of information source use.,,,
S0262885614001681," This paper presents yet another algorithm for finding polygonal approximations of digital planar curves however with a significant distinction the vertices of an approximating polygon need not lie on the contour itself . This approach gives us more flexibility to reduce the approximation error of the polygon compared to the conventional way where the vertices of the polygon are restricted to lie on the contour . To compute the approximation efficiently we adaptively define a local neighborhood of each point on the contour . The vertices of the polygonal approximation are allowed to move around in the neighborhoods . In addition we demonstrate a general approach where the error measure of an already computed polygonal approximation can possibly be reduced further by vertex relocation without increasing the number of dominant points . Moreover the proposed method is non parametric requiring no parameter to set for any particular application . Suitability of the proposed algorithm is validated by testing on several databases and comparing with existing methods . 
",Polygonal approximations using relaxed approximations allowing the vertices of an approximation to lie outside the contour. Adaptive estimation of contour neighborhoods where the vertices of an approximation are located. A general approach to reduce error measure of any polygonal approximations through vertex relocation.,,,
S0262885615001092," We propose a new methodology for facial landmark detection . Similar to other state of the art methods we rely on the use of cascaded regression to perform inference and we use a feature representation that results from concatenating 66 HOG descriptors one per landmark . However we propose a novel regression method that substitutes the commonly used Least Squares regressor . This new method makes use of the L 2 1 norm and it is designed to increase the robustness of the regressor to poor initialisations or partial occlusions . Furthermore we propose to use multiple initialisations consisting of both spatial translation and 4 head poses corresponding to different pan rotations . These estimates are aggregated into a single prediction in a robust manner . Both strategies are designed to improve the convergence behaviour of the algorithm so that it can cope with the challenges of in the wild data . We further detail some important experimental details and show extensive performance comparisons highlighting the performance improvement attained by the method proposed here . 
",Facial landmark detection using a cascade of regressors New regression model based on L21 norm Multiple initialisations are used to improve robustness. The method is evaluated on multiple datasets and on the 300W challenge.,,,
S0262885615000335,"Recent research trends in Content based Video Retrieval have shown topic models as an effective tool to deal with the semantic gap challenge. In this scenario this paper has a dual target 1 it is aimed at studying how the use of different topic models pLSA LDA and FSTM affects video retrieval performance 2 a novel incremental topic model IpLSA is presented in order to cope with incremental scenarios in an effective and efficient way. A comprehensive comparison among these four topic models using two different retrieval systems and two reference benchmarking video databases is provided. Experiments revealed that pLSA is the best model in sparse conditions LDA tend to outperform the rest of the models in a dense space and IpLSA is able to work properly in both cases. 
",A study of the use of topic models for video retrieval is presented. A new topic model to deal with incremental retrieval scenarios is proposed. Comparison of four topic models using two different retrieval functions The results highlight the performance differences among the topic models.,,,
S0262885616300580,"Great variances in visual features often present significant challenges in human action recognitions. To address this common problem this paper proposes a statistical adaptive metric learning SAML method by exploring various selections and combinations of multiple statistics in a unified metric learning framework. Most statistics have certain advantages in specific controlled environments and systematic selections and combinations can adapt them to more realistic in the wild scenarios. In the proposed method multiple statistics include means covariance matrices and Gaussian distributions are explicitly mapped or generated in the Riemannian manifolds. Typically d dimensional mean vectors in Rd are mapped to a Rd d space of symmetric positive definite SPD matrices . Subsequently by embedding the heterogeneous manifolds in their tangent Hilbert space subspace combination with minimal deviation is selected from multiple statistics. Then Mahalanobis metrics are introduced to map them back into the Euclidean space. Unified optimizations are finally performed based on the Euclidean distances. In the proposed method subspaces with smaller deviations are selected before metric learning. Therefore by exploring different metric combinations the final learning is more representative and effective than exhaustively learning from all the hybrid metrics. Experimental evaluations are conducted on human action recognitions in both static and dynamic scenarios. Promising results demonstrate that the proposed method performs effectively for human action recognitions in the wild. 
",A statistical adaptive metric learning SAML is proposed to classify action features. SAML explores multiple statistic combinations for feature sets in different scales. Discriminative statistic subspace is learned by a unified metric learning framework. High competitive performances are achieved by SAML on five benchmark databases.,,,
S0262885614001115," Texture classification is one of the most important tasks in computer vision field and it has been extensively investigated in the last several decades . Previous texture classification methods mainly used the template matching based methods such as Support Vector Machine and k Nearest Neighbour for classification . Given enough training images the state of the art texture classification methods could achieve very high classification accuracies on some benchmark databases . However when the number of training images is limited which usually happens in real world applications because of the high cost of obtaining labelled data the classification accuracies of those state of the art methods would deteriorate due to the overfitting effect . In this paper we aim to develop a novel framework that could correctly classify textural images with only a small number of training images . By taking into account the repetition and sparsity property of textures we propose a sparse representation based multi manifold analysis framework for texture classification from few training images . A set of new training samples are generated from each training image by a scale and spatial pyramid and then the training samples belonging to each class are modelled by a manifold based on sparse representation . We learn a dictionary of sparse representation and a projection matrix for each class and classify the test images based on the projected reconstruction errors . The framework provides a more compact model than the template matching based texture classification methods and mitigates the overfitting effect . Experimental results show that the proposed method could achieve reasonably high generalization capability even with as few as 3 training images and significantly outperforms the state of the art texture classification approaches on three benchmark datasets . 
",The prime pyramid expands the training dataset with less redundancy. Sparse representation provides a compact model for textural images in each class. Multi manifold analysis improves discriminative power while mitigates overfitting. Reasonably high classification accuracy is achieved with very few training images.,,,
S0262885614001632," An algorithm for fitting multiple models that characterize the projective relationships between point matches in pairs of images is proposed herein . Specifically the problem of estimating multiple algebraic varieties that relate the projections of 3 dimensional points in one or more views is predominantly turned into a problem of inference over a Markov random field using labels that include outliers and a set of candidate models estimated from subsets of the point matches . Thus not only the MRF can trivially incorporate the errors of fit in singleton factors but the sheer benefit of this approach is the ability to consider the interactions between data points . The proposed method refines the outlier posterior over the course of consecutive inference sweeps until the process settles at a local minimum . The inference engine employed is a Markov Chain Monte Carlo method which samples new labels from clusters of data points . The advantage of this technique pertains to the fact that cluster formation can be manipulated to favor common label assignments between points related to each other by image based criteria . Moreover although CSAMMFIT uses a Potts like pairwise factor the inference algorithm allows for arbitrary prior formulations thereby accommodating the needs for more elaborate feature based constraints . 
",Assignment of projective models becomes a problem of probabilistic inference through clustering. A Markov network formulation that models data points in terms of projective relationships in two views is introduced. An algorithm that fits multiple varieties to data points is specified using MCMC based inference. Use of a global energy measure to capture the quality of convergence. Comparative results indicate less susceptibility to parameter tuning and increased accuracy of convergence.,,,
S0262885614001127,"This paper proposes a novel robust texture descriptor based on Gaussian Markov random fields GMRFs . A spatially localized parameter estimation technique using local linear regression is performed and the distributions of local parameter estimates are constructed to formulate the texture features. The inconsistencies arising in localized parameter estimation are addressed by applying generalized inverse regularization and an estimation window size selection criterion. The texture descriptors are named as local parameter histograms LPHs and are used in texture segmentation with the k means clustering algorithm. The segmentation results on general texture datasets demonstrate that LPH descriptors significantly improve the performance of classical GMRF features and achieve better results compared to the state of the art texture descriptors based on local feature distributions. Impressive natural image segmentation results are also achieved and comparisons to the other standard natural image segmentation algorithms are also presented. LPH descriptors produce promising texture features that integrate both statistical and structural information about a texture. The region boundary localization can be further improved by integrating colour information and using advanced segmentation algorithms. 
",Distributions of GMRF local parameter estimates as improved texture descriptors. Spatially localized parameter estimation using local linear regression. Approaches to overcome inconsistencies in localized parameter estimation. Proposed descriptors capture both spatial dependencies and distributions of texture.,,,
S0306457313001003," Although most of the queries submitted to search engines are composed of a few keywords and have a length that ranges from three to six words more than 15 of the total volume of the queries are verbose introduce ambiguity and cause topic drifts . We consider verbosity a different property of queries from length since a verbose query is not necessarily long it might be succinct and a short query might be verbose . This paper proposes a methodology to automatically detect verbose queries and conditionally modify queries . The methodology proposed in this paper exploits state of the art classification algorithms combines concepts from a large linguistic database and uses a topic gisting algorithm we designed for verbose query modification purposes . Our experimental results have been obtained using the TREC Robust track collection thirty topics classified by difficulty degree four queries per topic classified by verbosity and length and human assessment of query verbosity . Our results suggest that the methodology for query modification conditioned to query verbosity detection and topic gisting is significantly effective and that query modification should be refined when topic difficulty and query verbosity are considered since these two properties interact and query verbosity is not straightforwardly related to query length . 
",We consider query verbosity and query length two distinct query properties. We propose a methodology to automatically detect and process verbose queries. The methodology uses concept expansion and syntactic features for query modification. Experiments were carried out on a test collection built on TREC2004 Robust collection. Retrieval effectiveness can be significantly improved by considering query verbosity.,,,
S0262885614001322," Extracting local keypoints and keypoint descriptions from images is a primary step for many computer vision and image retrieval applications . In the literature many researchers have proposed methods for representing local texture around keypoints with varying levels of robustness to photometric and geometric transformations . Gradient based descriptors such as the Scale Invariant Feature Transform are among the most consistent and robust descriptors . The SIFT descriptor a 128 element vector consisting of multiple gradient histograms computed from local image patches around a keypoint is widely considered as the gold standard keypoint descriptor . However SIFT descriptors require at least 128bytes of storage per descriptor . Since images are typically described by thousands of keypoints it may require more space to store the SIFT descriptors for an image than the original image itself . This may be prohibitive in extremely large scale applications and applications on memory constrained devices such as tablets and smartphones . In this paper with the goal of reducing the memory requirements of keypoint descriptors such as SIFT without affecting their performance we propose BIG OH a simple yet extremely effective method for binary quantization of any descriptor based on gradient orientation histograms . BIG OH s memory requirements are very small when it uses SIFT s default parameters for the construction of the gradient orientation histograms it only requires 16bytes per descriptor . BIG OH quantizes gradient orientation histograms by computing a bit vector representing the relative magnitudes of local gradients associated with neighboring orientation bins . In a series of experiments on keypoint matching with different types of keypoint detectors under various photometric and geometric transformations we find that the quantized descriptor has performance comparable to or better than other descriptors including BRISK CARD BRIEF D BRIEF SQ and PCA SIFT . Our experiments also show that BIG OH is extremely effective for image retrieval with modestly better performance than SIFT . BIG OH s drastic reduction in memory requirements obtained while preserving or improving the image matching and image retrieval performance of SIFT makes it an excellent descriptor for large image databases and applications running on memory constrained devices . 
",BIG OH binary quantization of gradient orientation based descriptors is proposed. Quantized SIFT descriptors reduce memory by 88 compared to classical SIFT. BIG OH has performance comparable to SIFT and GLOH. BIG OH has better performance than BRISK CARD BRIEF and other descriptors. BIG OH is effective for large scale applications such as copy detection.,,,
S0262885615000384," Various visual tracking approaches have been proposed for robust target tracking among which using sparse representation of the tracking target yields promising performance . Some earlier works in this line used a fixed subset of features to compress the target s appearance which has limited modeling capacity between the target and the background and could not accommodate their appearance change over long period of time . In this paper we propose a visual tracking method by modeling targets with online learned sparse features . We first extract high dimensional Haar like features as an over completed basis set and then solve the feature selection problem in an efficient L 1 regularized sparse coding process . The selected low dimensional representation best discriminates the target from its neighboring background . Next we use a naive Bayesian classifier to select the most likely target candidate by a binary classification process . The online feature selection process happens when there are significant appearance changes identified by a thresholding strategy . In this way our proposed method could work for long tracking tasks . At the same time our comprehensive experimental evaluation has shown that the proposed methods achieve excellent running speed and higher accuracy over many state of the art approaches . 
",Proposed an online sparse feature selection method for modeling tracking target from its neighboring background. Introduced a correlation based feature updating strategy to accommodate significant appearance change of the target Achieved more stable and accurate tracking results compared to several state of the art methods Real time processing speed,,,
S0306457313000459," In our paper we present an experimental study which investigated the possibility to project the need for information specialists serving knowledge workers in knowledge industries on the basis of an average university library serving their counterparts at a university . Information management functions i.e . functions and processes related to information evaluation acquisition metadata creation etc . performed in an average university library are the starting point of this investigation . The fundamental assumption is that these functions do not only occur in libraries but also in other contexts like for instance in knowledge industries . As a consequence we try to estimate the need for information professionals in knowledge industries by means of quantitative methods from library and information science and economics . Our study confirms the validity of our assumption . Accordingly the number of information specialists projected on the basis of university libraries is consistent with their actual number reported in national statistics . However in order to attain a close fit we had to revise the original research model by dismissing the split up of information specialists into reader services and technical services staff . 
",Knowledge industries KI are the largest segment of the Austrian information sector. University libraries can serve as basis for estimating the need of info specialists in KI. The ratio information specialists to knowledge workers is 1 130 in KI in Austria.,,,
S0306457314000508," Multi document summarization techniques aim to reduce documents into a small set of words or paragraphs that convey the main meaning of the original document . Many approaches to multi document summarization have used probability based methods and machine learning techniques to simultaneously summarize multiple documents sharing a common topic . However these techniques fail to semantically analyze proper nouns and newly coined words because most depend on an out of date dictionary or thesaurus . To overcome these drawbacks we propose a novel multi document summarization system called FoDoSu or Folksonomy based Multi Document Summarization that employs the tag clusters used by Flickr a Folksonomy system for detecting key sentences from multiple documents . We first create a word frequency table for analyzing the semantics and contributions of words using the HITS algorithm . Then by exploiting tag clusters we analyze the semantic relationships between words in the word frequency table . Finally we create a summary of multiple documents by analyzing the importance of each word and its semantic relatedness to others . Experimental results from the TAC 2008 and 2009 data sets demonstrate the improvement of our proposed framework over existing summarization systems . 
",A novel multi document summarization system that employs the tag cluster on Flickr. The FoDoSu detects meaningful words by exploiting tag cluster for summarizing multi documents. We demonstrate the superiority of FoDoSu through experiments on TAC2008 and TAC2009.,,,
S0262885615001316," Pedestrian detection is an important image understanding problem with many potential applications . There has been little success in creating an algorithm which exhibits a high detection rate while keeping the false alarm in a relatively low rate . This paper presents a method designed to resolve this problem . The proposed method uses the Kinect or any similar type of sensors which facilitate the extraction of a distinct foreground . Then potential regions which are candidates for the presence of human are detected by employing the widely used Histogram of Oriented Gradients technique which performs well in terms of good detection rates but suffers from significantly high false alarm rates . Our method applies a sequence of operations to eliminate the false alarms produced by the HOG detector based on investigating the fine details of local shape information . Local shape information can be identified by efficient utilization of the edge points which in this work are used to formulate the so called Shape Context model . The proposed detection framework is divided in four sequential stages with each stage aiming at refining the detection results of the previous stage . In addition our approach employs a pre evaluation stage to pre screen and restrict further detection results . Extensive experimental results on the dataset created by the authors involves 673 images collected from 11 different scenes demonstrate that the proposed method eliminates a large percentage of the false alarms produced by the HOG pedestrian detector . 
",Proposed a low false alarm rate pedestrian detection method Extracted foregrounds with the employment of depth sensor Designed a pre evaluation stage to save computation time and reduce false alarm rate Built a pedestrian database with 673 depth images collected from 11 different scenes,,,
S0262885616300051," This work deals with the challenging task of activity recognition in unconstrained videos. Standard methods are based on video encoding of low level features using Fisher Vectors or Bag of Features. However these approaches model every sequence into a single vector with fixed dimensionality that lacks any long term temporal information which may be important for recognition especially of complex activities. This work proposes a novel framework with two main technical novelties First a video encoding method that maintains the temporal structure of sequences and second a Time Flexible Kernel that allows comparison of sequences of different lengths and random alignment. Results on challenging benchmarks and comparison to previous work demonstrate the applicability and value of our framework. 
",TFK a kernel framework between arbitrary length sequences. Some complex activities are defined by the order of sub actions. The new kernel framework improves results in complex activities recognition. Combination of several levels of granularity in temporal divisions reduces clutter.,,,
S0262885614001565," We describe an Eikonal based algorithm for computing dense oversegmentation of an image often called superpixels . This oversegmentation respects local image boundaries while limiting undersegmentation . The proposed algorithm relies on a region growing scheme where the potential map used is not fixed and evolves during the diffusion . Refinement steps are also proposed to enhance at low cost the first oversegmentation . Quantitative comparisons on the Berkeley dataset show good performance on traditional metrics over current state of the art superpixel methods . 
",Fast and efficient superpixel algorithm based on the Eikonal equation Dynamic color based potential function Parameter free refinement of the superpixels Equivalent or better performances than the state of the art Flexibility of the approach,,,
S0262885614001371," The rotation scaling and translation invariant property of image moments has a high significance in image recognition . Legendre moments as a classical orthogonal moment have been widely used in image analysis and recognition . Since Legendre moments are defined in Cartesian coordinate the rotation invariance is difficult to achieve . In this paper we first derive two types of transformed Legendre polynomial substituted and weighted radial shifted Legendre polynomials . Based on these two types of polynomials two radial orthogonal moments named substituted radial shifted Legendre moments and weighted radial shifted Legendre moments are proposed . The proposed moments are orthogonal in polar coordinate domain and can be thought as generalized and orthogonalized complex moments . They have better image reconstruction performance lower information redundancy and higher noise robustness than the existing radial orthogonal moments . At last a mathematical framework for obtaining the rotation scaling and translation invariants of these two types of radial shifted Legendre moments is provided . Theoretical and experimental results show the superiority of the proposed methods in terms of image reconstruction capability and invariant recognition accuracy under both noisy and noise free conditions . 
",Two types of radial Legendre moments for image analysis are proposed. The proposed moments are transformed Legendre moments. These proposed moments can be thought as orthogonalized complex moments. A framework of deriving RST invariance of the transformed moments is investigated. The proposed invariants are robust to additive white noise.,,,
S0262885614001280," This paper deals with the problem of estimating the human upper body orientation . We propose a framework which integrates estimation of the human upper body orientation and the human movements . Our human orientation estimator utilizes a novel approach which hierarchically employs partial least squares based models of the gradient and texture features coupled with the random forest classifier . The movement predictions are done by projecting detected persons into 3D coordinates and running an Unscented Kalman Filter based tracker . The body orientation results are then fused with the movement predictions to build a more robust estimation of the human upper body orientation . We carry out comprehensive experiments and provide comparison results to show the advantages of our system over the other existing methods . 
",We propose a method for estimating the human upper body orientation. Our algorithm uses partial least squares based gradient and texture feature models. Integration with an UKF based movement prediction increases the performance. Comparison with the state of the art shows the benefit of our algorithm. Experiment results using image video and camera are provided.,,,
S0262885614001346," Recently sparse representation has been applied to object tracking where each candidate target is approximately represented as a sparse linear combination of target templates. In this paper we present a new tracking algorithm which is faster and more robust than other tracking algorithms based on sparse representation. First with an analysis of many typical tracking examples with various degrees of corruption we model the corruption as a Laplacian distribution. Then a LAD Lasso optimisation model is proposed based on Bayesian Maximum A Posteriori MAP estimation theory. Compared with L1 Tracker and APG L1 Tracker the number of optimisation variables is reduced greatly it is equal to the number of target templates regardless of the dimensions of the feature. Finally we use the Alternating Direction Method of Multipliers ADMM to solve the proposed optimisation problem. Experiments on some challenging sequences demonstrate that our proposed method performs better than the state of the art methods in terms of accuracy and robustness. 
",The representation error is modelled as a Laplacian distribution. We derive our new LAD Lasso model based on a Bayesian MAP estimate. LAD Lasso model is robust to outliers. The number of optimisation variable in the new model reduces greatly. We use ADMM algorithm to solve the new optimisation problem.,,,
S0262885615000372," This paper describes how to generate optimal projection patterns to supplement general stereo camera systems . In contrast to structured light the active stereo systems utilize the projected patterns only as auxiliary information in correspondence search whereas the structured light systems have to detect the patterns and decode them to compute depth . The concept of non recurring De Bruijn sequences is introduced and a few algorithms based on the non recurring De Bruijn sequence are designed to build optimized projection patterns for several stereo parameters . When only the search window size of a stereo system is given we show that a non recurring De Bruijn sequence with corresponding parameters makes the longest functional pattern and presents experimental results using real scenes to show the effectiveness of the proposed projection patterns . Additionally if the pattern length is given in the form of maximum disparity search range the algorithm using branch and bound search scheme to find an optimal sub sequence of a non recurring De Bruijn sequence is proposed . 
",The novel concept of non recurring De Bruijn NRDB sequence The algorithms to generate NRDB sequences for given parameters The optimal subsequence search algorithm using branch and bound Discussion on similarities and differences of active stereo and structured light,,,
S0262885615000554," This paper investigates the effects of adding texture to images with poorly textured regions on optical flow performance namely the accuracy of foreground boundary detection and computation time . Despite significant improvements in optical flow computations poor texture still remains a challenge to even the most accurate methods . Accordingly we explored the effects of simple modification of images rather than the algorithms . To localize and add texture to poorly textured regions in the background which induce the propagation of foreground optical flow we first perform a texture segmentation using Laws masks and generate a texture map . Next using a binary frame difference we constrain the poorly textured regions to those with negligible motion . Finally we calculate the optical flow for the modified images with added texture using the best optical flow methods available . It is shown that if the threshold used for binarizing the frame difference is in a specific range determined empirically variations in the final foreground detection will be insignificant . Employing the texture addition in conjunction with leading optical flow methods on multiple real and animation sequences with different texture distributions revealed considerable advantages including improvement in the accuracy of foreground boundary preservation prevention of object merging and reduction in the computation time . The F measure and the Boundary Displacement Error metrics were used to evaluate the similarity between detected and ground truth foreground masks . Furthermore preventing foreground optical flow propagation and reduction in the computation time are discussed using analysis of optical flow convergence . 
",An initial step for optical flow estimation in poorly textured images is proposed. The simple yet effective step preserves motion boundaries where other methods fail. The proposed algorithm reduces computation time meaningfully. Mathematical analysis is employed to explain the advantages provided. Quantitative measures have been introduced to assess the performance.,,,
S0262885615001079," We propose to represent a time of flight camera by the map of internal radial distances associating an intrinsic distance to each pixel as an alternative for the classic pinhole model . This representation is more general than the perspective model and appears to be a natural concept for 3D reconstruction and other applications of TOF cameras . In this new framework calibrating a ToF camera comes down to the determination of this IRD map . We show how this can be accomplished by images of flat surfaces without performing any feature detection . We prove deterministic calibration formulas using one or more plane images . We also offer a numerical optimization method that in principle needs only one image of a flat surface . This paper has been recommended for acceptance by Peter Sturm . 
",We introduce a parameter free calibration model for a TOF camera IRD map . This model partially compensates certain aberrations from the pinhole model. The IRD is computed by 1 or 2 depth images of a flat surface. Introducing featureless calibration procedures for TOF sensors blank planes . Satisfying calibration for low resolution sensors.,,,
S0306457313000502," Archives are an extremely valuable part of our cultural heritage since they represent the trace of the activities of a physical or juridical person in the course of their business . Despite their importance the models and technologies that have been developed over the past two decades in the Digital Library field have not been specifically tailored to archives . This is especially true when it comes to formal and foundational frameworks as the Streams Structures Spaces Scenarios Societies model is . Therefore we propose an innovative formal model called NEsted SeTs for Object hieRarchies for archives explicitly built around the concepts of context and hierarchy which play a central role in the archival realm . NESTOR is composed of two set based data models the Nested Sets Model and the Inverse Nested Sets Model that express the hierarchical relationships between objects through the inclusion property between sets . We formally study the properties of these models and prove their equivalence with the notion of hierarchy entailed by archives . We then use NESTOR to extend the 5S model in order to take into account the specific features of archives and to tailor the notion of digital library accordingly . This offers the possibility of opening up the full wealth of DL methods and technologies to archives . We demonstrate the impact of NESTOR on this problem through three example use cases . 
",A set theoretical formal model for digital archives is presented. The 5S model is extended for digital archives and concrete applications are given. Address EAD issues in the Web and with compound digital objects. Use cases based on OAI PMH OAI ORE and annotation frameworks for digital archives. Properties and mappings between the tree and NESTOR are formally proved.,,,
S0262885614001292," This paper presents an unsupervised deep learning framework that derives spatio temporal features for human robot interaction . The respective models extract high level features from low level ones through a hierarchical network viz . the Hierarchical Temporal Memory providing at the same time a solution to the curse of dimensionality in shallow techniques . The presented work incorporates the tensor based framework within the operation of the nodes and thus enhances the feature derivation procedure . This is due to the fact that tensors allow the preservation of the initial data format and their respective correlation and moreover attain more compact representations . The computational nodes form spatial and temporal groups by exploiting the multilinear algebra and subsequently express the samples according to those groups in terms of proximity . This generic framework may be applied in a diverse of visual data while it has been examined on sequences of color and depth images exhibiting remarkable performance . 
",We propose a novel deep learning model for action recognition. The deep learning model exploits multilinear algebra. The method derives spatio temporal features.,,,
S0304397515005277," In this article we propose a novel formalism to model and analyse gene regulatory networks using a well established formal verification technique . We model the possible behaviours of networks by logical formulae in linear temporal logic . By checking the satisfiability of LTL it is possible to check whether some or all behaviours satisfy a given biological property which is difficult in quantitative analyses such as the ordinary differential equation approach . Owing to the complexity of LTL satisfiability checking analysis of large networks is generally intractable in this method . To mitigate this computational difficulty we developed two methods . One is a modular checking method where we divide a network into subnetworks check them individually and then integrate them . The other is an approximate analysis method in which we specify behaviours in simpler formulae which compress or expand the possible behaviours of networks . In the approximate method we focused on network motifs and presented approximate specifications for them . We confirmed by experiments that both methods improved the analysis of large networks . 
",We propose a novel qualitative method for analysing gene networks based on formal verification technique. Behaviours and properties of networks are described in temporal logic formulae. By checking satisfiability of the formula we can analyse properties of the network. To improve the efficiency of analysis we developed the modular and approximate method.,,,
S0262885615001109," The ability of most existing approaches to classify abandoned and removed objects in images is affected by external environmental conditions such as illumination and traffic volume because the approaches use several pre defined threshold values and generate many falsely classified static regions . To reduce these effects we propose an accurate ARO classification method using a hierarchical finite state machine that consists of pixel layer region layer and event layer FSMs where the result of the lower layer FSM is used as the input of the higher layer FSM . Each FSM is defined by a Mealy state machine with three states and several state transitions where a support vector machine determines the state transition based on the current state and input features such as area intensity motion shape time duration color and edge . Because it uses the hierarchical FSM structure with features that are optimally trained by SVM classifiers the proposed ARO classification method does not require threshold values and guarantees better classification accuracy under severe environmental changes . In experiments the proposed ARO classification method provided much higher classification accuracy and lower false alarm rate than the state of the art methods in both public databases and a commercial database . The proposed ARO classification method can be applied to many practical applications such as detection of littering illegal parking theft and camouflaged soldiers . 
",We propose a novel and accurate ARO classification method. We propose a hierarchical FSM consisting of pixel region and event layers. State transition is done by the pre trained SVM using 7 different input features. The proposed ARO method shows higher classification and low false alarm. The proposed ARO method can be applied to many practical applications.,,,
S0262885614001401," When a videometric system operates over a long period temperature variations in the camera and its environment will affect the measurement results which can not be ignored . How to eliminate or compensate for the effects of such variations in temperature is an emergent problem . Starting with the image drift phenomenon this paper presents an image drift model that analyzes the relationship between variations in the camera parameters and drift in the coordinates of the image . A simplified model is then introduced by analyzing the coupling relationships among the variations in the camera parameters . Furthermore a model of the relationship between the camera parameters and temperature variations is established with the system identification method . Finally several compensation experiments on image drift are carried out using the parameter temperature relationship model calibrated with one arbitrary data set to compensate the others . The analyses and experiments demonstrate the feasibility and efficiency of the proposed method . 
",This paper proposed a compensation method for eliminating the effects of temperature variation in the long duration application of image vision. A model of the relationship between the camera parameters and temperature variations is established with the system identification method. Experiments are carried on. The analyses and experiments demonstrate the feasibility and efficiency of the proposed method.,,,
S0306437915000459," In recent years monitoring the compliance of business processes with relevant regulations constraints and rules during runtime has evolved as major concern in literature and practice . Monitoring not only refers to continuously observing possible compliance violations but also includes the ability to provide fine grained feedback and to predict possible compliance violations in the future . The body of literature on business process compliance is large and approaches specifically addressing process monitoring are hard to identify . Moreover proper means for the systematic comparison of these approaches are missing . Hence it is unclear which approaches are suitable for particular scenarios . The goal of this paper is to define a framework for Compliance Monitoring Functionalities that enables the systematic comparison of existing and new approaches for monitoring compliance rules over business processes during runtime . To define the scope of the framework at first related areas are identified and discussed . The CMFs are harvested based on a systematic literature review and five selected case studies . The appropriateness of the selection of CMFs is demonstrated in two ways a systematic comparison with pattern based compliance approaches and a classification of existing compliance monitoring approaches using the CMFs . Moreover the application of the CMFs is showcased using three existing tools that are applied to two realistic data sets . Overall the CMF framework provides powerful means to position existing and future compliance monitoring approaches . 
",Framework of ten compliance monitoring functionalities CMF in business processes is defined. Related techniques and enabling technologies are discussed. Systematic literature review and presentation of real world business constraints for compliance are extracted from five case studies. Systematic comparison with compliance patterns and classification of existing approaches using the framework is presented. CMF framework based on two realistic data sets using three different compliance monitoring tools is applied.,,,
S0305054816300041," In accordance with Basel Capital Accords the Capital Requirements for market risk exposure of banks is a nonlinear function of Value at Risk . Importantly the CR is calculated based on a bank s actual portfolio i.e . the portfolio represented by its current holdings . To tackle mean VaR portfolio optimization within the actual portfolio framework we propose a novel mean VaR optimization method where VaR is estimated using a univariate Generalized AutoRegressive Conditional Heteroscedasticity volatility model . The optimization was performed by employing a Nondominated Sorting Genetic Algorithm . On a sample of 40 large US stocks our procedure provided superior mean VaR trade offs compared to those obtained from applying more customary mean multivariate GARCH and historical VaR models . The results hold true in both low and high volatility samples . 
",We introduce mean univariate GARCH VaR actual portfolio optimization in accordance with Basel Capital Accords. We develop optimization software that combines NSGA II algorithm and R statistical software. Computational results show that our approach provides better mean univariate GARCH VaR trade offs of actual portfolios in comparison to benchmarks estimated by analytical methods. Empirical tests cover both low and high volatility samples.,,,
S0306457313001027," Knowledge acquisition and bilingual terminology extraction from multilingual corpora are challenging tasks for cross language information retrieval . In this study we propose a novel method for mining high quality translation knowledge from our constructed Persian English comparable corpus University of Tehran Persian English Comparable Corpus . We extract translation knowledge based on Term Association Network constructed from term co occurrences in same language as well as term associations in different languages . We further propose a post processing step to do term translation validity check by detecting the mistranslated terms as outliers . Evaluation results on two different data sets show that translating queries using UTPECC and using the proposed methods significantly outperform simple dictionary based methods . Moreover the experimental results show that our methods are especially effective in translating Out Of Vocabulary terms and also expanding query words based on their associated terms . 
",We propose novel method for mining high quality translation from comparable corpus. We introduce Term Association Network TAN for mining Translation knowledge. We propose a new method for term translation validity using cross outlier detection. Results show that proposed methods significantly outperforms dictionary based method. Our methods are specially effective in translating OOV terms by expanding query words.,,,
S0306457314000314," Expertise seeking is the activity of selecting people as sources for consultation about an information need . This review of 72 expertise seeking papers shows that across a range of tasks and contexts people in particular work group colleagues and other strong ties are among the most frequently used sources . Studies repeatedly show the influence of the social network of friendships and personal dislikes on the expertise seeking network of organisations . In addition people are no less prominent than documentary sources in work contexts as well as daily life contexts . The relative influence of source quality and source accessibility on source selection varies across studies . Overall expertise seekers appear to aim for sufficient quality composed of reliability and relevance while also attending to accessibility composed of access to the source and access to the source information . Earlier claims that seekers disregard quality to minimise effort receive little support . Source selection is also affected by task related seeker related and contextual factors . For example task complexity has been found to increase the use of information sources whereas task importance has been found to amplify the influence of quality on source selection . Finally the reviewed studies identify a number of barriers to expertise seeking . 
",Expertise seeking involves selecting people for consultation about info needs. People e.g. work group colleagues are among the most frequent sources. Seekers balance quality and accessibility in their selection of sources. Source selection is affected by task related seeker related and contextual factors. Multiple barriers complicate degrade or prevent expertise seeking.,,,
S0262885615000104," In this paper we propose a robust dense stereo reconstruction algorithm using a random walk with restart . The pixel wise matching costs are aggregated into superpixels and the modified random walk with restart algorithm updates the matching cost for all possible disparities between the superpixels . In comparison to the majority of existing stereo methods using the graph cut belief propagation or semi global matching our proposed method computes the final reconstruction through the determination of the best disparity at each pixel in the matching cost update . In addition our method also considers occlusion and depth discontinuities through the visibility and fidelity terms . These terms assist in the cost update procedure in the calculation of the standard smoothness constraint . The method results in minimal computational costs while achieving high accuracy in the reconstruction . We test our method on standard benchmark datasets and challenging real world sequences . We also show that the processing time increases linearly in relation to an increase in the disparity search range . 
",We develop a robust stereo matching algorithm using a random walk with restart. The proposed method considers occlusion and depth discontinuities. The method achieves high accuracy with low computational costs in the reconstruction. Our method works well on the varying illumination and exposure test.,,,
S0262885616300506," On the onset of the second decade of research in eye movement biometrics the already demonstrated results strongly support the promising perspectives of the field . This paper presents a description of the research conducted in eye movement biometrics based on an extended analysis of the characteristics and results of the BioEye 2015 Competition on Biometrics via Eye Movements . This extended presentation can contribute to the understanding of the current level of research in eye movement biometrics covering areas such as the previous work in the field the procedures for the creation of a database of eye movement recordings and the different approaches that can be used for the analysis of eye movements . Also the presented results from BioEye 2015 competition can demonstrate the potential identification accuracy that can be achieved under easier and more difficult scenarios . Based on the provided presentation we discuss topics related to the current status in eye movement biometrics and suggest possible directions for the future research in the field . 
",We present a review of the state of the art in eye movement biometrics. We explain the general steps for the creation of a database of eye movement recordings. We describe basic eye movement features and methodologies with application in biometrics. We present extended analysis and results for the BioEye 2015 competition.,,,
S0262885616300282," Human action recognition from still image has recently drawn increasing attention in human behavior analysis and also poses great challenges due to the huge inter ambiguity and intra variability . Vector of locally aggregated descriptors has achieved state of the art performance in many image classification tasks based on local features . The great success of VLAD is largely due to its high descriptive ability and computational efficiency . In this paper towards optimal VLAD representations for human action recognition from still images we improve VLAD by tackling three important issues including empty cavity ambiguity and pooling strategies . The empty cavity limits the performance of VLAD and has long been overlooked . We investigate the empty cavity and provide an effective solution to deal with it which improves the performance of VLAD we enhance the codewords with middle level of assignments which are more reliable and can provide more useful information for realistic activity we propose incorporating the generalized max pooling to replace sum pooling in VLAD which is more reliable for the final representation . We have conducted extensive experiments on four widely used benchmarks to validate the proposed method for human action recognition from still images . Our method produces competitive performance with state of the art algorithms . 
",Tackle the empty cavity issue by properly selecting reference points. Reveal codeword ambiguity by imbalance assignment and enhance the reliable codeword. Incorporate GMP into VLAD.,,,
S0262885615000773," This paper proposes a unified multi lateral filter to efficiently increase the spatial resolution of low resolution and noisy depth maps in real time . Time of Flight cameras have become a very promising alternative to stereo based range sensing systems as they provide depth measurements at a high frame rate . However there are actually two main drawbacks that restrict their use in a wide range of applications namely their fairly low spatial resolution as well as the amount of noise within the depth estimation . In order to address these drawbacks we propose a new approach based on sensor fusion . That is we couple a ToF camera of low resolution with a 2 D camera of higher resolution to which the low resolution depth map will be efficiently upsampled . In this paper we first review the existing depth map enhancement approaches based on sensor fusion and discuss their limitations . We then propose a unified multi lateral filter that accounts for the inaccuracy of depth edges position due to the low resolution ToF depth maps . By doing so unwanted artefacts such as texture copying and edge blurring are almost entirely eliminated . Moreover the proposed filter is configurable to behave as most of the alternative depth enhancement approaches . Using a convolution based formulation and data quantization and downsampling the described filter has been effectively and efficiently implemented for dynamic scenes in real time applications . The experimental results show a sensitive qualitative as well as quantitative improvement on raw depth maps outperforming state of the art multi lateral filters . 
",New multi lateral filter to efficiently increase the spatial resolution of low resolution and noisy depth maps in real time. ToF camera coupled with a 2 D camera of higher resolution to which the low resolution depth map will upsampled. We account for the inaccuracy of depth edges position due to the low resolution ToF depth maps. Unwanted artefacts such as texture copying and edge blurring are almost entirely eliminated. The proposed filter is convolution based and achives a real time performance by data quantization and downsampling. The proposed filter has been effectively and efficiently implemented for dynamic scenes in real time applications. The proposed filter can be easily adapted for alternative depth sensing systems than ToF cameras.,,,
S0262885614001668,"We propose a holistic approach to the problem of re identification in an environment of distributed smart cameras. We model the re identification process in a distributed camera network as a distributed multi class classifier composed of spatially distributed binary classifiers. We treat the problem of re identification as an open world problem and address novelty detection and forgetting. As there are many tradeoffs in design and operation of such a system we propose a set of evaluation measures to be used in addition to the recognition performance. The proposed concept is illustrated and evaluated on a new many camera surveillance dataset and SAIVT SoftBio dataset. 
",Formalization of object re identification problem in a distributed environment Re identification treated as an open world problem Novelty detection and forgetting included in the scheme A set of performance measures geared towards open world distributed surveillance Experiments on a many camera 36 surveillance dataset and publicly available source code,,,
S0262885614001851," This paper presents a novel stereo disparity estimation method which combines three different cost metrics defined using RGB information the CENSUS transform as well as Scale Invariant Feature Transform coefficients . The selected cost metrics are aggregated based on an adaptive weight approach in order to calculate their corresponding cost volumes . The resulting cost volumes are then merged into a combined one following a novel two phase strategy which is further refined by exploiting scanline optimization . A mean shift segmentation driven approach is exploited to deal with outliers in the disparity maps . Additionally low textured areas are handled using disparity histogram analysis which allows for reliable disparity plane fitting on these areas . Finally an efficient two step approach is introduced to refine disparity discontinuities . Experiments performed on the four images of the Middlebury benchmark demonstrate the accuracy of this methodology which currently ranks first among published methods . Moreover this algorithm is tested on 27 additional Middlebury stereo pairs for evaluating thoroughly its performance . The extended comparison verifies the efficiency of this work . 
",A two phase strategy for combining separate cost volumes is described. A mean shift segmentation driven approach to handle disparity outliers is utilized. Low textured area plane fitting is fostered by using disparity histogram analysis. Our method ranks first among published methods in the Middlebury evaluation system.,,,
S0262885616300294," Automatic lip reading is a challenging task because the visual speech signal is known to be missing some important information such as voicing . We propose an approach to ALR that acknowledges that this information is missing but assumes that it is substituted or deleted in a systematic way that can be modelled . We describe a system that learns such a model and then incorporates it into decoding which is realised as a cascade of weighted finite state transducers . Our results show a small but statistically significant improvement in recognition accuracy . We also investigate the issue of suitable visual units for ALR and show that visemes are sub optimal not but because they introduce lexical ambiguity but because the reduction in modelling units entailed by their use reduces accuracy . 
",A novel technique for automatic lip reading is proposed. A weighted finite state transducer cascade is used incorporating a confusion model. Performance was slightly better than a standard HMM system. The issue of suitable units for automatic lip reading was also studied. It was found that visemes are sub optimal because of reduced contextual modelling.,,,
S0262885615000992," This paper proposes a globally rotation invariant multi scale co occurrence local binary pattern feature for texture relevant tasks . In MCLBP we arrange all co occurrence patterns into groups according to properties of the co patterns and design three encoding functions to extract features from each group . The MCLBP can effectively capture the correlation information between different scales and is also globally rotation invariant . The MCLBP is substantially different from most existing LBP variants including the LBP the CLBP and the MSJ LBP that achieves rotation invariance by locally rotation invariant encoding . We fully evaluate the properties of the MCLBP and compare it with some powerful features on five challenging databases . Extensive experiments demonstrate the effectiveness of the MCLBP compared to the state of the art LBP variants including the CLBP and the LBPHF . Meanwhile the dimension and computational cost of the MCLBP is also lower than that of the CLBP S M C and LBPHF S M . 
",This paper proposes a globally rotation invariant multi scale co occurrence of LBPs MCLBP . The proposed MCLBP can effectively capture the correlation between the LBPs in different scales. Three globally rotation invariant encoding methods are introduced for MCLBP. The proposed MCLBP performs very well on texture material and medical cell classification.,,,
S0262885615001328," This paper proposed a new method based on spatial filter banks and discrete wavelet transform for invariant texture classification . The method used a multi resolution analysis method like DWT and applied the proposed filter bank on different resolutions . Then a simple fusion of features on different resolutions was used for invariant texture analysis . A comprehensive study was done to examine the effectiveness of the proposed method . Different datasets with different properties were used in this paper such as Brodatz Outex and KTH TIPS for the evaluation . Local binary pattern methods have been one of the powerful methods in recent years for invariant texture classification . A comparative study was performed with some state of the art LBP methods . This comparison indicated promising results for the proposed approach as compared with the LBP methods . 
",A novel idea has been presented for invariant texture classification. The idea uses a combination of wavelet analysis and spatial filter bank method. The method has been tested on a variety of texture databases.,,,
S0262885616300117," Learning based hashing methods are becoming the mainstream for approximate scalable multimedia retrieval . They consist of two main components hash codes learning for training data and hash functions learning for new data points . Tremendous efforts have been devoted to designing novel methods for these two components i.e . supervised and unsupervised methods for learning hash codes and different models for inferring hashing functions . However there is little work integrating supervised and unsupervised hash codes learning into a single framework . Moreover the hash function learning component is usually based on hand crafted visual features extracted from the training images . The performance of a content based image retrieval system crucially depends on the feature representation and such hand crafted visual features may degrade the accuracy of the hash functions . In this paper we propose a semi supervised deep learning hashing method for fast multimedia retrieval . More specifically in the first component we utilize both visual and label information to learn an optimal similarity graph that can more precisely encode the relationship among training data and then generate the hash codes based on the graph . In the second stage we apply a deep convolutional network to simultaneously learn a good multimedia representation and a set of hash functions . Extensive experiments on five popular datasets demonstrate the superiority of our DLH over both supervised and unsupervised hashing methods . 
",A semi supervised graph approach is proposed to infer the hash codes. We simultaneously learn visual features and hash functions. Extensive experiments are conducted to exploit the performance of our method.,,,
S0306457314000168," We digitized three years of Dutch election manifestos annotated by the Dutch political scientist Isaac Lipschits . We used these data to train a classifier that can automatically label new unseen election manifestos with themes . Having the manifestos in a uniform XML format with all paragraphs annotated with their themes has advantages for both electronic publishing of the data and diachronic comparative data analysis . The data that we created will be disclosed to the public through a search interface . This means that it will be possible to query the data and filter them on themes and parties . We optimized the Lipschits classifier on the task of classifying election manifestos using models trained on earlier years . We built a classifier that is suited for classifying election manifestos from 2002 onwards using the data from the 1980s and 1990s . We evaluated the results by having a domain expert manually assess a sample of the classified data . We found that our automatic classifier obtains the same precision as a human classifier on unseen data . Its recall could be improved by extending the set of themes with newly emerged themes . Thus when using old political texts to classify new texts work is needed to link and expand the set of themes to newer topics . 
",We digitized political texts from the 1980s and 1990s. We used these data to learn a classifier that can label more recent political texts. Change of themes over the years affects recall of the learned classifier. But precision is comparable to the precision obtained by a human expert labeller. For political themes a high level of detail seems to be preferred by domain experts.,,,
S0262885616300336,"In this paper we present a new algorithm for the computation of the focus of expansion in a video sequence. Although several algorithms have been proposed in the literature for its computation almost all of them are based on the optical flow vectors between a pair of consecutive frames so being very sensitive to noise optical flow errors and camera vibrations. Our algorithm is based on the computation of the vanishing point of point trajectories thus integrating information for more than two consecutive frames. It can improve performance in the presence of erroneous correspondences and occlusions in the field of view of the camera. The algorithm has been tested with virtual sequences generated with Blender as well as some real sequences from both the public KITTI benchmark and a number of challenging video sequences also proposed in this paper. For comparison purposes some algorithms from the literature have also been implemented. The results show that the algorithm has proven to be very robust outperforming the compared algorithms specially in outdoor scenes where the lack of texture can make optical flow algorithms yield inaccurate results. Timing evaluation proves that the proposed algorithm can reach up to 15fps showing its suitability for real time applications. 
",Development of an algorithm for the estimation of the focus of expansion Algorithm based on the cross ratio property applied to interest point trajectories The algorithm gives accurate focus of expansion localization even for low textured scenes.,,,
S0262885614001036,"Visual speech information plays an important role in automatic speech recognition ASR especially when audio is corrupted or even inaccessible. Despite the success of audio based ASR the problem of visual speech decoding remains widely open. This paper provides a detailed review of recent advances in this research area. In comparison with the previous survey 97 which covers the whole ASR system that uses visual speech information we focus on the important questions asked by researchers and summarize the recent studies that attempt to answer them. In particular there are three questions related to the extraction of visual features concerning speaker dependency pose variation and temporal information respectively. Another question is about audio visual speech fusion considering the dynamic changes of modality reliabilities encountered in practice. In addition the state of the art on facial landmark localization is briefly introduced in this paper. Those advanced techniques can be used to improve the region of interest detection but have been largely ignored when building a visual based ASR system. We also provide details of audio visual speech databases. Finally we discuss the remaining challenges and offer our insights into the future research on visual speech decoding. 
",A detailed review of the recent advances in the area of visual speech decoding. Visual features tackling speaker dependency head poses and temporal information. Dynamic audio visual speech information fusion. Recent techniques of facial landmark localization. Summary of audio visual speech databases and ASR performance on them.,,,
S0262885615001377," A trustworthy protocol is essential to evaluate a text detection algorithm in order to first measure its efficiency and adjust its parameters and second to compare its performances with those of other algorithms . However current protocols do not give precise enough evaluations because they use coarse evaluation metrics and deal with inconsistent matchings between the output of detection algorithms and the ground truth both often limited to rectangular shapes . In this paper we propose a new evaluation protocol named EvaLTex that solves some of the current problems associated with classical metrics and matching strategies . Our system deals with different kinds of annotations and detection shapes . It also considers different kinds of granularity between detections and ground truth objects and hence provides more realistic and accurate evaluation measures . We use this protocol to evaluate text detection algorithms and highlight some key examples that show that the provided scores are more relevant than those of currently used evaluation protocols . 
",We propose a two level annotation evaluation protocol for text detection algorithms. Algorithms with different granularity outputs are equitably compared. All matching strategies between the ground truth and the detections are handled. Quantity and quality scores are given to describe a detector s behavior. The protocol can manage any irregular text representation.,,,
S0262885615000475," With the increasing number of videos all over the Internet and the increasing number of cameras looking at people around the world one of the most interesting applications would be human activity recognition in videos . Many researches have been conducted in the literature for this purpose . But still recognizing activities in a video with unrestricted conditions is a challenging problem . Moreover finding the spatio temporal location of the activity in the video is another issue . In this paper we present a method based on a non negative matrix completion framework that learns to label videos with activity classes and localizes the activity of interest spatio temporally throughout the video . This approach has a multi label weakly supervised setting for activity detection with a convex optimization procedure . The experimental results show that the proposed approach is competitive with the state of the art methods . 
",Developing a multi label classification framework with a convex optimization process for activity detection. Histogram correction for activity representation in each class to localize activities in a weakly supervised setting. Proposing a new formulation for matrix completion to deal with classification localization in video. Developing an activity recognition system in a totally weakly supervised multi label setting. Developing a non negative matrix completion framework based on Alternating Direction Method ADM .,,,
S0262885615001407," The possibility of sharing multimedia contents in easy and ubiquitous way has brought to the creation of multiuser photo albums . Pictures and video sequences taken by different people attending common social events are gathered together into huge sets of heterogeneous multimedia data . These databases require effective compression strategies that exploit the common visual information related to the scene but compensate effectively the differences depending on the acquiring viewpoints camera models and acquisition time instants . The paper presents a predictive coding strategy for multi user photo gallery which initially localizes each picture in terms of viewpoint orientation time and acquired elements . This information permits ordering all the images in a prediction tree and associates to each of them a reference picture . From this structure it is possible to build a predictive coding strategy that exploits the redundant elements between the image to be coded and its reference . Experimental results show an average bit rate reduction up to 75 with respect to HEVC Intra low complexity coding . 
",The paper presents a new coding scheme for photo collections of social events. The approach is intended for server side storage of image databases. Heterogeneous users could contribute to the dataset. The algorithm implements a predictive coder exploiting the visual similarity. The coding efficiency significantly improves with respect to existing solutions.,,,
S0262885614001103," Recent studies witness the success of Bag of Features frameworks for video based human action recognition . The detection and description of local interest regions are two fundamental problems in BoF framework . In this paper we propose a motion boundary based sampling strategy and spatial temporal co occurrence descriptors for action video representation and recognition . Our sampling strategy is partly inspired by the recent success of dense trajectory based features for action recognition . Compared with DT we densely sample spatial temporal cuboids along a motion boundary which can greatly reduce the number of valid trajectories and preserve the discriminative power . Moreover we develop a set of 3D co occurrence descriptors which take account of the spatial temporal context within local cuboids and deliver rich information for recognition . Furthermore we decompose each 3D co occurrence descriptor at pixel level and bin level and integrate the decomposed components with a multi channel framework which can improve the performance significantly . To evaluate the proposed methods we conduct extensive experiments on three benchmarks including KTH YouTube and HMDB51 . The results show that our sampling strategy significantly reduces the computational cost of point tracking without degrading performance . Meanwhile we achieve superior performance than the state of the art methods . We report 95.6 on KTH 87.6 on YouTube and 51.8 on HMDB51 . 
",A motion boundary based sampling strategy is proposed for dense trajectory. A set of 3D co occurrence descriptors is developed to describe cuboids. Two decomposition strategies are presented to further improve performance. We achieve state of the art results on several human action datasets.,,,
S0306457314000284," This study aims to compare representations of Japanese personal and corporate name authority data in Japan South Korea China and the Library of Congress in order to identify differences and to bring to light issues affecting name authority data sharing projects such as the Virtual International Authority File . For this purpose actual data manuals formats and case reports of organizations as research objects were collected . Supplemental e mail and face to face interviews were also conducted . Subsequently five check points considered to be important in creating Japanese name authority data were set and the data of each organization were compared from these five perspectives . Before the comparison an overview of authority control in Chinese Japanese Korean speaking countries was also provided . The findings of the study are as follows the databases of China and South Korea have mixed headings in Kanji and other Chinese characters few organizations display the correspondence between Kanji and their yomi romanization is not mandatory in some organizations and is different among organizations some organizations adopt representations in their local language and some names in hiragana are not linked with their local forms and might elude a search . 
",China Korea and LC have mixed headings in Kanji and other Chinese characters. Few organizations display correspondences between Kanji and their yomi. Romanized descriptions are different between organizations. Some names in hiragana might escape a search.,,,
S0262885614001383,"This article focuses on the usability evaluation of biometric recognition systems in mobile devices. In particular a behavioural modality has been used the dynamic handwritten signature. Testing usability in behavioural modalities involves a big challenge due to the number of degrees of freedom that users have in interacting with sensors as well as the variety of capture devices to be used. In this context we propose a usability evaluation that allows users to interact freely with the system while minimizing errors at the same time. The participants signed in a smartphone with a stylus through the different phases in the use of a biometric system training enrolment and verification. In addition a profound study on the automation of the evaluation processes has been done so as to reduce the resources employed. The influence of the users stress has also been studied to obtain conclusions on its impact on both the usability systems in scenarios where the user may suffer a certain level of stress such as in courts banks or even shopping. In brief the results shown in this paper prove not only that a dynamic handwritten signature is a trustable solution for a large number of applications in the real world but also that the evaluation of the usability of biometric systems can be carried out at lower costs and shorter duration. 
",We made usability stress tests on mobile biometrics successfully. We optimized resources through automation. Stress is not a major drawback for handwritten signature recognition. The use of colours as a feedback of the recognition benefits usability and performance.,,,
S0306437914001550," Enabling process changes constitutes a major challenge for any process aware information system . This not only holds for processes running within a single enterprise but also for collaborative scenarios involving distributed and autonomous partners . In particular if one partner adapts its private process the change might affect the processes of the other partners as well . Accordingly it might have to be propagated to concerned partners in a transitive way . A fundamental challenge in this context is to find ways of propagating the changes in a decentralized manner . Existing approaches are limited with respect to the change operations considered as well as their dependency on a particular process specification language . This paper presents a generic change propagation approach that is based on the Refined Process Structure Tree i.e . the approach is independent of a specific process specification language . Further it considers a comprehensive set of change patterns . For all these change patterns it is shown that the provided change propagation algorithms preserve consistency and compatibility of the process choreography . Finally a proof of concept prototype of a change propagation framework for process choreographies is presented . Overall comprehensive change support in process choreographies will foster the implementation and operational support of agile collaborative process scenarios . 
",Enabling flexibility and change in process choreographies. Proposing change propagation algorithms for process choreographies. Handling transitive change propagation across several partners. Ensuring consistency and compatibility when propagating changes. First comprehensive proof of concept implementation of change propagation framework for process choreographies.,,,
S0262885614001875," 3D face recognition and emotion analysis play important roles in many fields of communication and edutainment . An effective facial descriptor with higher discriminating capability for face recognition and higher descriptiveness for facial emotion analysis is a challenging issue . However in the practical applications the descriptiveness and discrimination are independent and contradictory to each other . 3D facial data provide a promising way to balance these two aspects . In this paper a robust regional bounding spherical descriptor is proposed to facilitate 3D face recognition and emotion analysis . In our framework we first segment a group of regions on each 3D facial point cloud by shape index and spherical bands on the human face . Then the corresponding facial areas are projected to regional bounding spheres to obtain our regional descriptor . Finally a regional and global regression mapping technique is employed to the weighted regional descriptor for boosting the classification accuracy . Three largest available databases FRGC v2 CASIA and BU 3DFE are contributed to the performance comparison and the experimental results show a consistently better performance for 3D face recognition and emotion analysis . 
",A regional bounding spherical descriptor is used for 3D face and emotion analysis. Region segmentation is based on shape index and spherical band on 3D face. The regional descriptors are obtained by projecting regional bounding spheres. A regional and global regression is proposed for the weighted regional descriptor.,,,
S0262885614001590," In this paper we study the problem of Face Recognition when using Single Sensor Multi Wavelength imaging systems that operate in the Short Wave Infrared band . The contributions of our work are four fold First a SWIR database is collected when using our developed SSMW system under the following scenarios i.e . Multi Wavelength multi pose images were captured when the camera was focused at either 1150 1350 or 1550nm . Second an automated quality based score level fusion scheme is proposed for the classification of input MW images . Third a weighted quality based score level fusion scheme is proposed for the automated classification of full frontal vs. nonfrontal face images . Fourth a set of experiments is performed indicating that our proposed algorithms for the classification of multiwavelength images and FF vs. NFF face images are beneficial when designing different steps of multi spectral face recognition systems including face detection eye detection and face recognition . We also determined that when our SWIR based system is focused at 1350nm the identification performance increases compared to focusing the camera at any of the other SWIR wavelengths available . This outcome is particularly important for unconstrained FR scenarios where imaging at 1550nm at long distances and when operating at night time environments is preferable over different SWIR wavelengths . 
",Developed SSMW system in SWIR band can acquire a series of images in short duration. Proposed an automated quality score fusion scheme for classification of MW images. Proposed an automated method for classification of frontal vs non frontal face images. Proposed algorithms are beneficial when designing face recognition systems.,,,
S0306457313000757," Cross Lingual Link Discovery is a new problem in Information Retrieval . The aim is to automatically identify meaningful and relevant hypertext links between documents in different languages . This is particularly helpful in knowledge discovery if a multi lingual knowledge base is sparse in one language or another or the topical coverage in each language is different such is the case with Wikipedia . Techniques for identifying new and topically relevant cross lingual links are a current topic of interest at NTCIR where the CrossLink task has been running since the 2011 NTCIR 9 . This paper presents the evaluation framework for benchmarking algorithms for cross lingual link discovery evaluated in the context of NTCIR 9 . This framework includes topics document collections assessments metrics and a toolkit for pooling assessment and evaluation . The assessments are further divided into two separate sets manual assessments performed by human assessors and automatic assessments based on links extracted from Wikipedia itself . Using this framework we show that manual assessment is more robust than automatic assessment in the context of cross lingual link discovery . 
",We present an evaluation framework for the study of cross lingual link discovery. All aspects of this evaluation framework are detailed. The effectiveness of various cross lingual link discovery systems is discussed. Top approaches for realisation of English to Chinese link discovery are compared.,,,
S0262885615000980," Current semantic video analysis systems are usually hierarchical and consist of some levels to overcome semantic gaps between low level features and high level concepts . In these systems some features descriptors objects or concepts are extracted in each level and therefore total computational complexity of such systems is huge . In this paper we present a new general framework to impose attention control on a video analysis system using Q learning . Thus our proposed framework restructures a given system dynamically to direct attention to the blocks extracting the most informative features concepts and reduces computational complexity of the system . In other words the proposed framework directs flow of processing actively using a learning attention control method . The proposed framework is evaluated for event detection in broadcast soccer videos using limited numbers of training samples . Experiments show that the proposed framework is able to learn how to direct attention to informative features concepts and restructure the initial structure of the system dynamically to reach the final goal with less computational complexity . 
",A framework is proposed to restructure an initial video analysis system dynamically. The initial structure of the system must be hierarchical containing some units. The framework imposes a learning based dynamic feature selection method on each unit. The system learns how to direct attention to the informative units to achieve a goal. The framework is tested for goal and card event detection in broadcast soccer videos.,,,
S0262885614001619," This paper presents an accurate and efficient eye detection method using the discriminatory Haar features and a new efficient support vector machine . The DHFs are extracted by applying a discriminating feature extraction method to the 2D Haar wavelet transform . The DFE method is capable of extracting multiple discriminatory features for two class problems based on two novel measure vectors and a new criterion in the whitened principal component analysis space . The eSVM significantly improves the computational efficiency upon the conventional SVM for eye detection without sacrificing the generalization performance . Experiments on the Face Recognition Grand Challenge database and the BioID face database show that the DHFs exhibit promising classification capability for eye detection problem the eSVM runs much faster than the conventional SVM and the proposed eye detection method achieves near real time eye detection speed and better eye detection performance than some state of the art eye detection methods . 
",A discriminating feature extraction DFE method for two class problems is proposed. The DFE method is applied to derive the discriminatory Haar features DHFs for eye detection. An efficient support vector machine eSVM is proposed to improve the efficiency of the SVM. An accurate and efficient eye detection method is presented using the DHFs and the eSVM.,,,
S0262885615000955," In daily life humans demonstrate an amazing ability to remember images they see on magazines commercials TV web pages etc . but automatic prediction of intrinsic memorability of images using computer vision and machine learning techniques has only been investigated very recently . Our goal in this article is to explore the role of visual attention and image semantics in understanding image memorability . In particular we present an attention driven spatial pooling strategy and show that considering image features from the salient parts of images improves the results of the previous models . We also investigate different semantic properties of images by carrying out an analysis of a diverse set of recently proposed semantic features which encode meta level object categories scene attributes and invoked feelings . We show that these features which are automatically extracted from images provide memorability predictions as nearly accurate as those derived from human annotations . Moreover our combined model yields results superior to those of state of the art fully automatic models . 
",We examine the role of visual attention and image semantics in understanding image memorability. We propose an attention driven spatial pooling strategy for image memorability. Considering image features from the salient parts of images improves the results of the previous models. We also investigate different semantic properties of images. Combining attention driven pooling with semantic features yields state of the art results.,,,
S0306457314001058," We consider the problem of searching posts in microblog environments . We frame this microblog post search problem as a late data fusion problem . Previous work on data fusion has mainly focused on aggregating document lists based on retrieval status values or ranks of documents without fully utilizing temporal features of the set of documents being fused . Additionally previous work on data fusion has often worked on the assumption that only documents that are highly ranked in many of the lists are likely to be of relevance . We propose BurstFuseX a fusion model that not only utilizes a microblog post s ranking information but also exploits its publication time . BurstFuseX builds on an existing fusion method and rewards posts that are published in or near a burst of posts that are highly ranked in many of the lists being aggregated . We experimentally verify the effectiveness of the proposed late data fusion algorithm and demonstrate that in terms of mean average precision it significantly outperforms the standard state of the art fusion approaches as well as burst or time sensitive retrieval methods . 
",We tackle the problem of microblog search differently by data fusion. We propose to integrate temporal characteristics into data fusion. We propose a novel and effective probabilistic data fusion model for microblog search.,,,
S0306457315000801," Retrieval systems with non deterministic output are widely used in information retrieval . Common examples include sampling approximation algorithms or interactive user input . The effectiveness of such systems differs not just for different topics but also for different instances of the system . The inherent variance presents a dilemma What is the best way to measure the effectiveness of a non deterministic IR system Existing approaches to IR evaluation do not consider this problem or the potential impact on statistical significance . In this paper we explore how such variance can affect system comparisons and propose an evaluation framework and methodologies capable of doing this comparison . Using the context of distributed information retrieval as a case study for our investigation we show that the approaches provide a consistent and reliable methodology to compare the effectiveness of a non deterministic system with a deterministic or another non deterministic system . In addition we present a statistical best practice that can be used to safely show how a non deterministic IR system has equivalent effectiveness to another IR system and how to avoid the common pitfall of misusing a lack of significance as a proof that two systems have equivalent effectiveness . 
",We propose methods to compare non deterministic IR systems. We show pitfalls in using standard significance tests to compare such systems. We verify the applicability of proposed methods using simulations and a case study. We show how to compare a non deterministic IR system for equivalent effectiveness.,,,
S0306457314000879," Social media websites such as YouTube and Flicker are currently gaining in popularity . A large volume of information is generated by online users and how to appropriately provide personalized content is becoming more challenging . Traditional recommendation models are overly dependent on preference ratings and often suffer from the problem of data sparsity . Recent research has attempted to integrate sentiment analysis results of online affective texts into recommendation models however these studies are still limited . The one class collaborative filtering method is more applicable in the social media scenario yet it is insufficient for item recommendation . In this study we develop a novel sentiment aware social media recommendation framework referred to as SA OCCF in order to tackle the above challenges . We leverage inferred sentiment feedback information and OCCF models to improve recommendation performance . We conduct comprehensive experiments on a real social media web site to verify the effectiveness of the proposed framework and methods . The results show that the proposed methods are effective in improving the performance of the baseline OCCF methods . 
",We propose a sentiment aware social media recommendation framework. An ensemble learning based method is proposed to classify sentiments from affective texts. We conduct comprehensive experiments to verify the effectiveness of the proposed methods.,,,
S0306457316300085," Bibliographic collections in traditional libraries often compile records from distributed sources where variable criteria have been applied to the normalization of the data . Furthermore the source records often follow classical standards such as MARC21 where a strict normalization of author names is not enforced . The identification of equivalent records in large catalogues is therefore required for example when migrating the data to new repositories which apply modern specifications for cataloguing such as the FRBR and RDA standards . An open source tool has been implemented to assist authority control in bibliographic catalogues when external features are not available for the disambiguation of creator names . This tool is based on similarity measures between the variants of author names combined with a parser which interprets the dates and periods associated with the creator . An efficient data structure has been used to accelerate the identification of variants . The algorithms employed and the attribute grammar are described in detail and their implementation is distributed as an open source resource to allow for an easier uptake . 
",A tool assisting authority control in large bibliographic collections. An efficient data structure accelerates the identification of name variants. The space search is pruned using a parser of complex temporal expressions. The algorithm and the parser s grammar are distributed as open source resources.,,,
S0306457315001016," In the web environment most of the queries issued by users are implicit by nature . Inferring the different temporal intents of this type of query enhances the overall temporal part of the web search results . Previous works tackling this problem usually focused on news queries where the retrieval of the most recent results related to the query are usually sufficient to meet the user s information needs . However few works have studied the importance of time in queries such as Philip Seymour Hoffman where the results may require no recency at all . In this work we focus on this type of queries named time sensitive queries where the results are preferably from a diversified time span not necessarily the most recent one . Unlike related work we follow a content based approach to identify the most important time periods of the query and integrate time into a re ranking model to boost the retrieval of documents whose contents match the query time period . For that purpose we define a linear combination of topical and temporal scores which reflects the relevance of any web document both in the topical and temporal dimensions thus contributing to improve the effectiveness of the ranked results across different types of queries . Our approach relies on a novel temporal similarity measure that is capable of determining the most important dates for a query while filtering out the non relevant ones . Through extensive experimental evaluation over web corpora we show that our model offers promising results compared to baseline approaches . As a result of our investigation we publicly provide a set of web services and a web search interface so that the system can be graphically explored by the research community . 
",We propose a novel temporal re ranking algorithm. We devise and provide new datasets for time sensitive evaluation purposes. We conduct comparative experiments including algorithms with a temporal focus . We investigate the effectiveness of GRank by running a crowdsourcing experiment. We build a prototype system that can be tested by the research community.,,,
S0306457315000424," One of the major reasons why people find music so enjoyable is its emotional impact . Creating emotion based playlists is a natural way of organizing music . The usability of online music streaming services could be greatly improved by developing emotion based access methods and automatic music emotion recognition is the most quick and feasible way of achieving it . When resorting to music for emotional regulation purposes users are interested in the MER method to predict their induced or felt emotion . The progress of MER in this area is impeded by the absence of publicly accessible ground truth data on musically induced emotion . Also there is no consensus on the question which emotional model best fits the demands of the users and can provide an unambiguous linguistic framework to describe musical emotions . In this paper we address these problems by creating a sizeable publicly available dataset of 400 musical excerpts from four genres annotated with induced emotion . We collected the data using an online game with a purpose Emotify which attracted a big and varied sample of participants . We employed a nine item domain specific emotional model GEMS . In this paper we analyze the collected data and report agreement of participants on different categories of GEMS . We also analyze influence of extra musical factors on induced emotion . We suggest that modifications in GEMS model are necessary . 
",We collected ground truth data on induced musical emotion for 400 musical excerpts. We designed an online game with a purpose to attract a big number of participants. We analyzed inter rater agreement on emotional terms from GEMS model. We found that mood gender and liking or disliking the music influence induced emotion. We suggested improvements to GEMS scale.,,,
S0306457314000880," Social media is playing a growing role in elections world wide . Thus automatically analyzing electoral tweets has applications in understanding how public sentiment is shaped tracking public sentiment and polarization with respect to candidates and issues understanding the impact of tweets from various entities etc . Here for the first time we automatically annotate a set of 2012 US presidential election tweets for a number of attributes pertaining to sentiment emotion purpose and style by crowdsourcing . Overall more than 100 000 crowdsourced responses were obtained for 13 questions on emotions style and purpose . Additionally we show through an analysis of these annotations that purpose even though correlated with emotions is significantly different . Finally we describe how we developed automatic classifiers using features from state of the art sentiment analysis systems to predict emotion and purpose labels respectively in new unseen tweets . These experiments establish baseline results for automatic systems on this new data . 
",We automatically compile a dataset of 2012 US presidential election tweets. We annotate the tweets for sentiment emotion style and purpose. We show that the tweets convey negative emotions twice as often as positive. We describe two automatic systems that predict emotion and purpose in tweets.,,,
S0306457315001399," Information filtering has been a major task of study in the field of information retrieval for a long time focusing on filtering well formed documents such as news articles . Recently more interest was directed towards applying filtering tasks to user generated content such as microblogs . Several earlier studies investigated microblog filtering for focused topics . Another vital filtering scenario in microblogs targets the detection of posts that are relevant to long standing broad and dynamic topics i.e . topics spanning several subtopics that change over time . This type of filtering in microblogs is essential for many applications such as social studies on large events and news tracking of temporal topics . In this paper we introduce an adaptive microblog filtering task that focuses on tracking topics of broad and dynamic nature . We propose an entirely unsupervised approach that adapts to new aspects of the topic to retrieve relevant microblogs . We evaluated our filtering approach using 6 broad topics each tested on 4 different time periods over 4 months . Experimental results showed that on average our approach achieved 84 increase in recall relative to the baseline approach while maintaining an acceptable precision that showed a drop of about 8 . Our filtering method is currently implemented on TweetMogaz a news portal generated from tweets . The website compiles the stream of Arabic tweets and detects the relevant tweets to different regions in the Middle East to be presented in the form of comprehensive reports that include top stories and news in each region . 
",Broad topics on Twitter are highly dynamic. Boolean filtering retrieve high precision but limited number of tweets. A proposed adaptive filtering achieved 84 gain in recall with slight drop in prec. Proposed method showed robustness over time across domains and query formulations. Our method is currently adopted in a live service that follows news from Twitter.,,,
S0306457315000837," Recommender systems are filters which suggest items or information that might be interesting to users . These systems analyze the past behavior of a user build her profile that stores information about her interests and exploit that profile to find potentially interesting items . The main limitation of this approach is that it may provide accurate but likely obvious suggestions since recommended items are similar to those the user already knows . In this paper we investigate this issue known as overspecialization or serendipity problem by proposing a strategy that fosters the suggestion of surprisingly interesting items the user might not have otherwise discovered . The proposed strategy enriches a graph based recommendation algorithm with background knowledge that allows the system to deeply understand the items it deals with . The hypothesis is that the infused knowledge could help to discover hidden correlations among items that go beyond simple feature similarity and therefore promote non obvious suggestions . Two evaluations are performed to validate this hypothesis an in vitro experiment on a subset of the hetrec2011 movielens 2k dataset and a preliminary user study . Those evaluations show that the proposed strategy actually promotes non obvious suggestions by narrowing the accuracy loss . 
",We design a Knowledge Infusion KI process for providing systems with background knowledge. We design a KI based recommendation algorithm for providing serendipitous recommendations. An in vitro evaluation shows the effectiveness of the proposed approach. We collected implicit emotional feedback on serendipitous recommendations. Results show that serendipity is moderately correlated with surprise and happiness.,,,
S0377221713002488," In this paper we develop a supply contract for a two echelon manufacturer retailer supply chain with a bidirectional option which may be exercised as either a call option or a put option . Under the bidirectional option contract we derive closed form expressions for the retailer s optimal order strategies including the initial order strategy and the option purchasing strategy with a general demand distribution . We also analytically examine the feedback effects of the bidirectional option on the retailer s initial order strategy . In addition taking a chain wide perspective we explore how the bidirectional option contract should be set to attain supply chain coordination . 
",We model the supply chain that deals with products having high demand uncertainty. We characterize the optimal order strategies with bidirectional options provision. We examine the feedback effects that come from bidirectional options provision. We develop the bidirectional option contracts to coordinate the supply chain.,,,
S0306457314001095," Nowadays a large number of opinion reviews are posted on the Web . Such reviews are a very important source of information for customers and companies . The former rely more than ever on online reviews to make their purchase decisions and the latter to respond promptly to their clients expectations . Unfortunately due to the business that is behind there is an increasing number of deceptive opinions that is fictitious opinions that have been deliberately written to sound authentic in order to deceive the consumers promoting a low quality product or criticizing a potentially good quality one . In this paper we focus on the detection of both types of deceptive opinions positive and negative . Due to the scarcity of examples of deceptive opinions we propose to approach the problem of the detection of deceptive opinions employing PU learning . PU learning is a semi supervised technique for building a binary classifier on the basis of positive and unlabeled examples only . Concretely we propose a novel method that with respect to its original version is much more conservative at the moment of selecting the negative examples from the unlabeled ones . The obtained results show that the proposed PU learning method consistently outperformed the original PU learning approach . In particular results show an average improvement of 8.2 and 1.6 over the original approach in the detection of positive and negative deceptive opinions respectively . 
",Detection of negative deceptive opinion spam. Improved PU learning approach. Compares the performance of the proposed approach and the original PU learning method. The role of opinions polarity in the detection of deception. Reports experimental results on a set of negative deceptive opinions.,,,
S0306457315000990," Transductive classification is a useful way to classify texts when labeled training examples are insufficient . Several algorithms to perform transductive classification considering text collections represented in a vector space model have been proposed . However the use of these algorithms is unfeasible in practical applications due to the independence assumption among instances or terms and the drawbacks of these algorithms . Network based algorithms come up to avoid the drawbacks of the algorithms based on vector space model and to improve transductive classification . Networks are mostly used for label propagation in which some labeled objects propagate their labels to other objects through the network connections . Bipartite networks are useful to represent text collections as networks and perform label propagation . The generation of this type of network avoids requirements such as collections with hyperlinks or citations computation of similarities among all texts in the collection as well as the setup of a number of parameters . In a bipartite heterogeneous network objects correspond to documents and terms and the connections are given by the occurrences of terms in documents . The label propagation is performed from documents to terms and then from terms to documents iteratively . Nevertheless instead of using terms just as means of label propagation in this article we propose the use of the bipartite network structure to define the relevance scores of terms for classes through an optimization process and then propagate these relevance scores to define labels for unlabeled documents . The new document labels are used to redefine the relevance scores of terms which consequently redefine the labels of unlabeled documents in an iterative process . We demonstrated that the proposed approach surpasses the algorithms for transductive classification based on vector space model or networks . Moreover we demonstrated that the proposed algorithm effectively makes use of unlabeled documents to improve classification and it is faster than other transductive algorithms . 
",Scalable algorithm based on bipartite networks to perform transduction. Unlabeled data effectively employed to improve classification performance. Better performance than algorithms based on vector space model or networks. Rigorous evaluation to show the drawbacks of the existing transductive algorithms. Trade off analysis between inductive supervised and transductive classification.,,,
S0377221713002233," We consider a consignment contract with consumer non defective returns behavior . In our model an upstream vendor contracts with a downstream retailer . The vendor decides his consignment price charged to the retailer for each unit sold and his refund price for each returned item and then the retailer sets her retail price for selling the product . The vendor gets paid based on net sold units and salvages unsold units as well as returned items in a secondary market . Under the framework we study and compare two different consignment arrangements the retailer vendor manages consignment inventory programs . To study the impact of return policy we discuss a consignment contract without return policy as a benchmark . We show that whether or not the vendor offers a return policy it is always beneficial for the channel to delegate the inventory decision to the vendor . We find that the vendor s return policy depends crucially on the salvage value of returns . If the product has no salvage value the vendor s optimal decision is not to offer a return policy otherwise the vendor can gain more profit by offering a return policy when the salvage value turns out to be positive . 
",To study a general model considering consumer uncertain post purchase valuation. To explore inventory control problem under consignment contract with consumer returns. To propose optimal inventory control strategy characterizing vendor s return policy.,,,
S0377221713002701," In the Prize Collecting Steiner Tree Problem we are given a set of customers with potential revenues and a set of possible links connecting these customers with fixed installation costs . The goal is to decide which customers to connect into a tree structure so that the sum of the link costs plus the revenues of the customers that are left out is minimized . The problem as well as some of its variants is used to model a wide range of applications in telecommunications gas distribution networks protein protein interaction networks or image segmentation . In many applications it is unrealistic to assume that the revenues or the installation costs are known in advance . In this paper we consider the well known Bertsimas and Sim robust optimization approach in which the input parameters are subject to interval uncertainty and the level of robustness is controlled by introducing a control parameter which represents the perception of the decision maker regarding the number of uncertain elements that will present an adverse behavior . We propose branch and cut approaches to solve the robust counterparts of the PCStT and the Budget Constraint variant and provide an extensive computational study on a set of benchmark instances that are adapted from the deterministic PCStT inputs . We show how the Price of Robustness influences the cost of the solutions and the algorithmic performance . Finally we adapt our recent theoretical results regarding algorithms for a general class of B S robust optimization problems for the robust PCStT and its budget and quota constrained variants . 
",The paper deals with the Robust PCStT addressing interval uncertainty on its data. Different mathematical programming formulations are proposed. Branch and cut Algorithms are designed and extensively tested. Recent theoretical results are adapted for the Robust PCStT and some of it variants.,,,
S0306457315000436," Document filtering is a popular task in information retrieval . A stream of documents arriving over time is filtered for documents relevant to a set of topics . The distinguishing feature of document filtering is the temporal aspect introduced by the stream of documents . Document filtering systems up to now have been evaluated in terms of traditional metrics like precision recall MAP nDCG F1 and utility . We argue that these metrics do not capture all relevant aspects of the systems being evaluated . In particular they lack support for the temporal dimension of the task . We propose a time sensitive way of measuring performance of document filtering systems over time by employing trend estimation . In short the performance is calculated for batches a trend line is fitted to the results and the estimated performance of systems at the end of the evaluation period is used to compare systems . We detail the application of our proposed trend estimation framework and examine the assumptions that need to hold for valid significance testing . Additionally we analyze the requirements a document filtering metric has to meet and show that traditional macro averaged true positive based metrics like precision recall and utility fail to capture essential information when applied in a batch setting . In particular false positives returned in a batch for topics that are absent from the ground truth in that batch go unnoticed . This is a serious flaw as over generation of a system might be overlooked this way . We propose a new metric aptness that does capture false positives . We incorporate this metric in an overall score and show that this new score does meet all requirements . To demonstrate the results of our proposed evaluation methodology we analyze the runs submitted to the two most recent editions of a document filtering evaluation campaign . We re evaluate the runs submitted to the Cumulative Citation Recommendation task of the 2012 and 2013 editions of the TREC Knowledge Base Acceleration track and show that important new insights emerge . 
",We propose a new way of measuring document filtering system performance over time. Performance is calculated per batch and a trend line is fitted to the results. Systems are compared by their performance at the end of the evaluation period. Important insights emerge by re evaluating TREC KBA CCR runs of 2012 and 2013.,,,
S0306457314001137," This study proposes a new 4D framework for citation distribution analysis . The importance and differences in the breadth and depth of citation distribution are analyzed . Easily computable indices X Y and XY are proposed which provide estimates of the breadth and depth of citation distribution . A knowledge unit can be an article author institution journal or a set of something . Index X which represents the breadth of citation distribution is the number of different knowledge units that cite special knowledge units . Index Y which represents the depth of citation distribution is the maximum number of citations among several knowledge units that refer to specific knowledge units . Index XY which synthetically represents Indices X and Y the feature and focus impacts of a knowledge unit is index X divided by index Y . We analyze empirically the citation and reference distributions of 84 journals from the Information science and library science category of the Journal Citation Reports at the journal to journal level . Indices X Y and XY reflect the actual breadth and depth of citation distribution . Differences exist among Indices X Y and XY . Differences also exist between these indices and other bibliometric indicators . These indices can not be replaced by existing bibliometric indicators . Specifically the absolute values of indices X and Y are good supplements to existing bibliometric indicators . However index XY and the relative values of Indices X and Y represent new aspects of bibliometric indicators . 
",4D framework for citation distribution analysis are proposed. Easily computable indices X Y and XY are proposed. The index X represents the breadth of citation distribution. The index Y represents the depth of citation distribution. The index XY synthetically represents indices X and Y.,,,
S0306457315001053," The absence of diacritics in text documents or search queries is a serious problem for Turkish information retrieval because it creates homographic ambiguity . Thus the inappropriate handling of diacritics reduces the retrieval performance in search engines . A straightforward solution to this problem is to normalize tokens by replacing diacritic characters with their American Standard Code for Information Interchange counterparts . However this so called ASCIIfication produces either synthetic words that are not legitimate Turkish words or legitimate words with meanings that are completely different from those of the original words . These non valid synthetic words can not be processed by morphological analysis components which expect the input to be valid Turkish words . By contrast synthetic words are not a problem when no stemmer or a simple first n characters stemmer is used in the text analysis pipeline . This difference emphasizes the notion of the diacritic sensitivity of stemmers . In this study we propose and evaluate an alternative solution based on the application of deASCIIfication which restores accented letters in query terms or text documents . Our risk sensitive evaluation results showed that the diacritics restoration approach yielded more effective and robust results compared with normalizing tokens to remove diacritics . 
",Risk sensitive evaluation of approaches for handling diacritics in Turkish information retrieval. Application of diacritics restoration to Turkish information retrieval. Investigation of the diacritics sensitivity of stemming algorithms.,,,
S0306457315001004," The International Classification of Diseases is a type of meta data found in many Electronic Patient Records . Research to explore the utility of these codes in medical Information Retrieval applications is new and many areas of investigation remain including the question of how reliable the assignment of the codes has been . This paper proposes two uses of the ICD codes in two different contexts of search Pseudo Relevance Judgments and Pseudo Relevance Feedback . We find that our approach to evaluate the TREC challenge runs using simulated relevance judgments has a positive correlation with the TREC official results and our proposed technique for performing PRF based on the ICD codes significantly outperforms a traditional PRF approach . The results are found to be consistent over the two years of queries from the TREC medical test collection . 
",A new approach using ICD codes to substitute the manual relevance judgements. Improving IR effectiveness by using the proposed ICD based query expansion. A novel method to automatically map clinical queries into ICD codes.,,,
S0306457315000035," We use extractive multi document summarization techniques to perform complex question answering and formulate it as a reinforcement learning problem . Given a set of complex questions a list of relevant documents per question and the corresponding human generated summaries as training data the reinforcement learning module iteratively learns a number of feature weights in order to facilitate the automatic generation of summaries i.e . answers to previously unseen complex questions . A reward function is used to measure the similarities between the candidate summary sentences and the abstract summaries . In the training stage the learner iteratively selects the important document sentences to be included in the candidate summary analyzes the reward function and updates the related feature weights accordingly . The final weights are used to generate summaries as answers to unseen complex questions in the testing stage . Evaluation results show the effectiveness of our system . We also incorporate user interaction into the reinforcement learner to guide the candidate summary sentence selection process . Experiments reveal the positive impact of the user interaction component on the reinforcement learning framework . 
",Reinforcement learning formulation for complex question answering. Abstract summaries used for small amount of supervision using reward scores. User interaction component incorporated to guide candidate sentence selection. Experiments reveal that systems trained with user interaction perform better. The reinforcement system is able to learn automatically and effectively.,,,
S0377221713002464," We present a novel generic programming implementation of a column generation algorithm for the generalized staff rostering problem . The problem is represented as a generalized set partitioning model which is able to capture commonly occurring problem characteristics given in the literature . Columns of the set partitioning problem are generated dynamically by solving a pricing subproblem and constraint branching in a branch and bound framework is used to enforce integrality . The pricing problem is formulated as a novel three stage nested shortest path problem with resource constraints that exploits the inherent problem structure . A very efficient implementation of this pricing problem is achieved by using generic programming principles in which careful use of the C pre processor allows the generator to be customized for the target problem at compile time . As well as decreasing run times this new approach creates a more flexible modeling framework that is well suited to handling the variety of problems found in staff rostering . Comparison with a more standard run time customization approach shows that speedups of around a factor of 20 are achieved using our new approach . The adaption to a new problem is simple and the implementation is automatically adjusted internally according to the new definition . We present results for three practical rostering problems . The approach captures all features of each problem and is able to provide high quality solutions in less than 15minutes . In two of the three instances the optimal solution is found within this time frame . 
",We describe a nested column generation algorithm for staff rostering. We show how generic programming can be used within a column generation implementation. Our resulting system is faster and more flexible than traditional systems. We can obtain good solutions to practical staff rostering problems.,,,
S0377221713002683," Participatory budgets are becoming increasingly popular in many municipalities all around the world . The underlying idea is to allow citizens to participate in the allocation of a municipal budget . Many advantages have been suggested for such experiences including legitimization and more informed and transparent decisions . There are many conceivable variants of such processes . However in most cases both its design and implementation are carried out in an informal way . In this paper we propose a methodology to design a participatory budget process based on a multicriteria decision making model . 
",We propose a methodology to design a participatory budget processes. This methodology is based on a multicriteria decision making model. We have identified alternatives constraints criteria and a value function model. We illustrate the use of the methodology with an example. With this approach we hope to facilitate the design of participatory budget processes.,,,
S0306457314000703," We present IntoNews a system to match online news articles with spoken news from a television newscasts represented by closed captions . We formalize the news matching problem as two independent tasks closed captions segmentation and news retrieval . The system segments closed captions by using a windowing scheme sliding or tumbling window . Next it uses each segment to build a query by extracting representative terms . The query is used to retrieve previously indexed news articles from a search engine . To detect when a new article should be surfaced the system compares the set of retrieved articles with the previously retrieved one . The intuition is that if the difference between these sets is large enough it is likely that the topic of the newscast currently on air has changed and a new article should be displayed to the user . In order to evaluate IntoNews we build a test collection using data coming from a second screen application and a major online news aggregator . The dataset is manually segmented and annotated by expert assessors and used as our ground truth . It is freely available for download through the Webscope program . http webscope.sandbox.yahoo.com . Our evaluation is based on a set of novel time relevance metrics that take into account three different aspects of the problem at hand precision timeliness and coverage . We compare our algorithms against the best method previously proposed in literature for this problem . Experiments show the trade offs involved among precision timeliness and coverage of the airing news . Our best method is four times more accurate than the baseline . 
",IntoNews is an Intonow app for matching web news with news broadcasted on TV. We present a framework that models the task as 2 separate sub tasks Find retrieve. We design and publicly provide an evaluation testbed for the IntoNews problem. Quality evaluation has to trade off 2 competing metrics coverage and precision.,,,
S0377221713002555," Every item produced transported used and discarded within a Supply Chain generates costs and creates an impact on the environment . The increase of forward flows as effects of market globalization and reverse flows due to legislation warranty recycling and disposal activities affect the ability of a modern SC to be economically and environmentally sustainable . In this context the study considers an innovative sustainable closed loop SC problem . It first introduces a linear programming model that aims to minimize the total SC costs . Environmental sustainability is guaranteed by the complete reprocessing of an end of life product the re use of components the disposal of unusable parts sent directly from the manufacturers with a closed loop transportation system that maximizes transportation efficiency . Secondly the authors consider the problem by means of a parametrical study by analyzing the economical sustainability of the proposed CLSC model versus the classical Forward Supply Chain model from two perspectives Case 1 the traditional company perspective where the SC ends at the customers and the disposal costs are not included in the SC and Case 2 the social responsibility company perspective where the disposal costs are considered within the SC . The relative impact of the different variables in the SC structure and the applicability of the proposed model in terms of total costs SC structure and social responsibility are investigated thoroughly and the results are reported at the conclusion of the paper . 
",Environmental sustainability through the reprocessing of end of life products. Traditional perspective and social responsibility perspective analysis. Critical aspects for sustainability in supply chain design are analysis. Transportation efficiency maximization through closed loop transportation system.,,,
S0306457315000679," In this paper we present an efficient spectral clustering method for large scale data sets given a set of pairwise constraints . Our contribution is threefold clustering accuracy is increased by injecting prior knowledge of the data points constraints to a small affinity submatrix connected components are identified automatically based on the data points pairwise constraints generating thus isolated islands of points furthermore local neighborhoods of points of the same connected component are adapted dynamically and constraints propagation is performed so as to further increase the clustering accuracy finally the complexity is preserved low by following a sparse coding strategy of a landmark spectral clustering . In our experiments with three benchmark shape face and handwritten digit image data sets we show that the proposed method outperforms competitive spectral clustering methods that either follow semi supervised or scalable strategies . 
",We face the real world problem of having a limited set of pairwise constraints. Using pairwise constraints connected components CC are generated. The points local neighborhoods of the same CC are dynamically adapted. Constraints propagation to CC neighborhoods to increase the clustering accuracy. Scalability is ensured by following a landmark strategy.,,,
S0377221713002129," Under circumstances of increasing environmental pressures from markets and regulators focal companies in supply chains have recognized the importance of greening their supply chain through green supplier development programs . Various studies have started to explore the inter relationships between green supply chain management and supplier performance . Much of this performance can be achieved only with suppliers involvement in green supplier development programs . But the literature focusing on green supplier development programs and supplier involvement propensity is very limited . In addition formal tools and models for focal companies to evaluate these inter relationships especially considering propensity of suppliers involvement are even rarer . To help address this gap in the literature we introduce a grey analytical network process based model to identify green supplier development programs that will effectively improve suppliers performance . We further comprehensively evaluate green supplier development programs with explicit consideration of suppliers involvement propensity levels . A real world example is introduced to demonstrate the effectiveness of the model . We end with a discussion of managerial implications and present some directions for further research . 
",We develop a grey analytical network process based model for green supplier development programs selection. The model is applied to a leading manufacturer in the China s pivot irrigation equipment industry. We conclude that suppliers performance evaluation and involvement propensity should be simultaneously considered.,,,
S0306457315001491," General graph random walk has been successfully applied in multi document summarization but it has some limitations to process documents by this way . In this paper we propose a novel hypergraph based vertex reinforced random walk framework for multi document summarization . The framework first exploits the Hierarchical Dirichlet Process topic model to learn a word topic probability distribution in sentences . Then the hypergraph is used to capture both cluster relationship based on the word topic probability distribution and pairwise similarity among sentences . Finally a time variant random walk algorithm for hypergraphs is developed to rank sentences which ensures sentence diversity by vertex reinforcement in summaries . Experimental results on the public available dataset demonstrate the effectiveness of our framework . 
",We propose a novel hybrid method to capture group relation of sentences. We cluster sentences with a KL divergence based on word topic distribution. We proposed a vertex reinforcement random walk process in a hypergraph model. The process simultaneously consider the query similarity the centrality and the diversity of sentences. We implement our framework and verify improvement over appropriate baselines.,,,
S0306457316300073," Query suggestion is generally an integrated part of web search engines . In this study we first redefine and reduce the query suggestion problem as comparison of queries . We then propose a general modular framework for query suggestion algorithm development . We also develop new query suggestion algorithms which are used in our proposed framework exploiting query session and user features . As a case study we use query logs of a real educational search engine that targets K 12 students in Turkey . We also exploit educational features in our query suggestion algorithms . We test our framework and algorithms over a set of queries by an experiment and demonstrate a 66 90 statistically significant increase in relevance of query suggestions compared to a baseline method . 
",Hightlights Query suggestion QS problem is reduced to comparison of queries problem. A modular and practical framework is suggested for development of QS algorithms. Breadth First Search graph traversal method is better for query log traversing. Combining QS algorithms improves the performance. Suggested new QS algorithms achieved 66 90 performance increase.,,,
S0306457314000636," Applying text mining techniques to legal issues has been an emerging research topic in recent years . Although a few previous studies focused on assisting professionals in the retrieval of related legal documents to our knowledge no previous studies could provide relevant statutes to the general public using problem statements . In this work we design a text mining based method the three phase prediction algorithm which allows the general public to use everyday vocabulary to describe their problems and find pertinent statutes for their cases . The experimental results indicate that our approach can help the general public who are not familiar with professional legal terms to acquire relevant statutes more accurately and effectively . 
",In the legal domain this method for statute prediction is a new research topic. We predict relevant statutes for the problem described by everyday vocabulary. The gap between lay terms and legal terms was remedied without using a synopsis. Employing the Normalized Google Distance SVM and Apriori algorithms into TPP. The result shows the performance of TPP is accurately and effectively.,,,
S0306457315000254," Summary writing is a process for creating a short version of a source text . It can be used as a measure of understanding . As grading students summaries is a very time consuming task computer assisted assessment can help teachers perform the grading more effectively . Several techniques such as BLEU ROUGE N gram co occurrence Latent Semantic Analysis LSA Ngram and LSA ERB have been proposed to support the automatic assessment of students summaries . Since these techniques are more suitable for long texts their performance is not satisfactory for the evaluation of short summaries . This paper proposes a specialized method that works well in assessing short summaries . Our proposed method integrates the semantic relations between words and their syntactic composition . As a result the proposed method is able to obtain high accuracy and improve the performance compared with the current techniques . Experiments have displayed that it is to be preferred over the existing techniques . A summary evaluation system based on the proposed method has also been developed . 
",We proposed a specialized method that works well in assessing short summaries. It integrates the semantic relations between words and their syntactic composition. We experimentally examine the influence of the combination of semantic and syntactic information on short summary assessment. Experiments have displayed that it is to be preferred over the existing techniques. We have developed the method as an intelligent tool to grade students summaries.,,,
S0306457314000934," Nowadays opinion mining systems play a strategic role in different areas such as Marketing Decision Support Systems or Policy Support . Since the arrival of the Web 2.0 more and more textual documents containing information that express opinions or comments in different languages are available . Given the proven importance of such documents the use of effective multilingual opinion mining systems has become of high importance to different fields . This paper presents the experiments carried out with the objective to develop a multilingual sentiment analysis system . We present initial evaluations of methods and resources performed in two international evaluation campaigns for English and for Spanish . After our participation in both competitions additional experiments were carried out with the aim of improving the performance of both Spanish and English systems by using multilingual machine translated data . Based on our evaluations we show that the use of hybrid features and multilingual machine translated data can help to better distinguish relevant features for sentiment classification and thus increase the precision of sentiment analysis systems . 
",We study different strategies to classify sentiment from tweets using supervised learning with hybrid features. We experiment with English and Spanish data and compare against benchmark competitions. We employ machine translated data from other languages for training. We show that the use of multilingual data improves the sentiment classification accuracy.,,,
S0306457314000685," Background Our methodology describes a human activity recognition framework based on feature extraction and feature selection techniques where a set of time statistical and frequency domain features taken from 3 dimensional accelerometer sensors are extracted . This framework specifically focuses on activity recognition using on body accelerometer sensors . We present a novel interactive knowledge discovery tool for accelerometry in human activity recognition and study the sensitivity to the feature extraction parametrization . Results The implemented framework achieved encouraging results in human activity recognition . We have implemented a new set of features extracted from wearable sensors that are ambitious from a computational point of view and able to ensure high classification results comparable with the state of the art wearable systems . A feature selection framework is developed in order to improve the clustering accuracy and reduce computational complexity . The software OpenSignals was used for signal acquisition and signal processing algorithms were developed in Python Programming Language and Orange Software . Several clustering methods such as K Means Affinity Propagation Mean Shift and Spectral Clustering were applied . The K means methodology presented promising accuracy results for person dependent and independent cases with 99.29 and 88.57 respectively . Conclusions The presented study performs two different tests in intra and inter subject context and a set of 180 features is implemented which are easily selected to classify different activities . The implemented algorithm does not stipulate a priori any value for time window or its overlap percentage of the signal but performs a search to find the best parameters that define the specific data . A clustering metric based on the construction of the data confusion matrix is also proposed . The main contribution of this work is the design of a novel gesture recognition system based solely on data from a single 3 dimensional accelerometer . 
",The presented study performs two different tests in intra and inter subject context. A set of 180 features is implemented to be selected based on clustering performance. Our algorithm searches for the best feature extraction parameter. A new clustering metric based on the construction of the confusion matrix is proposed. A novel gesture recognition system based on data from a single 3 dimensional accelerometer.,,,
S0306457315000503," We propose two novel language models to improve the performance of sentence retrieval in Question Answering class based language model and trained trigger language model . As the search in sentence retrieval is conducted over smaller segments of text than in document retrieval the problems of data sparsity and exact matching become more critical . Different techniques such as the translation model are also proposed to overcome the word mismatch problem . Our class based and trained trigger language models however use different approaches to this aim and are shown to outperform the exiting models . The class model uses word clustering algorithm to capture term relationships . In this model we assume a relation between the terms that belong to the same clusters as a result they can be substituted when searching for relevant sentences . The trigger model captures pairs of trigger and target words while training on a large corpus . The model considers a relation between a question and a sentence if a trigger word appears in the question and the sentence contains the corresponding target word . For both proposed models we introduce different notions of co occurrence to find word relations . In addition we study the impact of corpus size and domain on the models . Our experiments on TREC QA collection verify that the proposed model significantly improves the sentence retrieval performance compared to the state of the art translation model . While the translation model based on mutual information has 0.3927 Mean Average Precision the class model achieves 0.4174 MAP and the trigger model enhances the performance to 0.4381 . 
",We introduce two novel LM based models to relax the exact matching assumption in IR. The class based model clusters words to provide a coarse grained word representation. The trigger model captures pairs of trigger and target words to find word relationships. Different types of word co occurrence and triggering are studied within the models. We further studied the combination of both models to achieve the best result.,,,
S0306457314000533," In this paper a new homomorphic image watermarking method implementing the Singular Value Decomposition algorithm is presented . The idea of the proposed method is based on embedding the watermark with the SVD algorithm in the reflectance component after applying the homomorphic transform . The reflectance component contains most of the image features but with low energy and hence watermarks embedded in this component will be invisible . A block by block implementation of the proposed method is also introduced . The watermark embedding on a block by block basis makes the watermark more robust to attacks . A comparison study between the proposed method and the traditional SVD watermarking method is presented in the presence of attacks . The proposed method is more robust to various attacks . The embedding of chaotic encrypted watermarks is also investigated in this paper to increase the level of security . 
",Information embedding and retrieval in digital images. Digital Watermarking. Image processing.,,,
S0377221713002154," We study a problem of tactical planning in a divergent supply chain . It involves decisions regarding production inventory internal transportation sales and distribution to customers . The problem is motivated by the context of a company in the speciality oils industry . The overall objective at tactical level is to maximize contribution and in order to achieve this the planning has been divided into two separate problems . The first problem concerns sales where the final sales and distribution planning is decentralized to individual sellers . The second problem concerns production transportation and inventory planning through refineries hubs and depots and is managed centrally with the aim of minimizing costs . Due to this decoupling the solution of the two problems needs to be coordinated in order to achieve the overall objective . In the company this is pursued through an internal price system aiming at giving the sellers the incentives needed to align their decisions with the overall objective . We propose and discuss linear programming models for the decoupled and integrated planning problems . We present numerical examples to illustrate potential effects of integration and coordination and discuss the advantages and disadvantages of the integrated over the decoupled approach . While the total contribution is higher in the integrated approach it has also been found that the sellers contribution can be considerably lower . Therefore we also suggest contribution sharing rules to achieve a solution where both the company and the sellers attain a better outcome under the integrated planning . 
",We propose linear models for the supply chain of a speciality oils company. We consider operation and sales decisions in decoupled and integrated approaches. The integrated outperforms the decoupled planning for the company. The sellers may get worse premiums in the integrated solution. Company and sellers are better off by reallocating the integrated contribution.,,,
S0306457315001193," Question Answering systems are developed to answer human questions . In this paper we have proposed a framework for answering definitional and factoid questions enriched by machine learning and evolutionary methods and integrated in a web based QA system . Our main purpose is to build new features by combining state of the art features with arithmetic operators . To accomplish this goal we have presented a Genetic Programming based approach . The exact GP duty is to find the most promising formulas made by a set of features and operators which can accurately rank paragraphs sentences and words . We have also developed a QA system in order to test the new features . The input of our system is texts of documents retrieved by a search engine . To answer definitional questions our system performs paragraph ranking and returns the most related paragraph . Moreover in order to answer factoid questions the system evaluates sentences of the filtered paragraphs ranked by the previous module of our framework . After this phase the system extracts one or more words from the ranked sentences based on a set of hand made patterns and ranks them to find the final answer . We have used Text Retrieval Conference QA track questions web data and AQUAINT and AQUAINT 2 datasets for training and testing our system . Results show that the learned features can perform a better ranking in comparison with other evaluation formulas . 
",A new framework for answering definitional and factoid questions. Producing new features by combining effective features with arithmetic operators. Genetic Programming GP algorithm has been employed for feature learning. Three discriminant based methods have been used for learning features weights.,,,
S0306457315001259," XML is a pervasive technology for representing and accessing semi structured data . XPath is the standard language for navigational queries on XML documents and there is a growing demand for its efficient processing . In order to increase the efficiency in executing four navigational XML query primitives namely descendants ancestors children and parent we introduce a new paradigm where traditional approaches based on the efficient traversing of nodes and edges to reconstruct the requested subtrees are replaced by a brand new one based on basic set operations which allow us to directly return the desired subtree avoiding to create it passing through nodes and edges . Our solution stems from the NEsted SeTs for Object hieRarchies formal model which makes use of set inclusion relations for representing and providing access to hierarchical data . We define in memory efficient data structures to implement NESTOR we develop algorithms to perform the descendants ancestors children and parent query primitives and we study their computational complexity . We conduct an extensive experimental evaluation by using several datasets digital archives INEX 2009 Wikipedia collection and two widely used synthetic datasets . We show that NESTOR based data structures and query primitives consistently outperform state of the art solutions for XPath processing at execution time and they are competitive in terms of both memory occupation and pre processing time . 
",Set based access to XML data. Improved XPath primitive operations. Up to eight orders of magnitude speed up.,,,
S0306457315000394," In recent years there has been a rapid growth of user generated data in collaborative tagging systems due to the prevailing of Web 2.0 communities . To effectively assist users to find their desired resources it is critical to understand user behaviors and preferences . Tag based profile techniques which model users and resources by a vector of relevant tags are widely employed in folksonomy based systems . This is mainly because that personalized search and recommendations can be facilitated by measuring relevance between user profiles and resource profiles . However conventional measurements neglect the sentiment aspect of user generated tags . In fact tags can be very emotional and subjective as users usually express their perceptions and feelings about the resources by tags . Therefore it is necessary to take sentiment relevance into account into measurements . In this paper we present a novel generic framework SenticRank to incorporate various sentiment information to various sentiment based information for personalized search by user profiles and resource profiles . In this framework content based sentiment ranking and collaborative sentiment ranking methods are proposed to obtain sentiment based personalized ranking . To the best of our knowledge this is the first work of integrating sentiment information to address the problem of the personalized tag based search in collaborative tagging systems . Moreover we compare the proposed sentiment based personalized search with baselines in the experiments the results of which have verified the effectiveness of the proposed framework . In addition we study the influences by popular sentiment dictionaries and SenticNet is the most prominent knowledge base to boost the performance of personalized search in folksonomy . 
",We present a framework SenticRank to incorporate sentiment for personalized search. Content based and collaborative sentiment ranking methods are proposed. We compare the proposed sentiment based search with baselines experimentally. We study the influence of sentiment corpora by using some sentiment dictionaries. Sentiment based information can significantly improve performance in folksonomy.,,,
S0377221713002762," This paper surveys the academic OR analytics literature describing research into the laws and rules of sports and sporting competitions . The literature is divided into post hoc analyses and proposals for future changes and is also divided into laws rules of sports themselves and rules organisation of tournaments or competitions . The survey outlines a large number of studies covering 21 sports in many parts of the world . The analytical approaches most commonly used are found to be various forms of regression analysis and simulation . Issues highlighted by this survey include the different views of what constitutes fairness and the frequency with which changes produce unintended consequences . 
",Reviews OR analysis of the effects of changes to the rules and laws of sports. Also reviews OR analysis of proposed future changes. Cites 67 references involving 21 different sports.,,,
S0308596113001900," As demand for mobile broadband services continues to explode mobile wireless networks must expand greatly their capacities . This paper describes and quantifies the economic and technical challenges associated with deepening wireless networks to meet this growing demand . Methods of capacity expansion divide into three general categories the deployment of more radio spectrum more intensive geographic reuse of spectrum and increasing the throughput capacity of each MHz of spectrum within a given geographic area . The paper describes these several basic methods to deepen mobile wireless capacity . It goes on to measure the contribution of each of these methods to historical capacity growth within U.S. networks . The paper then describes the capabilities of 4G LTE wireless technology and further innovations off of it to further improve network capacity . These capacity expansion capabilities of LTE Advanced along with traditional spectrum reuse are quantified and compared to forecasts of future demand to evaluate the ability of U.S. networks to match future demand . Without significantly increasing current spectrum allocations by 560MHz over the 2014 2022 period the presented model suggests that U.S. wireless capacity expansion will be inadequate to accommodate expected demand growth . This conclusion is in contrast to claims that the U.S. faces no spectrum shortage . 
",The three basic tools for deepening the capacity of mobile wireless networks are described. Historical growth in spectrum quantity geographic reuse and throughput efficiency is reported. The capability of 4G LTE and LTE Advanced to further improve network capacity is projected. These projected capacity improvements are compared with forecasts of future U.S. mobile demand. U.S. networks will require an additional 560MHz of spectrum by 2022 to meet forecasted demand.,,,
S0306457314001034," Applying natural language processing for mining and intelligent information access to tweets is a challenging emerging research area . Unlike carefully authored news text and other longer content tweets pose a number of new challenges due to their short noisy context dependent and dynamic nature . Information extraction from tweets is typically performed in a pipeline comprising consecutive stages of language identification tokenisation part of speech tagging named entity recognition and entity disambiguation . In this work we describe a new Twitter entity disambiguation dataset and conduct an empirical analysis of named entity recognition and disambiguation investigating how robust a number of state of the art systems are on such noisy texts what the main sources of error are and which problems should be further investigated to improve the state of the art . 
",We analyse the named entity recognition and disambiguation performance on tweets. Multiple state of the art systems are included. Commercial and academic systems suffer the same range of problems. Lack of context is a major problem demanding new custom NER NEL approaches. A named entity linking corpus is released with the paper.,,,
S0377221713002002," Production ramp up is an important phase in the lifecycle of a manufacturing system which still has significant potential for improvement and thereby reducing the time to market of new and updated products . Production systems today are mostly one of a kind complex engineered to order systems . Their ramp up is a complex order of physical and logical adjustments which are characterised by try and error decision making resulting in frequent reiterations and unnecessary repetitions . Studies have shown that clear goal setting and feedback can significantly improve the effectiveness of decision making in predominantly human decision processes such as ramp up . However few measurement driven decision aides have been reported which focus on ramp up improvement and no systematic approach for ramp up time reduction has yet been defined . In this paper a framework for measuring the performance during ramp up is proposed in order to support decision making by providing clear metrics based on the measurable and observable status of the technical system . This work proposes a systematic framework for data preparation ramp up formalisation and performance measurement . A model for defining the ramp up state of a system has been developed in order to formalise and capture its condition . Functionality quality and performance based metrics have been identified to formalise a clear ramp up index as a measurement to guide and support the human decision making . For the validation of the proposed framework two ramp up processes of an assembly station were emulated and their comparison was used to evaluate this work . 
",We analyse the ramp up process and identify the key performance metrics. We formalise the ramp up process as a state transition model. We provide a framework for measuring the overall ramp up performance based on the identified metrics. The performance measure has been shown to improve ramp up efficiency and reduce time.,,,
S0377221713002014," This paper considers the uncapacitated lot sizing problem with batch delivery focusing on the general case of time dependent batch sizes . We study the complexity of the problem depending on the other cost parameters namely the setup cost the fixed cost per batch the unit procurement cost and the unit holding cost . We establish that if any one of the cost parameters is allowed to be time dependent the problem is NP hard . On the contrary if all the cost parameters are stationary and assuming no unit holding cost we show that the problem is polynomially solvable in time O where T denotes the number of periods of the horizon . We also show that in the case of divisible batch sizes the problem with time varying setup costs a stationary fixed cost per batch and no unit procurement nor holding cost can be solved in time O . 
",We study the lot sizing problem with time dependent batch sizes. We prove that the problem is NP hard if one of the cost parameters is time varying. The problem is polynomial for stationary cost parameters and null holding cost.,,,
S0306457314000661," To avoid a sarcastic message being understood in its unintended literal meaning in microtexts such as messages on Twitter.com sarcasm is often explicitly marked with a hashtag such as sarcasm . We collected a training corpus of about 406 thousand Dutch tweets with hashtag synonyms denoting sarcasm . Assuming that the human labeling is correct we train a machine learning classifier on the harvested examples and apply it to a sample of a day s stream of 2.25 million Dutch tweets . Of the 353 explicitly marked tweets on this day we detect 309 with the hashtag removed . We annotate the top of the ranked list of tweets most likely to be sarcastic that do not have the explicit hashtag . 35 of the top 250 ranked tweets are indeed sarcastic . Analysis indicates that the use of hashtags reduces the further use of linguistic markers for signaling sarcasm such as exclamations and intensifiers . We hypothesize that explicit markers such as hashtags are the digital extralinguistic equivalent of non verbal expressions that people employ in live interaction when conveying sarcasm . Checking the consistency of our finding in a language from another language family we observe that in French the hashtag sarcasme has a similar polarity switching function be it to a lesser extent . 
",The use of hashtags such as sarcasm reduces the further use of linguistic markers of sarcasm in tweets. Hashtags such as sarcasm appear to be the extralinguistic equivalent of non verbal expressions in live interaction. Sarcastic hashtags are 90 appropriate. Sarcastic tweets without hashtags are hard to distinguish from non sarcastic hyperbolic tweets. In French tweets the hashtag sarcasme conveys a polarity switch less frequently than in Dutch.,,,
S0306457314000727," In this paper we study on effective and efficient processing of keyword based queries over graph databases . To produce more relevant answers to a query than the previous approaches we suggest a new answer tree structure which has no constraint on the number of keyword nodes chosen for each keyword in the query . For efficient search of answer trees on the large graph databases we design an inverted list index to pre compute and store connectivity and relevance information of nodes to keyword terms in the graph . We propose a query processing algorithm which aggregates from the pre constructed inverted lists the best keyword nodes and root nodes to find top k answer trees most relevant to the given query . We also enhance the method by extending the structure of the inverted list and adopting a relevance lookup table which enables more accurate estimation of the relevance scores of candidate root nodes and efficient search of top k answer trees . Performance evaluation by experiments with real graph datasets shows that the proposed method can find more effective top k answers than the previous approaches and provides acceptable and scalable execution performance for various types of keyword queries on large graph databases . 
",We define a new measure of relevance of a node in the graph to a keyword query. We propose an extended answer structure for a top k query over graph databases. We propose an inverted list index and search algorithm to find top k answer trees. We enhanced the basic method for more efficient and scalable processing the query. Experiments show that the proposed method can find effective top k answers efficiently.,,,
S0306457314001071," Teams are ranked to show their authority over each other . Existing methods rank the cricket teams using an ad hoc points system entirely based on the winning and losing of matches and ignores number of runs or wickets from which a team wins . In this paper adoptions of h index and PageRank are proposed for ranking teams to overcome the weakness of existing methods . Each team is represented by a node in the graph with two teams creates a weighted directed edge between each other by playing a match and the losing team points to the winning team . The intuition is to get more points for a team winning from a stronger team than winning from a weaker team by considering the number of runs or wickets also in addition to just winning and losing matches . The results show that proposed ranking methods provide quite promising insights of one day and test team rankings . The effect of damping factor d is also studied on the performance of PageRank based methods on both ODI and test matches teams ranking and interesting trends are found . 
",Proposal of graph and count weightage based ranking algorithms. Addition of parameters number of runs and wickets for graph weightage. Study of the affect of damping factor on different PageRank based ranking methods.,,,
S0306457314001083," In this work we develop new journal classification methods based on the h index . The introduction of the h index for research evaluation has attracted much attention in the bibliometric study and research quality evaluation . The main purpose of using an h index is to compare the index for different research units to differentiate their research performance . However the h index is defined by only comparing citations counts of one s own publications it is doubtful that the h index alone should be used for reliable comparisons among different research units like researchers or journals . In this paper we propose a new global h index where the publications in the core are selected in comparison with all the publications of the units to be evaluated . Furthermore we introduce some variants of the Gh index to address the issue of discrimination power . We show that together with the original h index they can be used to evaluate and classify academic journals with some distinct advantages in particular that they can produce an automatic classification into a number of categories without arbitrary cut off points . We then carry out an empirical study for classification of operations research and management science journals using this index and compare it with other well known journal ranking results such as the Association of Business Schools Journal Quality Guide and the Committee of Professors in OR ranking lists . 
",We argue that the h index has a deficiency in comparing journals for ranking purpose. We propose the global h index Gh index and its extensions to fix this problem. We apply the new indexes to produce an automatic journal classification without arbitrary cut off points. The new ranking results are compatible with other peer reviewed based ranking results.,,,
S0306457315000552," This study addresses the impact of domain expertise on the performance and query strategies used by users while searching for information . Twenty four experts and 24 non experts had to search for psychology information from the Universalis website in order to perform six information problems of varying complexity two simple problems two more difficult problems and two impossible problems . The results showed that participants with prior knowledge in the domain performed better than non experts . This difference was stronger as the complexity of the problems increased . This study also showed that experts and non experts displayed different query strategies . Experts reformulated the impossible problems more often than non experts because they produced new queries with psychology related keywords . The participants rarely used thematic category tool and when they did so this did not enhance their performance . 
",Three complexity levels of information problems are defined related to psychology domain. 40 students in psychology and in other domains performed information problems with an online encyclopedia. Students in psychology performed better than the others especially for complex problems. Students in psychologies used more relevant strategies than the others. These expertise related differences are stronger for the complex problems.,,,
S0377221713002518," Products that are not recycled at the end of their life increasingly damage the environment . In a collection remanufacturing scheme these end of life products can generate new profits . Designed on the personal computers industry this study defines an analytical model used to explore the implications of recycling on the reverse supply chain from an efficiency perspective for all participants in the process . The cases considered for analysis are the two and three echelon supply chains where we first look at the decentralized reverse setting followed by the coordinated setting through implementation of revenue sharing contract . We define customer willingness to return obsolete units as a function of the discount offered by the retailer in exchange for recycling devices with a remanufacturing value . The results show that performance measures and total supply chain profits improve through coordination with revenue sharing contracts on both two and three echelon reverse supply chains . 
",Proposed an analytical model for the two and three echelon reverse supply chain. Defined the PC recycling supply chain as foundation for generating the analytical model. Analyzed the model behavior and profit implications by the applicability of a numerical example.,,,
S0306457315001478," In contrast with their monolingual counterparts little attention has been paid to the effects that misspelled queries have on the performance of Cross Language Information Retrieval systems . The present work makes a first attempt to fill this gap by extending our previous work on monolingual retrieval in order to study the impact that the progressive addition of misspellings to input queries has this time on the output of CLIR systems . Two approaches for dealing with this problem are analyzed in this paper . Firstly the use of automatic spelling correction techniques for which in turn we consider two algorithms the first one for the correction of isolated words and the second one for a correction based on the linguistic context of the misspelled word . The second approach to be studied is the use of character n grams both as index terms and translation units seeking to take advantage of their inherent robustness and language independence . All these approaches have been tested on a from Spanish to English CLIR system that is Spanish queries on English documents . Real user generated spelling errors have been used under a methodology that allows us to study the effectiveness of the different approaches to be tested and their behavior when confronted with different error rates . The results obtained show the great sensitiveness of classic word based approaches to misspelled queries although spelling correction techniques can mitigate such negative effects . On the other hand the use of character n grams provides great robustness against misspellings . 
",We study the effects of misspelled queries on the performance of CLIR systems. Word based approaches as both indexing and translation units are highly sensitive to the presence of misspellings. The use of correction mechanisms can significantly reduce their negative effects. Classical techniques are suitable for shorter queries while context based corrections are suitable for longer queries. Our approach based on character n grams as both indexing and translation units shows remarkable strength.,,,
S0306457315000266," This paper examines the research patterns and trends of Recommendation System in China during the period of 2004 2013 . Data was collected from the China Academic Journal Network Publishing Database and the China Science Periodical Database . A co word analysis was conducted to measure correlation among the extracted keywords . The cluster analysis and social network analysis revealed 12 theme clusters network characteristics of the clusters the strategic diagram and the correlation network . The study results show that there are several important themes with a high correlation in Chinese RecSys research which is considered to be relatively focused mature and well developed overall . Some research themes have developed on a considerable scale while others remain isolated and undeveloped . This study also identified a few emerging themes with great potential for development . It was also determined that studies overall on the applications of RecSys are increasing . 
",We examine RecSys studies in China quantitatively empirically and longitudinally. Research on RecSys is relatively mature and well developed overall. Twelve theme clusters and six larger branches are identified. Some undeveloped or immature research themes continue to persist. Emerging themes with great potential for development are also identified.,,,
S0306457315000862," Learning to rank is an increasingly important scientific field that comprises the use of machine learning for the ranking task . New learning to rank methods are generally evaluated on benchmark test collections . However comparison of learning to rank methods based on evaluation results is hindered by the absence of a standard set of evaluation benchmark collections . In this paper we propose a way to compare learning to rank methods based on a sparse set of evaluation results on a set of benchmark datasets . Our comparison methodology consists of two components Normalized Winning Number which gives insight in the ranking accuracy of the learning to rank method and Ideal Winning Number which gives insight in the degree of certainty concerning its ranking accuracy . Evaluation results of 87 learning to rank methods on 20 well known benchmark datasets are collected through a structured literature search . ListNet SmoothRank FenchelRank FSMRank LRUF and LARF are Pareto optimal learning to rank methods in the Normalized Winning Number and Ideal Winning Number dimensions listed in increasing order of Normalized Winning Number and decreasing order of Ideal Winning Number . 
",We propose a novel way to compare learning to rank methods. We perform a meta analysis on a large set of papers that report ranking accuracy on a benchmark dataset. LRUF FSMRank FenchelRank SmoothRank and ListNet are the most accurate with increasing certainty.,,,
S0377221713001938," The worldwide propagation of mobile phone and the rapid development of location technologies have provided the chance to monitor freeway traffic conditions without requiring extra infrastructure investment . Over the past decade a number of research studies and operational tests have attempted to investigate the methods to estimate traffic measures using information from mobile phone . However most of these works ignored the fact that each vehicle has more than one phone due to the rapid popularity of mobile phone . This paper considered the circumstance of multi phones and proposed a relatively simplistic clustering technique to identify whether phones travel in the same vehicle . By using this technique mobile phone data can be used to determine not only speed but also vehicle counts by type and therefore density . A complex simulation covering different traffic condition and location accuracy of mobile phone has been developed to evaluate the proposed approach . Simulation results indicate that location accuracy of mobile phone is a crucial factor to estimate accurate traffic measures in case of a given location frequency and the number of continuous location data . In addition traffic demand and clustering method have a certain effect on the accuracy of traffic measures . 
",We considered the fact that multi phones exist in a traveling vehicle. We used a clustering method to estimate freeway traffic measures from mobile phone location data. Three factors influence estimation accuracy of traffic measures. Further development of location technologies will improve estimation accuracy.,,,
S0377221713002750," This paper presents a new relaxation technique to globally optimize mixed integer polynomial programming problems that arise in many engineering and management contexts . Using a bilinear term as the basic building block the underlying idea involves the discretization of one of the variables up to a chosen accuracy level . Multiparametric disaggregation technique for global optimization of polynomial programming problems . J. Glob . Optim . 55 227 251 by means of a radix based numeric representation system coupled with a residual variable to effectively make its domain continuous . Binary variables are added to the formulation to choose the appropriate digit for each position together with new sets of continuous variables and constraints leading to the transformation of the original mixed integer non linear problem into a larger one of the mixed integer linear programming type . The new underestimation approach can be made as tight as desired and is shown capable of providing considerably better lower bounds than a widely used global optimization solver for a specific class of design problems involving bilinear terms . 
",New approach for transforming polynomial MINLP into MILP. Approximating upper bounding and relaxation lower bounding formulations are given. Bilinear term is basic building block where one variable is discretized. Base 2 to base 10 numeric representation systems can be employed. Problem size increases logarithmically with number of discrete points optimality gap decreases linearly.,,,
S0306457315000849," We analyze the transitions from external search searching on web search engines to internal search searching on websites . We categorize 295 571 search episodes composed of a query submitted to web search engines and the subsequent queries submitted to a single website search by the same users . There are a total of 1 136 390 queries from all searches of which 295 571 are external search queries and 840 819 are internal search queries . We algorithmically classify queries into states and then use n grams to categorize search patterns . We cluster the searching episodes into major patterns and identify the most commonly occurring which are Explorers with a broad external search query and then broad internal search queries Navigators with an external search query containing a URL component and then specific internal search queries and Shifters with a different seemingly unrelated query types when transitioning from external to internal search . The implications of this research are that external search and internal search sessions are part of a single search episode and that online businesses can leverage these search episodes to more effectively target potential customers . 
",External search and internal search cohere searching sessions. The referral query from external search can be predictive of internal searching. There are common patterns that one can use to classify internal searchers.,,,
S0306457315000515," The Question Answering task aims to provide precise and quick answers to user questions from a collection of documents or a database . This kind of IR system is sorely needed with the dramatic growth of digital information . In this paper we address the problem of QA in the medical domain where several specific conditions are met . We propose a semantic approach to QA based on Natural Language Processing techniques which allow a deep analysis of medical questions and documents and semantic Web technologies at both representation and interrogation levels . We present our Semantic Question Answering System called MEANS and our proposed method for Answer Search based on semantic search and query relaxation . We evaluate the overall system performance on real questions and answers extracted from MEDLINE articles . Our experiments show promising results and suggest that a query relaxation strategy can further improve the overall performance . 
",We propose a semantic question answering system MEANS for the medical domain. We introduce a novel query relaxation approach for question answering. MEANS integrates NLP methods allowing a deep analysis of questions and documents. MEANS uses semantic Web technologies and standards for data sharing and integration. Our experiments show promising results in terms of MRR and precision.,,,
S0377221713002580," The equilibrium and socially optimal balking strategies are investigated for unobservable and observable single server classical retrial queues . There is no waiting space in front of the server . If an arriving customer finds the server idle he occupies the server immediately and leaves the system after service . Otherwise if the server is found busy the customer decides whether or not to enter a retrial pool with infinite capacity and becomes a repeated customer based on observation of the system and the reward cost structure imposed on the system . Accordingly two cases with respect to different levels of information are studied and the corresponding Nash equilibrium and social optimization balking strategies for all customers are derived . Finally we compare the equilibrium and optimal behavior regarding these two information levels through numerical examples . 
",Study the equilibrium strategic behavior and social optimization in the M M 1 retrial queues. Two different levels of information provided to arriving customers have been investigated extensively. Nash equilibrium and the equilibrium social benefit for all customers are derived. The effect of the information levels is illustrated by extensive numerical examples. It provides insight into the optimal design and control of the retrial queueing systems.,,,
S0377221713002725," We propose and apply a novel approach for modeling special day effects to predict electricity demand in Korea . Notably we model special day effects on an hourly rather than a daily basis . Hourly specified predictor variables are implemented in the regression model with a seasonal autoregressive moving average type error structure in order to efficiently reflect the special day effects . The interaction terms between the hour of day effects and the hourly based special day effects are also included to capture the unique intraday patterns of special days more accurately . The multiplicative SARMA mechanism is employed in order to identify the double seasonal cycles namely the intraday effect and the intraweek effect . The forecast results of the suggested model are evaluated by comparing them with those of various benchmark models for the following year . The empirical results indicate that the suggested model outperforms the benchmark models for both special and non special day predictions . 
",We model special day effects to predict hourly electricity demand in Korea. The regression model with hourly special day effect and the double SARMA error is suggested. The hourly special day effects are more important than the daily ones. The suggested model is effective during both the special day prediction and the non special day prediction.,,,
S0306457315000795," Analyzing and modeling users online search behaviors when conducting exploratory search tasks could be instrumental in discovering search behavior patterns that can then be leveraged to assist users in reaching their search task goals . We propose a framework for evaluating exploratory search based on implicit features and user search action sequences extracted from the transactional log data to model different aspects of exploratory search namely uncertainty creativity exploration and knowledge discovery . We show the effectiveness of the proposed framework by demonstrating how it can be used to understand and evaluate user search performance and thereby make meaningful recommendations to improve the overall search performance of users . We used data collected from a user study consisting of 18 users conducting an exploratory search task for two sessions with two different topics in the experimental analysis . With this analysis we show that we can effectively model their behavior using implicit features to predict the user s future performance level with above 70 accuracy in most cases . Further using simulations we demonstrate that our search process based recommendations improve the search performance of low performing users over time and validate these findings using both qualitative and quantitative approaches . 
",We propose a framework for evaluating exploratory search using implicit features. User search action sequences are also used to find behavior patterns. Show effectiveness with above 70 prediction accuracy for user search performance. Provide search process based recommendation to assist underperforming users. Demonstrate recommendation effectiveness both qualitatively and quantitatively.,,,
S0377221713002452," Emission trading schemes such as the European Union Emissions Trading System attempt to reconcile economic efficiency with ecological efficiency by creating financial incentives for companies to invest in climate friendly innovations . Using real options methodology we demonstrate that under uncertainty economic and ecological efficiency continue to be mutually exclusive . This problem is even worse if a climate friendly project depends on investing in of a whole supply chain . We model a sequential bargaining game in a supply chain where the parties negotiate over implementation of a carbon dioxide saving investment project . We show that the outcome of their bargaining is not economically efficient and even less ecologically efficient . Furthermore we show that a supply chain becomes less economically efficient and less ecologically efficient with every additional chain link . Finally we make recommendations for how managers or politicians can improve the situation and thereby increase economic as well as ecological efficiency and thus also the eco efficiency of supply chains . 
",We analyze the timing decision of a company which can invest in a CO2 saving project. Doing this we use a real options game theoretic model. Total emission of the supply chain increases with carbon price uncertainty. Furthermore inefficiency increases with the length of the supply chain. Cooperation and vertical integration improve ecologic as well as economic efficiency.,,,
S0306457315001417," Cross language plagiarism detection aims to detect plagiarised fragments of text among documents in different languages . In this paper we perform a systematic examination of Cross language Knowledge Graph Analysis an approach that represents text fragments using knowledge graphs as a language independent content model . We analyse the contributions to cross language plagiarism detection of the different aspects covered by knowledge graphs word sense disambiguation vocabulary expansion and representation by similarities with a collection of concepts . In addition we study both the relevance of concepts and their relations when detecting plagiarism . Finally as a key component of the knowledge graph construction we present a new weighting scheme of relations between concepts based on distributed representations of concepts . Experimental results in Spanish English and German English plagiarism detection show state of the art performance and provide interesting insights on the use of knowledge graphs . 
",Study of the impact of the implicit aspects of knowledge graphs for cross language plagiarism detection. We present a new weighting scheme for relations between concepts based on distributed representations of concepts. We obtain state of the art performance compared to several state of the art models.,,,
S0306457314000740," Aspect level sentiment analysis is important for numerous opinion mining and market analysis applications . In this paper we study the problem of identifying and rating review aspects which is the fundamental task in aspect level sentiment analysis . Previous review aspect analysis methods seldom consider entity or rating but only 2 tuples i.e . head and modifier pair e.g . in the phrase nice room room is the head and nice is the modifier . To solve this problem we novelly present a Quad tuple Probability Latent Semantic Analysis which incorporates entity and its rating together with the 2 tuples into the PLSA model . Specifically QPLSA not only generates fine granularity aspects but also captures the correlations between words and ratings . We also develop two novel prediction approaches the Quad tuple Prediction and the Expectation Prediction . For evaluation systematic experiments show that Quad tuple PLSA outperforms 2 tuple PLSA significantly on both aspect identification and aspect rating prediction for publication datasets . Moreover for aspect rating prediction QPLSA shows significant superiority over state of the art baseline methods . Besides the Quad tuple Prediction and the Expectation Prediction also show their strong ability in aspect rating on different datasets . 
",We propose two novel aspect rating prediction approaches i.e. Quad tuple prediction and Expectation prediction. We analyze and investigate the performance of the proposed aspect rating prediction methods in contrast with Local Prediction and Global Prediction. We experimentally inspect the influence of aspect rating variance for different rating prediction approaches.,,,
S0306457314001046," Word sense ambiguity has been identified as a cause of poor precision in information retrieval systems . Word sense disambiguation and discrimination methods have been defined to help systems choose which documents should be retrieved in relation to an ambiguous query . However the only approaches that show a genuine benefit for word sense discrimination or disambiguation in IR are generally supervised ones . In this paper we propose a new unsupervised method that uses word sense discrimination in IR . The method we develop is based on spectral clustering and reorders an initially retrieved document list by boosting documents that are semantically similar to the target query . For several TREC ad hoc collections we show that our method is useful in the case of queries which contain ambiguous terms . We are interested in improving the level of precision after 5 10 and 30 retrieved documents respectively . We show that precision can be improved by 8 above current state of the art baselines . We also focus on poor performing queries . 
",We propose a new unsupervised method that uses word sense discrimination in IR. The re ranking method is based on spectral clustering. The effectiveness over queries with ambiguous terms is proved on TREC corpora. Our interest regards improving the precision after 5 10 and 30 retrieved documents. The method improves results for queries with poor baseline performance.,,,
S0306457314001113," Video summarization aims at producing a compact version of a full length video while preserving the significant content of the original video . Movie summarization condenses a full length movie into a summary that still retains the most significant and interesting content of the original movie . In the past several movie summarization systems have been proposed to generate a movie summary based on low level video features such as color motion texture etc . However a generic summary which is common to everyone and is produced based only on low level video features will not satisfy every user . As users preferences for the summary differ vastly for the same movie there is a need for a personalized movie summarization system nowadays . To address this demand this paper proposes a novel system to generate semantically meaningful video summaries for the same movie which are tailored to the preferences and interests of a user . For a given movie shots and scenes are automatically detected and their high level features are semi automatically annotated . Preferences over high level movie features are explicitly collected from the user using a query interface . The user preferences are generated by means of a stored query . Movie summaries are generated at shot level and scene level where shots or scenes are selected for summary skim based on the similarity measured between shots and scenes and the user s preferences . The proposed movie summarization system is evaluated subjectively using a sample of 20 subjects with eight movies in the English language . The quality of the generated summaries is assessed by informativeness enjoyability relevance and acceptance metrics and Quality of Perception measures . Further the usability of the proposed summarization system is subjectively evaluated by conducting a questionnaire survey . The experimental results on the performance of the proposed movie summarization approach show the potential of the proposed system . 
",A novel personalized semantic movie summarization approach is proposed. User s subjective preferences are exploited for personalized movie summarization. An extensive user study demonstrates the advantages of the proposed system. A subjective study shows the users diverse behavior on the system usability. Proof of concept prototype of the proposed movie summarization system is provided.,,,
S0377221713002208," Considering the inherent connection between supplier selection and inventory management in supply chain networks this article presents a multi period inventory lot sizing model for a single product in a serial supply chain where raw materials are purchased from multiple suppliers at the first stage and external demand occurs at the last stage . The demand is known and may change from period to period . The stages of this production distribution serial structure correspond to inventory locations . The first two stages stand for storage areas for raw materials and finished products in a manufacturing facility and the remaining stages symbolize distribution centers or warehouses that take the product closer to customers . The problem is modeled as a time expanded transshipment network which is defined by the nodes and arcs that can be reached by feasible material flows . A mixed integer nonlinear programming model is developed to determine an optimal inventory policy that coordinates the transfer of materials between consecutive stages of the supply chain from period to period while properly placing purchasing orders to selected suppliers and satisfying customer demand on time . The proposed model minimizes the total variable cost including purchasing production inventory and transportation costs . The model can be linearized for certain types of cost structures . In addition two continuous and concave approximations of the transportation cost function are provided to simplify the model and reduce its computational time . 
",This article provides an inventory model for a serial supply chain with multiple suppliers and time varying demand. The model minimizes the total cost of purchasing production inventory setup and holding and transportation. Considering lead times necessary feasibility conditions regarding the demand at the last stage are established. Actual transportation costs are modeled by exact piecewise linear functions. Our results show that inventory setup and holding costs can affect supplier selection and order lot sizing decisions.,,,
S0306457316300450," In social networks identifying influential nodes is essential to control the social networks . Identifying influential nodes has been among one of the most intensively studies of analyzing the structure of networks . There are a multitude of evaluation indicators of node importance in social networks such as degree betweenness and cumulative nomination and so on . But most of the indicators only reveal one characteristic of the node . In fact in social networks node importance is not affected by a single factor but is affected by a number of factors . Therefore the paper puts forward a relatively comprehensive and effective method of evaluation node importance in social networks by using the multi objective decision method . Firstly we select several different representative indicators given a certain weight . We regard each node as a solution and different indicators of each node as the solution properties . Then through calculating the closeness degree of each node to the ideal solution we obtain evaluation indicator of node importance in social networks . Finally we verify the effectiveness of the proposed method experimentally on a few actual social networks . 
",Based on the theories of comprehensive decision making a multiattribute ranking method is presented. The proposed method is superior to other methods through contrast experiments. The theories of comprehensive decision making are utilized to detect important nodes in social networks. This method can be used in different types and different scales of networks.,,,
S0306457315000400," In order to successfully apply opinion mining to the large amounts of user generated content produced every day we need robust models that can handle the noisy input well yet can easily be adapted to a new domain or language . We here focus on opinion mining for YouTube by modeling classifiers that predict the type of a comment and its polarity while distinguishing whether the polarity is directed towards the product or video proposing a robust shallow syntactic structure that adapts well when tested across domains and evaluating the effectiveness on the proposed structure on two languages English and Italian . We rely on tree kernels to automatically extract and learn features with better generalization power than traditionally used bag of word models . Our extensive empirical evaluation shows that STRUCT outperforms the bag of words model both within the same domain it is particularly useful when tested across domains especially when little training data is available and the proposed structure is also effective in a lower resource language scenario where only less accurate linguistic processing tools are available . 
",We designed the first model for effectively carrying out opinion mining on YouTube comments. We propose kernel methods applied to a robust shallow syntactic structure which improves accuracy for both languages. Our approach greatly outperforms other basic models on cross domain settings. We created a YouTube corpus in Italian and English and made it available for the research community. Comments must be classified in subcategories to make opinion mining effective on YouTube.,,,
S0377221713002063," This paper highlights the subject of integrated projects planning in contemporary IS departments and presents a multi period multi project selection and assignment approach to assist the departments in handling continuous project based IS requests . The MPPA features a model to optimize the selection and assignment of IS projects . In the scope of multi project multi period planning the model innovatively considers the losses due to the accumulated postponement of a previously unselected IS request and the expected delay of ongoing projects when inserting a new project request . The MPPA also features an event based decisional process for cumulative selection and assignment on a multi period basis . Due to the complex and contextual nature of data in this paper a computerized system is implemented for aiding the execution of the model and the process . The paper reports on an industrial case for a demonstration of the proposed work . Finally the paper compares the MPPA with related work to summarize the value and role it may play in the IPP context . 
",The paper is an inter disciplinary research of applying OR in projects management. Address a sustainability issue of handling continuous IS project demands in contemporary IT departments. Features mixed integer programming model to optimize the selection and assignment of projects on a multi period multi project basis. It innovatively considers the losses due to unselecting or delaying projects in each time of selection and assignment. It implements a computerized solution that automatically enables the sustainability of continuous and cumulative selections and assignments.,,,
S0377221713002592," Apart from the well known weaknesses of the standard Malmquist productivity index related to infeasibility and not accounting for slacks already addressed in the literature we identify a new and significant drawback of the Malmquist Luenberger index decomposition that questions its validity as an empirical tool for environmental productivity measurement associated with the production of bad outputs . In particular we show that the usual interpretation of the technical change component in terms of production frontier shifts can be inconsistent with its numerical value thereby resulting in an erroneous interpretation of this component that passes on to the index itself . We illustrate this issue with a simple numerical example . Finally we propose a solution for this inconsistency issue based on incorporating a new postulate for the technology related to the production of bad outputs . 
",We show that the Malmquist Luenberger ML index may yield inconsistent results. The ML index can signal declining productivity when productivity is increasing. We discuss this issue within the standard decomposition of the ML index. We use a simple example and give cautionary advice to practitioners. We propose a solution for solving the inconsistency through a new postulate.,,,
S0306457315000278," This paper presents a Web intelligence portal that captures and aggregates news and social media coverage about Game of Thrones an American drama television series created for the HBO television network based on George R.R . Martin s series of fantasy novels . The system collects content from the Web sites of Anglo American news media as well as from four social media platforms Twitter Facebook Google and YouTube . An interactive dashboard with trend charts and synchronized visual analytics components not only shows how often Game of Thrones events and characters are being mentioned by journalists and viewers but also provides a real time account of concepts that are being associated with the unfolding storyline and each new episode . Positive or negative sentiment is computed automatically which sheds light on the perception of actors and new plot elements . 
",Westeros Sentinel a visual analytics dashboard for Game of Thrones. Extraction of affective and factual knowledge from news and social media coverage. Emotional categories from semantic knowledge bases. Automated annotation services for contextualized information spaces. Interactive visualizations to explore context features.,,,
S0306457314000855," Complex applications such as big data analytics involve different forms of coupling relationships that reflect interactions between factors related to technical business and environmental aspects . There are diverse forms of couplings embedded in poor structured and ill structured data . Such couplings are ubiquitous implicit and or explicit objective and or subjective heterogeneous and or homogeneous presenting complexities to existing learning systems in statistics mathematics and computer sciences such as typical dependency association and correlation relationships . Modeling and learning such couplings thus is fundamental but challenging . This paper discusses the concept of coupling learning focusing on the involvement of coupling relationships in learning systems . Coupling learning has great potential for building a deep understanding of the essence of business problems and handling challenges that have not been addressed well by existing learning theories and tools . This argument is verified by several case studies on coupling learning including handling coupling in recommender systems incorporating couplings into coupled clustering coupling document clustering coupled recommender algorithms and coupled behavior analysis for groups . 
",The concept of coupling and major coupling relationships. Coupling layers and forms appearing in complex data and applications. Modeling couplings measuring couplings and the curse of couplings. A new theoretical framework for the next generation recommender systems. Case studies of learning couplings in data mining and recommendation.,,,
S0308596115001238," Cloud computing combines established computing technologies and outsourcing advantages into a new ICT paradigm that is generally expected to foster productivity and economic growth . However despite a series of studies on the drivers of cloud adoption evidence of its economic effects is lacking possibly because many of the datasets on cloud computing are of insufficient size and often lack a time dimension as well as precise definitions of cloud computing thus making them unsuitable for rigorous quantitative analysis . To overcome these limitations we propose a proxy variable for cloud computing usage cloud adaptiveness based on survey panel data from European firms . Observations based on a descriptive analysis suggest three important aspects for further research . First cloud studies should be conducted at the industry level as cloud computing adaptiveness differs widely across industry sectors . Second it is important to know what firms do with cloud computing to understand the economic mechanisms and effects triggered by this innovation . And third cloud adaptiveness is potentially correlated to a firm s position in the supply chain and thus the type of output it produces as well as the market in which it operates . Our indicator can be employed to further analyze the effects of cloud computing in the context of firm heterogeneity . 
",We propose a measure for cloud computing using existing datasets. Our empirical findings map out an agenda for future research. Cloud studies should be at the industry level. Researchers need to figure out what firms actually do with cloud computing. Studying cloud usage across the value chain helps understanding the mechanisms.,,,
S0377221713002026," The attributes of vehicle routing problems are additional characteristics or constraints that aim to better take into account the specificities of real applications . The variants thus formed are supported by a well developed literature including a large variety of heuristics . This article first reviews the main classes of attributes providing a survey of heuristics and meta heuristics for Multi Attribute Vehicle Routing Problems . It then takes a closer look at the concepts of 64 remarkable meta heuristics selected objectively for their outstanding performance on 15 classic MAVRP with different attributes . This cross analysis leads to the identification of winning strategies in designing effective heuristics for MAVRP . This is an important step in the development of general and efficient solution methods for dealing with the large range of vehicle routing variants . 
",We identify classify and analyze 15 multi attribute vehicle routing problems. We analyze in detail 64 of the most efficient heuristics for these problems. We identify winning strategies for designing effective heuristics for MAVRP s.,,,
S0306457315000813," The rapid growth of information in the digital world especially on the web calls for automated methods of organizing the digital information for convenient access and efficient information retrieval . Topic modeling is a branch of machine learning and probabilistic graphical modeling that helps in arranging the web pages according to their topical structure . The topic distribution over a set of documents and the affinity of a document toward a specific topic can be revealed using topic modeling . Topic modeling algorithms are typically computationally expensive due to their iterative nature . Recent research efforts have attempted to parallelize specific topic models and are successful in their attempts . These parallel algorithms however have tightly coupled parallel processes which require frequent synchronization and are also tightly coupled with the underlying topic model which is used for inferring the topic hierarchy . In this paper we propose a parallel algorithm to infer topic hierarchies from a large scale document corpus . A key feature of the proposed algorithm is that it exploits coarse grained parallelism and the components running in parallel need not synchronize after every iteration thus the algorithm lends itself to be implemented on a geographically dispersed set of processing elements interconnected through a network . The parallel algorithm realizes a speed up of 53.5 on a 32 node cluster of dual core workstations and at the same time achieving approximately the same likelihood or predictive accuracy as that of the sequential algorithm with respect to the performance of Information Retrieval tasks . 
",We propose a novel parallel Algorithm for inferring topic hierarchies using HLDA. We use loosely coupled parallel tasks that do not require frequent synchronization. The parallel Algorithm is well suited to be run on distributed computing systems. The proposed Algorithm achieves a predictive accuracy on par with that of HLDA. The parallel Algorithm exhibits a near linear speed up and scales well.,,,
S0306457314000673," We live in the Information Age where most of the personal business and administrative data are collected and managed electronically . However poor data quality may affect the effectiveness of knowledge discovery processes thus making the development of the data improvement steps a significant concern . In this paper we propose the Multidimensional Robust Data Quality Analysis a domain independent technique aimed to improve data quality by evaluating the effectiveness of a black box cleansing function . Here the proposed approach has been realized through model checking techniques and then applied on a weakly structured dataset describing the working careers of millions of people . Our experimental outcomes show the effectiveness of our model based approach for data quality as they provide a fine grained analysis of both the source dataset and the cleansing procedures enabling domain experts to identify the most relevant quality issues as well as the action points for improving the cleansing activities . Finally an anonymized version of the dataset and the analysis results have been made publicly available to the community . 
",MRDQA a model based approach for supporting the Data Quality task on KDD. Evaluation of quality requirements of weakly structured data via model checking. A fine grained quality analysis of the cleansing procedures effectiveness. Automatic identification of error patterns and interactive visualisation. Experiments done on a real scenario making data publicly available.,,,
S0306457315001235," In this paper we focus on applying sentiment analysis to resources from online art collections by exploiting as information source tags intended as textual traces that visitors leave to comment artworks on social platforms . We present a framework where methods and tools from a set of disciplines ranging from Semantic and Social Web to Natural Language Processing provide us the building blocks for creating a semantic social space to organize artworks according to an ontology of emotions . The ontology is inspired by the Plutchik s circumplex model a well founded psychological model of human emotions . Users can be involved in the creation of the emotional space through a graphical interactive interface . The development of such semantic space enables new ways of accessing and exploring art collections . The affective categorization model and the emotion detection output are encoded into W3C ontology languages . This gives us the twofold advantage to enable tractable reasoning on detected emotions and related artworks and to foster the interoperability and integration of tools developed in the Semantic Web and Linked Data community . The proposal has been evaluated against a real word case study a dataset of tagged multimedia artworks from the ArsMeteo Italian online collection and validated through a user study . 
",Emotions evoked by artworks are captured by emotion analysis of social tags. An OWL 2 ontology based on Plutchik s circumplex model of emotions is developed. Ontologies linked data and affective lexicons are combined in a novel framework. Emotion driven organization and access to online art collections is enabled. The proposal is applied to a real dataset of tagged multimedia artworks.,,,
S0306457315000655," The emerging research area of opinion mining deals with computational methods in order to find extract and systematically analyze people s opinions attitudes and emotions towards certain topics . While providing interesting market research information the user generated content existing on the Web 2.0 presents numerous challenges regarding systematic analysis the differences and unique characteristics of the various social media channels being one of them . This article reports on the determination of such particularities and deduces their impact on text preprocessing and opinion mining algorithms . The effectiveness of different algorithms is evaluated in order to determine their applicability to the various social media channels . Our research shows that text preprocessing algorithms are mandatory for mining opinions on the Web 2.0 and that part of these algorithms are sensitive to errors and mistakes contained in the user generated content . 
",We carry out an empirical analysis to determine characteristics of social media channels. User generated content is noisy and contains mistakes emoticons etc. We evaluate text preprocessing algorithms regarding user generated content. Discussion of improvements to opinion mining process.,,,
S0377221713002051," This article introduces and solves a new rich routing problem integrated with practical operational constraints . The problem examined calls for the determination of the optimal routes for a vehicle fleet to satisfy a mix of two different request types . Firstly vehicles must transport three dimensional rectangular and stackable boxes from a depot to a set of predetermined customers . In addition vehicles must also transfer products between pairs of pick up and delivery locations . Service of both request types is subject to hard time window constraints . In addition feasible palletization patterns must be identified for the transported products . A practical application of the problem arises in the transportation systems of chain stores where vehicles replenish the retail points by delivering products stored at a central depot while they are also responsible for transferring stock between pairs of the retailer network . To solve this very complex combinatorial optimization problem our major objective was to develop an efficient methodology whose required computational effort is kept within reasonable limits . To this end we propose a local search based framework for optimizing vehicle routes in which feasible loading arrangements are identified via a simple structured packing heuristic . The algorithmic framework is enhanced with various memory components which store and retrieve useful information gathered through the search process in order to avoid any duplicate unnecessary calculations . The proposed solution approach is assessed on newly introduced benchmark instances . 
",New vehicle routing model with operational constraints. Routes satisfy a blend of plain delivery and pick up and delivery requests. Feasible pallet packing patterns must be identified for the transported goods. Local search framework equipped with appropriate memory components. Results are reported on new benchmark instances derived from well known routing problems.,,,
S0306457316000029," Electronic word of mouth communication is an important force in building a digital marketplace . The study of eWOM has implications for how to build an online community through social media design web communication and knowledge exchange . Innovative use of eWOM has significant benefits especially for start up firms . We focus on how users on the web communicate value related to online products . It is the premise of this paper that generating emotional value in social media and networking sites is critical for the survival of new e service ventures . Hence by introducing a formal value theory as a coding scheme we report a study on E value in SMNS by analyzing how a Swedish start up industrial design company attempted to build a global presence by creating followers on the web . The aim of the study was to investigate how the company s website design and communication can affect eWOM over time . This was done by capturing a series of emoticon and value expressions generated by community members from three different e communication campaigns with changing website content hence giving different stimuli to viewers . Those members who expressed emotional value often incorporating emoticons displayed both shorter verbal expressions and reaction time . These value expressions we suggest are important aspects of eWOM and need to be actively taken into account . The study has implications for information management strategies through using eWOM . 
",The paper is one of the few papers to explore the role of emotion in eWOM. It includes a practical study of a business using eWOM and the data is analyzed. It provides recommendations on how to use emotion in eWOM to encourage participation and improve sales.,,,
S0377221713002130," This paper presents a genetic algorithm for solving the resource constrained project scheduling problem . The innovative component of the algorithm is the use of a magnet based crossover operator that can preserve up to two contiguous parts from the receiver and one contiguous part from the donator genotype . For this purpose a number of genes in the receiver genotype absorb one another to have the same order and contiguity they have in the donator genotype . The ability of maintaining up to three contiguous parts from two parents distinguishes this crossover operator from the powerful and famous two point crossover operator which can maintain only two contiguous parts both from the same parent . Comparing the performance of the new procedure with that of other procedures indicates its effectiveness and competence . 
",A new precedence based crossover operator has been developed based on extensive research on related work. The operator can be applied to a variety of permutation problems. The procedure strikes a balance between diversification and intensification. The computational experiments show the effectiveness of the procedure. The operator is versatile the procedure is robust and the results are very promising.,,,
S0306457315000473," In this paper we propose a linguistically motivated query expansion framework that recognizes and encodes significant query constituents characterizing query intent in order to improve retrieval performance . Concepts of Interest are recognized as the core concepts that represent the gist of the search goal whilst the remaining query constituents which serve to specify the search goal and complete the query structure are classified as descriptive relational or structural . Acknowledging the need to form semantically associated base pairs for the purpose of extracting related potential expansion concepts an algorithm which capitalizes on syntactical dependencies to capture relationships between adjacent and non adjacent query concepts is proposed . Lastly a robust weighting scheme that duly emphasizes the importance of query constituents based on their linguistic role within the expanded query is presented . We demonstrate improvements in retrieval effectiveness in terms of increased mean average precision garnered by the proposed linguistic based query expansion framework through experimentation on the TREC ad hoc test collections . 
",Linguistically motivated query expansion framework. Classification of query constituents within four concept types descriptive relational structural and concepts of interest. Algorithm capitalizing on syntactical dependencies to capture relationships between adjacent and non adjacent query concepts. Weighting scheme emphasizing the importance of query constituents based on their linguistic role.,,,
S0306457314000715," Automatic document summarization using citations is based on summarizing what others explicitly say about the document by extracting a summary from text around the citations . While this technique works quite well for summarizing the impact of scientific articles other genres of documents as well as other types of summaries require different approaches . In this paper we introduce a new family of methods that we developed for legal documents summarization to generate catchphrases for legal cases . Our methods use both incoming and outgoing citations and we show how citances can be combined with other elements of cited and citing documents including the full text of the target document and catchphrases of cited and citing cases . On a legal summarization corpus our methods outperform competitive baselines . The combination of full text sentences and catchphrases from cited and citing cases is particularly successful . We also apply and evaluate the methods on scientific paper summarization where they perform at the level of state of the art techniques . Our family of citation based summarization methods is powerful and flexible enough to target successfully a range of different domains and summarization tasks . 
",We extend citation based summarization techniques to use both cited and citing documents. Our method combines citation text with other elements of cited and citing documents such as the full text and catchphrases. We investigate using citations to extract information from the target document. We apply these techniques on the domain of legal judgements a new area for citations based summarization. On a legal summarization corpus our methods outperform competitive baselines.,,,
S0306457315000540," Urban legends are a genre of modern folklore consisting of stories about rare and exceptional events just plausible enough to be believed which tend to propagate inexorably across communities . In our view while urban legends represent a form of sticky deceptive text they are marked by a tension between the credible and incredible . They should be credible like a news article and incredible like a fairy tale to go viral . In particular we will focus on the idea that urban legends should mimic the details of news to be credible while they should be emotional and readable like a fairy tale to be catchy and memorable . Using NLP tools we will provide a quantitative analysis of these prototypical characteristics . We also lay out some machine learning experiments showing that it is possible to recognize an urban legend using just these simple features . 
",Urban legends are viral deceptive texts in between credible and incredible. To be credible they mimic news articles while being incredible like a fairy tale. High level features who where when of news emotional readable of fairy tales. Quantitative analysis and machine learning experiments for recognizing urban legends.,,,
S0306457314000752," Online review mining has been used to help manufacturers and service providers improve their products and services and to provide valuable support for consumer decision making . Product aspect extraction is fundamental to online review mining . This research is aimed to improve the performance of aspect extraction from online consumer reviews . To this end we augment a frequency based extraction method with PMI IR which utilizes web search in measuring the semantic similarity between aspect candidates and target entities . In addition we extend RCut an algorithm originally developed for text classification to learn the threshold for selecting candidate aspects . Experiment results with Chinese online reviews show that our proposed method not only outperforms the state of the art frequency based method for aspect extraction but also generalizes across different product domains and various data sizes . 
",Augment frequency based aspect extraction with web based similarity measure. Improve the efficiency of aspect extraction methods that use PMI IR measures. Demonstrate the generality of the proposed method across various products.,,,
S0377221713002567," This paper presents a generalized weighted vertex p center model that represents uncertain nodal weights and edge lengths using prescribed intervals or ranges . The objective of the robust WVPC model is to locate p facilities on a given set of candidate sites so as to minimize worst case deviation in maximum weighted distance from the optimal solution . The RWVPC model is well suited for locating urgent relief distribution centers in an emergency logistics system responding to quick onset natural disasters in which precise estimates of relief demands from affected areas and travel times between URDCs and affected areas are not available . To reduce the computational complexity of solving the model this work proposes a theorem that facilitates identification of the worst case scenario for a given set of facility locations . Since the problem is NP hard a heuristic framework is developed to efficiently obtain robust solutions . Then a specific implementation of the framework based on simulated annealing is developed to conduct numerical experiments . Experimental results show that the proposed heuristic is effective and efficient in obtaining robust solutions . We also examine the impact of the degree of data uncertainty on the selected performance measures and the tradeoff between solution quality and robustness . Additionally this work applies the proposed RWVPC model to a real world instance based on a massive earthquake that hit central Taiwan on September 21 1999 . 
",Weighted p center model is proposed for locating urgent relief distribution centers. Uncertain relief demands and delivery times are represented by fixed intervals. The objective is to optimize worst case performances. Property of worst case scenarios is identified and applied in the algorithm. The proposed model is applied to a real world case based on a massive earthquake.,,,
S0377221713002658," Asset allocation among diverse financial markets is essential for investors especially under situations such as the financial crisis of 2008 . Portfolio optimization is the most developed method to examine the optimal decision for asset allocation . We employ the hidden Markov model to identify regimes in varied financial markets a regime switching model gives multiple distributions and this information can convert the static mean variance model into an optimization problem under uncertainty which is the case for unobservable market regimes . We construct a stochastic program to optimize portfolios under the regime switching framework and use scenario generation to mathematically formulate the optimization problem . In addition we build a simple example for a pension fund and examine the behavior of the optimal solution over time by using a rolling horizon simulation . We conclude that the regime information helps portfolios avoid risk during left tail events . 
",We applied the hidden Markov model for stock bond and commodity markets. HMM identified two extreme regimes and two transition regimes. We employed the stochastic programming under regime switching framework. Optimized portfolio performed well in crash periods by reducing the risky assets.,,,
S0306457315000412," Questionnaires are commonly used to measure attitudes toward systems and perceptions of search experiences . Whilst the face validity of such measures has been established through repeated use in information retrieval research their reliability and wider validity are not typically examined this threatens internal validity . The evaluation of self report questionnaires is important not only for the internal validity of studies and by extension increased confidence in the results but also for examining constructs of interest over time and across different domains and systems . In this paper we look at a specific questionnaire the User Engagement Scale for its robustness as a measure . We describe three empirical studies conducted in the online news domain and investigate the reliability and validity of the UES . Our results demonstrate good reliability of the UES sub scales however we argue that a four factor structure may be more appropriate than the original six factor structure proposed in earlier work . In addition we found evidence to suggest that the UES can differentiate between systems and experimental conditions . 
",We examined the robustness of the User Engagement Scale UES . Three studies were conducted in Canada and the United Kingdom. The UES sub scales were reliable across three samples of online news browser. A four factor structure rather than six may be more appropriate. The UES differentiated between online news sources and experimental conditions.,,,
S0306457315000667," This article describes in depth research on machine learning methods for sentiment analysis of Czech social media . Whereas in English Chinese or Spanish this field has a long history and evaluation datasets for various domains are widely available in the case of the Czech language no systematic research has yet been conducted . We tackle this issue and establish a common ground for further research by providing a large human annotated Czech social media corpus . Furthermore we evaluate state of the art supervised machine learning methods for sentiment analysis . We explore different pre processing techniques and employ various features and classifiers . We also experiment with five different feature selection algorithms and investigate the influence of named entity recognition and preprocessing on sentiment classification performance . Moreover in addition to our newly created social media dataset we also report results for other popular domains such as movie and product reviews . We believe that this article will not only extend the current sentiment analysis research to another family of languages but will also encourage competition potentially leading to the production of high end commercial solutions . 
",We explore state of the art supervised machine learning methods for sentiment analysis of Czech social media. We provide a large human annotated Czech social media corpus. We explore different pre processing techniques and employ various features and classifiers. We experiment with five different feature selection algorithms. Results are also reported on other widely popular domains such as movie and product reviews.,,,
S0306457314000521," The widespread availability of the Internet and the variety of Internet based applications have resulted in a significant increase in the amount of web pages . Determining the behaviors of search engine users has become a critical step in enhancing search engine performance . Search engine user behaviors can be determined by content based or content ignorant algorithms . Although many content ignorant studies have been performed to automatically identify new topics previous results have demonstrated that spelling errors can cause significant errors in topic shift estimates . In this study we focused on minimizing the number of wrong estimates that were based on spelling errors . We developed a new hybrid algorithm combining character n gram and neural network methodologies and compared the experimental results with results from previous studies . For the FAST and Excite datasets the proposed algorithm improved topic shift estimates by 6.987 and 2.639 respectively . Moreover we analyzed the performance of the character n gram method in different aspects including the comparison with Levenshtein edit distance method . The experimental results demonstrated that the character n gram method outperformed to the Levensthein edit distance method in terms of topic identification . 
",We used the character n gram method to predict topic changes in search engine queries. We obtained more successful estimations than previous studies and made remarkable contributions. We compared the character n gram method with the Levenshtein edit distance method. We analyzed ASPELL Google and Bing search engines as pre processed spelling correction methods. We conclude that Google could be used as a pre processed spelling correction method.,,,
S0377221713002476," Because individual interpretations of the analytic hierarchy process linguistic scale vary for each user this study proposes a novel framework that AHP decision makers can use to generate numerical scales individually based on the 2 tuple linguistic modeling of AHP scale problems . By using the concept of transitive calibration individual characteristics in understanding the AHP linguistic scale are first defined . An algorithm is then proposed for detecting the individual characteristics from the linguistic pairwise comparison data that is associated with each of the AHP individual decision makers . Finally a nonlinear programming model is proposed to generate individual numerical scales that optimally match the obtained individual characteristics . Two well known numerical examples are re examined using the proposed framework to demonstrate its validity . 
",This paper discusses the individual choice of numerical scale in AHP. We present an algorithm to obtain the linguistic individual characteristics in AHP. We set individual numerical scales to optimally match the individual characteristics.,,,
S0306457314000648," This paper proposes a novel query expansion method to improve accuracy of text retrieval systems . Our method makes use of a minimal relevance feedback to expand the initial query with a structured representation composed of weighted pairs of words . Such a structure is obtained from the relevance feedback through a method for pairs of words selection based on the Probabilistic Topic Model . We compared our method with other baseline query expansion schemes and methods . Evaluations performed on TREC 8 demonstrated the effectiveness of the proposed method with respect to the baseline . 
",A graph of terms can be effectively used for query expansion. Such a graph is extracted from documents thanks to a LDA based methodology. Proposed method achieves good performances on standard datasets.,,,
S0306457315001387," This investigation deals with the problem of language identification of noisy texts which could represent the primary step of many natural language processing or information retrieval tasks . Language identification is the task of automatically identifying the language of a given text . Although there exists several methods in the literature their performances are not so convincing in practice . In this contribution we propose two statistical approaches the high frequency approach and the nearest prototype approach . In the first one 5 algorithms of language identification are proposed and implemented namely character based identification word based identification special characters based identification sequential hybrid algorithm and parallel hybrid algorithm . In the second one we use 11 similarity measures combined with several types of character N Grams . For the evaluation task the proposed methods are tested on forum datasets containing 32 different languages . Furthermore an experimental comparison is made between the proposed approaches and some referential language identification tools such as LIGA NTC Google translate and Microsoft Word . Results show that the proposed approaches are interesting and outperform the baseline methods of language identification on forum texts . 
",This investigation deals with the problem of language identification of noisy texts. Two statistical approaches are proposed High Frequency Approach and Nearest Prototype Approach. The proposed methods are evaluated on forum datasets containing 32 different languages. An experimental comparison is made with LIGA NTC Google translate and Microsoft Word. Results show that the proposed approaches are interesting in language identification of forum texts.,,,
S0306457314000739," Probabilistic topic models are unsupervised generative models which model document content as a two step generation process that is documents are observed as mixtures of latent concepts or topics while topics are probability distributions over vocabulary words . Recently a significant research effort has been invested into transferring the probabilistic topic modeling concept from monolingual to multilingual settings . Novel topic models have been designed to work with parallel and comparable texts . We define multilingual probabilistic topic modeling and present the first full overview of the current research methodology advantages and limitations in MuPTM . As a representative example we choose a natural extension of the omnipresent LDA model to multilingual settings called bilingual LDA . We provide a thorough overview of this representative multilingual model from its high level modeling assumptions down to its mathematical foundations . We demonstrate how to use the data representation by means of output sets of per topic word distributions and per document topic distributions coming from a multilingual probabilistic topic model in various real life cross lingual tasks involving different languages without any external language pair dependent translation resource cross lingual event centered news clustering cross lingual document classification cross lingual semantic similarity and cross lingual information retrieval . We also briefly review several other applications present in the relevant literature and introduce and illustrate two related modeling concepts topic smoothing and topic pruning . In summary this article encompasses the current research in multilingual probabilistic topic modeling . By presenting a series of potential applications we reveal the importance of the language independent and language pair independent data representations by means of MuPTM . We provide clear directions for future research in the field by providing a systematic overview of how to link and transfer aspect knowledge across corpora written in different languages via the shared space of latent cross lingual topics that is how to effectively employ learned per topic word distributions and per document topic distributions of any multilingual probabilistic topic model in various cross lingual applications . 
",A systematic overview of multilingual probabilistic topic modeling MuPTM . A tutorial on methodology modeling training output inference and evaluation of MuPTM. Language independent and language pair independent data representations. A model independent framework and applications in various cross lingual tasks. A complete MuPTM based framework for cross lingual semantic similarity.,,,
S0306457315000291," The uncertainty children experience when searching for information influences their information seeking behavior by stimulating curiosity or hindering their search efforts . This study explored the interactions and the usability of various search interfaces and the enjoyment or uncertainty experienced by children when using them . Structural Equation Modeling was used to determine whether children feel uncertainty or a sense of control when using virtual game like interfaces to search for information associated with entertainment or as a means to satisfy an assigned learning task . We then analyzed the weight relationships among three latent variables using statistical analysis . Our results indicate that children prefer using a retrieval interface with situated affordance to satisfy entertainment related information needs as opposed to searching for information to solve specific problems . Furthermore their perceptions of text and graphic icons determined the degree to which they experienced a sense of uncertainty or control . When searching for entertainment related information they were better able to deal with uncertainty and sought greater control in their search interface compared to when they were searching for information related to assigned tasks . According to their information needs children may regard a game like interface as a toy or a tool for learning . The results of this study can serve as reference for the future development of information search interfaces aimed at arousing the interest of children . The use of virtual game like interfaces to guide the IS behavior of children warrants further study . 
",Elucidation of children s info seeking behaviors using virtual retrieval interfaces. The use of SEM to explore the relationship between fun and uncertainty. Children prefer a virtual retrieval interface when at play. Text sound channels are less definite but easier to use than icon channels. Children at play are willing to endure a high degree of uncertainty control.,,,
S0306457315000783," In this paper we investigate the impact of emotions on author profiling concretely identifying age and gender . Firstly we propose the EmoGraph method for modelling the way people use the language to express themselves on the basis of an emotion labelled graph . We apply this representation model for identifying gender and age in the Spanish partition of the PAN AP 13 corpus obtaining comparable results to the best performing systems of the PAN Lab of CLEF . 
",We take into account emotions for author profiling. We model the six basic emotions with a novel graph based approach EmoGraph . We achieve comparable results to the best performing systems of the PAN Lab of CLEF 2013.,,,
S0377221713001914," Majority of parallel machine scheduling studies consider machine as the only resource . However in most real life manufacturing environments jobs may require additional resources such as automated guided vehicles machine operators tools pallets dies and industrial robots for their handling and processing . This paper presents a review and discussion of studies on the parallel machine scheduling problems with additional resources . Papers are surveyed in five main categories machine environment additional resource objective functions complexity results and solution methods and other important issues . The strengths and weaknesses of the literature together with open areas for future studies are also emphasized . Finally extensions of integer programming models for two main classes of related problems are given and conclusions are drawn based on computational studies . number of jobs set of jobs number of machines set of machines indices of jobs i h 1 the set of possible modes belonging to job i index of resource modes processing time of job i when processed in mode k Ki processing time of job i on machine j when processed in mode k Ki the size of the single additional resource type the earliest time at which job i can start its processing i.e . release time due date of job i weight of job i denoting the importance of job i relative to other jobs completion time of job i maximum completion time of all jobs in the system i.e . makespan tardiness of job i equals to 1 if job i is tardy 0 otherwise maximum lateness 
",We review the literature on parallel machine scheduling with additional resources. We structure the review around five categories. We provide both the strengths and weaknesses of the studies together with open areas. We propose several extensions on some of the mathematical models given in the literature. We conclude with several remarks based on our computational studies and the review.,,,
S0377221713002531," Several major environmental issues like biodiversity loss and climate change currently concern the international community . These topics that are related to the development of human societies have become increasingly important since the United Nations Conference on Environment and Development or Earth Summit in Rio de Janeiro in 1992 . In this article we are interested in the first issue . We present here many examples of the help that using mathematical programming can provide to decision makers in the protection of biodiversity . The examples we have chosen concern the selection of nature reserves the control of adverse effects caused by landscape fragmentation including the creation or restoration of biological corridors the ecological exploitation of forests the control of invasive species and the maintenance of genetic diversity . Most of the presented models are or can be approximated with linear quadratic or fractional integer formulations and emphasize spatial aspects of conservation planning . Many of them represent decisions taken in a static context but temporal dimension is also considered . The problems presented are generally difficult combinatorial optimization problems some are well solved and others less well . Research is still needed to progress in solving them in order to deal with real instances satisfactorily . Moreover relations between researchers and practitioners have to be strengthened . Furthermore many recent achievements in the field of robust optimization could probably be successfully used for biodiversity protection a domain in which many data are uncertain . 
",We review representative optimization problems arising in biodiversity protection. They concern e.g. nature reserves landscape fragmentation and genetic diversity. Some of these problems come from the literature and others are new. For each problem we present one or several mathematical programming formulations. Some computational results are reported.,,,
S0306457315000850," The intention gap between users and queries results in ambiguous and broad queries . To solve these problems subtopic mining has been studied which returns a ranked list of possible subtopics according to their relevance popularity and diversity . This paper proposes a novel method to mine subtopics using simple patterns and a hierarchical structure of subtopic candidates . First relevant and various phrases are extracted as subtopic candidates using simple patterns based on noun phrases and alternative partial queries . Second a hierarchical structure of the subtopic candidates is constructed using sets of relevant documents from a web document collection . Finally the subtopic candidates are ranked considering a balance between popularity and diversity using this structure . In experiments our proposed methods outperformed the baselines and even an external resource based method at high ranked subtopics which shows that our methods can be effective and useful in various search scenarios like result diversification . 
",We use only web document collection instead of query logs and external resources. Our simple patterns are based on noun phrases and alternative partial queries. We maintain a balance between popularity and diversity of subtopics. Our method covered various search intentions of a query by its few subtopics. Our results were steadily improved by extracting more relevant and various subtopics.,,,
S0306457315000242," Sentiment analysis on Twitter has attracted much attention recently due to its wide applications in both commercial and public sectors . In this paper we present SentiCircles a lexicon based approach for sentiment analysis on Twitter . Different from typical lexicon based approaches which offer a fixed and static prior sentiment polarities of words regardless of their context SentiCircles takes into account the co occurrence patterns of words in different contexts in tweets to capture their semantics and update their pre assigned strength and polarity in sentiment lexicons accordingly . Our approach allows for the detection of sentiment at both entity level and tweet level . We evaluate our proposed approach on three Twitter datasets using three different sentiment lexicons to derive word prior sentiments . Results show that our approach significantly outperforms the baselines in accuracy and F measure for entity level subjectivity and polarity detections . For tweet level sentiment detection our approach performs better than the state of the art SentiStrength by 4 5 in accuracy in two datasets but falls marginally behind by 1 in F measure in the third dataset . 
",We propose a semantic sentiment representation of words called SentiCircle. SentiCircle captures the contextual semantic of words from their co occurrences. SentiCircle updates the sentiment of words based on their contextual semantics. SentiCircle can be used to perform entity and tweet level level sentiment analysis.,,,
S0306457315000527," Time is an important aspect of text documents . While some documents are atemporal many have strong temporal characteristics and contain contents related to time . Such documents can be mapped to their corresponding time periods . In this paper we propose estimating the focus time of documents which is defined as the time period to which document s content refers and which is considered complementary dimension to the document s creation time . We propose several estimators of focus time by utilizing statistical knowledge from external resources such as news article collections . The advantage of our approach is that document focus time can be estimated even for documents that do not contain any temporal expressions or contain only few of them . We evaluate the effectiveness of our methods on the diverse datasets of documents about historical events related to 5 countries . Our approach achieves average error of less than 21years on collections of Wikipedia pages extracts from history related books and web pages while using the total time frame of 113years . We also demonstrate an example classification method to distinguish temporal from atemporal documents . 
",Statistical approach for estimating the focus time of text documents. Classification framework for categorizing documents into temporal and atemporal. Bi Temporal Document Representation using document focus time and creation time.,,,
S0306457315001405," The task of finding groups or teams has recently received increased attention as a natural and challenging extension of search tasks aimed at retrieving individual entities . We introduce a new group finding task given a query topic we try to find knowledgeable groups that have expertise on that topic . We present five general strategies for this group finding task given a heterogenous document repository . The models are formalized using generative language models . Two of the models aggregate expertise scores of the experts in the same group for the task one locates documents associated with experts in the group and then determines how closely the documents are associated with the topic whilst the remaining two models directly estimate the degree to which a group is a knowledgeable group for a given topic . For evaluation purposes we construct a test collection based on the TREC 2005 and 2006 Enterprise collections and define three types of ground truth for our task . Experimental results show that our five knowledgeable group finding models achieve high absolute scores . We also find significant differences between different ways of estimating the association between a topic and a group . 
",We introduce a new information retrieval task given a topic try to find knowledgeable groups that have expertise on the topic. Five probabilistic language models are proposed to tackle the challenge of automatically finding groups of experts in heterogeneous document collections. For evaluation purpose a data set is created based on a publicly downloadable corpus used in the TREC Enterprise 2005 and 2006 tracks and three types of ground truth are defined. We provide a detailed analysis of the performance of the proposed group finding models.,,,
S0306457314000843," Research into unsupervised ways of stemming has resulted in the past few years in the development of methods that are reliable and perform well . Our approach further shifts the boundaries of the state of the art by providing more accurate stemming results . The idea of the approach consists in building a stemmer in two stages . In the first stage a stemming algorithm based upon clustering which exploits the lexical and semantic information of words is used to prepare large scale training data for the second stage algorithm . The second stage algorithm uses a maximum entropy classifier . The stemming specific features help the classifier decide when and how to stem a particular word . In our research we have pursued the goal of creating a multi purpose stemming tool . Its design opens up possibilities of solving non traditional tasks such as approximating lemmas or improving language modeling . However we still aim at very good results in the traditional task of information retrieval . The conducted tests reveal exceptional performance in all the above mentioned tasks . Our stemming method is compared with three state of the art statistical algorithms and one rule based algorithm . We used corpora in the Czech Slovak Polish Hungarian Spanish and English languages . In the tests our algorithm excels in stemming previously unseen words . Moreover it was discovered that our approach demands very little text data for training when compared with competing unsupervised algorithms . 
",New unsupervised stemming algorithm is introduced in this article. The algorithm exploits lexical as well as semantic information of words. Performance of stemming is measured on several languages Czech Slovak Polish Hungarian Spanish and English . We outperform competing stemmers in inflection removal test information retrieval task and language modeling task.,,,
S0377221713002105," In the paper we consider three quadratic optimization problems which are frequently applied in portfolio theory i.e . the Markowitz mean variance problem as well as the problems based on the mean variance utility function and the quadratic utility . Conditions are derived under which the solutions of these three optimization procedures coincide and are lying on the efficient frontier the set of mean variance optimal portfolios . It is shown that the solutions of the Markowitz optimization problem and the quadratic utility problem are not always mean variance efficient . The conditions for the mean variance efficiency of the solutions depend on the unknown parameters of the asset returns . We deal with the problem of parameter uncertainty in detail and derive the probabilities that the estimated solutions of the Markowitz problem and the quadratic utility problem are mean variance efficient . Because these probabilities deviate from one the above mentioned quadratic optimization problems are not stochastically equivalent . The obtained results are illustrated by an empirical study . 
",We consider three quadratic optimization problems applied in portfolio theory. Conditions are derived under which the solutions of these three optimization procedures are lying on the efficient frontier. We show that the three quadratic optimization problems are mathematically equivalent but not from a stochastic point of view. We derive the probabilities that the estimated solutions of the Markowitz problem and the quadratic utility problem are mean variance efficient.,,,
S0306457315000874," In reputation management knowing what impact a tweet has on the reputation of a brand or company is crucial . The reputation polarity of a tweet is a measure of how the tweet influences the reputation of a brand or company . We consider the task of automatically determining the reputation polarity of a tweet . For this classification task we propose a feature based model based on three dimensions the source of the tweet the contents of the tweet and the reception of the tweet i.e . how the tweet is being perceived . For evaluation purposes we make use of the RepLab 2012 and 2013 datasets . We study and contrast three training scenarios . The first is independent of the entity whose reputation is being managed the second depends on the entity at stake but has over 90 fewer training samples per model on average . The third is dependent on the domain of the entities . We find that reputation polarity is different from sentiment and that having less but entity dependent training data is significantly more effective for predicting the reputation polarity of a tweet than an entity independent training scenario . Features related to the reception of a tweet perform significantly better than most other features . 
",We find that reputation polarity of a post is different from sentiment. We model reputation polarity using feature classes from communication theory. We introduce new features based on the replies to a post. We propose different ways to operationalise the RepLab 2012 and 2013 tasks.,,,
S0377221713002993," This paper focuses on detecting nuclear weapons on cargo containers using port security screening methods where the nuclear weapons would presumably be used to attack a target within the United States . This paper provides a linear programming model that simultaneously identifies optimal primary and secondary screening policies in a prescreening based paradigm where incoming cargo containers are classified according to their perceived risk . The proposed linear programming model determines how to utilize primary and secondary screening resources in a cargo container screening system given a screening budget prescreening classifications and different device costs . Structural properties of the model are examined to shed light on the optimal screening policies . The model is illustrated with a computational example . Sensitivity analysis is performed on the ability of the prescreening in correctly identifying prescreening classifications and secondary screening costs . Results reveal that there are fewer practical differences between the screening policies of the prescreening groups when prescreening is inaccurate . Moreover devices that can better detect shielded nuclear material have the potential to substantially improve the system s detection capabilities . 
",We apply knapsack analysis to the problem of screening for nuclear materials. The analysis considers various levels of intelligence based pre screening . We illustrate the effectiveness of screening methods using speculative values. The analysis models a range of threats when screening for nuclear material.,,,
S0377221713002099," This paper clarifies the relation between decisions of a risk averse decision maker based on expected utility theory on the one hand and spectral risk measures on the other . We first demonstrate that recent approaches to this problem generally do not provide strongly consistent results i.e . they fail to induce identical preference orders simultaneously with both concepts . Then we detail the relation between risk averse decisions under the dual theory of choice and spectral risk measures . This relation is identified as the fundamental reason why it is not in general possible to establish a simple one to one mapping between expected utility theory and spectral risk measures . We are nonetheless able to use spectral risk measures to model decisions obtained using expected utility theory . Interestingly this implies that a given utility function corresponds to a whole family of risk spectra . 
",We establish a formal link between expected utility and spectral risk measures. We give sufficient conditions for existence and uniqueness of such a link. We provide a spectral version of the Pratt Arrow risk aversion measure.,,,
S0306457315001247," The explosion of online user generated content and the development of big data analysis provide a new opportunity and challenge to understand and respond to public opinions in the G2C e government context . To better understand semantic searching of public comments on an online platform for citizens opinions about urban affairs issues this paper proposed an approach based on the latent Dirichlet allocation a probabilistic topic modeling method and designed a practical system to provide users municipal administrators of B city with satisfying searching results and the longitudinal changing curves of related topics . The system is developed to respond to actual demand from B city s local government and the user evaluation experiment results show that a system based on the LDA method could provide information that is more helpful to relevant staff members . Municipal administrators could better understand citizens online comments based on the proposed semantic search approach and could improve their decision making process by considering public opinions . 
",The explosion of online user generated content UGC and the development of big data analysis provide a new opportunity and challenge to understand and respond to public opinions in the G2C e government context. We proposed an approach based on the latent Dirichlet allocation LDA and designed a practical system to provide users with satisfying searching results and the longitudinal changing curves of related topics. Municipal administrators could better understand citizens online comments based on the proposed semantic search approach and could improve their decision making process by considering public opinions.,,,
S0306457314000922," We propose TripBuilder an unsupervised framework for planning personalized sightseeing tours in cities . We collect categorized Points of Interests from Wikipedia and albums of geo referenced photos from Flickr . By considering the photos as traces revealing the behaviors of tourists during their sightseeing tours we extract from photo albums spatio temporal information about the itineraries made by tourists and we match these itineraries to the Points of Interest of the city . The task of recommending a personalized sightseeing tour is modeled as an instance of the Generalized Maximum Coverage problem where a measure of personal interest for the user given her preferences and visiting time budget is maximized . The set of actual trajectories resulting from the GMC solution is scheduled on the tourist s agenda by exploiting a particular instance of the Traveling Salesman Problem . Experimental results on three different cities show that our approach is effective efficient and outperforms competitive baselines . 
",The formal problem of planning a tourist visit as a fully automatic two step process. An unsupervised method for mining common patterns of movements of tourists. A comprehensive evaluation of TripBuilder.,,,
S0377221713002075," Data envelopment analysis allows us to evaluate the relative efficiency of each of a set of decision making units . However the methodology does not permit us to identify specific sources of inefficiency because DEA views the DMU as a black box that consumes a mix of inputs and produces a mix of outputs . Thus DEA does not provide a DMU manager with insight regarding the internal source of the organization s inefficiency . Recent methodological developments have extended the basic DEA methodology to allow the analyst to look inside the DMU and model the network of production processes that comprise the organization . In such models sub DMUs consume inputs from outside the DMU and intermediate products from other sub DMUs to produce outputs that flow out of the DMU and intermediate products that flow into other sub DMUs . In this paper we present an unoriented two stage DEA model to measure efficiency in situations in which analysts seek to simultaneously reduce input quantities and increase output quantities . The methodology extends previous work in which the model must be either input oriented or output oriented . The key to the methodology is an iterative algorithm that alternates between an input oriented push backward step and an output oriented push forward step that is characterized by damped oscillations in the intermediate products . We apply the methodology to Major League Baseball teams during the 2009 season to demonstrate how this approach provides a deeper understanding of each team s operations . 
",We introduce an iterative approach to solve the unoriented two stage DEA model using the classical radial objective. Our approach determines optimal levels for each input and output by simultaneously reducing inputs and increasing outputs while the intermediate products oscillate and converge. Unlike the slacks based approach our model does not require arbitrary weights on inputs and outputs. We demonstrate our model by applying it to Major League Baseball teams during the 2009 season. The model has one input two intermediate products and one output.,,,
S0306457314001125," Researchers have shown that a weighted linear combination in data fusion can produce better results than an unweighted combination . Many techniques have been used to determine the linear combination weights . In this work we have used the Genetic Algorithm for the same purpose . The GA is not new and it has been used earlier in several other applications . But to the best of our knowledge the GA has not been used for fusion of runs in information retrieval . First we use GA to learn the optimum fusion weights using the entire set of relevance assessment . Next we learn the weights from the relevance assessments of the top retrieved documents only . Finally we also learn the weights by a twofold training and testing on the queries . We test our method on the runs submitted in TREC . We see that our weight learning scheme using both full and partial sets of relevance assessment produces significant improvements over the best candidate run CombSUM CombMNZ Z Score linear combination method with performance level performance level square weighting scheme multiple linear regression based weight learning scheme mixture model result merging scheme LambdaMerge ClustFuseCombSUM and ClustFuseCombMNZ . Furthermore we study how the correlation among the scores in the runs can be used to eliminate redundant runs in a set of runs to be fused . We observe that similar runs have similar contributions in fusion . So eliminating the redundant runs in a group of similar runs does not hurt fusion performance in any significant way . 
",Genetic Algorithm is an effective scheme in determining data fusion weights. Tuning Genetic Algorithm increases time efficiency. Weight learning from only top ranked documents is useful. Redundant runs can be removed based on correlation between scores.,,,
S0306457315001028," A main challenge in Cross Language Information Retrieval is to estimate a proper translation model from available translation resources since translation quality directly affects the retrieval performance . Among different translation resources we focus on obtaining translation models from comparable corpora because they provide appropriate translations for both languages and domains with limited linguistic resources . In this paper we employ a two step approach to build an effective translation model from comparable corpora without requiring any additional linguistic resources for the CLIR task . In the first step translations are extracted by deriving correlations between source target word pairs . These correlations are used to estimate word translation probabilities in the second step . We propose a language modeling approach for the first step where modeling based on probability distribution provides two key advantages . First our approach can be tuned easier in comparison with heuristically adjusted previous work . Second it provides a principled basis for integrating additional lexical and translational relations to improve the accuracy of translations from comparable corpora . As an indication we integrate monolingual relations of word co occurrences into the process of translation extraction which helps to extract more reliable translations for low frequency words in a comparable corpus . Experimental results on an English Persian comparable corpus show that our method outperforms the previous approaches in terms of both translation quality and the performance of CLIR . Indeed the proposed method is naturally applicable to any comparable corpus regardless of its languages . In addition we demonstrate the significant impact of word translation probabilities estimated in the second step of our approach on the performance of CLIR . 
",Proposing a language modeling method to extract translations from comparable corpora. Comparing two similarity functions for deriving bilingual word correlations. Improving translation quality by integrating co occurrence relations into word models. Comparing different estimations of translation probabilities from word correlations. Showing the significant impact of probability estimation methods on CLIR performance.,,,
S0377221713002671," A popular assumption in the current literature on remanufacturing is that the whole new product is produced by an integrated manufacturer which is inconsistent with most industries . In this paper we model a decentralised closed loop supply chain consisting of a key component supplier and a non integrated manufacturer and demonstrate that the interaction between these players significantly impacts the economic and environmental implications of remanufacturing . In our model the non integrated manufacturer can purchase new components from the supplier to produce new products and remanufacture used components to produce remanufactured products . Thus the non integrated manufacturer is not only a buyer but also a rival to the supplier . In a steady state period we analyse the performances of an integrated manufacturer and the decentralised supply chain . We find that although the integrated manufacturer always benefits from remanufacturing the remanufacturing opportunity may constitute a lose lose situation to the supplier and the non integrated manufacturer making their profits be lower than in an identical supply chain without remanufacturing . In addition the non integrated manufacturer may be worse off with a lower remanufacturing cost or a larger return rate of used products due to the interaction with the supplier . We further demonstrate that the government subsidised remanufacturing in the non integrated manufacturer is detrimental to the environment . 
",We model a closed loop supply chain in which the manufacturer can remanufacture used products. The interaction with the supplier significantly impacts the performance of remanufacturing. The remanufacturing opportunity can form a lose lose situation making the players profits shrunk. The manufacturer may be worse off with a lower remanufacturing cost or a larger return rate. The remanufacturing may be detrimental to the environment.,,,
S0306457315001430," This practical study aims to enrich the current literature by providing new practical evidence of the positive and negative influence factors of the Internet on generations Y and Z in Australia and Portugal . The Internet has become a powerful force among these Gens in relation to communication cooperation collaboration and connection but numerous problems from cognitive social and physical developments perspective are developed throughout the Internet usage . A quantitative approach was used to collect new practical evidence from 180 Australian and 85 Portuguese respondents with a total of 265 respondents completing an online survey . This study identifies new positive factors to the Internet usage as problem solving skills proactive study information gathering and awareness globally and locally communication and collaboration with their peers and family were improved and enhanced . Alternatively this study identifies new negative factors as physical contact and physical activities were prevented thinking concentrating and memory skills were reduced depressed and isolated laziness having increased nevertheless the Internet encourages Gens Y and Z to play physical and virtual games . Finally this study concluded that the Internet is becoming an essential part of the everyday routines and practices of Gens Y and Z . 
",Examine the influence factors of the Internet on Gens Y and Z. Empirical evidence was collected via an online survey in Australia and Portugal. This study composed new positive and negative factors of the Internet usage. Set of recommendations are raised to reduce the negativities of the Internet usage.,,,
S0377221713002506," We consider the optimal ship navigation problem wherein the goal is to find the shortest path between two given coordinates in the presence of obstacles subject to safety distance and turn radius constraints . These obstacles can be debris rock formations small islands ice blocks other ships or even an entire coastline . We present a graph theoretic solution on an appropriately weighted directed graph representation of the navigation area obtained via 8 adjacency integer lattice discretization and utilization of the A algorithm . We explicitly account for the following three conditions as part of the turn radius constraints the ship s left and right turn radii are different ship s speed reduces while turning and the ship needs to navigate a certain minimum number of lattice edges along a straight line before making any turns . The last constraint ensures that the navigation area can be discretized at any desired resolution . Once the optimal path is determined we smoothen it to emulate the actual navigation of the ship . We illustrate our methodology on an ice navigation example involving a 100 000 DWT merchant ship and present a proof of concept by simulating the ship s path in a full mission ship handling simulator . 
",We consider the optimal ship navigation problem in the presence of obstacles. We explicitly account for safety distance and realistic ship turn radius constraints. We present a graph theoretic solution on an appropriately weighted directed graph. Our methodology is illustrated on a merchant ship ice navigation example.,,,
S0377221713002737," Competitive location problems can be characterized by the fact that the decisions made by others will affect our own payoffs . In this paper we address a discrete competitive location game in which two decision makers have to decide simultaneously where to locate their services without knowing the decisions of one another . This problem arises in a franchising environment in which the decision makers are the franchisees and the franchiser defines the potential sites for locating services and the rules of the game . At most one service can be located at each site and one of the franchisees has preferential rights over the other . This means that if both franchisees are interested in opening the service in the same site only the one that has preferential rights will open it . We consider that both franchisees have budget constraints but the franchisee without preferential rights is allowed to show interest in more sites than the ones she can afford . We are interested in studying the influence of the existence of preferential rights and overbidding on the outcomes for both franchisees and franchiser . A model is presented and an algorithmic approach is developed for the calculation of Nash equilibria . Several computational experiments are defined and their results are analysed showing that preferential rights give its holder a relative advantage over the other competitor . The possibility of overbidding seems to be advantageous for the franchiser as well as the inclusion of some level of asymmetry between the two decision makers . 
",We develop a model where two players simultaneously choose their facilities sites. One of the players has preferential rights over the other. Nash equilibria are calculated by an algorithmic approach using linear programming. Including some level of asymmetry between players can benefit the franchiser.,,,
S0377221713001999," Robust optimization problems which have uncertain data are considered . We prove surrogate duality theorems for robust quasiconvex optimization problems and surrogate min max duality theorems for robust convex optimization problems . We give necessary and sufficient constraint qualifications for surrogate duality and surrogate min max duality and show some examples at which such duality results are used effectively . Moreover we obtain a surrogate duality theorem and a surrogate min max duality theorem for semi definite optimization problems in the face of data uncertainty . 
",We show a set containment characterization with data uncertainty. We investigate surrogate strong duality theorem for robust quasiconvex programming with its constraint qualification. We investigate surrogate min max duality theorem for robust quasiconvex programming with its constraint qualification. We obtain a surrogate duality theorems for semi definite optimization problems in the face of data uncertainty.,,,
S0306457315000825," A series of events generates multiple types of time series data such as numeric and text data over time and the variations of the data types capture the events from different angles . This paper aims to integrate the analyses on such numerical and text time series data influenced by common events with a single model to better understand the events . Specifically we present a topic model called an associative topic model which finds the soft cluster of time series text data guided by time series numerical value . The identified clusters are represented as word distributions per clusters and these word distributions indicate what the corresponding events were . We applied ATM to financial indexes and president approval rates . First ATM identifies topics associated with the characteristics of time series data from the multiple types of data . Second ATM predicts numerical time series data with a higher level of accuracy than does the iterative model which is supported by lower mean squared errors . 
",Introduce a probabilistic graphical model extracting topics with numerical guidance. Enhance the regression performance with unified PGM of text and numbers. Tightly links the analysis on numeric and text data over time.,,,
S0377221713002579," Robust portfolios reduce the uncertainty in portfolio performance . In particular the worst case optimization approach is based on the Markowitz model and form portfolios that are more robust compared to mean variance portfolios . However since the robust formulation finds a different portfolio from the optimal mean variance portfolio the two portfolios may have dissimilar levels of factor exposure . In most cases investors need a portfolio that is not only robust but also has a desired level of dependency on factor movement for managing the total portfolio risk . Therefore we introduce new robust formulations that allow investors to control the factor exposure of portfolios . Empirical analysis shows that the robust portfolios from the proposed formulations are more robust than the classical mean variance approach with comparable levels of exposure on fundamental factors . 
",We introduce the robustness measure and its properties. We propose new robust formulations that allow investors to control the factor exposure of portfolios. The optimal portfolio of our model is not only robust but also has a desired factor exposure.,,,
S0377221713002166," Natural earthquake disasters are unprecedented incidents which take many lives as a consequence and cause major damages to lifeline infrastructures . Various agencies in a country are responsible for reducing such adverse impacts within specific budgets . These responsibilities range from before to after the incident targeting one of the main phases of disaster management . Use of OR in disaster management and coordination of its phases has been mostly ignored and highly recommended in former reviews . This paper presents a formulation to coordinate three main agencies and proposes a heuristic approach to solve the different introduced sub problems . The results show an improvement of 7.5 24 when the agencies are coordinated . entire population of sub region r in region l vulnerable population ratio of sub region r in region l vulnerable population of sub region r in region l improvement ratio of sub region r in region l budget for investment in the building renovation sector existing emergency supplies in region k inventory cost of one unit of humanitarian goods additional level of humanitarian goods level of relief supplies sent from sub region kto sub region rl budget for investment in the emergency response sector travel time from sub region k to sub region rl survival function which describes the efficiency of goods mobility full cost of retrofitting link failure probability of link retrofitting ratio of link budget for investment in the transportation sector 
",We model three different agencies involved in disaster management through OR. We coordinate agencies responsible for different phases of disaster management. Coordination indicates an average improvement of 7.5 24 in the death toll. Through multi agent optimization connections between the agencies are observed. Efficiency of relief operations is dependent on transportation network reliability.,,,
S0306457315000448," Temporal aspects have been receiving a great deal of interest in Information Retrieval and related fields . Although previous studies have proposed designed and implemented temporal aware systems and solutions understanding of people s temporal information searching behaviour is still limited . This paper reports the findings of a user study that explored temporal information searching behaviour and strategies in a laboratory setting . Information needs were grouped into three temporal classes to systematically study their characteristics . The main findings of our experiment are as follows . It is intuitive for people to augment topical keywords with temporal expressions such as history recent or future as a tactic of temporal search . However such queries produce mixed results and the success of query reformulations appears to depend on topics to a large extent . Search engine interfaces should detect temporal information needs to trigger the display of temporal search options . Finding a relevant Wikipedia page or similar summary page is a popular starting point of past information needs . Current search engines do a good job for information needs related to recent events but more work is needed for past and future tasks . Participants found it most difficult to find future information . Searching for domain experts was a key tactic in Future search and file types of relevant documents are different from other temporal classes . Overall the comparison of search across temporal classes indicated that Future search was the most difficult and the least successful followed by the search for the Past and then for Recency information . This paper discusses the implications of these findings on the design of future temporal IR systems . 
",Temporal information searching behaviour and strategies were investigated. Searching patterns were identified for past recency and future search tasks. Implications for the development of temporal IR systems are discussed.,,,
S0306457315001223," The paper reports on some of the results of a research project into how changes in digital behaviour and services impacts on concepts of trust and authority held by researchers in the sciences and social sciences in the UK and the USA . Interviews were used in conjunction with a group of focus groups to establish the form and topic of questions put to a larger international sample in an online questionnaire . The results of these 87 interviews were analysed to determine whether or not attitudes have indeed changed in terms of sources of information used citation behaviour in choosing references and in dissemination practices . It was found that there was marked continuity in attitudes though an increased emphasis on personal judgement over established and new metrics . Journals were more highly respected than other sources and still the vehicle for formal scholarly communication . The interviews confirmed that though an open access model did not in most cases lead to mistrust of a journal a substantial number of researchers were worried about the approaches from what are called predatory OA journals . Established researchers did not on the whole use social media in their professional lives but a question about outreach revealed that it was recognised as effective in reaching a wider audience . There was a remarkable similarity in practice across research attitudes in all the disciplines covered and in both the countries where interviews were held . 
",Digital transition had resulted in changes in researcher behaviour. It is now easier for scientists to discover and disseminate research. The way scientists exercise trust has not changed. Metrics are less important than experience and personal recommendation.,,,
S0377221713001902," Statistical process control and maintenance planning have long been treated as two separate problems . The interdependence between these two activities has not been adequately addressed in the literature despite their apparent connections . Information obtained in the course of statistical process control signals the need for possible maintenance actions and thus affects the preventive maintenance schedules . Preventive maintenance actions can prevent a production process from further deterioration and improve product quality in conjunction with statistical process control . This paper presents an integrated model for the joint optimization of statistical process control and preventive maintenance . The proposed model is developed for a production process that deteriorates according to a discrete time Markov chain . It is assumed that preventive maintenance is imperfect and both preventive and corrective maintenance are instantaneous . The formulation of the deterioration process with maintenance interventions formulated as a Markov chain provides a breakthrough in designing an efficient solution algorithm and obtaining analytical results . A numerical example is used to illustrate the proposed integrated statistical process control and preventive maintenance policies . Sensitivity analysis is conducted to analyze the impact of model parameters on optimal policies . Sensitivity analysis further indicates the interrelationship between statistical process control and maintenance actions . Numerical results indicate that potential cost savings can be achieved from the proposed integrated policies . 
",We developed a model for the joint optimization of process control and maintenance. Information from the control charts is used to facilitate maintenance decisions. Process control and maintenance procedures are inter dependent. Potential cost savings can be obtained from joint SPC and maintenance policies.,,,
S0306457314000909," Search task difficulty has been attracting much research attention in recent years mostly regarding its relationship with searchers behaviors and the prediction of task difficulty from search behaviors . However it remains unknown what makes searchers feel the difficulty . A study consisting of 48 undergraduate students was conducted to explore this question . Each participant was given 4 search tasks that were carefully designed following a task classification scheme . Questionnaires were used to elicit participants ratings on task difficulty and why they gave those ratings . Based on the collected difficulty reasons a coding scheme was developed which covered various aspects of task user and user task interaction . Difficulty reasons were then categorized following this scheme . Results showed that searchers reported some common reasons leading to task difficulty in different tasks but most of the difficulty reasons varied across tasks . In addition task difficulty had some common reasons between searchers with low and high levels of topic knowledge although there were also differences in top task difficulty reasons between high and low knowledge users . These findings further our understanding of search task difficulty the relationship between task difficulty and task type and that between task difficulty and knowledge level . The findings can also be helpful with designing tasks for information search experiments and have implications on search system design both in general and for personalization based on task type and searchers knowledge . 
",Development of a search task difficulty reason scheme. Relationship between task difficulty and task type common and different reasons across task types. Relationship between task difficulty and user knowledge common and different reasons among different knowledge groups. Implications for general and personalized information retrieval system design.,,,
S0306457315000230," Vendors of Business Intelligence software have recently started extending their systems by features from social software . The generated reports may include profiles of report authors and later be supplemented by information about users who accessed the report user evaluations of the report or other social cues . With these features users can support each other in discovering and filtering valuable information in the context of BI . Users who consider reusing an existing report that was not designed by or for them can now not only peruse the report content but also take the social cues into consideration . We analyze which report features influence their perception of report usefulness . Our analysis is based on the elaboration likelihood model which assumes that information recipients are either influenced by the quality of information or peripheral cues . We conduct an experiment with knowledge workers from different companies . The results confirm most hypotheses derived from ELM in the context of BI reports but we also find a deviation from the basic ELM expectations . We find that even people who are able and motivated to scrutinize the report content use community cues to decide on report usefulness in addition to report quality considerations . 
",We analyze which social software features influence the usefulness of BI reports. We test theory based hypotheses in an experiment with knowledge workers. Users of BI reports are mostly influenced by the report s argument quality. Personal characteristics also influence the users perception of report usefulness.,,,
S0306457315001454," Query auto completion models recommend possible queries to web search users when they start typing a query prefix . Most of today s QAC models rank candidate queries by popularity and in doing so they tend to follow a strict query matching policy when counting the queries . That is they ignore the contributions from so called homologous queries queries with the same terms but ordered differently or queries that expand the original query . Importantly homologous queries often express a remarkably similar search intent . Moreover today s QAC approaches often ignore semantically related terms . We argue that users are prone to combine semantically related terms when generating queries . We propose a learning to rank based QAC approach where for the first time features derived from homologous queries and semantically related terms are introduced . In particular we consider the observed and predicted popularity of homologous queries for a query candidate and the semantic relatedness of pairs of terms inside a query and pairs of queries inside a session . We quantify the improvement of the proposed new features using two large scale real world query logs and show that the mean reciprocal rank and the success rate can be improved by up to 9 over state of the art QAC models . 
",We propose a learning to rank based query auto completion model L2R QAC that exploits contributions from so called homologous queries for a QAC candidate in which two kinds of homologous queries are taken into account. We propose semantic features for QAC using the semantic relatedness of terms inside a query candidate and of pairs of terms from a candidate and from queries previously submitted in the same session. We analyze the effectiveness of our L2R QAC model with newly added features and find that it significantly outperforms state of the art QAC models either based on learning to rank or on popularity.,,,
S0306457314000892," With the rise of Web 2.0 platforms personal opinions such as reviews ratings recommendations and other forms of user generated content have fueled interest in sentiment classification in both academia and industry . In order to enhance the performance of sentiment classification ensemble methods have been investigated by previous research and proven to be effective theoretically and empirically . We advance this line of research by proposing an enhanced Random Subspace method POS RS for sentiment classification based on part of speech analysis . Unlike existing Random Subspace methods using a single subspace rate to control the diversity of base learners POS RS employs two important parameters i.e . content lexicon subspace rate and function lexicon subspace rate to control the balance between the accuracy and diversity of base learners . Ten publicly available sentiment datasets were investigated to verify the effectiveness of proposed method . Empirical results reveal that POS RS achieves the best performance through reducing bias and variance simultaneously compared to the base learner i.e . Support Vector Machine . These results illustrate that POS RS can be used as a viable method for sentiment classification and has the potential of being successfully applied to other text classification problems . 
",The rise of social media has fueled interest in sentiment classification. POS RS is proposed for sentiment analysis based on part of speech analysis. Ten public datasets were investigated to verify the effectiveness of POS RS. Experimental results reveal POS RS can be used as a viable method.,,,
S0377221713002087," Clusterwise regression consists of finding a number of regression functions each approximating a subset of the data . In this paper a new approach for solving the clusterwise linear regression problems is proposed based on a nonsmooth nonconvex formulation . We present an algorithm for minimizing this nonsmooth nonconvex function . This algorithm incrementally divides the whole data set into groups which can be easily approximated by one linear regression function . A special procedure is introduced to generate a good starting point for solving global optimization problems at each iteration of the incremental algorithm . Such an approach allows one to find global or near global solution to the problem when the data sets are sufficiently dense . The algorithm is compared with the multistart Sp th algorithm on several publicly available data sets for regression analysis . 
",We develop an incremental algorithm to solve clusterwise linear regression problems. The algorithm gradually computes clusters and linear regression functions. Two special procedures to construct initial solutions are proposed. The algorithm finds global or near global minimizers of the overall fit function.,,,
S0377221713002221," In this paper we consider a latent Markov process governing the intensity rate of a Poisson process model for software failures . The latent process enables us to infer performance of the debugging operations over time and allows us to deal with the imperfect debugging scenario . We develop the Bayesian inference for the model and also introduce a method to infer the unknown dimension of the Markov process . We illustrate the implementation of our model and the Bayesian approach by using actual software failure data . 
",A new model for software reliability is proposed. Proposed model enables us to infer performance of the debugging process. Bayesian inference is developed. An approach is introduced to assess dimension of the Markov process.,,,
S0306457315001429," This work presents a content based semantics and image retrieval system for semantically categorized hierarchical image databases . Each module is designed with an aim to develop a system that works closer to human perception . Images are mapped to a multidimensional feature space where images belonging a semantic are clustered and indexed to acquire its efficient representation . This helps in handling the existing variability or heterogeneity within this semantic . Adaptive combinations of the obtained depictions are utilized by the branch selection and pruning algorithms to identify some closer semantics and select only a part of the large hierarchical search space for actual search . So obtained search space is finally used to retrieve desired semantics and similar images corresponding to them . The system is evaluated in terms of accuracy of the retrieved semantics and precision recall curves . Experiments show promising semantics and image retrieval results on hierarchical image databases . The results reported with non hierarchical but categorized image databases further prove the efficacy of the proposed system . 
",Proposes automatic semantics and image retrieval system for hierarchical databases. System uses 1 3 search space to retrieve semantics and finally similar images. 77 semantic retrieval accuracy on ImageNet. Uses 4 space to retrieve images. System reports precision of 0.78 @ 20 and 0.67 @ 100 images on categorized WANG. The study explores adequacy of visual signatures set used to represent a semantic.,,,
S0306457314001022," Websites can learn what their users do on their pages to provide better content and services to those users . A website can easily find out where a user has been but in order to find out what content is consumed and how it was consumed at a sub page level prior work has proposed client side tracking to record cursor activity which is useful for computing the relevance for search results or determining user attention on a page . While recording cursor interactions can be done without disturbing the user the overhead of recording the cursor trail and transmitting this data over the network can be substantial . In our work we investigate methods to compress cursor data taking advantage of the fact that not every cursor coordinate has equal value to the website developer . We evaluate 5 lossless and 5 lossy compression algorithms over two datasets reporting results about client side performance space savings and how well a lossy algorithm can replicate the original cursor trail . The results show that different compression techniques may be suitable for different goals LZW offers reasonable lossless compression but lossy algorithms such as piecewise linear interpolation and distance thresholding offer better client side performance and bandwidth reduction . 
",We study 10 techniques to compress users cursor interactions on the Web. A systematic evaluation of both lossy and lossless algorithms is performed. We found that different compression techniques excel depending on the objective. LZW and piecewise linear interpolation balance well accuracy and efficiency.,,,
S0306457314000697," Community question answering services that enable users to ask and answer questions have become popular on the internet . However lots of new questions usually can not be resolved by appropriate answerers effectively . To address this question routing task in this paper we treat it as a ranking problem and rank the potential answerers by the probability that they are able to solve the given new question . We utilize tensor model and topic model simultaneously to extract latent semantic relations among asker question and answerer . Then we propose a learning procedure based on the above models to get optimal ranking of answerers for new questions by optimizing the multi class AUC . Experimental results on two real world CQA datasets show that the proposed method is able to predict appropriate answerers for new questions and outperforms other state of the art approaches . 
",We formally described the answerer ranking problem in CQA service. A novel model with tensor decomposition is proposed to solve the problem. The parameters of the above model are learned by maximizing the multi class AUC. Introducing the asker dimension improves the answerer ranking performance. Our approach outperforms the previous methods on two real world CQA datasets.,,,
S0306457314000867," Media sharing applications such as Flickr and Panoramio contain a large amount of pictures related to real life events . For this reason the development of effective methods to retrieve these pictures is important but still a challenging task . Recognizing this importance and to improve the retrieval effectiveness of tag based event retrieval systems we propose a new method to extract a set of geographical tag features from raw geo spatial profiles of user tags . The main idea is to use these features to select the best expansion terms in a machine learning based query expansion approach . Specifically we apply rigorous statistical exploratory analysis of spatial point patterns to extract the geo spatial features . We use the features both to summarize the spatial characteristics of the spatial distribution of a single term and to determine the similarity between the spatial profiles of two terms i.e . term to term spatial similarity . To further improve our approach we investigate the effect of combining our geo spatial features with temporal features on choosing the expansion terms . To evaluate our method we perform several experiments including well known feature analyzes . Such analyzes show how much our proposed geo spatial features contribute to improve the overall retrieval performance . The results from our experiments demonstrate the effectiveness and viability of our method . 
",We have developed a new approach that effectively retrieves event based images. We have proposed a rigorous technique to extract spatial features from image tags. We have developed a method for summarizing spatial distributions of single tags. We have developed new techniques for spatial relatedness between two tag terms. Our spatio temporal IR method improves the retrieval performance significantly.,,,
S0306457315001363," With constant growth in size of analyzable data ranking of academic entities is becoming an attention grabbing task . For ranking of authors this study considers the author s own contribution as well as the impact of mutual influence of the co authors along with exclusivity in their received citations . The ranking of researchers is influenced by the ranking of their co authors more so if co authors are seniors . Tracking the citations received by an author is also an important factor to measure standing of an author . This study proposes Mutual Influence and Citation Exclusivity Author Rank algorithm . We performed a sequence of experiments to calculate the MuICE Rank . First we calculated Mutual Influence considering three different factors the number of papers the number of citations and the author s appearance as first author . Secondly we computed MuICE incorporating all three factors of MuInf along with the exclusivity in citations received by an author . Empirically it is shown that the proposed methods generate substantial results . 
",A mutual influence and exclusive citations based method for author ranking is proposed. It considers effect of publications citations and publications as first author for authors and co authors. It also considers exclusivity in citations received by an author.,,,
S0306457315001211," Nowadays the increasing demand for group recommendations can be observed . In this paper we address the problem of recommendation performance for groups of users . We focus on the performance of very Top N recommendations which are important when recommending the long lasting items . To improve existing group recommenders we propose a mixed hybrid recommender for groups combining content based and collaborative strategies . The principle of proposed group recommender is to generate content and collaborative recommendations for each user apply an aggregation strategy to solve the group conflict preferences for the content and collaborative sets separately and finally reorder the collaborative candidates based on the content based ones . It is based on an idea that candidates recommended by both recommendation strategies at the same time are presumably more appropriate for the group than the candidates recommended by individual strategies . The evaluation is performed by several experiments in the multimedia domain . Both online and offline experiments were performed in order to compare real users satisfaction to the standard group recommenders and also to compare performance of proposed approach to the state of the art recommenders based on the MovieLens dataset . Finally we experimented with the proposed hybrid recommender to generate the recommendation for a group of size one . Obtained results support our hypothesis that proposed mixed hybrid approach improves the precision of the recommendation for groups of users and for the single user recommendation respectively on very Top N recommended items . 
",Novel group hybrid method combining collaborative and content based recommendation. Proposed method improves the quality of recommended items ordering. Proposed method increases the recommendation precision for very Top N results. Applicable for single user as well as group recommendation.,,,
S0377221713001963," To reduce labor intensive and costly order picking activities many distribution centers are subdivided into a forward area and a reserve area . The former is a small area where most popular stock keeping units can conveniently be picked and the latter is applied for replenishing the forward area and storing SKUs that are not assigned to the forward area at all . Clearly reducing SKUs stored in forward area enables a more compact forward area but requires a more frequent replenishment . To tackle this basic trade off different versions of forward reserve problems determine the SKUs to be stored in forward area the space allocated to each SKU and the overall size of the forward area . As previous research mainly focuses on simplified problem versions where the forward area can continuously be subdivided we investigate discrete forward reserve problems . Important subproblems are defined and computation complexity is investigated . Furthermore we experimentally analyze the model gaps between the different fluid models and their discrete counterparts . 
",Existing literature mainly focuses on fluid models . We formulate discrete versions of basic and extended forward reserve problems. We discuss their computational complexity and present suitable solution procedures. We show the basic allocation problem to be solvable in polynomial time. We propose and computationally examine a very effective repair heuristic .,,,
S0377221713001975," This study proposes an efficient exact algorithm for the precedence constrained single machine scheduling problem to minimize total job completion cost where machine idle time is forbidden . The proposed algorithm is based on the SSDP method and is an extension of the authors previous algorithms for the problem without precedence constraints . In this method a lower bound is computed by solving a Lagrangian relaxation of the original problem via dynamic programming and then it is improved successively by adding constraints to the relaxation until the gap between the lower and upper bounds vanishes . Numerical experiments will show that the algorithm can solve all instances with up to 50 jobs of the precedence constrained total weighted tardiness and total weighted earliness tardiness problems and most instances with 100 jobs of the former problem . 
",The precedence constrained single machine scheduling problem is considered. An efficient exact algorithm is proposed based on our previous studies. The SSDP Successive Sublimation Dynamic Programming method is employed. Instances of the total weighted tardiness problem with up to 100 jobs are solved.,,,
S0306457315000023," Semantic similarity assessment between concepts is an important task in many language related applications . In the past several approaches to assess similarity by evaluating the knowledge modeled in an ontology have been proposed . However there are some limitations such as the facts of relying on predefined ontologies and fitting non dynamic domains in the existing measures . Wikipedia provides a very large domain independent encyclopedic repository and semantic network for computing semantic similarity of concepts with more coverage than usual ontologies . In this paper we propose some novel feature based similarity assessment methods that are fully dependent on Wikipedia and can avoid most of the limitations and drawbacks introduced above . To implement similarity assessment based on feature by making use of Wikipedia firstly a formal representation of Wikipedia concepts is presented . We then give a framework for feature based similarity based on the formal representation of Wikipedia concepts . Lastly we investigate several feature based approaches to semantic similarity measures resulting from instantiations of the framework . The evaluation based on several widely used benchmarks and a benchmark developed in ourselves sustains the intuitions with respect to human judgements . Overall several methods proposed in this paper have good human correlation and constitute some effective ways of determining similarity between Wikipedia concepts . 
",A formal representation of Wikipedia concepts is presented. A framework for feature based similarity is proposed. Some novel feature based approaches to semantic similarity measures are presented. Results show that several proposed methods have good human correlation.,,,
S0377221713002695," This paper presents a novel solution heuristic to the General Lotsizing and Scheduling Problem for Parallel production Lines . The GLSPPL addresses the problem of simultaneously deciding about the sizes and schedules of production lots on parallel heterogeneous production lines with respect to scarce capacity sequence dependent setup times and deterministic dynamic demand of multiple products . Its objective is to minimize inventory holding sequence dependent setup and production costs . The new heuristic iteratively decomposes the multi line problem into a series of single line problems which are easier to solve . Different approaches for decomposition and for the iteration between a modified multi line master problem and the single line subproblems are proposed . They are compared with an existing solution method for the GLSPPL by means of medium sized and large practical problem instances from different types of industries . The new methods prove to be superior with respect to both solution quality and computation time . 
",A novel solution heuristic for the General Lotsizing and Scheduling Problem for Parallel production Lines is presented. The idea is to iteratively decompose the parallel line problem into a series of single line problems. The new heuristic improves already existing approaches. Large practical applications can now be solved.,,,
S0306457315000047," Daily deals have emerged in the last three years as a successful form of online advertising . The downside of this success is that users are increasingly overloaded by the many thousands of deals offered each day by dozens of deal providers and aggregators . The challenge is thus offering the right deals to the right users i.e . the relevance ranking of deals . This is the problem we address in our paper . Exploiting the characteristics of deals data we propose a combination of a term and a concept based retrieval model that closes the semantic gap between queries and documents expanding both of them with category information . The method consistently outperforms state of the art methods based on term matching alone and existing approaches for ad classification and ranking . 
",We formalize the problem of retrieving daily deals in the context of Web search. We effectively combine keyword based retrieval with automated classification. Our solution outperforms state of the art query expansion and prior ad ranking work.,,,
S0377221713002713," This paper proposes a test for whether data are over represented in a given production zone i.e . a subset of a production possibility set which has been estimated using the non parametric Data Envelopment Analysis approach . A binomial test is used that relates the number of observations inside such a zone to a discrete probability weighted relative volume of that zone . A Monte Carlo simulation illustrates the performance of the proposed test statistic and provides good estimation of both facet probabilities and the assumed common inefficiency distribution in a three dimensional input space . Potential applications include tests for whether benchmark units dominate more observations than expected . 
",This paper proposes a test for whether data are over represented in a subset of a DEA estimated production possibility set. The binomial test the number of observations inside a zone to a discrete probability weighted relative volume of that zone. A Monte Carlo simulation illustrates the performance of the proposed test.,,,
S0306457315000771," In the area of Information Retrieval the task of automatic text summarization usually assumes a static underlying collection of documents disregarding the temporal dimension of each document . However in real world settings collections and individual documents rarely stay unchanged over time . The World Wide Web is a prime example of a collection where information changes both frequently and significantly over time with documents being added modified or just deleted at different times . In this context previous work addressing the summarization of web documents has simply discarded the dynamic nature of the web considering only the latest published version of each individual document . This paper proposes and addresses a new challenge the automatic summarization of changes in dynamic text collections . In standard text summarization retrieval techniques present a summary to the user by capturing the major points expressed in the most recent version of an entire document in a condensed form . In this new task the goal is to obtain a summary that describes the most significant changes made to a document during a given period . In other words the idea is to have a summary of the revisions made to a document over a specific period of time . This paper proposes different approaches to generate summaries using extractive summarization techniques . First individual terms are scored and then this information is used to rank and select sentences to produce the final summary . A system based on Latent Dirichlet Allocation model is used to find the hidden topic structures of changes . The purpose of using the LDA model is to identify separate topics where the changed terms from each topic are likely to carry at least one significant change . The different approaches are then compared with the previous work in this area . A collection of articles from Wikipedia including their revision history is used to evaluate the proposed system . For each article a temporal interval and a reference summary from the article s content are selected manually . The articles and intervals in which a significant event occurred are carefully selected . The summaries produced by each of the approaches are evaluated comparatively to the manual summaries using ROUGE metrics . It is observed that the approach using the LDA model outperforms all the other approaches . Statistical tests reveal that the differences in ROUGE scores for the LDA based approach is statistically significant at 99 over baseline . 
",The summarization of changes addresses a new challenge the automatic summarization of changes in dynamic text collections. Four different approaches are proposed for the summarization of changes. A system based on Latent Dirichlet Allocation model is used to find the hidden topic structures of changes. The approach based on LDA model outperforms all the other approaches. The differences in ROUGE scores for LDA based approach is statistically significant at 99 over baseline.,,,
S0308596113000724," This paper investigates the contributions of digital infrastructure policies of provincial governments in Canada to the development of broadband networks . Using measurements of broadband network speeds between 2007 and 2011 the paper analyzes potential causes for observed differences in network performance growth across the provinces including geography Internet use intensity platform competition and provincial broadband policies . The analysis suggests provincial policies that employed public sector procurement power to open access to essential facilities and channeled public investments in Internet backbone infrastructure were associated with the emergence of relatively high quality broadband networks . However a weak essential facilities regime and regulatory barriers to entry at the national level limit the scope for decentralized policy solutions . 
",The impact of multilevel broadband policies on network development in Canada. Analyzes variations in the levels and patterns of network performance growth. Federal regulations limited the scope for policy decentralization. Higher rate of progress in provinces that promoted access to essential facilities.,,,
S0377221713002543," The Sequential Probability Ratio Test control chart is a powerful tool for monitoring manufacturing processes . It is highly suitable for the applications where testing is destructive or very expensive such as the automobile airbags test . This article studies the effect of the Average Sample Number on the chart s performance . A design algorithm is proposed to develop the optimal SPRT chart for monitoring the fraction nonconforming p of Bernoulli processes . By optimizing the ASN and other charting parameters the average detection speed of the SPRT chart is almost doubled . It is also found that the optimal SPRT chart significantly outperforms the optimal np and binomial CUSUM charts in terms of Average Number of Defectives under different combinations of the design specifications . It is observed that the SPRT chart using a relatively smaller ASN and a shorter sampling interval has a higher overall detection effectiveness . 
",This article studies the effect of the Average Sample Number ASN on the performance of the SPRT chart. A design algorithm is proposed to explore the optimal ASN of the SPRT chart for monitoring the fraction nonconforming p. The optimal SPRT chart significantly outperforms the optimal np chart optimal CUSUM chart and basic SPRT chart.,,,
S0306457315001442," Cluster analysis using multiple representations of data is known as multi view clustering and has attracted much attention in recent years . The major drawback of existing multi view algorithms is that their clustering performance depends heavily on hyperparameters which are difficult to set . In this paper we propose the Multi View Normalized Cuts approach a two step algorithm for multi view clustering . In the first step an initial partitioning is performed using a spectral technique . In the second step a local search procedure is used to refine the initial clustering . MVNC has been evaluated and compared to state of the art multi view clustering approaches using three real world datasets . Experimental results have shown that MVNC significantly outperforms existing algorithms in terms of clustering quality and computational efficiency . In addition to its superior performance MVNC is parameter free which makes it easy to use . 
",A new multi view clustering algorithm is proposed. The proposed MVNC algorithm uses spectral partitioning and local refinement. MVNC is compared to state of the art algorithms using three real world datasets. MVNC significantly outperforms the other algorithms. MVNC is parameter free unlike existing multi view clustering algorithms.,,,
S0377221713002178," This note proposes an alternative procedure for identifying violated subtour elimination constraints in branch and cut algorithms for elementary shortest path problems . The procedure is also applicable to other routing problems such as variants of travelling salesman or shortest Hamiltonian path problems on directed graphs . The proposed procedure is based on computing the strong components of the support graph . The procedure possesses a better worst case time complexity than the standard way of separating SECs which uses maximum flow algorithms and is easier to implement . 
",Presents alternative procedure for separating subtour elimination constraints. Procedure is based on computing the strong components of the support graph. Procedure has better worst case time complexity than standard way of separating SECs. Moreover procedure is easier to implement. Computational experiments verify practical usefulness of the procedure.,,,
S0306457315001351," Many problems in data mining involve datasets with multiple views where the feature space consists of multiple feature groups . Previous studies employed view weighting method to find a shared cluster structure underneath different views . However most of these studies applied gradient optimization method to optimize the cluster centroids and feature weights iteratively and made the final partition local optimal . In this work we proposed a novel bi level weighted multi view clustering method with emphasizing fuzzy weighting on both view and feature . Furthermore an efficient global search strategy that combines particle swarm optimization and gradient optimization was proposed to solve the induced non convex loss function . In the experimental analysis the performance of the proposed method was compared with five state of the art weighted clustering algorithms on three real world high dimensional multi view datasets . 
",We propose a bi level fuzzy weighting to discriminate view and feature simultaneously. We develop a hybrid optimization based on particle swarm optimization. We propose a representative based method to determine hyper parameters. Our method is compared with six clustering algorithm on three real world datasets.,,,
S0306457316300371," In this paper we propose and evaluate the Block Max WAND with Candidate Selection and Preserving Top K Results algorithm or BMW CSP . It is an extension of BMW CS a method previously proposed by us . Although very efficient BMW CS does not guarantee preserving the top k results for a given query . Algorithms that do not preserve the top results may reduce the quality of ranking results in search systems . BMW CSP extends BMW CS to ensure that the top k results will have their rankings preserved . In the experiments we performed for computing the top 10 results the final average time required for processing queries with BMW CSP was lesser than the ones required by the baselines adopted . For instance when computing top 10 results the average time achieved by MBMW the best multi tier baseline we found in the literature was 36.29 ms per query while the average time achieved by BMW CSP was 19.64 ms per query . The price paid by BMW CSP is an extra memory required to store partial scores of documents . As we show in the experiments this price is not prohibitive and in cases where it is acceptable BMW CSP may constitute an excellent alternative query processing method . 
",We present a new query processing method for text search. We extend the BMW CS algorithm to now preserve the top k results proposing BMW CSP. We show through experiments that the method is competitive when compared to baselines.,,,
S0306457315001508," Object matching is an important task for finding the correspondence between objects in different domains such as documents in different languages and users in different databases . In this paper we propose probabilistic latent variable models that offer many to many matching without correspondence information or similarity measures between different domains . The proposed model assumes that there is an infinite number of latent vectors that are shared by all domains and that each object is generated from one of the latent vectors and a domain specific projection . By inferring the latent vector used for generating each object objects in different domains are clustered according to the vectors that they share . Thus we can realize matching between groups of objects in different domains in an unsupervised manner . We give learning procedures of the proposed model based on a stochastic EM algorithm . We also derive learning procedures in a semi supervised setting where correspondence information for some objects are given . The effectiveness of the proposed models is demonstrated by experiments on synthetic and real data sets . 
",We propose a probabilistic model for matching clusters in different domains without correspondence information. The proposed method can handle data with more than two domains and the number of objects in each domain can be different. We extend the proposed method for a semi supervised setting. We demonstrate that the proposed method achieve better matching performance than existing methods using synthetic and real world data sets.,,,
S0377221713001987," The Team Orienteering Problem is a particular vehicle routing problem in which the aim is to maximize the profit gained from visiting customers without exceeding a travel cost time limit . This paper proposes a new and fast evaluation process for TOP based on an interval graph model and a Particle Swarm Optimization inspired Algorithm to solve the problem . Experiments conducted on the standard benchmark of TOP clearly show that our algorithm outperforms the existing solving methods . PSOiA reached a relative error of 0.0005 whereas the best known relative error in the literature is 0.0394 . Our algorithm detects all but one of the best known solutions . Moreover a strict improvement was found for one instance of the benchmark and a new set of larger instances was introduced . 
",A new fast optimal split for the team orienteering problem. An effective particle swarm algorithm PSOiA . PSOiA is robust and outperforms the state of the art algorithms in the literature.,,,
S0377221713002749," Lateral transshipments are an effective strategy to pool inventories . We present a Semi Markov decision problem formulation for proactive and reactive transshipments in a multi location continuous review distribution inventory system with Poisson demand and one for one replenishment policy . For a two location model we state the monotonicity of an optimal policy . In a numerical study we compare the benefits of proactive and different reactive transshipment rules . The benefits of proactive transshipments are the largest for networks with intermediate opportunities of demand pooling and the difference between alternative reactive transshipment rules is negligible . 
",Semi Markov decision problem formulation. Benefits of proactive over different reactive transshipment rules. Networks with intermediate opportunities of demand pooling. The difference between reactive transshipment rules is negligible.,,,
S0306457315000461," Topic time reflects the temporal feature of topics in Web news pages which can be used to establish and analyze topic models for many time sensitive text mining tasks . However there are two critical challenges in discovering topic time from Web news pages . The first issue is how to normalize different kinds of temporal expressions within a Web news page e.g . explicit and implicit temporal expressions into a unified representation framework . The second issue is how to determine the right topic time for topics in Web news . Aiming at solving these two problems we propose a systematic framework for discovering topic time from Web news . In particular for the first issue we propose a new approach that can effectively determine the appropriate referential time for implicit temporal expressions and further present an effective defuzzification algorithm to find the right explanation for a fuzzy temporal expression . For the second issue we propose a relation model to describe the relationship between news topics and topic time . Based on this model we design a new algorithm to extract topic time from Web news . We build a prototype system called Topic Time Parser and conduct extensive experiments to measure the effectiveness of our proposal . The results suggest that our proposal is effective in both temporal expression normalization and topic time extraction . 
",We present a systematic framework for discovering topic time from Web news. We propose a new approach to determine the referential time for implicit temporal expressions. We devise a relation model between news topics and topic time for topic time extraction. We build a prototype system and conduct comparative experiments on real datasets.,,,
S0377221713002142," Interactive approaches employing cone contraction for multi criteria mixed integer optimization are introduced . In each iteration the decision maker is asked to give a reference point . The subsequent Pareto optimal point is the reference point projected on the set of admissible objective vectors using a suitable scalarizing function . Thereby the procedures solve a sequence of optimization problems with integer variables . In such a process the DM provides additional preference information via pair wise comparisons of Pareto optimal points identified . Using such preference information and assuming a quasiconcave and non decreasing value function of the DM we restrict the set of admissible objective vectors by excluding subsets which can not improve over the solutions already found . The procedures terminate if all Pareto optimal solutions have been either generated or excluded . In this case the best Pareto point found is an optimal solution . Such convergence is expected in the special case of pure integer optimization indeed numerical simulation tests with multi criteria facility location models and knapsack problems indicate reasonably fast convergence in particular under a linear value function . We also propose a procedure to test whether or not a solution is a supported Pareto point . 
",An interactive approach for a mixed integer multi criteria optimization is introduced. The DM makes pairwise comparisons of identified Pareto optimal points and gives reference points. Assuming a quasi concave value function pairwise comparisons are used to contract the cone of admissible objective vectors. Numerical simulation tests indicate reasonably fast convergence. Convergence is guaranteed for the pure integer case.,,,
S0306457314000910," One of the problems of opinion mining is the domain adaptation of the sentiment classifiers . There are several approaches to tackling this problem . One of these is the integration of a list of opinion bearing words for the specific domain . This paper presents the generation of several resources for domain adaptation to polarity detection . On the other hand the lack of resources in languages different from English has orientated our work towards developing sentiment lexicons for polarity classifiers in Spanish . The results show the validity of the new sentiment lexicons which can be used as part of a polarity classifier . 
",A lexicon based domain adaptation method is proposed. Several domain polar lexicons were compiled following a corpus based approach. The new resources are assessed over a Spanish corpus. The promising results encourage us to follow improving this domain adaptation method.,,,
S0377221713002117," In this article we present a new exact algorithm for solving the simple assembly line balancing problem given a determined cycle time . The algorithm is a station oriented bidirectional branch and bound procedure based on a new enumeration strategy that explores the feasible solutions tree in a non decreasing idle time order . The procedure uses several well known lower bounds dominance rules and a new logical test based on the assimilation of the feasibility problem for a given cycle time and number of stations to a maximum flow problem . The algorithm has been tested with a series of computational experiments on a well known set of problem instances . The tests show that the proposed algorithm outperforms the best existing exact algorithm for solving SALBP 1 verifying optimality for 264 of the 269 benchmark instances . 
",A new branch and bound is proposed for the simple assembly line balancing problem. A novel flow based logical test and a strengthened dominance rule are put forward. The results from the proposed algorithm outperform those provided by the previous procedures found in the literature.,,,
S0377221713002038," We consider single item and inventory systems with integer valued demand processes . While most of the inventory literature studies continuous approximations of these models and establishes joint convexity properties of the policy parameters in the continuous space we show that these properties no longer hold in the discrete space in the sense of linear interpolation extension and L convexity . This nonconvexity can lead to failure of optimization techniques based on local optimality to obtain the optimal inventory policies . It can also make certain comparative properties established previously using continuous variables invalid . We revise these properties in the discrete space . 
",First paper to focus on discrete r q and s T inventory systems. Some well known properties of the continuous systems no longer hold. Show exactly where joint convexity breaks down and establish other useful properties. Revise comparative properties of the continuous model for the discrete model.,,,
S0306457315001375," The purpose of this article is to validate through two empirical studies a new method for automatic evaluation of written texts called Inbuilt Rubric based on the Latent Semantic Analysis technique which constitutes an innovative and distinct turn with respect to LSA application so far . In the first empirical study evidence of the validity of the method to identify and evaluate the conceptual axes of a text in a sample of 78 summaries by secondary school students is sought . Results show that the proposed method has a significantly higher degree of reliability than classic LSA methods of text evaluation and displays very high sensitivity to identify which conceptual axes are included or not in each summary . A second study evaluates the method s capacity to interact and provide feedback about quality in a real online system on a sample of 924 discursive texts written by university students . Results show that students improved the quality of their written texts using this system and also rated the experience very highly . The final conclusion is that this new method opens a very interesting way regarding the role of automatic assessors in the identification of presence absence and quality of elaboration of relevant conceptual information in texts written by students with lower time costs than the usual LSA based methods . 
",We model how to implement a rubric in latent semantic analysis. The proposed method change abstract dimensions into meaningful dimensions. The method allows to detect easily written contents. Inbuilt rubric method has been used to give feedback to 924 university students.,,,
S0377221713002245," Trade credit arises when a buyer delays payment for purchased goods or services . Its nature has predominantly been an area of inquiry for researchers from the disciplines of finance marketing and economics but it has received relatively little attention in other domains . In our article we provide an integrative review of the existing literature and discuss conflicting study outcomes . We organize the relevant literature into seven areas of inquiry and analyze four in detail trade credit motives order quantity decisions credit term decisions and settlement period decisions . Additionally we derive a detailed agenda for future research in these areas . 
",We provide an integrative review of the existing literature of trade credit in the interface of operations and finance. We analyze in detail four literature areas trade credit motives order quantity decisions credit term decisions and settlement period decisions. The main findings of the literature review is that trade credit increases the economic order quantity and could serve as a buyer supplier coordination mechanism. We derive a detailed agenda for future research around two core themes opportunities arising from inside operations management and opportunities arising from outside operations management.,,,
S0306457315000497," Social media represents an emerging challenging sector where the natural language expressions of people can be easily reported through blogs and short text messages . This is rapidly creating unique contents of massive dimensions that need to be efficiently and effectively analyzed to create actionable knowledge for decision making processes . A key information that can be grasped from social environments relates to the polarity of text messages . To better capture the sentiment orientation of the messages several valuable expressive forms could be taken into account . In this paper three expressive signals typically used in microblogs have been explored adjectives emoticon emphatic and onomatopoeic expressions and expressive lengthening . Once a text message has been normalized to better conform social media posts to a canonical language the considered expressive signals have been used to enrich the feature space and train several baseline and ensemble classifiers aimed at polarity classification . The experimental results show that adjectives are more discriminative and impacting than the other considered expressive signals . 
",To capture the sentiment of messages several expressive forms are investigated. Expressive signals enrich the feature space of baseline and ensemble classifiers. Only adjectives play a fundamental role as expressive signal. Pragmatic particles and expressive lengthening could lead to the de finition of erratic polarity classifiers.,,,
S037722171501111X," We develop a dynamic continuous time theory of the competitive firm under multiple correlated uncertainties . In doing so we completely generalize and extend the previous comparative statics results . Particularly we relax the assumption of the statistical independence between the risks and the restrictions on the coefficient of absolute relative risk aversion . Furthermore we generally show the impact of one risk on the aversion to another . Moreover we show the role of the factor of correlation between risks on the decisions of the firm . 
",A dynamic model of the firm under multiple correlated uncertainties. Completely generalize the previous static results. The impact of one risk on the aversion to another.,,,
S037722171501084X," Autonomous word of mouth as a channel of social influence that is out of firms direct control has acquired particular importance with the development of the Internet . Depending on whether a given product or service is a good or a bad deal this can significantly contribute to commercial success or failure . Yet the existing dynamic models of sales in marketing still assume that the influence of word of mouth on sales is at best advertising dependent . This omission can produce ineffective management and therefore misleading marketing policies . This paper seeks to bridge the gap by introducing a contagion sales model of a monopolist firm s product where sales are affected by advertising dependent as well as autonomous word of mouth . We assume that the firm s attraction rate of new customers is determined by the degree at which the current sales price is advantageous or not compared with the current customers reservation price . A primary goal of the paper is to determine the optimal sales price and advertising effort . We show that despite costly price adjustments the interactions between sales price advertising dependent and autonomous word of mouth can result in complex dynamic pricing policies involving history dependence or limit cycling consisting of alternating attraction of new customers and attrition of current customers . 
",We study dynamic pricing and advertising policies in a sales model for a monopolist firm. Both autonomous and advertising dependent word of mouth are considered. Optimal limit cycles alternating building and leveraging of the price advantage are found. Multiple equilibria separated by Skiba curves are numerically ascertained.,,,
S037722171500990X," The existence of positive and negative externalities ought to be considered in a productivity analysis in order to obtain unbiased measures of efficiency . In this research we present an additive style data envelopment analysis model that considers the production of both negative and positive externalities and permits a limited increase in input utilisation where relevant . The directional economic environmental distance function is a unified approach based on a linear program that evaluates the relative inefficiency of the units under examination with respect to a unique reference technology . We discuss the impact of disposability assumptions in depth and demonstrate how different versions of the DEED model improve on models presented in the literature to date . 
",Positive and negative externalities ought to be modelled to obtain unbiased measures of productivity. We develop an additive style data envelopment analysis model that accounts for both externalities. We model limited increases in input utilisation in order to further reduce negative externalities. The directional economic environmental distance DEED function is a unified approach. DEED draws from a unique reference technology enabling reasonable productivity measure comparisons.,,,
S037722171501022X," Faced with a short turn around request to characterise several hand held mine detection systems the authors developed and applied an analytical methodology that was sufficiently robust and pragmatic to satisfy the needs of the various military stakeholders involved yet it was appropriately rigorous and transparent to bear external scrutiny . The methodology can be applied in situations where data collection and analysis must be done quickly while preserving scientific veracity . For mine detection systems considerable uncertainties existed that needed to be characterised including application location operational situation and involvement of human operators . Constraints on the time and expertise available implied there would be difficulties ensuring a sufficient number of trials could be conducted to levels of statistical confidence that would assure appropriate credibility across all of the parameters . This problem was effectively rectified through experimental design and by heavily involving the sponsor stakeholders and subject matter experts throughout the study thus boosting the credibility and acceptance of its results . The process followed involved liaison with the sponsor identification of critical issues measurements in field environments reporting mechanisms and discussion on implementation and further development . The critical focus was operational capability rather than specific equipment characteristics . A robust data presentation technique was developed to deal with the complexities associated with different needs of multiple stakeholders . This technique enabled the results to be reviewed from different stakeholders perspectives the formation of a common understanding and the results to be reusable in future analyses . 
",We develop and apply a methodology to characterise hand held detection systems. Methodology which incorporates field trials and workshops allows scrutiny. Robust data presentation technique was developed to facilitate common understanding. Methodology and presentation satisfies the needs of multiple stakeholder groups. Can be applied to future detectors allowing direct comparisons to be made.,,,
S037722171501019X," We analyze a time fenced planning system where both expediting and canceling are allowed inside the time fence but only with a penalty . Previous research has allowed only for the case of expediting inside the time fence and has overlooked the opportunity for additional improvement by also allowing for cancelations . Some researchers also have found that for traditional time fenced models the choice of the more complex stochastic linear programming approach versus the simpler deterministic approach is not justified . We formulate both the deterministic and stochastic problems as dynamic programs and develop analytic bounds that limit the search space of the stochastic approach . We run extensive simulations and numerical experiments to understand better the benefit of adding cancelation and to compare the performance of the stochastic model with the more common deterministic model when they are employed as heuristics in a rolling horizon setting . Across all experiments we find that allowing expediting lowered costs by 11.3 using the deterministic approach but costs were reduced by 27.8 if both expediting and canceling are allowed . We find that the benefit of using the stochastic model versus the deterministic model varies widely across demand distributions and levels of recourse the ratio of stochastic average costs to deterministic average costs ranged from 43.3 to 78.5 . 
",We analyze a time fenced planning system with penalized expediting and canceling. We formulate deterministic and stochastic dynamic programs and develop bounds. Allowing both expediting and canceling lowered costs 28 versus 11 for expediting only.,,,
S037722171501070X," In this paper the resource constrained project scheduling problem with general temporal constraints is extended by the concept of break calendars in order to incorporate the possible absence of renewable resources . Three binary linear model formulations are presented that use either start based or changeover based or execution based binary decision variables . In addition a priority rule method as well as three different versions of a scatter search procedure are proposed in order to solve the problem heuristically . All exact and heuristic solution procedures use a new and powerful time planning method which identifies all time and calendar feasible start times for activities as well as all corresponding absolute time lags between activities . In a comprehensive performance analysis small and medium scale instances are solved with CPLEX 12.6 . Furthermore large scale instances of the problem are tackled with scatter search where the results of the three versions are compared to each other and to the priority rule method . 
",We consider the RCPSP with general temporal constraints and break calendars. A new and powerful time planning method is proposed. Three binary linear model formulations are presented. A priority rule method and three different versions of a scatter search procedure are developed. We provide a test set and perform an extensive computational study.,,,
S037722171500987X," This note takes up a shortcoming of Coelli et al . s popular environmental efficiency measure and its extension to economic environmental trade off analysis namely that they do not reward emission reductions by pollution control . A new environmental efficiency measure that overcomes this issue and similar to Coelli et al . s efficiency measure is in line with the materials balance principle is proposed and further decomposed into technical environmental efficiency and material and nonmaterial allocative environmental efficiencies . The new efficiency measure collapses into Coelli et al . s efficiency measure if none of the considered Decision Making Units control pollutants . A numerical example using Data Envelopment Analysis is provided to further explore the properties of the new efficiency measure . 
",Coelli et al. s 2007 environmental efficiency measure does not reward pollution control. Neglecting efforts to control pollutants may lead to biased environmental efficiency scores. A new environmental efficiency measure that rewards pollution control efforts is proposed. A numerical example using Data Envelopment Analysis is provided.,,,
S037722171501067X," We introduce and solve the Vehicle Routing Problem with Simultaneous Pick ups and Deliveries and Two Dimensional Loading Constraints . The 2L SPD model covers cases where customers raise delivery and pick up requests for transporting non stackable rectangular items . 2L SPD belongs to the class of composite routing packing optimization problems . However it is the first such problem to consider bi directional material flows dictated in practice by reverse logistics policies . The aspect of simultaneously satisfying deliveries and pick ups has a major impact on the underlying loading constraints feasible loading patterns must be identified for every arc traveled in the routing plan . This implies that 2L SPD generalizes previous routing problem variants with two dimensional loading constraints which call for one feasible loading per route . From a managerial perspective the simultaneous service of deliveries and pick ups may bring substantial cost savings but the generalized loading constraints are very hard to tackle in reasonable computational times . To this end we propose an optimization framework which employs memorization techniques designed for the 2L SPD model to accelerate the solution methodology . To assess the performance of our routing and packing algorithmic components we have solved the Vehicle Routing Problem with Simultaneous Pick ups and Deliveries and the Vehicle Routing Problem with Two Dimensional Constraints . Computational results are also reported on newly constructed 2L SPD benchmark problems . Apart from the basic 2L SPD version we introduce the 2L SPD with LIFO constraints which prohibit item rearrangements along the routes . Computational experiments are conducted to understand the impact of the LIFO constraints on the routing plans obtained . 
",We introduce a problem generalizing previous VRPs with 2 D loading constraints. Memorization strategies are employed for keeping the CPU times in acceptable levels. Extensive computational results on well known benchmark instances are reported. The proposed model promotes high loading space utilization throughout the routes. Item rearrangements along vehicle routes bring significant routing cost savings.,,,
S037722171630073X," The characterization of a technology from an economic point of view often uses the first derivatives of either the transformation or the production function . In a parametric setting these quantities are readily available as they can be easily deduced from the first derivatives of the specified function . In the standard framework of data envelopment analysis models these quantities are not so easily obtained . The difficulty resides in the fact that marginal changes of inputs and outputs might affect the position of the frontier itself while the calculation of first derivatives for economic purposes assumes that the frontier is held constant . We develop here a procedure to recover first derivatives of transformation functions in DEA models and we show how we can evacuate the problem of the shift of the frontier . We show how the knowledge of the first derivatives of the frontier estimated by DEA can be used to deduce and compute marginal products marginal rates of substitution and returns to scale for each decision making unit in the sample . 
",This paper raises the problem of measuring partial derivatives of transformation functions in DEA models. The derivatives of the DEA frontier imply a displacement on the frontier and a shift of the frontier. We reconcile the theoretical partial derivatives and the empirical DEA version of these derivatives. We show the invariance of the marginal rate of substitution to the partial derivatives definition. We show the invariance of the returns to scale to the partial derivatives definition.,,,
S037722171501173X," A characteristic aspect of risks in a complex modern society is the nature and degree of the public response sometimes significantly at variance with objective assessments of risk . A large part of the risk management task involves anticipating explaining and reacting to this response . One of the main approaches we have for analysing the emergent public response the social amplification of risk framework has been the subject of little modelling . The purpose of this paper is to explore how social risk amplification can be represented and simulated . The importance of heterogeneity among risk perceivers and the role of their social networks in shaping risk perceptions makes it natural to take an agent based approach . We look in particular at how to model some central aspects of many risk events the way actors come to observe other actors more than external events in forming their risk perceptions the way in which behaviour both follows risk perception and shapes it and the way risk communications are fashioned in the light of responses to previous communications . We show how such aspects can be represented by availability cascades but also how this creates further problems of how to represent the contrasting effects of informational and reputational elements and the differentiation of private and public risk beliefs . Simulation of the resulting model shows how certain qualitative aspects of risk response time series found empirically such as endogenously produced peaks in risk concern can be explained by this model . 
",Managing major societal risks involves the need to understand public risk responses. The social amplification of risk framework has been our main theoretical approach. We explore how to model endogenised risk observation behaviour and communication. Agent simulation shows characteristic outcomes like peaks and drift in risk beliefs. The model indicates the key areas where further empirical research is needed.,,,
S037722171600076X," We consider robust one way trading with limited information on price fluctuations . Our analysis finds the best guarantee of difference from the optimal offline performance . We provide closed form solution and reveal for the first time all possible worst case scenarios . Numerical experiments show that our policy is more tolerant of information inaccuracy than Bayesian policies and can earn higher average revenue than other robust policies while keeping a lower standard deviation . 
",We provide a competitive difference analysis of the online one way trading problem with limited information on prices. We formulate the problem recursively and obtain closed form solutions via backward induction. Our competitive difference analysis reveals for the first time all possible worst case scenarios. Simulations show that our method exhibits robustness against information inaccuracy. Our method can outperform trading policies by competitive ratio analysis in simulations.,,,
S037722171630162X," Discontinuities are common in the pricing of financial derivatives and have a tremendous impact on the accuracy of quasi Monte Carlo method . While if the discontinuities are parallel to the axes good efficiency of the QMC method can still be expected . By realigning the discontinuities to be axes parallel succeeded in recovering the high efficiency of the QMC method for a special class of functions . Motivated by this work we propose an auto realignment method to deal with more general discontinuous functions . The k means clustering algorithm a classical algorithm of machine learning is used to select the most representative normal vectors of the discontinuity surface . By applying this new method the discontinuities of the resulting function are realigned to be friendly for the QMC method . Numerical experiments demonstrate that the proposed method significantly improves the performance of the QMC method . 
",The idea of machine learning is introduced to the QMC area. A new path generation method called the auto realignment method is proposed. The k means clustering algorithm is used to select representative normal vectors. Experiments demonstrate the efficiency and robustness of the proposed method.,,,
S037722171600093X," We propose a Hybrid Scenario Cluster Decomposition heuristic for solving a large scale multi stage stochastic mixed integer programming model corresponding to a supply chain tactical planning problem . The HSCD algorithm decomposes the original scenario tree into smaller sub trees that share a certain number of predecessor nodes . Then the MS MIP model is decomposed into smaller scenario cluster multi stage stochastic sub models coordinated by Lagrangian terms in their objective functions in order to compensate the lack of non anticipativity corresponding to common ancestor nodes of sub trees . The sub gradient algorithm is then implemented in order to guide the scenario cluster sub models into an implementable solution . Moreover a Variable Fixing Heuristic is embedded into the sub gradient algorithm in order to accelerate its convergence rate . Along with the possibility of parallelization the HSCD algorithm provides the possibility of embedding various heuristics for solving scenario cluster sub models . The algorithm is specialized to lumber supply chain tactical planning under demand and supply uncertainty . An ad hoc heuristic based on Lagrangian Relaxation is proposed to solve each scenario cluster sub model . Our experimental results on a set of realistic scale test cases reveal the efficiency of HSCD in terms of solution quality and computation time . set of harvesting blocks set of manufacturing mills set of raw materials set of available raw material scenarios set of time periods corresponding to node n in the scenario tree the demand scenario tree set of immediate predecessor of each node n in scenario tree the total harvesting capacity in period t the total transportation capacity in period t unit cost to harvest block bl during period t unit cost to transport raw material rm from block bl to mill m during period t unit cost to store raw material rm in block bl during period t stumpage fee for raw material rm in block bl during period t inventory holding cost of raw material rm at mill m inventory capacity at mill m during period t supply capacity of block bl in period t maximum number of periods over which harvesting can occur in block bl lead time of procuring raw material rm from block bl unit purchase cost of raw material rm from block m in period t maximum number of blocks in which harvesting can occur during period t probability of node n in scenario tree probability of supply scenario sc minimum contract purchase quantity from block bl safety stock of raw material rm at mill m volume of available raw material rm in block bl for supply scenario sc forecasted demand of raw material rm for mill m in period t at node n of the scenario tree binary variable that takes 1 if harvesting occurs in block bl during time period t at node n of the scenario tree and 0 otherwise inventory of raw material rm in block bl at the end of period t for supply scenario sc at. 
",A heuristic is proposed for solving large scale multi stage stochastic mixed integer programs. The algorithm significantly accelerates the scenario cluster decomposition approach. An accelerated sub gradient algorithm is used to solve scenario cluster sub models. The algorithm is specialized to lumber supply chain tactical planning under uncertainty.,,,
S037722171630100X," Demand based pricing is often used to moderate demand fluctuations so as to level resource utilization and increase profitability . However such pricing policies may not be effective when customers purchase decisions are influenced by social interactions . This paper investigates the demand dynamics under a demand based pricing policy of a frequently purchased service when social interactions are at work . Customers are heterogeneous and adaptively forward looking . Existing customers re purchase decisions are based on adaptively formed price expectations and reservation prices . Potential customers are attracted through social interactions with existing customers . The demand process is characterized by a two dimensional dynamical system . It is shown that the equilibrium demand can be unstable . For a given reservation price distribution we first analyze the stability of the equilibrium demand under various scenarios of social interactions and customers adaptively forward looking behavior and then characterize their dynamics using the bifurcation plots Lyapunov exponents and return maps . The results indicate that the demand process can be stable periodic or chaotic . The study shows that the intended effect of a demand based pricing policy may be offset by customers adaptively forward looking behavior under the influence of social interactions . In fact the interplay of these factors may even lead to chaotic demand dynamics . The result highlights the complex dynamics produced by a simple demand price mechanism under social interactions . For a demand based pricing strategy to be effective companies must take social interactions into account . 
",The effectiveness of demand based pricing under social interactions in moderating demand fluctuations is studied. Results show that the demand dynamics can exhibit chaos when social interactions are at work. Social interactions and customers forward looking behavior may offset the intended effect of a demand based pricing. Operational policies must be examined in the appropriate context e.g. under the influence of social interactions.,,,
S037722171630039X," We consider storage loading problems where items with uncertain weights have to be loaded into a storage area taking into account stacking and payload constraints . Following the robust optimization paradigm we propose strict and adjustable optimization models for finite and interval based uncertainties . To solve these problems exact decomposition and heuristic solution algorithms are developed . For strict robustness we also propose a compact formulation based on a characterization of worst case scenarios . Computational results for randomly generated data with up to 300 items are presented showing that the robustness concepts have different potential depending on the type of data being used . 
",We present a new model for stacking problems with payload constraints. Due to uncertain item weights two robust optimization approaches are developed. Problems are solved using exact decomposition and heuristic algorithms. Using a characterization of worst case scenarios a compact model is presented.,,,
S037722171600103X," We consider a Cournot duopoly under general demand and cost functions where an incumbent patentee has a cost reducing technology that it can license to its rival by using combinations of royalties and upfront fees . We show that for drastic technologies licensing occurs and both firms stay active if the cost function is superadditive and licensing does not occur and the patentee monopolizes the market if the cost function is additive or subadditive . For non drastic technologies licensing takes place provided the average efficiency gain from the cost reducing technology is higher than the marginal gain computed at the licensee s reservation output . Optimal licensing policies have both royalties and fees for significantly superior technologies if the cost function is superadditive . By contrast for additive and certain subadditive cost functions optimal licensing policies have only royalties and no fees . 
",We analyze the licensing of a cost reducing innovation in a Cournot duopoly. We focus on two part tariff policies for both drastic and non drastic innovations. A drastic innovation is licensed not licensed if cost is super additive sub additive . A non drastic innovation is licensed if technology s average gain exceeds marginal gain. Royalties are positive fees could be zero.,,,
S037722171600014X," This study addresses the optimal pipe sizing problem of a tree shaped gas distribution network with a single supply source . An algorithm was developed with the aim of minimizing the investment for constructing a gas distribution network with a tree shaped layout in which demands are fixed . The construction cost is known to depend on the pipe diameters used for each arc in the network . However under the assumption that pipe diameters are continuous we prove that it is possible to obtain the minimum construction cost directly and analytically by an iterating procedure that converts the original tree into a single equivalent arc . In addition we also show that expanding the converted single arc inversely to the original tree computes the optimal continuous pipe diameter for each arc . Following this we present an additional heuristic to convert optimal continuous pipe diameters into approximate discrete pipe diameters . The algorithms were evaluated by applying them to sample networks . The numerical results obtained by comparing the approximate discrete diameters with the optimal discrete diameters confirm the efficiency of our algorithms thereby demonstrating their suitability for designing real gas distribution networks . 
",Algorithm to minimize investment cost for tree shaped gas distribution networks. Minimum cost obtained by converting the original tree into a single equivalent arc. Optimal continuous pipe diameters are converted to discrete approximate diameters. The algorithms are suitable for designing real gas distribution networks.,,,
S037722171630203X," Given the evolution in the agricultural sector and the new challenges it faces managing agricultural supply chains efficiently has become an attractive topic for researchers and practitioners . Against this background the integration of uncertain aspects has continuously gained importance for managerial decision making since it can lead to an increase in efficiency responsiveness business integration and ultimately in market competitiveness . In order to capture appropriately the uncertain conjuncture of most agricultural real life applications an increasing amount of research effort is especially dedicated to treating uncertainty . In particular quantitative modeling approaches have found extensive use in agricultural supply chain management . This paper provides an overview of the latest advances and developments in the application of operations research methodologies to handling uncertainty occurring in the agricultural supply chain management problems . It seeks to offer a representative overview of the predominant research topics highlight the most pertinent and widely used frameworks and discuss the emergence of new operations research advances in the agricultural sector . The broad spectrum of reviewed contributions is classified and presented with respect to three most relevant discerned features uncertainty modeling types programming approaches and functional application areas . Ultimately main review findings are pointed out and future research directions which emerge are suggested . 
",This study provides a state of the art of OR use in agriculture under uncertainty. It offers an overview of the commonly used OR approaches for handling uncertainty. Main review findings are pointed out. Future research directions are derived and suggested.,,,
S037722171501139X," We study the regulation of one way station based vehicle sharing systems through parking reservation policies . We measure the performance of these systems in terms of the total excess travel time of all users caused as a result of vehicle or parking space shortages . We devise mathematical programming based bounds on the total excess travel time of vehicle sharing systems under any passive regulation and in particular under any parking space reservation policy . These bounds are compared to the performance of several partial parking reservation policies a parking space overbooking policy and to the complete parking reservation and no reservation policies introduced in a previous paper . A detailed user behavior model for each policy is presented and a discrete event simulation is used to evaluate the performance of the system under various settings . The analysis of two case studies of real world systems shows the following a significant improvement of what can theoretically be achieved is obtained via the CPR policy the performances of the proposed partial reservation policies monotonically improve as more reservations are required and parking space overbooking is not likely to be beneficial . In conclusion our results reinforce the effectiveness of the CPR policy and suggest that parking space reservations should be used in practice even if only a small share of users are required to place reservations . 
",The exertion of parking reservation policies in vehicle sharing systems is studied. Partial parking reservations are introduced and analyzed. Mathematical programming based lower bounds on the quality of service are devised. The complete parking reservation policy is shown to be both simple and effective. Parking overbooking policies are demonstrated not to be worthwhile.,,,
S037722171600045X," In this paper we present an approach to generate cyclic production schemes for multiple products with stochastic demand to be produced on a single production unit . This scheme specifies a fixed and periodic production sequence called a cycle where each product may occur more than once in the sequence . In order to stabilize the cycle length alternative control strategies are introduced . These strategies keep the cycle length close to a target length by means of adding idle time to the cycle cutting production short or overproduction . On basis of the selected control strategy as well as a base stock policy target inventory levels are determined for each production run in the cycle . Demand can be backlogged but a certain service level is required . Setup times are sequence dependent and the storage is capacitated . Employing the developed strategies we investigate the tradeoff between stability of cycle length and total cost induced by the production schedule in a computational study based on both real world data and random test instances . 
",We present a cyclic production scheme for multiple products with stochastic demand. We impose bounds for the cycle length in order to achieve a regular schedule. We consider sequence dependent setup times service levels and storage capacity. The computational study based on real world data compares six strategies that control the cycle length.,,,
S037722171630025X," This paper addresses an integrated framework for deciding about the supplier selection in the processed food industry under uncertainty . The relevance of including tactical production and distribution planning in this procurement decision is assessed . The contribution of this paper is three fold . Firstly we propose a new two stage stochastic mixed integer programming model for the supplier selection in the process food industry that maximizes profit and minimizes risk of low customer service . Secondly we reiterate the importance of considering main complexities of food supply chain management such as perishability of both raw materials and final products uncertainty at both downstream and upstream parameters and age dependent demand . Thirdly we develop a solution method based on a multi cut Benders decomposition and generalized disjunctive programming . Results indicate that sourcing and branding actions vary significantly between using an integrated and a decoupled approach . The proposed multi cut Benders decomposition algorithm improved the solutions of the larger instances of this problem when compared with a classical Benders decomposition algorithm and with the solution of the monolithic model . 
",We present a stochastic programming model for supplier selection. We incorporate uncertainty in the demand and in the supplier operation. We propose an enhanced Benders decomposition algorithm for this problem. Sourcing actions vary between using an integrated and a decoupled approach. Solving a single master problem in the Benders decomposition is more efficient.,,,
S037722171630159X," In this paper we consider the Asymmetric Quadratic Traveling Salesman Problem . Given a directed graph and a function that maps every pair of consecutive arcs to a cost the problem consists in finding a cycle that visits every vertex exactly once and such that the sum of the costs is minimal . We propose an extended Linear Programming formulation that has a variable for each cycle in the graph . Since the number of cycles is exponential in the graph size we propose a column generation approach . Moreover we apply a particular reformulation linearization technique on a compact representation of the problem and compute lower bounds based on Lagrangian relaxation . We compare our new bounds with those obtained by some linearization models proposed in the literature . Computational results on some set of benchmarks used in the literature show that our lower bounding procedures are very promising . 
",Present different reformulations for the quadratic traveling salesman problem. Develop new bound based on reformulation linearization technique. Propose a cycle based model and solve it by column generation approach. Develop a stabilized column generation.,,,
S037722171630234X," We propose a novel meta approach to support collaborative multi objective supplier selection and order allocation decisions by integrating multi criteria decision analysis and linear programming . The proposed model accounts for suppliers performance synergy effects within a hierarchical decision structure . It incorporates both heterogeneous objective data and subjective judgments of the decision makers representing various groups with different voting powers . We maximize the total value of purchasing by optimizing order quantity assignment to suppliers and taking into consideration their synergies encountered in different time horizons . We apply the proposed model to a contractor selection and order quantity assignment problem in an agricultural commodity trading company . We maximize the strategic effectiveness of both the customers and the suppliers minimize risks increase the degree of cooperation between trading partners on all levels of supply chain integration enhance transparent knowledge sharing and aggregation and support collaborative decision making . 
",A meta approach is proposed for collaborative multi criteria supplier selection and order allocation. The model incorporates both heterogeneous objective data and subjective judgments. The model considers synergistic effects within a hierarchical decision structure. The suppliers strategic performance indicators are monitored in the proposed model. The applicability of the model is demonstrated in commodity trading.,,,
S037722171630220X," The linguistic term set is an applicable and flexible technique in qualitative decision making . To further develop the linguistic term set this paper proposes a generalized asymmetric linguistic term set based on the asymmetric sigmoid semantics which belongs to an asymmetric and non uniform linguistic term set and can be used to address the QDM problems involving risk appetites of the decision maker . Then a value at risk fitting approach is designed for obtaining the risk appetite parameters of the GALTS and six desirable properties of the GALTS are analyzed i.e . asymmetry non uniformity generality variability range consistency and diminishing utility . Based on the above approaches and the generalized asymmetric linguistic preference relations a QDM process involving risk appetites of the DM is designed . Because the GALPRs consist of subjective information provided by the DM the process is not perfectly consistent and is usually difficult to change or repeat . Thus a transitivity improvement approach is investigated and the corresponding calculation steps are provided . Finally an example dealing with the problem of investment decision making is provided and the results fully demonstrate the validity of the proposed methods for QDM involving risk appetites . 
",We develop a generalized asymmetric linguistic term set GALTS with six properties. The value at risk fitting method is designed to obtain the risk appetite parameters. The transitivity improvement approach of the GALTS is provided. A new QDM process involving risk appetites of the decision maker is proposed.,,,
S037722171630114X," Today s power systems are experiencing a transition from primarily fossil fuel based generation toward greater shares of renewable energy sources . It becomes increasingly costly to manage the resulting uncertainty and variability in power system operations solely through flexible generation assets . Incorporating demand side flexibility through appropriately designed incentive structures can add an additional lever to balance demand and supply . Based on a supply model using empirical wind generation data and a discrete model of flexible demand with temporal constraints we design and evaluate a local online market mechanism for matching flexible load and uncertain supply . Under this mechanism truthful reporting of flexibility is a dominant strategy for consumers reducing payments and increasing the likelihood of allocation . Suppliers during periods of scarce supply benefit from elevated critical value payments as a result of flexibility induced competition on the demand side . We find that for a wide range of the key parameters the cost of ensuring incentive compatibility in a smart grid market relative to the welfare optimal matching is relatively small . This suggests that local matching of demand and supply can be organized in a decentral manner in the presence of a sufficiently flexible demand side . Extending the stylized demand model to include complementary demand structures we demonstrate that decentral matching induces only minor efficiency losses if demand is sufficiently flexible . Furthermore by accounting for physical grid limitations we show that flexibility and grid capacity exhibit complementary characteristics . 
",We present a model of flexible loads and renewable supply in the smart grid. Supply and demand are matched locally via an incentive compatible online mechanism. Demand flexibility reduces payments and increases allocation probability. Increasing demand flexibility will increase suppliers profits. The cost of establishing incentive compatibility is decreasing in flexibility.,,,
S037722171501142X," This manuscript reviews recent advances in deterministic global optimization for Mixed Integer Nonlinear Programming as well as Constrained Derivative Free Optimization . This work provides a comprehensive and detailed literature review in terms of significant theoretical contributions algorithmic developments software implementations and applications for both MINLP and CDFO . Both research areas have experienced rapid growth with a common aim to solve a wide range of real world problems . We show their individual prerequisites formulations and applicability but also point out possible points of interaction in problems which contain hybrid characteristics . Finally an inclusive and complete test suite is provided for both MINLP and CDFO algorithms which is useful for future benchmarking . 
",We review the recent advances in global optimization for Mixed Integer Nonlinear Programming MINLP. We review the recent advances in global optimization for Constrained Derivative Free optimization CDFO. We present theoretical contributions software implementations and applications for both MINLP and CDFO. We discuss possible interactions between the two areas of MINLP and CDFO. We present a complete test suite for MINLP and CDFO algorithms.,,,
S074756321630125X," Provocative messages targeting childhood obesity are a central means to increase problem awareness . But what happens when different online media platforms take up the campaign comment re contextualize and evaluate it Relating to preliminary findings of persuasion research we postulate that source credibility perceptions vary across types of online media platforms and contextualization of the message . Individual characteristics in particular weight related factors are assumed to influence message effects . A 3 2 experimental design with students aged between 13 and 18 years was conducted . Results show an interaction between media type and argumentation for affective self perceptions of weight . Self relevance varies based on different source credibility perceptions . Overall campaign re contextualization of provocative messages may result in negative persuasion effects and needs to be considered in campaign development . Childhood anti obesity campaigns are often designed in a highly provocative way in order to increase attention . Social media platforms are a prominent online environment to spread such health related public service announcements raise awareness and motivate discussion . However when applying this strategy public healthcare runs the risk of losing control over the effects of its campaigns . Follow up communication in different media platforms such as blogs online news sites or social networking sites takes up the campaign comments re contextualizes and evaluates it . On the one hand this strengthens the attention focused on the campaign on the other hand the question emerges how different context formats affect campaign perceptions . Even if the core messages of the campaigns are still equal variations in the argumentative contextualization and the media platform change the representation of the message or in other words the frame of the message . In terms of how campaigns are embedded two aspects stand out . First the contextualization of the content could either reinforce the argumentation of the campaign or impair its original message . The arguments of the original message are either supported or rejected which resembles value framing in political communication . Second follow up communication regarding the campaigns could be placed on different online media platforms reaching from more traditional journalistic contexts such as online newspapers to social media environments such as Facebook or blogs . Both re contextualization factors address the credibility and trustworthiness of the context the message and of the communicator . However research has failed to analyze the comparative effects of social campaigns in these different online media environments . A highly relevant example in regard to such concerns is the perception of obesity related health messages especially when these campaigns address children and adolescents since such campaigns are increasingly at risk of offending and stigmatizing children affected by obesity . However there is little research on the effects of campaigns against childhood obesity and the fact that they convey negative or even shocking messages in online media . Considering the lack of research on this subject our study aims to close the gap and investigate the effects of different. 
",First systematic research on variations of online source credibility perceptions. Experimental study on online re contextualization effects. Stratified sample for education levels of 700 secondary school students. Provocative campaigning takes the risk of boomerang effects. Media and argumentation type affects affective self perceptions of weight.,,,
S088523081300048X," Language is being increasingly harnessed to not only create natural human machine interfaces but also to infer social behaviors and interactions . In the same vein we investigate a novel spoken language task of inferring social relationships in two party conversations whether the two parties are related as family strangers or are involved in business transactions . For our study we created a corpus of all incoming and outgoing calls from a few homes over the span of a year . On this unique naturalistic corpus of everyday telephone conversations which is unlike Switchboard or any other public domain corpora we demonstrate that standard natural language processing techniques can achieve accuracies of about 88 82 74 and 80 in differentiating business from personal calls family from non family calls familiar from unfamiliar calls and family from other personal calls respectively . Through a series of experiments with our classifiers we characterize the properties of telephone conversations and find that 30 words of openings are sufficient to predict business from personal calls which could potentially be exploited in designing context sensitive interfaces in smart phones our corpus based analysis does not support Schegloff and Sack s manual analysis of exemplars in which they conclude that pre closings differ significantly between business and personal calls closing fared no better than a random segment and the distribution of different types of calls are stable over durations as short as 1 2 months . In summary our results show that social relationships can be inferred automatically in two party conversations with sufficient accuracy to support practical applications . 
",We introduce a novel task that of inferring social relationships from everyday conversations. We collected a corpus of natural telephone conversations unlike any other publicly available corpora. We show that 30 words of the beginning of a conversation is sufficient to infer the relationship accurately. We show that classifiers are useful in estimating the social engagement using conversations spanning 3 months.,,,
S088523081400120X," In recent years the use of rhythm based features in speech processing systems has received growing interest . This approach uses a wide array of rhythm metrics that have been developed to capture speech timing differences between and within languages . However the reliability of rhythm metrics is being increasingly called into question . In this paper we propose two modifications to this approach . First we describe a model that is based on auditory cues that simulate the external middle and inner parts of the ear . We evaluate this model by performing experiments to discriminate between native and non native Arabic speech . Data are from the West Point Arabic Speech Corpus testing is done on standard classifiers based on Gaussian Mixture Models Support Vector Machines and a hybrid GMM SVM . Results show that the auditory based model consistently outperforms a traditional rhythm metric approach that includes both duration and intensity based metrics . Second we propose a framework that combines the rhythm metrics and the auditory based cues in the context of a Logistic Regression method that can optimize feature combination . Further results show that the proposed LR based method improves performance over the standard classifiers in the discrimination between the native and non native Arabic speech . 
",New models of speech rhythm and auditory knowledge are proposed. Use both duration and intensity metrics to classify native and non native accents. Perform accent classification by logistic regression LR and with baseline systems. LR based approach provides the best classification of native non native Arabic speech. Combination of auditoryc indicative features and rhythm metrics provides the best classification.,,,
S088523081300020X," Sentiment analysis is the natural language processing task dealing with sentiment detection and classification from texts . In recent years due to the growth in the quantity and fast spreading of user generated contents online and the impact such information has on events people and companies worldwide this task has been approached in an important body of research in the field . Despite different methods having been proposed for distinct types of text the research community has concentrated less on developing methods for languages other than English . In the above mentioned context the present work studies the possibility to employ machine translation systems and supervised methods to build models able to detect and classify sentiment in languages for which less no resources are available for this task when compared to English stressing upon the impact of translation quality on the sentiment classification performance . Our extensive evaluation scenarios show that machine translation systems are approaching a good level of maturity and that they can in combination to appropriate machine learning algorithms and carefully chosen features be used to build sentiment analysis systems that can obtain comparable performances to the one obtained for English . 
",We study the possibility to employ Machine Translation MT systems and supervised methods for multilingual sentiment analysis. Experiments are done for English German Spanish and French. We use three MT systems Google Bing and Moses different supervised learning algorithms and various types of features. We show how meta classifiers can be employed to mitigate the noise introduced by translation. Our extensive evaluations show that MT systems can be used for multilingual sentiment analysis.,,,
S088523081500042X," Phonological studies suggest that the typical subword units such as phones or phonemes used in automatic speech recognition systems can be decomposed into a set of features based on the articulators used to produce the sound . Most of the current approaches to integrate articulatory feature representations into an automatic speech recognition system are based on a deterministic knowledge based phoneme to AF relationship . In this paper we propose a novel two stage approach in the framework of probabilistic lexical modeling to integrate AF representations into an ASR system . In the first stage the relationship between acoustic feature observations and various AFs is modeled . In the second stage a probabilistic relationship between subword units and AFs is learned using transcribed speech data . Our studies on a continuous speech recognition task show that the proposed approach effectively integrates AFs into an ASR system . Furthermore the studies show that either phonemes or graphemes can be used as subword units . Analysis of the probabilistic relationship captured by the parameters has shown that the approach is capable of adapting the knowledge based phoneme to AF representations using speech data and allows different AFs to evolve asynchronously . 
",Approach for AF based ASR in framework of probabilistic lexical modeling is proposed. Most approaches in literature use a knowledge based deterministic phoneme to AF map. Approach incorporates a probabilistic phoneme to AF map learned through acoustic data. Analysis has shown that the approach allows different AFs to evolve asynchronously. Approach has potential to reduce word error rates by incorporating AFs in an ASR system.,,,
S088523081530036X," In this work we present a comprehensive study on the use of deep neural networks for automatic language identification . Motivated by the recent success of using DNNs in acoustic modeling for speech recognition we adapt DNNs to the problem of identifying the language in a given utterance from its short term acoustic features . We propose two different DNN based approaches . In the first one the DNN acts as an end to end LID classifier receiving as input the speech features and providing as output the estimated probabilities of the target languages . In the second approach the DNN is used to extract bottleneck features that are then used as inputs for a state of the art i vector system . Experiments are conducted in two different scenarios the complete NIST Language Recognition Evaluation dataset 2009 and a subset of the Voice of America data from LRE 09 in which all languages have the same amount of training data . Results for both datasets demonstrate that the DNN based systems significantly outperform a state of art i vector system when dealing with short duration utterances . Furthermore the combination of the DNN based and the classical i vector system leads to additional performance improvements . 
",This work presents a comprehensive study on the use of deep neural networks for automatic language identification. It includes a detailed performance analysis for different data selection strategies and DNN architectures. Proposed systems are tested on the NIST Language Recognition Evaluation 2009 against an state of the art i vector baseline. It also presents a novel approach that combines DNN and i vector systems by using bottleneck features. The combination of i vector and bottleneck systems outperforms our baseline system by 45 in EER and Cavg on 3s and 10s.,,,
S088523081400093X," In this paper we present a survey on the application of recurrent neural networks to the task of statistical language modeling . Although it has been shown that these models obtain good performance on this task often superior to other state of the art techniques they suffer from some important drawbacks including a very long training time and limitations on the number of context words that can be taken into account in practice . Recent extensions to recurrent neural network models have been developed in an attempt to address these drawbacks . This paper gives an overview of the most important extensions . Each technique is described and its performance on statistical language modeling as described in the existing literature is discussed . Our structured overview makes it possible to detect the most promising techniques in the field of recurrent neural networks applied to language modeling but it also highlights the techniques for which further research is required . 
",We explain in detail the different steps in computing a language model based on a recurrent neural network. We survey the applications and findings based on the current literature. We survey the methods for reducing computational complexity.,,,
S088523081300034X," In this paper we suggest a list of high level features and study their applicability in detection of cyberpedophiles . We used a corpus of chats downloaded from http www.perverted justice.com and two negative datasets of different nature cybersex logs available online and the NPS chat corpus . The classification results show that the NPS data and the pedophiles conversations can be accurately discriminated from each other with character n grams while in the more complicated case of cybersex logs there is need for high level features to reach good accuracy levels . In this latter setting our results show that features that model behaviour and emotion significantly outperform the low level ones and achieve a 97 accuracy . 
",High level features for cyberpedophilia detection are proposed. The fixated discourse model is suggested. Experiments on distinguishing between pedophiles and non pedophiles chats are performed. Feature analysis is presented.,,,
S088523081300079X," Discriminative confidence based on multi layer perceptrons and multiple features has shown significant advantage compared to the widely used lattice based confidence in spoken term detection . Although the MLP based framework can handle any features derived from a multitude of sources choosing all possible features may lead to over complex models and hence less generality . In this paper we design an extensive set of features and analyze their contribution to STD individually and as a group . The main goal is to choose a small set of features that are sufficiently informative while keeping the model simple and generalizable . We employ two established models to conduct the analysis one is linear regression which targets for the most relevant features and the other is logistic linear regression which targets for the most discriminative features . We find the most informative features are comprised of those derived from diverse sources and the two models deliver highly consistent feature ranks . STD experiments on both English and Spanish data demonstrate significant performance gains with the proposed feature sets . 
",Feature analysis for spoken term detection STD on English meeting domain and Spanish read speech data in a discriminative confidence estimation framework. Feature analysis is based on groups that are defined according to their information sources lattice based features duration based features lexical features Levenshtein distance based features position and prosodic features pitch and energy . Feature analysis employs two well known and established models linear regression a generative approach and logistic linear regression a discriminative approach . Individual and incremental analyses are presented for both models. Results demonstrate significant improvement with the 3 5 most informative features compared with using the single best feature for STD confidence estimation. The best feature set comprises features from different groups lattice based and lexical features are among the most informative groups in general and duration and energy are more informative for read speech data.,,,
S088523081400103X," Globalization has dramatically increased the need of translating information from one language to another . Frequently such translation needs should be satisfied under very tight time constraints . Machine translation techniques can constitute a solution to this overly complex problem . However the documents to be translated in real scenarios are often limited to a specific domain such as a particular type of medical or legal text . This situation seriously hinders the applicability of MT since it is usually expensive to build a reliable translation system no matter what technology is used due to the linguistic resources that are required to build them such as dictionaries translation memories or parallel texts . In order to solve this problem we propose the application of automatic post editing in an online learning framework . Our proposed technique allows the human expert to translate in a specific domain by using a base translation system designed to work in a general domain whose output is corrected by means of an automatic post editing module . This automatic post editing module learns to make its corrections from user feedback in real time by means of online learning techniques . We have validated our system using different translation technologies to implement the base translation system as well as several texts involving different domains and languages . In most cases our results show significant improvements in terms of BLEU with respect to the baseline systems . The proposed technique works effectively when the n grams of the document to be translated presents a certain rate of repetition situation which is common according to the document internal repetition property . 
",We present a method to customize machine translation systems when in domain data is not available. For that we perform an online learning automatic post editing from ready to use generic machine translation systems. The results show that the method is very effective on rule based machine translation systems. On statistical machine translation systems the method performs well if no in domain data was used in the training. Finally if there is not enough repetition our method has limited use.,,,
S088523081400028X," Natural languages are known for their expressive richness . Many sentences can be used to represent the same underlying meaning . Only modelling the observed surface word sequence can result in poor context coverage and generalization for example when using n gram language models . This paper proposes a novel form of language model the paraphrastic LM that addresses these issues . A phrase level paraphrase model statistically learned from standard text data with no semantic annotation is used to generate multiple paraphrase variants . LM probabilities are then estimated by maximizing their marginal probability . Multi level language models estimated at both the word level and the phrase level are combined . An efficient weighted finite state transducer based paraphrase generation approach is also presented . Significant error rate reductions of 0.5 0.6 absolute were obtained over the baseline n gram LMs on two state of the art recognition tasks for English conversational telephone speech and Mandarin Chinese broadcast speech using a paraphrastic multi level LM modelling both word and phrase sequences . When it is further combined with word and phrase level feed forward neural network LMs a significant error rate reduction of 0.9 absolute and 0.5 absolute were obtained over the baseline n gram and neural network LMs respectively . 
",Paraphrastic language models proposed. Statistical paraphrase learning from standard texts. Improved LM context coverage and generalization performance. Combination with word and phrase level neural network LMs. Significant error rate reductions of 5 9 relative.,,,
S088523081300017X," SAMAR is a system for subjectivity and sentiment analysis for Arabic social media genres . Arabic is a morphologically rich language which presents significant complexities for standard approaches to building SSA systems designed for the English language . Apart from the difficulties presented by the social media genres processing the Arabic language inherently has a high number of variable word forms leading to data sparsity . In this context we address the following 4 pertinent issues how to best represent lexical information whether standard features used for English are useful for Arabic how to handle Arabic dialects and whether genre specific features have a measurable impact on performance . Our results show that using either lemma or lexeme information is helpful as well as using the two part of speech tagsets . However the results show that we need individualized solutions for each genre and task but that lemmatization and the ERTS POS tagset are present in a majority of the settings . 
",We present a system for subjectivity and sentiment analysis SSA for Arabic social media data. Individual settings are required per genre and task. Using either lemmas or lexemes improves SSA results. Using a POS tagset leads improved results as do standard features. Processing dialects does not improve when it is know which sentences are in dialect. Genre specific features tend to be helpful for sentiment analysis but not for subjectivity.,,,
S088523081500073X," Non verbal communication involves encoding transmission and decoding of non lexical cues and is realized using vocal or visual channels during conversation . These cues perform the function of maintaining conversational flow expressing emotions and marking personality and interpersonal attitude . In particular non verbal cues in speech such as paralanguage and non verbal vocal events are used to nuance meaning and convey emotions mood and attitude . For instance laughters are associated with affective expressions while fillers are used to hold floor during a conversation . In this paper we present an automatic non verbal vocal events detection system focusing on the detect of laughter and fillers . We extend our system presented during Interspeech 2013 Social Signals Sub challenge for frame wise event detection and test several schemes for incorporating local context during detection . Specifically we incorporate context at two separate levels in our system the raw frame wise features and the output decisions . Furthermore our system processes the output probabilities based on a few heuristic rules in order to reduce erroneous frame based predictions . Our overall system achieves an Area Under the Receiver Operating Characteristics curve of 95.3 for detecting laughters and 90.4 for fillers on the test set drawn from the data specifications of the Interspeech 2013 Social Signals Sub challenge . We perform further analysis to understand the interrelation between the features and obtained results . Specifically we conduct a feature sensitivity analysis and correlate it with each feature s stand alone performance . The observations suggest that the trained system is more sensitive to a feature carrying higher discriminability with implications towards a better system design . 
",We present a sequential algorithm for detecting laughters and fillers in speech. The algorithm performs stepwise probability prediction context inclusion masking. We test several architectures for each of the above steps. Our models are more sensitive to change in feature carrying higher predictive power.,,,
S088523081500087X," Due to the increasing aging population in modern society and to the proliferation of smart devices there is a need to enhance speech recognition among smart devices in order to make information easily accessible to the elderly as it is to the younger population . In general speech recognition systems are optimized to an average adult s voice and tend to exhibit a lower accuracy rate when recognizing an elderly person s voice due to the effects of speech articulation and speaking style . Additional costs are bound to be incurred when adding modifications to current speech recognitions systems for better speech recognition among elderly users . Thus using a preprocessing application on a smart device can not only deliver better speech recognition but also substantially reduce any added costs . Audio samples of 50 words uttered by 80 elderly and young adults were collected and comparatively analyzed . The speech patterns of the elderly have a slower speech rate with longer inter syllabic silence length and slightly lower speech intelligibility . The speech recognition rate for elderly adults could be improved by means of increasing the speech rate adding a 1.5 increase in accuracy eliminating silence periods adding another 4.2 increase in accuracy and boosting the energy of the formant frequency bands for a 6 boost in accuracy . After all the preprocessing a 12 increase in the accuracy of elderly speech recognition was achieved . Through this study we show that speech recognition of elderly voices can be improved through modifying specific aspects of differences in speech articulation and speaking style . In the future we will conduct studies on methods that can precisely measure and adjust speech rate and find additional factors that impact intelligibility . 
",Preprocessed elderly voice signals were tested with an android smart phone. Speech recognition accuracy increased to 1.5 by increasing the speech rate. Speech recognition accuracy increased to 4.2 by eliminating intersyllabic pauses. Speech recognition accuracy increased to 6 by boosting formant frequency bands. After all the preprocessing 12 increase in the recognition accuracy was achieved.,,,
S088832701630005X," This work presents a conceptually simple experiment consisting of a cantilever beam with a nonlinear spring at the tip . The configuration allows manipulation of the relative spacing between the modal frequencies of the underlying linear structure and this permits the deliberate introduction of internal resonance . A 3 1 resonance is studied in detail the response around the first mode shows a classic stiffening response with the addition of more complex dynamic behaviour and an isola region . Quasiperiodic responses are also observed but in this work the focus remains on periodic responses . Predictions using Normal Form analysis and continuation methods show good agreement with experimental observations . The experiment provides valuable insight into frequency responses of nonlinear modal structures and the implications of nonlinearity for vibration tests . Young s modulus vector of Fourier components of the forcing signal nth modal variable first modal response amplitude taken at drive frequency second modal response amplitude taken at third harmonic of drive frequency vector of Fourier components of the voltage signal sent to the shaker amplifier resonant component of the nth modal variable response amplitude of u axial coordinate lateral coordinate linear modal damping ratio phase Poisson s ratio mass density nth mode shape function shorthand for linear natural angular frequency of nth mode resonant response frequency of nth modal variable 
",Experimental demonstration of 3 1 internal resonance in an easy to reproduce structure with transparent underlying physics. Experimental demonstration of isolated region in frequency response. Normal forms backbone analysis of the free structure used to explain the rich dynamics seen. Reasonable match with continuation analysis in AUTO the revealed structure of bifurcations sheds further light on the response of the forced and damped system.,,,
S074756321400538X," For acquiring new skills or knowledge contemporary learners frequently rely on the help of educational technologies supplementing human teachers as a learning aid . In the interaction with such systems speech based communication between the human user and the technical system has increasingly gained importance . Since spoken computer output can take on a variety of forms depending on the method of speech generation and the employment of prosodic modulations the effects of such auditory variations on the user s learning achievement require systematic investigation . The experiment reported here examined the specific effects of speech generation method and prosody of spoken system feedback in a computer supported learning environment and may serve as validational tool for future investigations of spoken computer feedback effects on learning . Learning performance in a basic cognitive task was compared between users receiving pre recorded naturally spoken system feedback with neutral prosody pre recorded feedback with motivating prosody or computer synthesized feedback . The observed results provide empirical evidence that users of technical tutoring systems benefit from pre recorded naturally spoken feedback and do even more so from feedback with motivational prosodic modulations matching their performance success . Theoretical implications and considerations for future implementations of spoken feedback in computer based educational systems are discussed . 
",Users of tutorial systems are affected by auditory modulations in system feedback. Users learn faster with naturally spoken than with synthesized computer feedback. Users learning rates benefit from system feedback with motivational prosody.,,,
S074756321500268X," There has been much debate surrounding the potential benefits and costs of online interaction . The present research argues that engagement with online discussion forums can have underappreciated benefits for users well being and engagement in offline civic action and that identification with other online forum users plays a key role in this regard . Users of a variety of online discussion forums participated in this study . We hypothesized and found that participants who felt their expectations had been exceeded by the forum reported higher levels of forum identification . Identification in turn predicted their satisfaction with life and involvement in offline civic activities . Formal analyses confirmed that identification served as a mediator for both of these outcomes . Importantly whether the forum concerned a stigmatized topic moderated certain of these relationships . Findings are discussed in the context of theoretical and applied implications . 
",Online discussion forums have benefits at individual and society level. They are positively linked to well being for stigmatised group members. Online discussion forum use is linked to offline civic engagement in related areas. Identification with other forum users mediates the above relationships. Online discussion forums are of greater applied importance than has been realized.,,,
S074756321500360X," On Facebook users are exposed to posts from both strong and weak ties . Even though several studies have examined the emotional consequences of using Facebook less attention has been paid to the role of tie strength . This paper aims to explore the emotional outcomes of reading a post on Facebook and examine the role of tie strength in predicting happiness and envy . Two studies one correlational based on a sample of 207 American participants and the other experimental based on a sample of 194 German participants were conducted in 2014 . In Study 2 envy was further distinguished into benign and malicious envy . Based on a multi method approach the results showed that positive emotions are more prevalent than negative emotions while browsing Facebook . Moreover tie strength is positively associated with the feeling of happiness and benign envy whereas malicious envy is independent of tie strength after reading a post on Facebook . New communication technologies such as social media have made social news more pervasive . Facebook continuously keeps users updated with a variety of posts and passive consumption of news updates is the main Facebook activity that people engage in . The majority of these updates are positive . There is evidence for emotional contagion showing happiness can spread through the news updates on online social networks . However recent studies also indicate that exposure to positive posts on Facebook may induce envy and lead to depression and reduced well being over time . Given that Facebook has over 1.35 billion active users and there are on average 1500 potential stories for users to check per visit we are eager to understand how Facebook affects users emotions and identify relevant factors that will determine emotional reactions . We argue that tie strength between the user and the poster is one important factor that should affect emotional outcomes . The use of social media such as Facebook can cause both positive and negative feelings and the results of prior studies on the psychological effects of social media usage are quite mixed . From a long term perspective the use of social media offers benefits such as the possibility of developing and maintaining social capital and social connectedness Nevertheless it may also lead to negative outcomes such as social overload an over optimistic perception towards others lives and a decrease in life satisfaction . From a short term perspective the use of Facebook can evoke a feeling of flow which is characterized by high positive valence and high arousal and joyful and fun are the most common positive feelings reported by users while using Facebook . Nonetheless the consumption of social news on Facebook can also trigger invidious emotions such as jealousy and envy . Faced with mixed results from prior research on the psychological effects of Facebook usage it is important to differentiate between interactive and non interactive social media behavior . Previous research has shown a consistent relation between using FB for interpersonal interaction. 
",Positive emotions are more prevalent than negative emotions while browsing Facebook. Users are happier when a positive post comes from a strong tie rather than a weak tie. Similarly users experience more benign envy when a post comes from a strong tie. The experience of malicious envy is independent of tie strength.,,,
S088523081400059X," This paper regards social question and answer collections such as Yahoo Answers as knowledge repositories and investigates techniques to mine knowledge from them to improve sentence based complex question answering systems . Specifically we present a question type specific method that extracts question type dependent cue expressions from social Q A pairs in which the question types are the same as the submitted questions . We compare our approach with the question specific and monolingual translation based methods presented in previous works . The question specific method extracts question dependent answer words from social Q A pairs in which the questions resemble the submitted question . The monolingual translation based method learns word to word translation probabilities from all of the social Q A pairs without considering the question or its type . Experiments on the extension of the NTCIR 2008 Chinese test data set demonstrate that our models that exploit social Q A collections are significantly more effective than baseline methods such as LexRank . The performance ranking of these methods is QTSM QSM MTM . The largest F3 improvements in our proposed QTSM over QSM and MTM reach 6.0 and 5.8 respectively . 
",The proposed approach leverages social Q A collections to improve automatic complex QA system. There is no need to manually collect training Q A pairs that are necessary for supervised machine learning approaches. Extensive comparison experiments are conducted i.e. LexRank question specific and translation based approaches are compared. Experiments on the extension of NTCIR 2008 test questions indicate that the proposed approach is more effective.,,,
S074756321630348X," The increasing convergence of the gambling and gaming industries has raised questions about the extent to which social casino game play may influence gambling . This study aimed to examine the relationship between social casino gaming and gambling through an online survey of 521 adults who played social casino games in the previous 12 months . Most social casino game users reported that these games had no impact on how much they gambled . However 9.6 reported that their gambling overall had increased and 19.4 reported that they had gambled for money as a direct result of these games . Gambling as a direct result of social casino games was more common among males younger users those with higher levels of problem gambling severity and more involved social casino game users in terms of game play frequency and in game payments . The most commonly reported reason for gambling as a result of playing social casino games was to win real money . As social casino games increased gambling for some users this suggests that simulated gambling may influence actual gambling expenditure particularly amongst those already vulnerable to or affected by gambling problems . 
",The social casino games and gambling industries and products are converging. 521 adults who played social casino games in the previous 12 months completed an online survey. A minority 19.4 of social casino game players gambled as a result of these games. Gambling was typically motivated by a desire to win money which is not possible in games. Social casino games may increase gambling for some players including those vulnerable to gambling problems.,,,
S088523081400014X," Pathological speech usually refers to the condition of speech distortion resulting from atypicalities in voice and or in the articulatory mechanisms owing to disease illness or other physical or biological insult to the production system . Although automatic evaluation of speech intelligibility and quality could come in handy in these scenarios to assist experts in diagnosis and treatment design the many sources and types of variability often make it a very challenging computational processing problem . In this work we propose novel sentence level features to capture abnormal variation in the prosodic voice quality and pronunciation aspects in pathological speech . In addition we propose a post classification posterior smoothing scheme which refines the posterior of a test sample based on the posteriors of other test samples . Finally we perform feature level fusions and subsystem decision fusion for arriving at a final intelligibility decision . The performances are tested on two pathological speech datasets the NKI CCRT Speech Corpus and the TORGO database by evaluating classification accuracy without overlapping subjects data among training and test partitions . Results show that the feature sets of each of the voice quality subsystem prosodic subsystem and pronunciation subsystem offer significant discriminating power for binary intelligibility classification . We observe that the proposed posterior smoothing in the acoustic space can further reduce classification errors . The smoothed posterior score fusion of subsystems shows the best classification performance . 
",We propose novel sentence level features to capture atypical variation. Our sentence level features are effective for intelligibility classification. We propose a post classification posterior smoothing scheme. Our smoothing scheme improves classification accuracy of our systems. We test feature level and subsystem fusions for the final intelligibility decision.,,,
S088523081300051X," This paper describes a noisy channel approach for the normalization of informal text such as that found in emails chat rooms and SMS messages . In particular we introduce two character level methods for the abbreviation modeling aspect of the noisy channel model a statistical classifier using language based features to decide whether a character is likely to be removed from a word and a character level machine translation model . A two phase approach is used in the first stage the possible candidates are generated using the selected abbreviation model and in the second stage we choose the best candidate by decoding using a language model . Overall we find that this approach works well and is on par with current research in the field . 
",Normalization of abbreviations in noisy informal text. Collection filtering and annotation of Twitter status messages. Comparison of statistical and machine translation approaches. Effects of language model order on accuracy. Combination of methods to achieve best results.,,,
S088523081600005X," Traditional concept retrieval is based on usual word definition dictionaries with simple performance they just map words to their definitions . This approach is mostly helpful for readers and language students but writers sometimes need to find a word that encompasses a set of ideas that they have in mind . For this task inverse dictionaries are ready to help however in some cases a sought word does not correspond to a single definition but to a composite meaning of several concepts . A language producer then tends to require a concept search that starts with a group of words or a series of related terms looking for a target word . This paper aims to assist on this task by presenting a new approach for concept blending through the development of a search by concept method based on vector space representation using semantic analysis and statistical natural language processing techniques . Words are represented as numeric vectors based on different semantic similarity measures and probabilistic measures the semantic properties of a word are captured in the vector elements determined by a given linguistic context . Three different sources are used as context for word vector construction WordNet a distributional thesaurus and the Latent Dirichlet Allocation algorithm each source is used for building a different semantic vector space . The concept blender input is then conformed by a set of n nouns . All input members are read and substituted by their corresponding vectors . Then a semantic space analysis including a filtering and ranking process is carried out to deploy a list of target words . A test set of 50 concepts was created in order to evaluate the system s performance . A group of 30 evaluators found our integrated concept blending model to provide better results for finding an adequate word for the provided set of concepts . 
",A method for merging an arbitrary number of nouns mixing their meanings. Successful model using vector representation scantily explored for concept retrieval. Experiments with 3 semantic space models WN a thesaurus and a topic based model. Good performance with an automatically obtained resource comparable to a manual one. Evaluation by qualified reviewers and comparison with a traditional dictionary.,,,
S092054891300069X," Living Labs are innovation infrastructures where software companies and research organizations collaborate with lead users to design and develop new products and services . There is not any reference model related to the processes or practices to manage a living lab . This article presents a reference model to manage effectively the synergies of software companies with the other stakeholders participating in a living lab . The article describes the approach used to create the reference model through the analysis of a multiple case study considering six living labs and discusses the lessons learned during the creation of the process reference model . 
",ISO IEC 15504 philosophy helps to formalize effective practices to manage living labs. The PRM covers the absence of a formalized approach to guide living lab management. The reference model contributes to facilitating benchmarking among living labs. The maturity levels provide a feasible path to create and evolve a living lab.,,,
S092054891300086X," The development of connected mobile applications is a complex task due to device diversity . Therefore device independent approaches are aimed at hiding the differences among the distinct mobile devices in the market . This work proposes DIMAG a software framework to generate connected mobile applications for multiple software platforms following a declarative approach . DIMAG provides transparent data and state synchronization between the server and the client side applications . The proposed platform has been designed making use of existing standards extending them when a required functionality is not provided . 
",The DIMAG framework generates platform independent connected mobile applications. Existing standards were used and refined in the design and implementation of DIMAG. Client server applications are specified in a declarative way. Server side of applications is generated for the Java EE platform. It dynamically generates client applications for Android Java ME and Windows Phone.,,,
S092054891400004X," Queuing network models provide powerful notations and tools for modeling and analyzing the performance of many different kinds of systems . Although several powerful tools currently exist for solving QNMs some of these tools define their own model representations have been developed in platform specific ways and are normally difficult to extend for coping with new system properties probability distributions or system behaviors . This paper shows how Domain Specific Languages when used in conjunction with Model driven engineering techniques provide a high level and very flexible approach for the specification and analysis of QNMs . We build on top of an existing metamodel for QNMs to define a DSL and its associated tools able to provide a high level notation for the specification of different kinds of QNMs and easy to extend for dealing with other probability distributions or system properties such as system reliability . 
",We have developed an application for specifying and simulating queuing network models. We reuse and extend a de facto standard metamodel for queuing network models PMIF. A modular and easily extensible architecture has been achieved using MDE techniques. The behavior for queuing network models is easily extensible with our approach.,,,
S092054891300127X," Flexibility maintainability and evolvability are very desirable properties for modern automation control systems . In order to achieve these characteristics modularity is regarded as an important concept in several scientific domains . The reuse of modules facilitates the reproduction of functionality or extensions of existing systems in similar environments . However it is often necessary to prepare such an environment to be able to reuse the intended programmed functionality . In an IEC 61131 3 environment cross vendor reuse of modules is problematic due to dependencies in proprietary programming environments and existing configurations . In this paper we aim to enable cross vendor reuse of modules by controlling these dependencies . Our approach is based on the Normalized Systems Theory from which we derived three guidelines for the design of reusable modules in an IEC 61131 3 environment for automation control projects . These guidelines are intended to support programmers in controlling dependencies regardless of the commercial programming environment they work with . 
",We illustrate that designing cross vendor reusable modules in automation systems is hard. Some theoretical foundations on how to design reusable modules are given. Five dependency problems in IEC 61131 3 projects are illustrated. Three rules are proposed in order to avoid these dependencies.,,,
S092054891300130X," Food consumption data are collected and used in several fields of science . The data are often combined from various sources and interchanged between different systems . There is however no harmonized and widely used data interchange format . In addition food consumption data are often combined with other data such as food composition data . In the field of food composition successful harmonization has recently been achieved by the European Food Information Resource Network which is now the basis of a standard draft by the European Committee for Standardization . We present an XML based data interchange format for food consumption based on work and experiences related to food composition . The aim is that the data interchange format will provide a basis for wider harmonization in the future . 
",Data interchange format for food consumption data Definition of the key concepts used with food consumption data A generic data model for food consumption data Linking food consumption data with food composition data,,,
S089561111500141X," A variety of vision ailments are indicated by anomalies in the choroid layer of the posterior visual section . Consequently choroidal thickness and volume measurements usually performed by experts based on optical coherence tomography images have assumed diagnostic significance . Now to save precious expert time it has become imperative to develop automated methods . To this end one requires choroid outer boundary detection as a crucial step where difficulty arises as the COB divides the choroidal granularity and the scleral uniformity only notionally without marked brightness variation . In this backdrop we measure the structural dissimilarity between choroid and sclera by structural similarity index and hence estimate the COB by thresholding . Subsequently smooth COB estimates mimicking manual delineation are obtained using tensor voting . On five datasets each consisting of 97 adult OCT B scans automated and manual segmentation results agree visually . We also demonstrate close statistical match between choroidal thickness distributions obtained algorithmically and manually . Further quantitative superiority of our method is established over existing results by respective factors of 27.67 and 76.04 in two quotient measures defined relative to observer repeatability . Finally automated choroidal volume estimation being attempted for the first time also yields results in close agreement with that of manual methods . 
",Choroid is statistically separated from sclera using structural similarity index. Smoothness in choroid delineation is achieved using tensor voting. Automated choroidal volume estimation is attempted for the first time. Choroidal thickness and volume estimates are given vis vis observer repeatability. Quotients are defined to fairly compare among methods tested on disparate datasets.,,,
S089561111400158X," Conventional in room cone beam computed tomography lacks explicit representation of patient respiratory motion and usually has poor image quality and inaccurate CT numbers for target delineation and or adaptive treatment planning . In room four dimensional CBCT image acquisition is still time consuming and suffers the same issue of poor image quality . To overcome this limitation we developed a computational framework to digitally synthesize high quality daily 4D CBCT images using the prior knowledge of motion and appearance learned from the planning 4D CT dataset . A patient specific respiratory motion model was first constructed from the planning 4D CT images using principal component analysis of displacement vector fields across different respiratory phases . Subsequently the respiratory motion model as well as the image content of the planning CT was spatially mapped onto the daily CBCT using deformable image registration . The synthesized 4D images possess explicit patient motion while maintaining the geometric accuracy of patient s anatomy at the time of treatment . We validated our model by quantitatively comparing the synthesized 4D CBCT against the 4D CT dataset acquired in the same day from protocol patients undergoing daily in room CBCT setup and weekly 4D CT for treatment evaluation . Our preliminary results have demonstrated good agreement of contours in different motion phases between the synthesized and acquired scans . Various imaging artifacts were also suppressed and soft tissue visibility was enhanced . 
",By taking advantages of deformable image registration we can successfully convert a single a low quality cone beam CT into a high quality multi phase cone beam CT images.,,,
S089561111400189X," Volume visualization is a very important work in medical imaging and surgery plan . However determining an ideal transfer function is still a challenging task because of the lack of measurable metrics for quality of volume visualization . In the paper we presented the voxel vibility model as a quality metric to design the desired visibility for voxels instead of designing transfer functions directly . Transfer functions are obtained by minimizing the distance between the desired visibility distribution and the actual visibility distribution . The voxel model is a mapping function from the feature attributes of voxels to the visibility of voxels . To consider between class information and with class information simultaneously the voxel visibility model is described as a Gaussian mixture model . To highlight the important features the matched result can be obtained by changing the parameters in the voxel visibility model through a simple and effective interface . Simultaneously we also proposed an algorithm for transfer functions optimization . The effectiveness of this method is demonstrated through experimental results on several volumetric data sets . 
",We propose the voxel visibility model for transfer function in volume rendering. We propose an optimization algorithm for automatic transfer function design. The voxel visibility model is a feature and voxel level model. The voxel visibility model can be efficiently used by users. The voxel visibility model provides an importance based strategy.,,,
S089561111500083X," We propose a novel 3D 2D registration approach for micro computed tomography and histology constructed for dental implant biopsies that finds the position and normal vector of the oblique slice from CT that corresponds to HI . During image pre processing the implants and the bone tissue are segmented using a combination of thresholding morphological filters and component labeling . After this chamfer matching is employed to register the implant edges and fine registration of the bone tissues is achieved using simulated annealing . The method was tested on n 10 biopsies obtained at 20 weeks after non submerged healing in the canine mandible . The specimens were scanned with CT 100 and processed for hard tissue sectioning . After registration we assessed the agreement of bone to implant contact using automated and manual measurements . Statistical analysis was conducted to test the agreement of the BIC measurements in the registered samples . Registration was successful for all specimens and agreement of the respective binary images was high . Direct comparison of BIC yielded that automated and manual measures from CT were significant positively correlated with HI between CT and HI groups . The results show that this method yields promising results and that CT may become a valid alternative to assess osseointegration in three dimensions . 
",We developed a novel method for automated 3D 2D registration of microCT and histology. We used the model of dental implant biopsies and found high agreement among the registered specimens. We directly compared the bone to implant contact BIC using the registered samples. We found significant positive association for the BIC measurements. Metal artifacts increased automated BIC measurements systematically.,,,
S089561111630009X," Prostate cancer is one of the major causes of cancer death for men . Magnetic resonance imaging is being increasingly used as an important modality to localize prostate cancer . Therefore localizing prostate cancer in MRI with automated detection methods has become an active area of research . Many methods have been proposed for this task . However most of previous methods focused on identifying cancer only in the peripheral zone or classifying suspicious cancer ROIs into benign tissue and cancer tissue . Few works have been done on developing a fully automatic method for cancer localization in the entire prostate region including central gland and transition zone . In this paper we propose a novel learning based multi source integration framework to directly localize prostate cancer regions from in vivo MRI . We employ random forests to effectively integrate features from multi source images together for cancer localization . Here multi source images include initially the multi parametric MRIs and later also the iteratively estimated and refined tissue probability map of prostate cancer . Experimental results on 26 real patient data show that our method can accurately localize cancerous sections . The higher section based evaluation combined with the ROC analysis result of individual patients shows that the proposed method is promising for in vivo MRI based prostate cancer localization which can be used for guiding prostate biopsy targeting the tumor in focal therapy planning triage and follow up of patients with active surveillance as well as the decision making in treatment selection . The common ROC analysis with the AUC value of 0.832 and also the ROI based ROC analysis with the AUC value of 0.883 both illustrate the effectiveness of our proposed method . 
",We propose an automatic detection method to localize prostate cancer in MRI. We localize prostate cancer in peripheral zone PZ as well as in central gland CG and transition zone TZ . The random forest is employed to effectively integrate features from multi source images. Experimental results show that our method can accurately localize the cancerous sections. The results of our proposed method are better than that of other four conventional methods.,,,
S089561111500097X," Diabetes increases the risk of developing any deterioration in the blood vessels that supply the retina an ailment known as Diabetic Retinopathy . Since this disease is asymptomatic it can only be diagnosed by an ophthalmologist . However the growth of the number of ophthalmologists is lower than the growth of the population with diabetes so that preventive and early diagnosis is difficult due to the lack of opportunity in terms of time and cost . Preliminary affordable and accessible ophthalmological diagnosis will give the opportunity to perform routine preventive examinations indicating the need to consult an ophthalmologist during a stage of non proliferation . During this stage there is a lesion on the retina known as microaneurysm which is one of the first clinically observable lesions that indicate the disease . In recent years different image processing algorithms which allow the detection of the DR have been developed however the issue is still open since acceptable levels of sensitivity and specificity have not yet been reached preventing its use as a pre diagnostic tool . Consequently this work proposes a new approach for MA detection based on reduction of non uniform illumination normalization of image grayscale content to improve dependence of images from different contexts application of the bottom hat transform to leave reddish regions intact while suppressing bright objects binarization of the image of interest with the result that objects corresponding to MAs blood vessels and other reddish objects are completely separated from the background application of the hit or miss Transformation on the binary image to remove blood vessels from the ROIs two features are extracted from a candidate to distinguish real MAs from FPs where one feature discriminates round shaped candidates from elongated shaped ones through application of Principal Component Analysis the second feature is a count of the number of times that the radon transform of the candidate ROI evaluated at the set of discrete angle values 0 1 2 180 is characterized by a valley between two peaks . The proposed approach is tested on the public databases DiaretDB1 and Retinopathy Online Challenge competition . The proposed MA detection method achieves sensitivity specificity and precision of 92.32 93.87 and 95.93 for the diaretDB1 database and 88.06 97.47 and 92.19 for the ROC database . Theory results challenges and performance related to the proposed MA detecting method are presented . 
",A new approach for microaneurysm detection is tested in two public image databases. Normalization of grayscale content is an essential preprocessing stage. Coarse segmentation for candidate selection is based on morphologic processing. Principal component analysis and Radon transform are used during feature extraction. The number of features to extract is very small with only two features.,,,
S089561111400202X," The automatization of the analysis of Indirect Immunofluorescence images is of paramount importance for the diagnosis of autoimmune diseases . This paper proposes a solution to one of the most challenging steps of this process the segmentation of HEp 2 cells through an adaptive marker controlled watershed approach . Our algorithm automatically conforms the marker selection pipeline to the peculiar characteristics of the input image hence it is able to cope with different fluorescent intensities and staining patterns without any a priori knowledge . Furthermore it shows a reduced sensitivity to over segmentation errors and uneven illumination that are typical issues of IIF imaging . 
",We propose automated segmentation of HEp 2 cells in immunofluorescence imaging. We apply the same pipeline to images with different fluorescent pattern and intensity. Our segmentation approach is based on adaptive marker controlled watershed. We assess the accuracy of our approach on a public dataset. We compare our performance with significant works from literature.,,,
S089561111400192X," A computerized framework for monitoring four dimensional dose distributions during stereotactic body radiation therapy based on a portal dose image based 2D 3D registration approach has been proposed in this study . Using the PDI based registration approach simulated 4D treatment CT images were derived from the deformation of 3D planning CT images so that a 2D planning PDI could be similar to a 2D dynamic clinical PDI at a breathing phase . The planning PDI was calculated by applying a dose calculation algorithm to the geometry of the planning CT image and a virtual water equivalent phantom . The dynamic clinical PDIs were estimated from electronic portal imaging device dynamic images including breathing phase data obtained during a treatment . The parameters of the affine transformation matrix were optimized based on an objective function and a gamma pass rate using a Levenberg Marquardt algorithm . The proposed framework was applied to the EPID dynamic images of ten lung cancer patients which included 183 frames . The 4D dose distributions during the treatment time were successfully obtained by applying the dose calculation algorithm to the simulated 4D treatment CT images . The mean standard deviation of the percentage errors between the prescribed dose and the estimated dose at an isocenter for all cases was 3.25 4.43 . The maximum error for the ten cases was 14.67 and the minimum error was 0.00 . The proposed framework could be feasible for monitoring the 4D dose distribution and dose errors within a patient s body during treatment . 
",We have investigated a framework for monitoring 4D dose distributions. A portal dose image based 2D 3D registration has been proposed. Dose errors can be calculated to ensure the quality of the radiation therapy.,,,
S092054891300072X," This paper proposes a QoS active queue management mechanism for multi QoS classes named as M GREEN which includes the consideration of QoS parameters and provides service differentiation among different flows classes . M GREEN extends the concept of Random and Early Detection in RED to Global Random and Early Estimation respectively . Furthermore M GREEN extends the linear concept of RED to an exponential one to enhance the efficiency of AQM . For performance evaluation extensive numerical cases are employed to compare M GREEN with some popular AQM schemes and to show the superior performance and characteristics of M GREEN . Consequently M GREEN is a possible way to provide the future multimedia Internet with differential services for different traffic classes of diverse QoS requirements . 
",We proposes a QoS AQM mechanism M GREEN for multi QoS classes. It extends the concept of Random in RED to Global Random . It extends the concept of Early Detection in RED to Early Estimation . It extends the linear concept of RED to an exponential one.,,,
S089561111400113X," Shape based 3D surface reconstructing methods for liver vessels have difficulties to tackle with limited contrast of medical images and the intrinsic complexity of multi furcation parts . In this paper we propose an effective and robust technique called Gap Border Pairing to reconstruct surface of liver vessels with complicated multi furcations . The proposed method starts from a tree like skeleton which is extracted from segmented liver vessel volumes and preprocessed as a number of simplified smooth branching lines . Secondly for each center point of any branching line an optimized elliptic cross section ring is generated by optimizedly fitting its actual cross section outline based on its tangent vector . Thirdly a tubular surface mesh is generated for each branching line by weaving all of its adjacent rings . Then for every multi furcation part a transitional regular mesh is effectively and regularly reconstructed by using GBP . An initial model is generated after reconstructing all multi furcation parts . Finally the model is refined by using just one time subdivision and its topologies can be re maintained by grouping its facets according to the skeleton providing high level editability . Our method can be automatically implemented in parallel if the segmented vessel volume and corresponding skeletons are provided . The experimental results show that GBP model is accurate enough in terms of the boundary deviations between segmented volume and the model . 
",Traditional methods for reconstructing complicated multi furcation are limited. Turing points are yield on borders if three branches intersect with each other. Turning points having anisotropic opposite relationship exactly make a triangle. Any border segment can pair one and only opposite segment. Topology of refined surface can be re maintained by checking original vertices.,,,
S092054891300024X," Many organizations are implementing process improvement models seeking to increase their organizational maturity for software development . However implementing traditional maturity models involves a large investment which is beyond the reach of vast majority of small organizations . This paper presents the use and adaptation of some ISO models in the creation of an organizational maturity model for the Spanish software industry . This model was used satisfactorily to improve the software processes of several Spanish small firms and obtain an organizational maturity certification for software development granted by the Spanish Association for Standardization and Certification . 
",A Software Engineering Maturity Model was produced for Spanish software industry. The model is based on standards ISO IEC 12207 ISO IEC 15504 and ISO IEC 17021. It allows a certification of the organizational maturity for software enterprises. Helping to improve the software development quality in all types of enterprises. Currently there are 38 development enterprises certified by AENOR in this model.,,,
S089561111500004X," Colorectal cancer usually appears in polyps developed from the mucosa . Carcinoma is frequently found in those polyps larger than 10mm and therefore only this kind of polyps is sent for pathology examination . In consequence accurate estimation of a polyp size determines the surveillance interval after polypectomy . The follow up consists in a periodic colonoscopy whose frequency depends on the estimation of the size polyp . Typically this polyp measure is achieved by examining the lesion with a calibrated endoscopy tool . However measurement is very challenging because it must be performed during a procedure subjected to a complex mix of noise sources namely anatomical variability drastic illumination changes and abrupt camera movements . This work introduces a semi automatic method that estimates a polyp size by propagating an initial manual delineation in a single frame to the whole video sequence using a spatio temporal characterization of the lesion during a routine endoscopic examination . The proposed approach achieved a Dice Score of 0.7 in real endoscopy video sequences when comparing with an expert . In addition the method obtained a root mean square error of 0.87mm in videos artificially captured in a cylindric structure with spheres of known size that simulated the polyps . Finally in real endoscopy sequences the diameter estimation was compared with measures obtained by a group of four experts with similar experience obtaining a RMSE of 4.7mm for a set of polyps measuring from 5 to 20mm . An ANOVA test performed for the five groups of measurements showed no significant differences . 
",The approach segments a polyp by propagating an initial manual delineation. Polyp segmentation is performed by a combination of motion and appearance features. A defocus strategy estimates the camera distance and the actual polyp size. Four experts and the method did not show significant differences in endoscopy. The approach is robust to different types of noise and can be used in real scenarios.,,,
S089561111500107X," Mitral valve diseases are among the most common types of heart diseases while heart diseases are the most common cause of death worldwide . MV repair surgery is connected to higher survival rates and fewer complications than the total replacement of the MV but MV repair requires extensive patient specific therapy planning . The simulation of MV repair with a patient specific model could help to optimize surgery results and make MV repair available to more patients . However current patient specific simulations are difficult to transfer to clinical application because of time constraints or prohibitive requirements on the resolution of the image data . As one possible solution to the problem of patient specific MV modeling we present a mass spring MV model based on 3D transesophageal echocardiographic images already routinely acquired for MV repair therapy planning . Our novel approach to the rest length estimation of springs allows us to model the global support of the MV leaflets through the chordae tendinae without the need for high resolution image data . The model is used to simulate MV annuloplasty for five patients undergoing MV repair and the simulated results are compared to post surgical TEE images . The comparison shows that our model is able to provide a qualitative estimate of annuloplasty surgery . In addition the data suggests that the model might also be applied to simulating the implantation of artificial chordae . 
",We present a mass spring model for simulating mitral valve repair. The patient specific model geometry is based on pre surgical 3D TEE images. We use a rest length estimation algorithm for modeling of the tendinous cords. A comparison with post surgical 3D TEE images proves the feasibility of our approach. Our simulation is faster than comparable state of the art finite element simulations.,,,
S089561111500066X," The present research was directed to effective image restoration with the extraction of ischemic edema signs . Computerized support of hyperacute stroke diagnosis based on routinely used computerized tomography scans was optimized to visualize the infarct extent more precisely . In particular a beneficial support of time limited appropriate decision of whether to treat the patient by thrombolysis is expected . Because of a limited accuracy in determining the area of core infarction particularly in the early hours of symptoms onset a variational approach to sensed data recovery was applied . Proposed methodology adjusts fidelity norms and regularization priors integrated with simulated sensing procedures in a compressed sensing framework . Experimental study confirmed almost perfect recognition of ischemic stroke in a test set of over 500 CT scans . 
",We designed and verified computerized support of acute stroke diagnosis. Novelty is application of integrated variational approach to detect ischemic edema. New content oriented image restoration and descriptors of asymmetric hypodensity distribution were proposed. Clarified visualization of direct symptoms of pathology was completed with automated recognition of ischemic stroke. Achieved recognition accuracy was 1 for test set of over 500 CT scans.,,,
S089561111500124X," Comparison of human brain MR images is often challenged by large inter subject structural variability . To determine correspondences between MR brain images most existing methods typically perform a local neighborhood search based on certain morphological features . They are limited in two aspects pre defined morphological features often have limited power in characterizing brain structures thus leading to inaccurate correspondence detection and correspondence matching is often restricted within local small neighborhoods and fails to cater to images with large anatomical difference . To address these limitations we propose a novel method to detect distinctive landmarks for effective correspondence matching . Specifically we first annotate a group of landmarks in a large set of training MR brain images . Then we use regression forest to simultaneously learn the optimal sets of features to best characterize each landmark and the non linear mappings from the local patch appearances of image points to their 3D displacements towards each landmark . The learned regression forests are used as landmark detectors to predict the locations of these landmarks in new images . Because each detector is learned based on features that best distinguish the landmark from other points and also landmark detection is performed in the entire image domain our method can address the limitations in conventional methods . The deformation field estimated based on the alignment of these detected landmarks can then be used as initialization for image registration . Experimental results show that our method is capable of providing good initialization even for the images with large deformation difference thus improving registration accuracy . 
",We propose a novel method to detect distinctive landmarks for effective correspondence matching. The method can accurately detect corresponding landmarks in the case of large anatomical variations. The method can automatically identify informative features and search correspondence in the entire image domain. The method can provide good initialization to registration especially for images with large deformation difference.,,,
S092188901500216X," An interactive loop between motion recognition and motion generation is a fundamental mechanism for humans and humanoid robots . We have been developing an intelligent framework for motion recognition and generation based on symbolizing motion primitives . The motion primitives are encoded into Hidden Markov Models which we call motion symbols . However to determine the motion primitives to use as training data for the HMMs this framework requires a manual segmentation of human motions . Essentially a humanoid robot is expected to participate in daily life and must learn many motion symbols to adapt to various situations . For this use manual segmentation is cumbersome and impractical for humanoid robots . In this study we propose a novel approach to segmentation the Real time Unsupervised Segmentation method which comprises three phases . In the first phase short human movements are encoded into feature HMMs . Seamless human motion can be converted to a sequence of these feature HMMs . In the second phase the causality between the feature HMMs is extracted . The causality data make it possible to predict movement from observation . In the third phase movements having a large prediction uncertainty are designated as the boundaries of motion primitives . In this way human whole body motion can be segmented into a sequence of motion primitives . This paper also describes an application of RUS to AUtonomous Symbolization of motion primitives . Each derived motion primitive is classified into an HMM for a motion symbol and parameters of the HMMs are optimized by using the motion primitives as training data in competitive learning . The HMMs are gradually optimized in such a way that the HMMs can abstract similar motion primitives . We tested the RUS and AUS frameworks on captured human whole body motions and demonstrated the validity of the proposed framework . 
",This paper proposes a framework for real time unsupervised segmentation of human motions and automatic symbolization of the motions. The segmentation is based on prediction uncertainty and symbolization is based on competitive learning of human motion. Their integration was verified on the human motion datasets.,,,
S092427161400241X," In this study we compare three commonly used methods for hyperspectral image classification namely Support Vector Machines Gaussian Processes and the Spectral Angle Mapper . We assess their performance in combination with different kernels . The assessment is done in two experiments under ideal conditions in the laboratory and separately in the field using natural light . For both experiments independent training and test sets are used . Results show that GPs generally outperform the SVMs irrespective of the kernel used . Furthermore angle based methods including the Spectral Angle Mapper outperform GPs and SVMs when using distance based kernels in the field experiment . A new GP method using an angle based kernel the Observation Angle Dependent covariance function outperforms SAM and SVMs in both experiments using only a small number of training spectra . These findings show that distance based kernels are more affected by changes in illumination between the training and test set than are angular based methods kernels . Taken together this study shows that independent training data can be used for classification of hyperspectral data in the field such as in open pit mines by using Bayesian machine learning methods and non stationary kernels such as GPs and the OAD kernel . This provides a necessary component for automated classifications such as autonomous mining where many images have to be classified without user interaction . 
",Gaussian Processes incorporating SAM at core for hyperspectral data classification. Systematic comparison of machine learning methods under various conditions. Data acquired from different materials sensors and different illumination. Classification using such independent training and test data sets. The presented method provides a Bayesian framework as basis for data fusion.,,,
S092054891500015X," The development of distributed testing frameworks is more complex where the implementation process must consider the mechanisms and functions required to support interaction as long as the communication and the coordination between distributed testing components . The typical reactions of such systems are the generation of errors set time outs locks observability controllability and synchronization problems . In other side the distributed testing process must not only check if the output events have been observed but also the dates when these events have been occurred . In this paper we show how to cope with these problems by using a distributed testing method including timing constraints . Afterwards a multi agent architecture is proposed in the design process to describe the behavior of testing a distributed chat group application on high level of abstraction . 
",We present some technical issues for testing distributed frameworks with timing constraints. We model an architecture taking into account the delay of messages exchanged between the components of the testing distributed applications. We propose a Multi Agent based system to capture the complex monitoring tasks of distributed tester behaviors,,,
S092054891500063X," The rise of Internet of Things has been improving the so called mobile Online Social Networks in sense of more ubiquitous inter communication and information sharing . Meanwhile location sharing service is known as the key cornerstone of mOSNs . Unfortunately location sharing has also caused similarly serious concerns on the potential privacy leakage . We propose BMobishare a security enhanced privacy preserving location sharing mechanism . It employs the Bloom Filter to mask sensitive data exchanges such that exchange of both sides can not obtain unauthorized privacy information . Analyses and evaluations show that BMobishare s enhanced location sharing procedure achieves significantly better performance when compared to existing approaches . 
",We make a further research on privacy preserving location sharing on mobile online social networks. BMobishare is proposed a security improved mechanism which employs dummy query and the Bloom Filter to protect privacy. A performance evaluation that attests to the practicality of our solution.,,,
S092054891500121X," The driving force behind software development of the Electronic Medical Record has been gradually changing . Heterogeneous software requirements have emerged so how to correctly carry out development project has become a complex task . This paper adopts the knowledge engineering and management mechanism i.e . CommonKADS and software quality engineering to improve existing strategic information management plan as a design methodology to help software implementation for medical institutes . We evaluate the adopting performance by a real case that examines the maturity level of the architecture alignment between the target solution in the proposed SIM plan and the built medical system . 
",This proposed methodology can be used to improve the SIM plan. With the proposed SIM plan software development process is improved. The related software architecture is examined in different models. A real case with the working scenario to verify this proposed SIM plan.,,,
S092188901400178X," Due to the advancements of robotic systems they are able to be employed in more unstructured outdoor environments . In such environments the robot terrain interaction becomes a highly non linear function . Several methods were proposed to estimate the robot terrain interaction machine learning methods iterative geometric methods quasi static and fully dynamic physics simulations . However to the best of our knowledge there has been no systematic evaluation comparing those methods . In this paper we present a newly developed iterative contact point estimation method for static stability estimation of actively reconfigurable robots . This new method is systematically compared to a physics simulation in a comprehensive evaluation . Both interaction models determine the contact points between robot and terrain and facilitate a subsequent static stability prediction . Hence they can be used in our state space global planner for rough terrain to evaluate the robot s pose and stability . The analysis also compares deterministic versions of both methods to stochastic versions which account for uncertainty in the robot configuration and the terrain model . The results of this analysis show that the new iterative method is a valid and fast approximate method . It is significantly faster compared to a physics simulation while providing good results in realistic robotic scenarios . 
",We present a new robot pose prediction method for static stability estimation. The method approximates the terrain by least squares planes to reduce the runtime. A stochastic version accounts for noise in the robot state and the terrain model. We systematically compared it with a physics simulation in many distinct scenarios. The new method is significantly faster and competitive in realistic situations.,,,
S092054891500135X," Controller Area Network is very popular in networked embedded systems . On the other hand intranets are now ubiquitous in office home and factory environments . Namely the Internet Protocol is the glue that permits any kind of information to be exchanged between devices in heterogeneous systems . In this paper a network architecture to seamlessly integrate CAN busses in intranets is described . Flexibility and scalability were the key design requirements to conceive a comprehensive solution that suits both inexpensive and very complex applications . A prototype implementation has been tested to confirm architecture feasibility and assess the performance it can achieve . 
",An architecture to integrate Controller Area Network CAN in intranets is proposed. Key design requirements were flexibility and scalability. The proposed architecture suits both inexpensive and very complex applications. A prototype implementation has been tested to assess feasibility and performance.,,,
S095070511300186X," A significant number of recommender systems utilize the k nearest neighbor algorithm as the collaborative filtering core . This algorithm is simple it utilizes updated data and facilitates the explanations of recommendations . Its greatest inconveniences are the amount of execution time that is required and the non scalable nature of the algorithm . The algorithm is based on the repetitive execution of the selected similarity metric . In this paper an innovative similarity metric is presented HwSimilarity . This metric attains high quality recommendations that are similar to those provided by the best existing metrics and can be processed by employing low cost hardware circuits . This paper examines the key design concepts and recommendation quality results of the metric . The hardware design cost of implementation and improvements achieved during execution are also explored . 
",kNN algorithm performance. Collaborative filtering hardware similarity measure. Low cost recommender systems hardware circuits.,,,
S092054891630006X," The Future Internet is expected to be composed of a mesh of interoperable web services accessed from all over the Web . This approach has been supported by many software providers who have provided a wide range of mash up tools for creating composite applications based on components prepared by the respective provider . These tools aim to achieve the end user development of rich internet applications however most having failed to meet the needs of end users without programming knowledge have been unsuccessful . Thus many studies have investigated success factors in order to propose scales of success factor objectives and assess the adequacy of mashup tools for their purpose . After reviewing much of the available literature this paper proposes a new success factor scale based on human factors human computer interaction factors and the specialization functionality relationship . It brings together all these factors offering a general conception of EUD success factors . The proposed scale was applied in an empirical study on current EUD tools which found that today s EUD tools have many shortcomings . In order to achieve an acceptable success rate among end users we then designed a mashup tool architecture called FAST Wirecloud which was built taking into account the proposed EUD success factor scale . The results of a new empirical study carried out using this tool have demonstrated that users are better able to successfully develop their composite applications and that FAST Wirecloud has scored higher than all the other tools under study on all scales of measurement and particularly on the scale proposed in this paper . 
",Shows a mashup platform that lets end users make their own web applications Proposes a new scale based on a set of objective factors Proposes a reference architecture built taking into account the factors of success This architecture achieves more success among end users that current platforms It is useful to know which decisions are relevant for achieving end user satisfaction.,,,
S093336571530066X," Objective Machine learning techniques can be used to extract predictive models for diseases from electronic medical records . However the nature of EMRs makes it difficult to apply off the shelf machine learning techniques while still exploiting the rich content of the EMRs . In this paper we explore the usage of a range of natural language processing techniques to extract valuable predictors from uncoded consultation notes and study whether they can help to improve predictive performance . Methods We study a number of existing techniques for the extraction of predictors from the consultation notes namely a bag of words based approach and topic modeling . In addition we develop a dedicated technique to match the uncoded consultation notes with a medical ontology . We apply these techniques as an extension to an existing pipeline to extract predictors from EMRs . We evaluate them in the context of predictive modeling for colorectal cancer a disease known to be difficult to diagnose before performing an endoscopy . Results Our results show that we are able to extract useful information from the consultation notes . The predictive performance of the ontology based extraction method moves significantly beyond the benchmark of age and gender alone of 0.870 versus 0.831 . We also observe more accurate predictive models by adding features derived from processing the consultation notes compared to solely using coded data although the difference is not significant . The extracted features from the notes are shown be equally predictive compared to the coded data of the consultations . Conclusion It is possible to extract useful predictors from uncoded consultation notes that improve predictive performance . Techniques linking text to concepts in medical ontologies to derive these predictors are shown to perform best for predicting CRC in our EMR dataset . 
",The paper studies several natural language processing NLP techniques to extract predictors from uncoded data in electronic medical records EMRs . Some techniques are well known while other have been developed specifically for this research. The approaches have been applied to a large dataset we have access to covering 90 000 patients in general practices. We focus on predictive modelling of colorectal cancer which is a challenging disease to study as it is a common type of cancer while the symptoms are very a specific for the disease. The results show that some of the NLP techniques studied can complement the coded EMR data and hence result in improved predictive models.,,,
S093336571630015X," Objective Radiotherapy treatment planning aims at delivering a sufficient radiation dose to cancerous tumour cells while sparing healthy organs in the tumour surrounding area . It is a time consuming trial and error process that requires the expertise of a group of medical experts including oncologists and medical physicists and can take from 2 to 3h to a few days . Our objective is to improve the performance of our previously built case based reasoning system for brain tumour radiotherapy treatment planning . In this system a treatment plan for a new patient is retrieved from a case base containing patient cases treated in the past and their treatment plans . However this system does not perform any adaptation which is needed to account for any difference between the new and retrieved cases . Generally the adaptation phase is considered to be intrinsically knowledge intensive and domain dependent . Therefore an adaptation often requires a large amount of domain specific knowledge which can be difficult to acquire and often is not readily available . In this study we investigate approaches to adaptation that do not require much domain knowledge referred to as knowledge light adaptation . Methodology We developed two adaptation approaches adaptation based on machine learning tools and adaptation guided retrieval . They were used to adapt the beam number and beam angles suggested in the retrieved case . Two machine learning tools neural networks and naive Bayes classifier were used in the adaptation to learn how the difference in attribute values between the retrieved and new cases affects the output of these two cases . The adaptation guided retrieval takes into consideration not only the similarity between the new and retrieved cases but also how to adapt the retrieved case . Results The research was carried out in collaboration with medical physicists at the Nottingham University Hospitals NHS Trust City Hospital Campus UK . All experiments were performed using real world brain cancer patient cases treated with three dimensional conformal radiotherapy . Neural networks based adaptation improved the success rate of the CBR system with no adaptation by 12 . However naive Bayes classifier did not improve the current retrieval results as it did not consider the interplay among attributes . The adaptation guided retrieval of the case for beam number improved the success rate of the CBR system by 29 . However it did not demonstrate good performance for the beam angle adaptation . Its success rate was 29 versus 39 when no adaptation was performed . Conclusions The obtained empirical results demonstrate that the proposed adaptation methods improve the performance of the existing CBR system in recommending the number of beams to use . However we also conclude that to be effective the proposed adaptation of beam angles requires a large number of relevant cases in the case base . 
",This paper is concerned with a case based reasoning CBR system for radiotherapy treatment planning for brain cancer patients which has been developed in collaboration with medical physicists at the Nottingham University Hospitals NHS Trust City Hospital Campus UK. The developed CBR system generates the parameters of a treatment plan by capturing the subjective and intuitive knowledge of medical physicists. In this research we focus on the adaptation stage of the CBR system in which the solution treatment plan of the retrieved case is adapted to meet the needs of the new case patient by considering differences between the retrieved and new cases. We investigate approaches to adaptation that do not require much domain knowledge referred to as knowledge light adaptation. Results obtained by three developed adaptation approaches including adaptation based on Neural Networks and na ve Bayes classifier and adaptation guided retrieval are presented.,,,
S092054891500046X," Hidden and exposed terminal problems are known to negatively impact wireless communications degrading potential computing services on top . These effects are more significant in Wireless Mesh Sensor Networks and particularly in those based on the IEEE 802.15.5 Low Rate Wireless Personal Area Network standard a promising solution for enabling low power WMSNs . The first contribution of this paper is a quantitative evaluation of these problems under the IEEE 802.15.5 Asynchronous Energy Saving mode which is intended for asynchronous data collection applications . The results obtained show a sharp deterioration of the network performance . Therefore this paper also reviews the most relevant approaches that cope with these problems and are compatible with ASES . Finally a set of these proposals is assessed to find out those more suitable for their potential integration with ASES which constitutes the second major contribution of the paper . 
",The hidden and exposed effects are assessed on Wireless Mesh Sensor Networks WMSNs . Performance evaluation reveals a clear negative impact on WMSNs. Different scientific proposals are discussed in order to mitigate these problems. Some of these proposals are stressed and compared according to WMSN premises. Best solutions of our comparative study are shown to the audience.,,,
S092054891500118X," Enterprise Resource Planning packages are information systems that automate the business processes of organizations thereby improving their operational efficiency substantially . ERP projects that involve customization are often affected by inaccurate estimation of efforts . Size of the software forms the basis for effort estimation . Methods used for effort estimation either employ function points or lines of code to measure the size of customized ERP packages . Literature review reveals that the existing software size methods which are meant for custom built software products may not be suitable for COTS products such as customized ERP packages . Hence the effort estimation using conventional methods for customized ERP packages may not be accurate . This paper proposes a new approach to estimate the size of customized ERP packages using Package Points . The proposed approach was validated with data collected from 14 ERP projects delivered by the same company . A positive correlation was observed between Package Points and the efforts of these projects . This result indicates the feasibility of our proposed approach as well as the positive climate for its utility by the project managers of future ERP projects . Lastly we examine the implication of these results for practice and future research scope . 
",A unique approach to measure size of the customized ERP package has been proposed. Case study research method has been used to validate the proposed approach. A positive correlation was observed between Package Points and the efforts of these projects. The study presents theoretical and practical implications for better understanding of ERP package size measurement process.,,,
S092054891630023X," In this study we propose a novel solution for collecting smart meter data by merging Vehicular Ad Hoc Networks and smart grid communication technologies . In our proposed mechanism Vehicular Ad Hoc Networks are utilized for collecting data from smart meters eliminating the need for manpower . To the best of our knowledge this is the first study proposing the utilization of public transportation vehicles for collecting data from smart meters . With this work the use of the IEEE 802.11p protocol has been proposed for the first time for use in smart grid applications . In our scheme data flows first from smart meters to a bus through infrastructure to vehicle communication and then from the bus to a bus stop through vehicle to infrastructure communication . The performance of our proposed mechanism has been investigated in detail in terms of end to end delay and delivery ratio by using Network Simulator 2 and with different routing protocols . 
",The first study which uses public transportation for collecting data from SM Our study only needs the IEEE 802.11p communication protocol capability on WAMR. Unlike other VANET studies our study uses both I2V V2I communications.,,,
S093336571500086X," Objective Recurrence of cancer after treatment increases the risk of death . The ability to predict the treatment outcome can help to design the treatment planning and can thus be beneficial to the patient . We aim to select predictive features from clinical and PET based features in order to provide doctors with informative factors so as to anticipate the outcome of the patient treatment . Methods In order to overcome the small sample size problem of datasets usually met in the medical domain we propose a novel wrapper feature selection algorithm named HFS which searches forward in a hierarchical feature subset space . Feature subsets are iteratively evaluated with the prediction performance using SVM . All feature subsets performing better than those at the preceding iteration are retained . Moreover as SUV based features have been recognized as significant predictive factors for a patient outcome we propose to incorporate this prior knowledge into the selection procedure to improve its robustness and reduce its computational cost . Results Two real world datasets from cancer patients are included in the evaluation . We extract dozens of clinical and PET based features to characterize the patient s state including SUV parameters and texture features . We use leave one out cross validation to evaluate the prediction performance in terms of prediction accuracy and robustness . Using SVM as the classifier our HFS method produces accuracy values of 100 and 94 on the two datasets respectively and robustness values of 89 and 96 . Without accuracy loss the prior based version improves the robustness up to 100 and 98 on the two datasets respectively . Conclusions Compared with other feature selection methods the proposed HFS and pHFS provide the most promising results . For our HFS method we have empirically shown that the addition of prior knowledge improves the robustness and accelerates the convergence . 
",A novel wrapper method searches forward in a hierarchical feature subset space. Prior domain knowledge is incorporated into the selection procedure. Promising results are obtained on two cancer patient datasets.,,,
S093336571500041X," Background Evidence based medicine practice requires practitioners to obtain the best available medical evidence and appraise the quality of the evidence when making clinical decisions . Primarily due to the plethora of electronically available data from the medical literature the manual appraisal of the quality of evidence is a time consuming process . We present a fully automatic approach for predicting the quality of medical evidence in order to aid practitioners at point of care . Methods Our approach extracts relevant information from medical article abstracts and utilises data from a specialised corpus to apply supervised machine learning for the prediction of the quality grades . Following an in depth analysis of the usefulness of features they are extracted from the text via rule based approaches and from the meta data associated with the articles and then applied in the supervised classification model . We propose the use of a highly scalable and portable approach using a sequence of high precision classifiers and introduce a simple evaluation metric called average error distance that simplifies the comparison of systems . We also perform elaborate human evaluations to compare the performance of our system against human judgments . Results We test and evaluate our approaches on a publicly available specialised annotated corpus containing 1132 evidence based recommendations . Our rule based approach performs exceptionally well at the automatic extraction of publication types of articles with F scores of up to 0.99 for high quality publication types . For evidence quality classification our approach obtains an accuracy of 63.84 and an AED of 0.271 . The human evaluations show that the performance of our system in terms of AED and accuracy is comparable to the performance of humans on the same data . Conclusions The experiments suggest that our structured text classification framework achieves evaluation results comparable to those of human performance . Our overall classification approach and evaluation technique are also highly portable and can be used for various evidence grading scales . 
",We present a classification model for the automatic quality grading of clinical evidence. We propose NLP based approaches for extraction of informative features from text. We present a supervised learning approach using SVM classifiers for evidence grading. We show that the performance of our approach is comparable to human performance. Our quality grading approach can significantly reduce practitioners time needs.,,,
S093336571400150X," Objective Proteins are considered to be the most important individual components of biological systems and they combine to form physical protein complexes which are responsible for certain molecular functions . Despite the large availability of protein protein interaction information not much information is available about protein complexes . Experimental methods are limited in terms of time efficiency cost and performance constraints . Existing computational methods have provided encouraging preliminary results but they phase certain disadvantages as they require parameter tuning some of them can not handle weighted PPI data and others do not allow a protein to participate in more than one protein complex . In the present paper we propose a new fully unsupervised methodology for predicting protein complexes from weighted PPI graphs . Methods and materials The proposed methodology is called evolutionary enhanced Markov clustering and it is a hybrid combination of an adaptive evolutionary algorithm and a state of the art clustering algorithm named enhanced Markov clustering . EE MC was compared with state of the art methodologies when applied to datasets from the human and the yeast Saccharomyces cerevisiae organisms . Results Using public available datasets EE MC outperformed existing methodologies . Moreover when applied to new human datasets its performance was encouraging in the prediction of protein complexes which consist of proteins with high functional similarity . In specific 5737 protein complexes were predicted and 72.58 of them are enriched for at least one gene ontology function term . Conclusions EE MC is by design able to overcome intrinsic limitations of existing methodologies such as their inability to handle weighted PPI networks their constraint to assign every protein in exactly one cluster and the difficulties they face concerning the parameter tuning . This fact was experimentally validated and moreover new potentially true human protein complexes were suggested as candidates for further validation using experimental techniques . 
",EE MC is a new unsupervised methodology for predicting protein complexes from weighted PPI graphs. It is by design able to overcome intrinsic limitations of existing methodologies. It outperformed existing methodologies increasing the separation metric by 10 20 . 72.58 of the predicted protein complexes in human are enriched for at least one GO function term.,,,
S093336571400147X," Introduction The length of stay of critically ill patients in the intensive care unit is an indication of patient ICU resource usage and varies considerably . Planning of postoperative ICU admissions is important as ICUs often have no nonoccupied beds available . Problem statement Estimation of the ICU bed availability for the next coming days is entirely based on clinical judgement by intensivists and therefore too inaccurate . For this reason predictive models have much potential for improving planning for ICU patient admission . Objective Our goal is to develop and optimize models for patient survival and ICU length of stay based on monitored ICU patient data . Furthermore these models are compared on their use of sequential organ failure scores as well as underlying raw data as input features . Methodology Different machine learning techniques are trained using a 14 480 patient dataset both on SOFA scores as well as their underlying raw data values from the first five days after admission in order to predict the patient LOS and the patient mortality . Furthermore to help physicians in assessing the prediction credibility a probabilistic model is tailored to the output of our best performing model assigning a belief to each patient status prediction . A two by two grid is built using the classification outputs of the mortality and prolonged stay predictors to improve the patient LOS regression models . Results For predicting patient mortality and a prolonged stay the best performing model is a support vector machine with G A D 65.9 of 0.77 and G S L 73.2 . In terms of LOS regression the best performing model is support vector regression achieving a mean absolute error of 1.79 days and a median absolute error of 1.22 days for those patients surviving a nonprolonged stay . Conclusion Using a classification grid based on the predicted patient mortality and prolonged stay allows more accurate modeling of the patient LOS . The detailed models allow to support the decisions made by physicians in an ICU setting . 
",A dataset of 14 480 critically ill patients within the ICU was collected. Machine learning models are constructed to predict patient length of stay and mortality. Prediction accuracy was improved by using a two by two classification grid. Probabilistic model outputs can improve interpretation by physicians. Moving data window shows potential for real time ICU patient analysis.,,,
S092054891500094X," Context awareness enables the personalization of computer systems according to the users needs and their particular situation at a given time . The personalization capabilities are usually implemented by programmers due to the complex processes that are involved . However an important trend in software development is that more and more software systems are being implemented not only by programmers but also by people with expertise in other domains . Since most of the existing context aware development toolkits are designed for programmers non technical users can not develop these kinds of systems . The design of tools to create context aware systems by users that do not have programming skills but are experts in the domain where the system is going to be deployed will contribute to speed up the adoption of these kinds of services by the society . This paper presents a cloud based platform to ease the development of context aware mobile applications by people without programming skills . The platform has been designed to be used in a tourism domain . This way tourism experts can send tourist information to mobile users according to their context data . An energy efficient mobile app has been developed in order to obtain context data from the user s device and to receive personalized information in real time based on these data . The architecture and implementation details of the system are presented and the evaluation of the platform by tourism domain experts is discussed . 
",We implement a cloud based platform to obtain and manage context data The platform is designed for people with no programming skills in order to develop context aware mobile apps The reasoning process of the platform has been optimized using the conditions as facts rule pattern The context data gathering process has been optimized using an algorithm that detects the activity of the user,,,
S093336571600004X," Objectives To develop a rigorous and repeatable method for building effective Bayesian network models for medical decision support from complex unstructured and incomplete patient questionnaires and interviews that inevitably contain examples of repetitive redundant and contradictory responses To exploit expert knowledge in the BN development since further data acquisition is usually not possible To ensure the BN model can be used for interventional analysis To demonstrate why using data alone to learn the model structure and parameters is often unsatisfactory even when extensive data is available . Method The method is based on applying a range of recent BN developments targeted at helping experts build BNs given limited data . While most of the components of the method are based on established work its novelty is that it provides a rigorous consolidated and generalised framework that addresses the whole life cycle of BN model development . The method is based on two original and recent validated BN models in forensic psychiatry known as DSVM MSS and DSVM P. Results When employed with the same datasets the DSVM MSS demonstrated competitive to superior predictive performance against the state of the art and the DSVM P demonstrated superior predictive performance against the state of the art . More importantly the resulting models go beyond improving predictive accuracy and into usefulness for risk management purposes through intervention and enhanced decision support in terms of answering complex clinical questions that are based on unobserved evidence . Conclusions This development process is applicable to any application domain which involves large scale decision analysis based on such complex information rather than based on data with hard facts and in conjunction with the incorporation of expert knowledge for decision support via intervention . The novelty extends to challenging the decision scientists to reason about building models based on what information is really required for inference rather than based on what data is available and hence forces decision scientists to use available data in a much smarter way . 
",A method for developing Bayesian networks based on both data and knowledge. The method targets complex questionnaire and interviewing medical data. The objective is to provide real world decision support and risk management. The method is based on two original and validated applications in forensic psychiatry. Both applications provide improvements in predictive accuracy and decision support.,,,
S092054891500104X," Indexing the Web is becoming a laborious task for search engines as the Web exponentially grows in size and distribution . Presently the most effective known approach to overcome this problem is the use of focused crawlers . A focused crawler employs a significant and unique algorithm in order to detect the pages on the Web that relate to its topic of interest . For this purpose we proposed a custom method that uses specific HTML elements of a page to predict the topical focus of all the pages that have an unvisited link within the current page . These recognized on topic pages have to be sorted later based on their relevance to the main topic of the crawler for further actual downloads . In the Treasure Crawler we use a hierarchical structure called T Graph which is an exemplary guide to assign appropriate priority score to each unvisited link . These URLs will later be downloaded based on this priority . This paper embodies the implementation test results and performance evaluation of the Treasure Crawler system . The Treasure Crawler is evaluated in terms of specific information retrieval criteria such as recall and precision both with values close to 50 . Gaining such outcome asserts the significance of the proposed approach . 
",We present the experimental results of a focused Web crawler that combines link based and content based approaches to predict the topical focus of an unvisited page. We present a custom method using Dewey decimal classification system to best classify the subject of an unvisited page into standard human knowledge categories. To prioritize an unvisited URL we use a dynamic flexible and updating hierarchical data structure called T Graph which helps find the shortest path to get to on topic pages on the Web. For the background review the experimental results from several crawlers are presented. We compare our results against other significant focused Web crawlers.,,,
S095070511500324X," Accurate and reliable prediction of diarrhoea outpatient visits is necessary for the health authorities to ensure the appropriate action for the control of the outbreak . In this study a novel method based on time series decomposition and multi local predictor fusion has been proposed to predict the diarrhoea outpatient visits . For time series decomposition the Ensemble Empirical Mode Decomposition with Adaptive Noise is used to decompose diarrhoea outpatient visits time series into a finite set of Intrinsic Mode Function components and a residue . The IMF components and residue are modeled and predicted respectively by means of Generalized Regression Neural Network as local predictor . Then the prediction results of all components are fusioned using another independent GRNN as fusion predictor to obtain final prediction results . This is the first study on using a EEMDAN and GRNN to constructing an prediction model for diarrhoea outpatient visits prediction problems . The pre procession and post processing techniques are used to take into account the seasonal and trend effects in the datasets for improving the prediction precision of proposed model . The performance of the proposed EEMDAN GRNN model has been compared with Seasonal Auto Regressive Moving Average Single GRNN Wavelet GRNN and also with EEMD GRNN by applying them to predict four real world diarrhoea outpatient visits . The results indicate that the proposed EEMDAN GRNN model provides more accurate prediction results compared to the other traditional techniques . Thus EEMDAN GRNN can be an alternate tool to facilitate the prediction of diarrhoea outpatient visits . 
",A EEMDAN GRNN algorithm is proposed for diarrhoea outpatient visits prediction. Predictor based on decomposition and ensemble principle superior to single predictor. GRNN is a good candidate as predictor for diarrhoea outpatient visits prediction. EEMDAN is superior to EMD and Wavelet for diarrhoea outpatient visits time series.,,,
S096599781300152X," In the simulation of a chain of manufacturing processes several finite element packages can be employed and for each process or package a different mesh density or element type may be the most suitable . Therefore there is a need for transferring finite element analysis data among packages and mapping it between meshes . This paper presents efficient algorithms for mapping FEA data between meshes with different densities and element types . An in core spatial index is created on the mesh from which FEA data is transferred . The index is represented by a dynamic grid partitioning the underlying space from which nodes and elements are drawn into equal sized cells . Buckets containing references to the nodes indexed are associated with the cells in a many to one correspondence . Such an index makes nearest neighbour searches of nodes and elements much faster than sequential scans . An experimental evaluation of the mapping techniques using the index is conducted . The algorithms have been implemented in the open source finite element data exchange system FEDES . 
",We present efficient algorithms for mapping FEA data between meshes. An in core spatial index is created on the source mesh. Nearest neighbour searches of nodes and elements in the index are very fast. An experimental evaluation of the mapping techniques using the index is conducted. The algorithms have been implemented in the finite element data exchange system FEDES.,,,
S096599781300094X," A new hybrid mixed stress finite element model for the static and dynamic non linear analysis of concrete structures is presented and discussed in this paper . The main feature of this model is the simultaneous and independent approximation of the stress the strain and the displacement fields in the domain of each element . The displacements along the static boundary which is considered to include inter element boundaries are also directly approximated . To define the approximation bases in the space domain complete sets of orthonormal Legendre polynomials are used . The adoption of these functions enables the use of analytical closed form solutions for the computation of all linear structural operators and leads to the development of very effective p refinement procedures . To represent the material quasi brittle behaviour a physically non linear model is considered by using damage mechanics . A simple isotropic damage model is adopted and to control strain localisation problems a non local integral formulation is considered . To solve the dynamic non linear governing system a time integration procedure based on the use of the HHT method is used . For each time step the solution of the non linear governing system is achieved using an iterative algorithm based on a secant method . The model being discussed is applied to the solution of two dimensional structures . To validate the model to illustrate its potential and to assess its accuracy and numerical efficiency several numerical examples are discussed and comparisons are made with solutions provided by experimental tests and with other numerical results obtained using conventional finite elements . 
",A new hybrid mixed stress finite element model is presented. Independent approximation of stress strain and displacement fields. Static and dynamic physically non linear analysis with isotropic damage models. Use of effective p refinement procedures. Legendre polynomials are used to define the approximation bases.,,,
S095070511500502X," This paper proposes a new variant of particle swarm optimization namely multiple learning PSO with space transformation perturbation to improve the performance of PSO . The proposed MLPSO STP uses a novel learning strategy and STP . The novel learning strategy allows each particle to learn from the average information on the personal historical best position of all particles and from the information on multiple best positions that are randomly chosen from the top 100p of pbest . This learning strategy enables the preservation of swarm diversity to prevent premature convergence . Meanwhile STP increases the chance to find optimal solutions . The performance of MLPSO STP is comprehensively evaluated in 21 unimodal and multimodal benchmark functions with or without rotation . Compared with eight popular PSO variants and seven state of the art metaheuristic search algorithms MLPSO STP performs more competitively on the majority of the benchmark functions . Finally MLPSO STP shows satisfactory performance in optimizing the operating conditions of an ethylene cracking furnace to improve the yields of ethylene and propylene . 
",A new variant of PSO abbreviated as MLPSO STP is proposed. A novel learning strategy is used to enhance the global search ability. Space transformation perturbation is used to obtain better solutions. MLPSO STP outperforms its peers in terms of searching accuracy and reliability. MLPSO STP is used to optimize the operating conditions of ethylene cracking furnace.,,,
S096599781300183X," Evidence theory employs a much more general and flexible framework to quantify the epistemic uncertainty and thereby it is adopted to conduct reliability analysis for engineering structures recently . However the large computational cost caused by its discrete property significantly influences the practicability of evidence theory . This paper proposes an efficient response surface method to evaluate the reliability for structures using evidence theory and hence improves its applicability in engineering problems . A new design of experiments technique is developed whose key issue is the search of the important control points . These points are the intersections of the limit state surface and the uncertainty domain thus they have a significant contribution to the accuracy of the subsequent established RS . Based on them a high precise radial basis functions RS to the actual limit state surface is established . With the RS the reliability interval can be efficiently computed for the structure . Four numerical examples are investigated to demonstrate the effectiveness of the proposed method . 
",A RS approach for structural reliability analysis using evidence theory is proposed. A design of experiments technique is proposed to guarantee the RS precision. The results of the numerical examples show pretty good efficiency and precision.,,,
S095070511500386X," The induction of decision tree searches for relevant characteristics in the data which would allow it to precisely model a certain concept but it also worries about the comprehensibility of the generated model helping human specialists to discover new knowledge something very important in the medical and biological areas . On the other hand such inducers present some instability . The main problem handled here refers to the behavior of those inducers when it comes to high dimensional data more specifically to gene expression data irrelevant attributes may harm the learning process and many models with similar performance may be generated . In order to treat those problems we have explored and revised windowing pruning of the trees generated during intermediary steps of the algorithm the use of the estimated error instead of the training error the use of the error weighted according to the size of the current window and the use of the classification confidence as the window update criterion . The results show that the proposed algorithm outperform the classical one especially considering measures of complexity and comprehensibility of the induced models . 
",We propose several improvements for the windowing algorithm. We evaluated model performance interpretability and stability. Our methodology focuses on the interpretability of the model. Our approach shows differences in terms of interpretability without harming performance. Our approach may yield better classification models.,,,
S095070511500132X," The integration of multiple features is important for action categorization and object recognition in videos because single feature based representation hardly captures imaging variations and individual attributes . In this paper a novel formulation named Multivariate video Information Bottleneck is defined . It is an extensional type of multivariate information bottleneck and can discover categories from a collection of unlabeled videos automatically . Differing from the original multivariate information bottleneck the novel approach extracts the video categories from multiple features simultaneously such as local static and dynamic feature each type of feature is treated as a relevant variable . Specifically by preserving the relevant information with respect to these feature variables maximally the MvIB method is able to integrate various aspects of semantic information into the final video partitioning results and thus captures the complementary information resided in multiple feature variables . Extensive experimental results on five challenging video data sets show that the proposed approach can consistently and significantly outperform other state of the art unsupervised learning methods . 
",Novel multivariate IB model is proposed for unsupervised video categorization. Effective solution is designed to integrate multiple features simultaneously. Information theoretic optimization is constructed to alleviate the semantic gap.,,,
S096599781300029X," A parallel computing region growing algorithm for surface reconstruction from unorganized point clouds is proposed in this research . The traditional region growing algorithm belongs to sequential process and needs to update the topology information continuously to maintain the boundaries of the growing region . This constraint becomes a bottleneck for efficiency improvement . The proposed GPU based region growing algorithm is to decompose the traditional sequence and re plan specific framework for the purpose of utilizing parallel computation . Then a graphics card with multi processing units will be used to build triangles in the parallel computing mode . In our GPU based reconstruction process each sampling point is regarded as an independent seed and expands simultaneously until all surrounding patches overlap each other . Following this the overlapping patches are removed and holes are filled by the GPU based calculation . Finally a complete model is created . In order to validate the algorithm proposed the unorganized point cloud was obtained by a 3D scanner and then reconstructed using the parallel computing region growing algorithm . According to the results obtained the algorithm proposed here shows 10 times better performance when compared to the traditional region growing method . 
",We develop a region growing algorithm based on parallel computation architecture. The old algorithm needs to be redesigned to fit the requirement of GPU architecture. Our algorithm can speed up the mesh reconstruction over 10 times. The proposed algorithm also have the same quality compared to original algorithm.,,,
S095070511500307X," In multi agent systems stereotypical trust models are widely used to bootstrap a priori trust in case historical trust evidences are unavailable . These models can work well if and only if malicious agents share some common features in their profiles and these features can be detected . However this condition may not hold for all the adversarial scenarios . Smart attackers can show different trustworthiness to different agents and services . In this paper we propose CAST a novel Context Aware Stereotypical Trust deep learning framework . CAST coins a comprehensive set of seven context aware stereotypes each of which can capture a unique type of context correlated attacks as well as a deep learning architecture to keep the trust stereotyping robust . The basic idea is to construct a multi layer perceptive structure to learn the latent correlations between context aware stereotypes and the trustworthiness and thus can estimate the new trust by taking into account the context information . We have evaluated CAST using a rich set of experiments over a simulated multi agent system . The experimental results have successfully confirmed that our CAST can achieve approximately tens of times higher trust inference accuracy in average than the competing algorithms in the presence of context correlated attacks and more importantly can maintain a much better trust inference robustness against stereotyping errors . 
",We have proposed CAST a new context aware stereotypical trust model. We have considered a comprehensive set of seven context aware stereotypes. We have applied a deep learning architecture to keep trust stereotyping robust. We have confirmed the effectiveness of CAST using a rich set of experiments.,,,
S096599781400012X," The paper is focussed on the robustness of parallel computation in the case of buckling and post buckling analyses . In the nonlinear context domain decomposition methods are mainly used as a solver for the tangent problem solved at each iteration of a Newton Raphson algorithm . In case of strongly nonlinear and heterogeneous problems as those encountered in buckling and post buckling this procedure may lead to severe difficulties regarding convergence and efficiency . The problem of convergence is regarded as the most critical issue at the industrial level . Indeed if a method which can show efficiency for some problems is not robust with respect to convergence the method will not be implemented by industrial end users . Therefore two paths are explored to gain robustness when making use of domain decomposition methods a nonlinear localization strategy which may also improve the robustness by treating the nonlinearity at the subdomain level and a mixed framework allowing to circumvent the problem of local divergence . It is to be noted that those two ingredients may also be used to improve the numerical efficiency of the method but this is not the main focus of the paper . Simple structures are first considered to illustrate the method performances . Results obtained in the case of a boxed structure and of a stiffened panel are then discussed . 
",Nonlinear localization can deal with larger increments than the classical NKS methods. Mixed DDM can use larger increments and pass limit points with respect to NKS methods. Important reduction on the number of global iterations. The amount of exchanged data between processors is reduced. Sensitivity study for the Robin operator.,,,
S095070511500009X," Accurate interval forecasting of agricultural commodity futures prices over future horizons is challenging and of great interests to governments and investors by providing a range of values rather than a point estimate . Following the well established linear and nonlinear modeling framework this study extends it to forecast interval valued agricultural commodity futures prices with vector error correction model and multi output support vector regression which is capable of capturing the linear and nonlinear patterns exhibited in agricultural commodity futures prices . Two agricultural commodity futures prices from Chinese futures market are used to justify the performance of the proposed VECM MSVR method against selected competitors . The quantitative and comprehensive assessments are performed and the results indicate that the proposed VECM MSVR method is a promising alternative for forecasting interval valued agricultural commodity futures prices . 
",Proposing an interval forecasting method for agricultural commodity futures prices. Extending the linear and nonlinear modeling framework for ITS forecasting. VECM and MSVR are integrated abbreviated as VECM MSVR . The experimental analysis is based on one step ahead and multi step ahead forecasts. VECM MSVR is a promising method for interval forecasting in future markets.,,,
S095070511400149X," We study the problem of clustering uncertain objects whose locations are uncertain and described by probability density functions . We analyze existing pruning algorithms and experimentally show that there exists a new bottleneck in the performance due to the overhead of pruning candidate clusters for assignment of each uncertain object in each iteration . In this article we will show that by considering squared Euclidean distance UK means is reduced to K means and performs much faster than pruning algorithms however with some discrepancies in the clustering results due to using different distance functions . Thus we propose Approximate UK means to heuristically identify objects of boundary cases and re assign them to better clusters . Three models for the representation of cluster representative are proposed to calculate expected squared Euclidean distance between objects and cluster representatives in this paper . Our experimental results show that on average the execution time of Approximate UK means is only 25 more than K means and our approach reduces the discrepancies of K means clustering results by up to 70 . 
",We reduce UK means to K means. We experimentally show that K means performs much faster than existing pruning algorithms. We propose Approximate UK means to heuristically identify boundary objects and re assign them to better clusters. We propose three models for the representation of cluster representative. To our knowledge this is the first time to introduce uncertain model of cluster representative.,,,
S095070511500204X," Covering rough sets are a generalization of Pawlak rough sets in which a partition of the universal set induced by an equivalence relation is replaced by a covering . In this paper covering rough sets are transformed into generalized rough sets induced by binary relations . The paper discusses three theoretical topics . First we consider a special type of covering in which the neighborhoods form a reduction of the covering and we obtain necessary and sufficient conditions for neighborhoods in a covering form a reduction of the covering . Second we study another special type of covering and give conditions for the covering lower and upper approximations to be dual to each other . Finally we give an axiomatic system that characterizes the lower and upper approximations of rough sets based on a partial order . 
",Give conditions for neighborhoods in a covering form a reduction of the covering. Give conditions for covering lower and upper approximations are dual to each other. Axiomatize lower and upper rough approximations based on partial orders.,,,
S096599781300077X," This study proposes a new robust multi objective maintenance planning approach of the deteriorating bridges against uncertainty in performance degradation model . The main focus is to guarantee the performance requirements of the bridge by the scheduled maintenance interventions even in the presence of uncertainty in time dependent performance degradation model . The uncertainties are modeled as the perturbation of the system parameters . These are simulated by a sampling method and incorporated into the GA based multi objective optimization framework which produces a set of optimal preventive maintenance scenarios . In order to focus the searching on the most preferable region the performance models of the bridge components are all integrated into single overall performance measure by using the preference based objective space reduction method . Numerical example of a typical prestressed concrete girder bridge is provided to demonstrate the new robust maintenance scheduling approach . For comparison purpose non robust multi objective maintenance planning without considering uncertainty of the bridge performance is also provided . It is verified that the proposed approach can produce successfully performing maintenance scenarios under the perturbation of bridge condition grades while maintaining well balanced maintenance strategy both in terms of bridge performance and maintenance cost . 
",We propose a new robust multi objective maintenance planning method of deteriorating bridges against model uncertainties. Simulated system uncertainties are incorporated into the GA based multi objective optimization framework. The preference based objective space reduction method is utilized to enhance searching efficiency. Numerical example of a typical prestressed concrete girder bridge is provided. Proposed method produces well balanced maintenance strategy both in terms of bridge performance and maintenance cost.,,,
S096599781300032X," This work describes a technique for generating two dimensional triangular meshes using distributed memory parallel computers based on a master slaves model . This technique uses a coarse quadtree to decompose the domain and a serial advancing front technique to generate the mesh in each subdomain concurrently . In order to advance the front to a neighboring subdomain each subdomain suffers a shift to a Cartesian direction and the same advancing front approach is performed on the shifted subdomain . This shift and remesh procedure is repeatedly applied until no more mesh can be generated shifting the subdomains to different directions each turn . A finer quadtree is also employed in this work to help estimate the processing load associated with each subdomain . This load estimation technique produces results that accurately represent the number of elements to be generated in each subdomain leading to proper runtime prediction and to a well balanced algorithm . The meshes generated with the parallel technique have the same quality as those generated serially within acceptable limits . Although the presented approach is two dimensional the idea can be easily extended to three dimensions . 
",We present a parallel technique for generating two dimensional triangular meshes. Generated meshes show the same quality as those generated with serial approaches. Our parallel technique presents a fairly good speed up. Our load estimation is very effective and can be used for efficient load balancing.,,,
S095070511400032X," This paper aims to develop and compare several elicitation criterions for decision making of incomplete soft sets which are generated by restricted intersection . One time elicitation process is divided into two steps . Using the greedy idea four criterions for elicitation of objects are built based on maximax maximin minimax regret and combination of expected choice values and elicitation times . Then these initial unknown values which produce incomplete values together with known information are in priority . Fast methods for computing possibly and necessarily optimal solutions before or in the elicitation process are invented . As far as the sizes of soft sets used in the simulation experiments it is found statistically that we should choose the criterion based on the combination of expected choice value and expected elicitation times in the first step of one time elicitation . The developed methods can be used for decision making of incomplete 0 1 information systems which are generated by the conjunction of two experts incomplete 0 1 evaluation results . Whenever the available information is not enough for choosing a necessarily optimal solution the elicitation algorithms can help elicitate as few unknown values as possible until an optimal result is found . An elicitation system is made to show that our elicitation methods can potentially be embedded in recommender or decision support systems . The elicitation problems are proposed for decision making of operation generated soft sets by extracting from some practical problems . The concept of expected elicitation times of objects is defined and used for developing one type of elicitation strategy . 
",A fast method for computing necessarily optimal solutions is developed. Four kinds of elicitation criterions for one time elicitation are explored. Idea of expected elicitation times is used in one elicitation criterion. A system is built to implement the proposed elicitation criterions.,,,
S096599781300121X," Ultra large scale systems are a new generation of distributed software system that are composed of various changing inconsistent or even conflicting components that are distributed in a wide domain . Some important characteristics of these systems include their very large size global geographical distribution operational and managerial independence of their member systems . The main function of these systems arises from the interoperability between their components . Nowadays one of the most important challenges facing ultra large scale systems is the interoperability of their component systems . Interoperability is the ability by which system elements can exchange and understand the information required with each other . This paper aims to solve the mentioned challenge which is divided into two main parts . In the first part this paper presents a maturity model for the interoperability of ultra large scale systems by using the interoperability level of the component system of one ultra large scale system its maturity level can be determined . In the second part by proposing a framework we try to increase the interoperability of the component systems in ultra large scale systems based on the interoperability maturity levels determined in the first part . Consequently their interoperability is improved . 
",We provide a maturity model for interoperability of ultra large scale systems. We provide a framework for interoperability of ultra large scale systems. Increasing interoperability of component systems in ultra large scale systems.,,,
S095070511500218X," We mainly study the low rank image recovery problem by proposing a bilinear low rank coding framework called Tensor Low Rank Representation . For enhanced low rank recovery and error correction our method constructs a low rank tensor subspace to reconstruct given images along row and column directions simultaneously by computing two low rank matrices alternately from a nuclear norm minimization problem so both column and row information of data can be effectively preserved . Our bilinear approach seamlessly integrates the low rank coding and dictionary learning into a unified framework . Thus our formulation can be treated as enhanced Inductive Robust Principal Component Analysis with noise removed by low rank representation and can also be considered as the enhanced low rank representation with a clean informative dictionary via low rank embedding . To enable our method to include outside images the out of sample extension is also presented by regularizing the model to correlate image features with the low rank recovery of the images . Comparison with other criteria shows that our model exhibits stronger robustness and enhanced performance . We also use the outputted bilinear low rank codes for feature learning . Two unsupervised local and global low rank subspace learning methods are proposed for extracting image features for classification . Simulations verified the validity of our techniques for image recovery representation and classification . 
",We mainly explore the low rank image recovery problem. A bilinear low rank image coding framework is proposed. For recovery TLRR preserves both column and row information of given data. Out of sample extension of TLRR is presented for handling outside data. We propose two local and global low rank subspace learning methods for feature learning.,,,
S095070511400416X," In this paper an effective bi population estimation of distribution algorithm is presented to solve the no idle permutation flow shop scheduling problem with the total tardiness criterion . To enhance the search efficiency and maintain the diversity of the whole population two sub populations are used in the BEDA . The two sub populations are generated by sampling the probability models that are updated differently for the global exploration and the local exploitation respectively . Meanwhile the two sub populations collaborate with each other to share search information for adjusting the models . To well adjust the models for generating promising solutions the global probability model is updated during the evolution with the superior population and the local probability model is updated with the best solution that has been explored . To further enhance exploitation in the promising region the insertion operator is used iteratively as the local search procedure . To investigate the influence of parameter setting numerical study based on the Taguchi method of design of experiment is carried out . The effectiveness of the bi population strategy and local search procedure is shown by numerical comparisons and the comparisons with the recently published algorithms by using the benchmarking instances also demonstrate the effectiveness of the proposed BEDA . 
",A novel bi population EDA for NIFPSP. Suitable adjusting mechanisms for adjusting probability models. Insertion based local search procedure to enhance exploitation. Better results obtained than those by the existing meta heuristics.,,,
S095070511500372X," Noise components are a major cause of poor performance in document analysis . To reduce undesired components most recent research works have applied an image processing technique . However the effectiveness of these techniques is suitable only for a Latin script document but not a non Latin script document . The characteristics of the non Latin script document such as Thai are considerably more complicated than the Latin script document and include many levels of character alignment no word or sentence separator and variability in a character s size . When applying an image processing technique to a Thai document we usually remove the characters that are relatively close to noise . Hence in this paper we propose a novel noise reduction method by applying a machine learning technique to classify and reduce noise in document images . The proposed method uses a semi supervised cluster and label approach with an improved labeling method namely feature selected sub cluster labeling . Feature selected sub cluster labeling focuses on the clusters that are incorrectly labeled by conventional labeling methods . These clusters are re clustered into small groups with a new feature set that is selected according to class labels . The experimental results show that this method can significantly improve the accuracy of labeling examples and the performance of classification . We compared the performance of noise reduction and character preservation between the proposed method and two related noise reduction approaches i.e . a two phased stroke like pattern noise removal and a commercial noise reduction software called ScanFix Xpress 6.0 . The results show that semi supervised noise reduction is significantly better than the compared methods of which an F measure of character and noise is 86.01 and 97.82 respectively . 
",We proposed a novel noise reduction method for document images. Semi supervised learning is applied to classify noise from character components. The proposed method is suitable for Non Latin based scripts i.e. Thai document image. We proposed an enhance labeling method of semi supervised cluster and label approach. The performance of proposed methods are significantly better than comparison methods.,,,
S095070511500355X," This paper is devoted to two issues involved in the one class support vector machine i.e . the optimization algorithm and the kernel parameter selection . For appropriate choices of parameters the primal maximum margin problem of OCSVM is equivalent to a nearest point problem . A generalized Gilbert algorithm is proposed to solve the nearest point problem . Compared with the algebraic algorithms developed for OCSVM such as the well known sequential minimal optimization algorithm the GG algorithm is a novel geometric algorithm that has an intuitive and explicit optimization target at each iteration . Moreover an improved MIES is developed for the Gaussian kernel parameter selection . IMIES is implemented by constraining the geometric locations of edge and interior sample mappings relative to OCSVM separating hyper planes . The experimental results on 2 D artificial datasets and benchmark datasets show that IMIES is able to select suitable kernel parameters and the GG algorithm is computationally more efficient while achieving comparable accuracies to the SMO algorithm . 
",The primal maximum margin problem of OCSVM is equivalent to a nearest point problem. A generalized Gilbert GG algorithm is proposed to solve the nearest point problem. An improved MIES is developed for the Gaussian kernel parameter selection. The GG algorithm is computationally more efficient than the SMO algorithm.,,,
S095070511400375X," Knowledge reduction is a basic issue in knowledge representation and data mining . Although various methods have been developed to reduce the size of classical formal contexts the reduction of formal fuzzy contexts based on fuzzy lattices remains a difficult problem owing to its complicated derivation operators . To address this problem we propose a general method of knowledge reduction by reducing attributes and objects in formal fuzzy contexts based on the variable threshold concept lattices . Employing the proposed approaches we remove attributes and objects which are non essential to the structure of a variable threshold concept lattice i.e . with a given threshold level the concept lattice constructed from a reduced formal context is made identical to that constructed from the original formal context . Discernibility matrices and Boolean functions are respectively employed to compute the attribute reducts and object reducts of the formal fuzzy contexts by which all the attribute reducts and object reducts of the formal fuzzy contexts are determined without changing the structure of the lattice . 
",We proposed a general method of knowledge reduction in formal fuzzy contexts. We gave some judging methods of attribute characteristics in fomal fuzzy contexts. We constucted the discernibility functions to calculating the attribute reducts.,,,
S095070511400118X," This study analyzed the Cost efficiency and Revenue efficiency of 207 certified public accountant firms in Taiwan by using the additive efficiency decomposition DEA approach . Furthermore this study applied the Tobit regression to explore the relationship between CPA firms and intellectual capital . The study found that the Big 4 and CPA firms that practiced auditing in China were relatively efficient in both cost and revenue . In addition this research discovered that CPA firms relying mainly on auditing are more efficient in creating revenue and utilizing costs . Furthermore the Tobit regression was employed to evaluate whether IC affected CPA firms cost efficiency and revenue efficiency . This study found that IC played an important role in performance representation both in cost efficiency and revenue efficiency . Therefore this study suggests that CPA firms should manage IC efficiently to enhance CPA firms competitive abilities . 
",We apply a two stage DEA model to evaluate cost efficiency and revenue efficiency. The additive efficiency decomposition DEA approach is utilized. We examine whether intellectual capital affects performance through Tobit models. We find that intellectual capital has a positive impact on CPA firms performance.,,,
S095070511600068X," From traditional clusters to cloud systems job scheduling is one of the most critical factors for achieving high performance in any distributed environment . In this paper we propose an adaptive algorithm for scheduling modular non linear parallel jobs in meteorological Cloud which has a unique parallelism that can only be configured at the very beginning of the execution . Different from existing work our algorithm takes into account four characteristics of the jobs at the same time including the average execution time the deadlines of jobs the number of assigned resources and the overall system loads . We demonstrate the effectiveness and efficiency of our scheduling algorithm through simulations using WRF that which is widely used in scientific computing . Our evaluation results show that the proposed algorithm has multiple advantages compared with previous methods including more than 10 reduction in terms of execution time a higher completion ratio in terms of meeting soft deadlines and a much smaller standard deviation of the average weighted execution time . Moreover we show that the proposed algorithm can tolerate inaccuracy in system load estimation . 
",WRF is used to discuss characteristic of non linear speedup parallel jobs. We give an analysis under different loads for non linear speedup parallel jobs. An adaptive scheduling method is used to schedule resources for parallel jobs. Simulations based on WRF are used to test our method.,,,
S095070511400046X," In multiple attribute decision making different attribute weights may generate different solutions which means that attribute weights significantly influence solutions . When there is a lack of sufficient data knowledge and experience for a decision maker to generate attribute weights the decision maker may expect to find the most satisfactory solution based on unknown attribute weights called a robust solution in this study . To generate such a solution this paper proposes a robust evidential reasoning approach to compare alternatives by measuring their robustness with respect to attribute weights in the ER context . Alternatives that can become the best with the support of one or more sets of attribute weights are firstly identified . The measurement of robustness of each identified alternative from two perspectives i.e . the optimal situation of the alternative and the insensitivity of the alternative to a variation in attribute weights is then presented . The procedure of the proposed approach is described based on the combination of such identification of alternatives and the measurement of their robustness . A problem of car performance assessment is investigated to show that the proposed approach can effectively produce a robust solution to a MADM problem with unknown attribute weights . 
",We propose a robust evidential reasoning approach for multiple attribute decision making. The possible set of best alternatives is identified using unknown attribute weights. The robustness of the alternatives in the set is measured from two perspectives. A robust rank order of the alternatives in the set is generated by their robustness. Intervals of utilities and relevant constraints are handled in the proposed approach.,,,
S096599781300063X," In order to follow modern trends in contemporary building architecture which is moving off the limits of current fire design models assumption of homogeneous temperature conditions used for structural fire analysis needs to be validated . In this paper it is described how temperature distribution in a medium size fire compartment has been investigated experimentally by conducting fire test in two storey experimental building in September 2011 in the Czech Republic . In the upper floor a scenario of travelling fire was prepared . It has been observed that as flames were spreading across the compartment considerable temperature gradients appeared . Numerical simulation of the travelling fire test conducted using FDS has been compared with simulation of compartment fire under uniform temperature conditions to highlight the potential impact of the gas temperature heterogeneity on structural behaviour . The temperature measurements from the fire test have been used for validation of the numerical simulation of travelling fire . The fire test has provided important data for design model of travelling fire and shown that its impact on structural behaviour is not in agreement with the assumption of homogenous temperature conditions . 
",Temperature distribution in medium fire compartment is investigated experimentally. Numerical simulation of the travelling fire test is completed by using FDS. Measurements from the travelling fire test are used to validate the simulation. Travelling fire is compared to compartment fire with uniform temperature conditions. A potential impact of temperature heterogeneity on a structure is highlighted.,,,
S096599781300166X," Smoke is a leading cause of death in fire . To minimize the potential harm from the smoke hazards in the course of a fire a rational virtual reality based fire training simulator taking full account of the various aspects of smoke hazards has been developed and is described herein . In this simulator a visualization technique based on volume rendering and fire dynamics data has been especially designed to create a realistic and accurate smoke environment for the purposes of effective virtual training which allows the trainees to experience a realistic and yet non threatening fire scenario . In addition an integrated assessment model of smoke hazards is also established in order to assess the safety of different paths for evacuation or rescue in virtual training which allows the trainees to learn to identify the safest path . Two case studies of a subway station and a primary school demonstrated a high level of accuracy and smooth interactive performance of the proposed simulator which is thus shown to be valuable for the training of both people who might become trapped in fire and firefighters engaged in learning the proper rescue procedures . 
",A VR fire training simulator with smoke hazard assessment capacity is proposed. Realistic and accurate smoke environment is created for virtual training. Integrated smoke hazard in the path for evacuation or rescue is accurately assessed. A subway station and a primary school are investigated for virtual fire training. The simulator helps trainee identify the safest path and minimize smoke hazards.,,,
S096599781400091X," This paper presents an open and integrated framework that performs the structural design optimization by associating the improved sequential approximation optimization algorithm with the CAD CAE integration technique . In the improved SAO algorithm a new estimate of the width of Gaussian kernel functions is proposed to enhance the surrogate models for SAO . Based on the improved surrogate models an adaptive sampling strategy is developed to balance the exploration exploitation in the sampling process which better balances between the competence to locate the global optimum and the computation efficiency in the optimization process . Fewer function evaluations are required to seek the optimum which is of great significance for computation intensive structural optimization problems . Moreover based on scripting program languages and Application Programming Interfaces integration between commercial CAD and CAE software packages is implemented to expand the applications of the SAO algorithm in mechanical practices . Two benchmark tests from simple to complex from low dimension to moderate dimension were performed to validate the efficacy of the proposed framework . Results show that the proposed approach facilitates the structural optimization process and reduces the computing cost immensely compared to other approaches . 
",An improved sequential approximation optimization algorithm is developed. An estimate of width of Gaussian kernel functions for surrogate models is proposed. An adaptive sampling strategy is developed based on the improved surrogate model. A framework that integrates CAD CAE and SAO tools is developed.,,,
S096599781500143X," The main goal of this paper was to develop an integrated simulation design of experiments model to optimize a petrol station queuing system and sales rate . Initially the petrol station operating system was simulated using Witness 2014 simulation software . Then the responses of simulation were deployed as the input of DOE . Two level full factorial experiments with center points were performed where the simulated model parameter studied were number of pump number of cashier and inter arrival times . The response variables analyzed were queue length and sales rate . The obtained model from experimental design revealed that number of cashier and inter arrival time were significant in determining the queue length while all the factors and their interaction were significantly affecting the sales rate . 
",A case study was simulated to provide a realistic depiction of an operating system. Investigated factors were number of pump number of cashier and IATs. 2 level full factorial experiments were used for further analysis of simulation. The performance of a service industry queuing system has been analyzed. The influential parameters on queue length and sales rate have been obtained.,,,
S096599781500085X," The present article proposes an advanced methodology for numerically simulating complex noise problems . More precisely we consider the so called multi stage acoustic hybrid approach which principle is to couple sound generation and acoustic propagation stages . Under that approach we propose an advanced hybrid method which acoustic propagation stage relies on Computational AeroAcoustics techniques . To this end first an innovative weak coupling technique is developed which allows an implicit forcing of the CAA stage with a given source signal coming from an a priori evaluation whether the latter evaluation is of analytical or computational nature . Then thanks to additional innovative solutions the resulting CAA based hybrid approach is optimized so that it can be applied to realistic and complex acoustic problems in an easier and safer way . All these innovative features are then validated on the basis of an academic test case before the resulting advanced CAA based hybrid methodology is applied to two problems of flow induced noise radiation . This demonstrates the ability of the here proposed method to address realistic problems by offering to handle at the same time both acoustic generation and propagation phenomena despite their intrinsic multiscale character . 
",An advanced hybrid method for computational acoustics is proposed. Acoustic propagation is simulated via the Perturbed Euler Equations. Acoustic generation and propagation stages are weakly coupled through an interface. The method is optimized via innovative features forcing derivation interpolation . The method is validated and illustrated via noise problems of increasing complexity.,,,
S096599781500023X," In reinforced concrete structural experiments the development of concrete surface cracks is an important factor of concern to experts . One conventional crack observation method is to suspend a test at a few selected testing steps and send inspectors to mark pen strokes on visible cracks but this method is dangerous and labor intensive . Many image analysis methods have been proposed to detect and measure the dark shadow lines of cracks reducing the need for manual pen marking . However these methods are not applicable for thin cracks which do not present clear dark lines in images . This paper presents an image analysis method to capture thin cracks and minimize the requirement for pen marking in reinforced concrete structural tests . The paper presents the mathematical models procedures and limitations of our image analysis method as well as the analysis flowchart the adopted image processing and analysis methods and the software implementation . Finally the results of applying the proposed method in full scale reinforced concrete bridge experiments are presented to demonstrate its performance . Results demonstrate that this method can capture concrete surface cracks even before dark crack lines visible to the naked eye appear . 
",An image analysis method for crack observation in a concrete pier is proposed. This method manifests concrete cracks before there became visible to the naked eyes. We present the procedures flowchart and software implementation of this method.,,,
S096599781630076X," One of the most important activities in software project planning involves scheduling tasks and assigning them to developers . Project managers must decide who will do what and when in a software project with the aim of minimizing both its duration and cost . However project managers often struggle to efficiently allocate developers and schedule tasks in a way that balances these conflicting goals . Furthermore the different criteria used to select developers could lead to inaccurate estimation of the duration and cost of tasks resulting in budget overruns delays or reduced software quality . This paper proposes an approach that makes use of multi objective optimization to handle the simultaneous minimization of project cost and duration taking into account several productivity related attributes for better estimation of task duration and cost . In particular we focus on dealing with the non interchangeable nature of human resources and the different ways in which teams carry out work by considering the relationship between the type of task interdependence and the productivity rate of developers as well as the communication overhead incurred among developers . The approach is applied to four well known optimization algorithms whose performance and scalability are compared using generated software project instances . Additionally several real world case studies are explored to help discuss the implications of such approach in the software development industry . The results and observations show positive indications that using a productivity based multi objective optimization approach has the potential to provide software project managers with more accurate developer allocation and task scheduling solutions in a more efficient manner . 
",Proposed approach adopts MOGAs to minimize software project cost and duration. Solutions representing resource allocations and task schedules are evolved. Objective functions consider productivity of developers and task interdependence. The performance and scalability of four MOGAs were compared using several datasets. MOCell NSGA II and SPEA2 outperform PAES in the majority of project instances.,,,
S105120041300064X," Automatic segmentation of non stationary signals such as electroencephalogram electrocardiogram and brightness of galactic objects has many applications . In this paper an improved segmentation method based on fractal dimension and evolutionary algorithms for non stationary signals is proposed . After using Kalman filter to reduce existing noises FD which can detect the changes in both the amplitude and frequency of the signal is applied to reveal segments of the signal . In order to select two acceptable parameters of FD in this paper two authoritative EAs namely genetic algorithm and imperialist competitive algorithm are used . The proposed approach is applied to synthetic multi component signals real EEG data and brightness changes of galactic objects . The proposed methods are compared with some well known existing algorithms such as improved nonlinear energy operator Varri s and wavelet generalized likelihood ratio methods . The simulation results demonstrate that segmentation by using KF FD and EAs have greater accuracy which proves the significance of this algorithm . 
",Recognition of physiological signal patters involving non stationarity. The non stationary signal patterns are partitioned into variable size stationary segments. Deterministic features are recognized using Kalman filter. Indeterministic features are estimated using fractal dimensions. A revolutionary approach is proposed to determine the best feature sets.,,,
S096599781630062X," The Colliding Bodies Optimization algorithm is a metaheuristic algorithm inspired by the physics laws of collision in which each candidate solution is modeled as an agent with mass body in proportion to the fitness of the solution . In this paper a modified version of CBO denoted by MCBO is utilized to optimize the cost of bridge superstructures . The problem consists of 17 variables and 101 implicit constraints based on AASHTO standard specifications and construction limitations . The optimization is performed for bridges with different span lengths and deck widths and with various unit costs of concrete . A comparison among the PSO CBO and MCBO algorithms is conducted which shows the efficiency and robustness of the MCBO algorithm . 
",A modified version of CBO denoted by MCBO is utilized to optimize the cost of bridge superstructures. The problem consists of 17 variables and 101 implicit constraints based on AASHTO standard. Optimization is performed for bridges with different span lengths deck widths and with various unit costs of concrete. A comparison among the PSO CBO and MCBO algorithms is conducted.,,,
S096599781400129X," Colliding Bodies Optimization is a new multi agent algorithm inspired by a collision between two objects in one dimension . Each agent is modeled as a body with a specified mass and velocity . A collision occurs between pairs of objects and the new positions of the colliding bodies are updated based on the collision laws . In this paper Enhanced Colliding Bodies Optimization which uses memory to save some best solutions is developed . In addition a mechanism is utilized to escape from local optima . The performance of the proposed algorithm is compared to those of standard CBO and some optimization techniques on some benchmark mathematical functions and three standard discrete and continuous structural design problems . Optimization results confirm the validity of the proposed approach . 
",A new multi agent algorithm inspired by a collision between two objects in one dimension is presented. An enhanced colliding bodies optimization which uses memory to save some best solutions is developed. A mechanism is utilized to escape from local optima. Performance of the proposed algorithm is compared to those of standard CBO and some optimization techniques.,,,
S096599781500040X," Computational efficiency is still a great challenge for the generation of the Medial Axis for complicated CAD models . Current research mainly focuses on CPU based MA generation methods . However most of the methods emphasize using a single CPU . The highly efficient methods based on parallel computing are still missing . In this study a parallel method based on multi CPU is proposed for the efficient MA generation of CAD models using distance dilation . By dividing the whole model into several parts for which MAs are calculated in parallel and then combined computational efficiency can be greatly improved in theory and the computation time can be reduced nearly K times if K CPUs are used . Firstly an adaptive division method is proposed to divide the voxelized model into blocks which have nearly the same number of voxels to balance the computational burden . Secondly the local Euclidean Distance Transform is calculated for each block based on the existing distance dilation method . Thirdly the complete inter dilation method is proposed to compute the influence between different blocks to get a global EDT for each block . Finally each block generates a sub MA separately and then all the generated MAs are combined to obtain the final MA . The last three processes can be efficiently conducted in parallel by using multiple CPUs . Several groups of experiments are conducted which demonstrate the good performance of the proposed methods in terms of efficiency . 
",An efficient approach to generating MA in parallel by multiple CPUs is proposed. An adaptive division method is proposed to maximize the parallelism degree. Inter dilation is proposed to ensure the correctness of final MA.,,,
S096599781500068X," Micro and nanomanipulators are essential for a broad range of applications requiring precise micro and nanoscopic spatial control such as those in micromanufacturing and single cell analysis . These manipulators are often manually controlled using an attached joystick and can be difficult for operators to use efficiently . This paper describes a system developed in MATLAB to control a well known commercial micromanipulator in a user friendly and versatile manner through a graphical user interface . The control system and interface allows several types of flexible movement controls in three axis Cartesian space including single movements multiple queued movements and mouse following continuous movements . The system uses image processing for closed loop feedback to ensure precise and accurate control over the movement of the manipulator s end effector . The system can be used on any electronic device capable of running the free MATLAB Runtime Environment and the system is extensible to simultaneously control other instruments capable of serial communication . 
",A GUI for commercial micro and nanomanipulators was developed. Multiple control options include point and click movement and mouse following. Tip detection using image processing corrects for the manipulator s error. The GUI provides faster and more accurate control even for inexperienced users.,,,
S096599781630059X," In a previous work we presented algorithms which allow obtaining three dimensional models from graphs which represent a projection in conical parallel perspectives and conical oblique perspectives of polyhedral models with normalon and quasi normalon typology . In this paper the new advances that we have achieved in this field are presented allowing increasing the set of models which can be reconstructed to other typologies different from the normalon and quasi normalon ones . Moreover we present a new technique which extends the previous work in order to be implemented to conical perspectives with three vanishing points and the method proposed for the detection of the type of conical perspective represented by the graph including the detection and subsequent reconstruction of graphs which represent a flat shape has been improved . The results obtained on a total of 336 tests with a success ratio of 100 make the method a proposal to be considered for obtaining models from conical perspectives automatically . 
",The algorithms proposed allow automatically obtaining models from the graphs representing a conical perspective. The system proposed does not need to know the faces defined in the model. The algorithms proposed allow expanding the reconstruction model through some oblique edges in the model. The methods have been implemented and simulated obtaining a success rate of 100 .,,,
S105120041400164X," A text independent speaker recognition system using a hybrid Probabilistic Principal Component Analysis and conventional i vector modeling technique is proposed . In this framework the total variability space is estimated using PPCA while the i vectors of target speakers and test utterances are extracted using the conventional method . This leads to appreciable decrease in development time while the time required for training and testing remains unchanged . In this a paper an algorithmic optimization to the PPCA s EM algorithm is developed . This is observed to provide a speed up of 3.7 . To simplify the testing procedure two different approximation procedures are proposed to be used in this framework . The first approximation assumes a covariance matrix computed based on the PPCA framework . The second approximation proposes an optimization to avoid inverting the precision matrix of the i vector . The comparison of time taken by these approximations with the baseline i vector extraction procedure shows speed gains with some deterioration in performance in terms of the Equal Error Rate . Among the proposed techniques a best case trade off is obtained with a speed up of 81.2 with deterioration in performance by 0.7 in absolute terms . Speaker recognition performances are studied on the telephone conditions of the benchmark NIST SRE 2010 dataset with systems built on the Mel Frequency Cepstral Co efficient feature . A trade off in the performance is observed when the proposed approximations are used . The scalability of these trade offs is tested on the Mel Filterbank Slope feature . The trade offs observed with the approximations are reduced when the two systems are fused . 
",The hybrid FA PPCA system is presented. Two approaches termed AN and AC are proposed to speed up the i vector estimation during testing in a speaker recognition. Significant speed ups are obtained for each proposed approach. The scalability of the hybrid system is demonstrated using two suitable features MFCC and MFS . Fusion of systems that use AC type approximation perform similar to that of the corresponding Hybrid system baseline.,,,
S096599781630014X," Medial axis is used as an effective description for objects in many engineering fields . A difficulty for the current methods for the generation of MA of CAD models is the balance between the efficiency and the quality . In this study an approach to iteratively generating hierarchical multi resolution MA is proposed . In each iteration only a small part of MA that affects MA quality is refined by which the time cost and the space cost are reduced greatly . First the model is voxelized and its initial MA is generated by distance dilation method . Meanwhile the MA quality is computed and evaluated . Second if the MA quality does not satisfy the requirement upgrade the MA level and re compute the local MA in the affected region until the MA quality does . Finally by combining the local MA in the affected region with the reused MA in other regions hierarchical multi resolution MA is obtained . Several examples are given to demonstrate the outperformance of the proposed method in terms of time and space . 
",An efficient approach to generating hierarchical multi resolution MA of a CAD model is proposed. The affected region and the re voxelization region are proposed to reuse voxels when the MA level is upgraded. An adaptive double queue distance dilation based algorithm is proposed to refine MA automatically. The MA quality metric is proposed to evaluate the MA.,,,
S096599781400101X," Many computer applications such as racing games and driving simulations demand high fidelity 3D road network models . However few methods exist for the automatic generation of 3D realistic road networks especially for those in the real world . On the other hand vast 2D road network data in various geographical information systems have been collected in the past and are used by a wide range of applications . A method that can automatically produce 3D high fidelity road network models from 2D real road GIS data will significantly reduce both the labor and time cost and greatly benefit applications involving road networks . Based on a set of carefully selected civil engineering rules for road design this paper proposes a novel approach that transforms existing road GIS data that contain only 2D road centerline information into high fidelity 3D road network models . The proposed method consists of several major components including road GIS data preprocessing 3D centerline modeling and 3D geometric modeling . With this approach basic road elements such as road segments road intersections and traffic interchanges are generated automatically to compose sophisticated road networks in a seamless manner . Results show that this approach provides a rapid and efficient 3D road modeling method for applications that have stringent requirements on high fidelity road models . 
",Procedural generation of high fidelity 3D road models from 2D road GIS data. A set of carefully selected civil engineering rules for road design. A novel approach that generates road geometry from centerline information. Parameterization of network data into elements which compose sophisticated geometry. The algorithm produces seamless road geometry quickly and efficiently.,,,
S096599781400146X," Application of techniques for modelling of boundary value problems implies two conflicting requirements obtaining high accuracy of the results and speed of the solution . Accurate results can be obtained only by using appropriate models and algorithms . In the previous papers the authors applied the parametric integral equations system in modelling and solving boundary value problems . The first requirement was satisfied the results were obtained with very high accuracy . This paper fulfils the second requirement by novel approach to accelerate PIES . Graphics cards programming for numerical calculations in general purpose applications using NVIDIA CUDA is used for this purpose . The speed of calculations increased up to 80 times whereas high accuracy of the solutions was maintained . Examples included in this paper concern solving elasticity problems which are modelled by three dimensional Navier Lam equations . 
",A novel parallel approach of numerical implementation of PIES named GPU accelerated PIES is proposed. Processing power of GPU is used to accelerate computations. The accuracy of the solutions obtained using GPU accelerated PIES was examined. Computational time of GPU accelerated PIES was examined. The effectiveness of the serial version of PIES program and GPU accelerated PIES was compared.,,,
S096599781500071X," This work presents a contribution on the numerical modelling capabilities for the simulation of fluid flow and heat transfer in cellular solids in particular we focus on open cell aluminium foams . Rather than applying one of the classical academical or commercial numerical finite volume finite difference or finite element interface tracking methods we base our models on an interface capturing phase field method . A coupled diffuse interface lattice Boltzmann fluid flow solver and a diffuse interface heat transfer approach are combined in view of dealing with even more convoluted geometries incorporating the dynamics of interfaces and complex multiphysics applications . Numerical results for the combined fluid flow and heat transfer simulations in open cell metal foams are in very good agreement with experimental data . 
",Diffuse interface method for heat and mass transfer simulation of cellular solids. Method for realistic modelling of complex microscale structures of cellular solids. Novel tensorial mobility approach is employed for diffuse interface heat transfer. Pore scale level CFD simulation using diffuse interface lattice Boltzmann method. Validation on complex engineering type geometries show excellent agreement.,,,
S096599781400115X," The paper describes an efficient numerical model for better understanding the influence of the microstructure on the thermal conductivity of heterogeneous media . This is the extension of an approach recently proposed for simulating and evaluating effective thermal conductivities of alumina Al composites . A C code called MultiCAMG taking into account all steps of the proposed approach has been implemented in order to satisfy requirements of efficiency optimization and code unification . Thus on the one hand numerical tools such as the efficient Eyre Milton scheme for computing the thermal response of composites have been implemented for reducing the calculation cost . On the other hand statistical parameters such as the covariance and the distribution of contact angles between particles are now estimated for better analyzing the microstructure . In the present work we focus our investigations on the effects of anisotropy on the effective thermal conductivity of alumina Al composites . First of all an isotropic benchmark is set up for comparison purposes . Secondly anisotropic configurations are studied in order to direct the heat flux . A transversally isotropic structure taking benefit of wall effects is finally proposed for controlling the orientation of contact angles . Its thermal capabilities are related to the current issue of heat dissipation in automotive engine blocks . 
",A new C code is implemented for generating and analyzing granular materials. We set up a numerical method based on the DEM for generating alumina Al materials. A statistical analysis is performed for ensuring the randomness of granular packings. Thermal conductivities are estimated using FFT based scheme of Eyre and Milton. Effects of anisotropy are investigated and related to the thermal conductivity.,,,
S096599781400132X," We examine the rotational variance of the continuous parameter genetic algorithm . We show that a standard CPGA using blend crossover and standard mutation is rotationally variant . To construct a rotationally invariant CPGA it is possible to modify the crossover operation to be rotationally invariant . This however results in a loss of diversity . Hence we introduce diversity in two ways firstly using a modified mutation scheme and secondly by adding a self scaling random vector with a standard normal distribution sampled uniformly from the surface of a n dimensional unit sphere to the offspring vector . This formulation is strictly invariant albeit in a stochastic sense only . We compare the three formulations in terms of numerical efficiency for a modest set of test problems the intention not being the contribution of yet another competitive and or superior CPGA variant but rather to present formulations that are both diverse and invariant in the hope that this will stimulate additional future contributions since rotational invariance in general is a desirable salient feature for an optimization algorithm . 
",We show that the standard CPGA is rotationally variant. We then construct a rotationally invariant CPGA. We ensure diversity using a modified mutation scheme. We also ensure diversity by adding a self scaling random vector.,,,
S105120041300184X," A simple physical model consisting of a point source displaced from its center of rotation in combination with a directivity model that includes backwards emitted energy is considered for the problem of estimating the orientation of a directional acoustic source . Such a problem arises for instance in voice commanded devices in a smart room and is usually tackled with a large or distributed microphone array . We show however that when the time difference of arrival is also taken into account a small array of only two microphones is sufficiently robust against unaccounted factors such as microphone directivity variation and mild reverberation . This is shown by comparing predicted and measured values of binaural cues and by using them and pairwise frame energies as inputs for an artificial neural network in order to estimate source orientation . 
",We estimate an acoustic source orientation using only two microphones. A simple physical model that explains ITD and ILD variations is proposed. We propose a new directivity model that includes backwards emitted energy. Multiframe estimation is more robust than frame estimation in adverse conditions. In multiframe estimation there is a tradeoff between COR and estimation delay.,,,
S105120041400342X," This paper introduces a method to perform a Time Scale Local Hurst Exponent analysis for time series . The traditional Hurst exponent methods usually analyze time series as a whole providing a single value that characterizes their global behavior . In contrast the methods based on the Local Hurst Exponent allow the evaluation of the fractal structure of a time series on local events . However a critical parameter in these methods is the selection of scale . Here a TS LHE method is presented based on a systematic implementation of the rescaled range method in a set of sliding windows of different sizes . This method allows calculating instantaneous values of Local Hurst Exponents at different scales associating them with individual samples of a time series . This paper is organized as follows first an overview of the TS LHE is provided then a proof of concept of this analysis is presented considering different fractional Brownian motion series a synthetic seismic signal under different noise conditions and a group of real seismic traces . Finally the obtained results show that the TS LHE analysis is particularly sensitive to sudden behavior changes of the time series such as frequency or phase variations . This sensitivity is independent of the amplitude of the data and thus it can be used to identify pattern changes as well as long and short range correlations within a time series . 
",We present a method for estimating a time scale local Hurst exponent on time series. The method has proven to be sensitive to sudden behavior changes on time series. Lower scales evaluate short range correlations. Larger scales evaluate long range correlations. The analysis evaluates pattern changes regardless the amplitude and scale.,,,
S096599781630045X," Photo and physically realistic techniques are often insufficient for visualization of fluid flow simulations especially for 3D and time varying studies . Substantial research effort has been dedicated to the development of non photorealistic and illustration inspired visualization techniques for compact and intuitive presentation of such complex datasets . However a great deal of work has been reproduced in this field as many research groups have developed specialized visualization software . Additionally interoperability between illustrative visualization software is limited due to diverse processing and rendering architectures employed in different studies . In this investigation a framework for illustrative visualization is proposed and implemented in MarmotViz a ParaView plug in enabling its use on a variety of computing platforms with various data file formats and mesh geometries . Region of interest identification and feature tracking algorithms incorporated into this tool are described . Implementations of multiple illustrative effect algorithms are also presented to demonstrate the use and flexibility of this framework . By providing an integrated framework for illustrative visualization of CFD data MarmotViz can serve as a valuable asset for the interpretation of simulations of ever growing scale . cell centroid of a cell or region distance of contour points from a central axis edge feature graph constructed from mesh thickness for feature halos products of inertia tensor for a region volumetric angular momentum of a region number of unmatched regions of interest camera projection plane normal octree used for feature matching point on a contour queue used for feature matching region of interest position vector strobe silhouette curve bounding contour around a feature time user specified threshold velocity of a cell or volume average velocity of a region volume of a cell or region spatial coordinates initial value bounding contour around a feature rear point on a contour cell count threshold for regions of interest gradient threshold for regions of interest counting indices left most point on a contour inset contour around a feature minimum maximum values along a contour offset contour around a feature right most point on a contour estimated value similarity parameter for feature matching user defined time step similarity criterion for feature matching relaxation parameter for feature matching hard lower limit for the feature matching criterion volumetric average angular velocity of a region 
",A framework for illustrative visualization of fluid simulation datasets is presented. New algorithms are developed for feature identification and matching in field data. Novel implementations are described for multiple illustrative visualization effects.,,,
S105120041400222X," This study deals with the asymptotic performance of a multiple spur cancellation scheme . Radio frequency transceivers are now multi standard and specific impairment can occur . The clock harmonics called spurs can leak into the signal band of the reception stage and thus degrade the performance . The performance of a fully digital approach is presented here . A one spur cancellation scheme is first described for which we exploit the a priori knowledge of the spur frequency to create a reference of the polluting tone with the same frequency . A least mean square algorithm block that uses this reference to mitigate the polluter is designed . However due to imperfections in the physical components there is a shift between the a priori frequency and the actual frequency of the spur and the spur is affected by Brownian phase noise . Under these circumstances we study the asymptotic and transient performance of the algorithm . We next improve the transient performance by adding a previously proposed adaptive step size process . In a second part of this paper we present a multiple spur parallel approach that is based on the one spur cancellation scheme for which we provide a closed form expression of the asymptotic signal plus noise interference ratio in the presence of frequency shifts and phase noise . 
",We model a multi spur pollution and take into account the impairments affecting the spurs pulsation shift phase noise . We derive all closed form formulae optimal step Signal to Interference Ratio of the LMS based spurs cancellation scheme. We add an original adaptive step overlay to improve the transitional mode while keeping the same asymptotic performance.,,,
S105120041300242X," Fitting a pair of coupled geometric objects to a number of coordinate points is a challenging and important problem in many applications including coordinate metrology petroleum engineering and image processing . This paper derives two asymptotically efficient estimators one for concentric circles fitting and the other for concentric ellipses fitting based on the weighted equation error formulation and non linear parameter transformation . The Kanatani Cram r Rao lower bounds for the parameter estimates of the concentric circles and concentric ellipses under zero mean Gaussian noise are provided to serve as the performance benchmark . Small noise analysis shows that the proposed estimators reach the KCR lower bound performance asymptotically . The accuracy of the proposed estimators is corroborated by experiments with synthetic data and realistic images . 
",New solutions for coupled circles and coupled ellipses fittings in explicit forms. The non iterative solutions have computation advantage. The iterative solutions are self initialized and achieve optimum performance. They have higher noise tolerance levels before the thresholding effects happen. They can be reduced back to the fittings of a single circle and a single ellipse.,,,
S096599781400204X," This paper proposes a novel training algorithm for radial basis function neural networks based on fuzzy clustering and particle swarm optimization . So far fuzzy clustering has proven to be a very efficient tool in designing such kind of networks . The motivation of the current work is to quantify the exact effect of fuzzy cluster analysis on the network s performance and use it in order to substantially improve this performance . There are two key theoretical findings resulting from the present work . First it is analytically proved that when the standard fuzzy c means algorithm is used to generate the input space fuzzy partition the main effect this partition imposes to the network s square error can be written down in terms of a distortion function that measures the ability of the partition to recreate the original data . Second using the aforementioned distortion function an upper bound of the network s square error can be constructed . Then the particle swarm optimization is put in place to minimize the above upper bound and determine the network s parameters . To further improve the accuracy the basis function widths and the connection weights are fine tuned by employing a steepest descent approach . The main experimental findings are the implementation of the PSO obtains a significant reduction of the square error while exhibiting a smooth dynamic behavior although the steepest descent further decreases the error it finally obtains smaller reduction rates meaning that the strongest impact on the error reduction is provided by the PSO and the improved performance of the proposed network is demonstrated through an extensive comparison with other related methods using a 10 fold cross validation analysis . 
",This paper quantifies the effect of fuzzy clustering in the design process of a typical RBF network. It is analytically shown that the fuzzy clustering acts to minimize an upper bound of the network s square error. The PSO algorithm is used to minimize the upper bound and to provide an estimation of the network s parameters. Finally the widths and connection weights are further tuned using a steepest descent approach.,,,
S105120041400075X," We propose a new method to incorporate priors on the solution of nonnegative matrix factorization . The NMF solution is guided to follow the minimum mean square error estimates of the weight combinations under a Gaussian mixture model prior . The proposed algorithm can be used for denoising or single channel source separation applications . NMF is used in SCSS in two main stages the training stage and the separation stage . In the training stage NMF is used to decompose the training data spectrogram for each source into a multiplication of a trained basis and gains matrices . In the separation stage the mixed signal spectrogram is decomposed as a weighted linear combination of the trained basis matrices for the source signals . In this work to improve the separation performance of NMF the trained gains matrices are used to guide the solution of the NMF weights during the separation stage . The trained gains matrix is used to train a prior GMM that captures the statistics of the valid weight combinations that the columns of the basis matrix can receive for a given source signal . In the separation stage the prior GMMs are used to guide the NMF solution of the gains weights matrices using MMSE estimation . The NMF decomposition weights matrix is treated as a distorted image by a distortion operator which is learned directly from the observed signals . The MMSE estimate of the weights matrix under the trained GMM prior and log normal distribution for the distortion is then found to improve the NMF decomposition results . The MMSE estimate is embedded within the optimization objective to form a novel regularized NMF cost function . The corresponding update rules for the new objectives are derived in this paper . The proposed MMSE estimates based regularization avoids the problem of computing the hyper parameters and the regularization parameters . MMSE also provides a better estimate for the valid gains matrix . Experimental results show that the proposed regularized NMF algorithm improves the source separation performance compared with using NMF without a prior or with other prior models . 
",Single channel source separation using nonnegative matrix factorization NMF . Using MMSE under GMM for regularizing the NMF gains online estimation of uncertainty. Less sensitive to the regularization parameter. Efficient update rules for gain parameters. Improved results in speech music separation experiments.,,,
S096599781500112X," One of the most important challenges in the computer vision has long been to obtain three dimensional models from the information given by a projection of the model . In this work we show an automatic system which allows obtaining three dimensional models from entities that represent the conical projection of a polyhedral model with normalon or quasi normalon typology . The results obtained on a total of 160 tests with a success ratio of 100 make the method a proposal to be considered for obtaining models from conical perspectives automatically . 
",The proposed algorithms represent a significant advance in solid geometry reconstruction. The reconstruction process is carried out automatically without user interaction. The methods have been implemented and simulated obtaining a success rate of 100 .,,,
S107731421300091X," Object recognition systems constitute a deeply entrenched and omnipresent component of modern intelligent systems . Research on object recognition algorithms has led to advances in factory and office automation through the creation of optical character recognition systems assembly line industrial inspection systems as well as chip defect identification systems . It has also led to significant advances in medical imaging defence and biometrics . In this paper we discuss the evolution of computer based object recognition systems over the last fifty years and overview the successes and failures of proposed solutions to the problem . We survey the breadth of approaches adopted over the years in attempting to solve the problem and highlight the important role that active and attentive approaches must play in any solution that bridges the semantic gap in the proposed object representations while simultaneously leading to efficient learning and inference algorithms . From the earliest systems which dealt with the character recognition problem to modern visually guided agents that can purposively search entire rooms for objects we argue that a common thread of all such systems is their fragility and their inability to generalize as well as the human visual system can . At the same time however we demonstrate that the performance of such systems in strictly controlled environments often vastly outperforms the capabilities of the human visual system . We conclude our survey by arguing that the next step in the evolution of object recognition algorithms will require radical and bold steps forward in terms of the object representations as well as the learning and inference algorithms used . 
",We survey the literature on passive and active object recognition during the last 50years. We survey some of the best performing object recognition algorithms. We discuss the limitations and drawbacks of current methodologies. We discuss future novel research directions.,,,
S107731421300204X," We present TouchCut a robust and efficient algorithm for segmenting image and video sequences with minimal user interaction . Our algorithm requires only a single finger touch to identify the object of interest in the image or first frame of video . Our approach is based on a level set framework with an appearance model fusing edge region texture and geometric information sampled local to the touched point . We first present our image segmentation solution then extend this framework to progressive video segmentation encouraging temporal coherence by incorporating motion estimation and a shape prior learned from previous frames . This new approach to visual object cut out provides a practical solution for image and video segmentation on compact touch screen devices facilitating spatially localized media manipulation . We describe such a case study enabling users to selectively stylize video objects to create a hand painted effect . We demonstrate the advantages of TouchCut by quantitatively comparing against the state of the art both in terms of accuracy and run time performance . 
",TouchCut requires only a single touch to bootstrap the object segmentation. It incorporates a new model that fuses edge region geometric and shape cues. A new fast dominant color extraction scheme to generate edge probability. Temporally propagated shape prior enables extension to video segmentation. Comprehensive qualitative and quantitative evaluations on images and videos.,,,
S105120041600004X," This paper introduces a new variation of the p norm detector which is designed for application to coherent multilook detection in compound Gaussian clutter with inverse Gamma texture . By applying what is termed a compensator enhanced detection performance can be achieved independently of the number of looks used . This is particularly useful in the case of a fast scan rate radar where the number of looks may be quite small . Conventional coherent detectors tend to experience saturation in such scenarios and so this new detection process complements recent advances in this area . Further validation is provided by applying this new decision rule to synthetic target detection in real X band radar clutter . 
",The p norm detector investigated for coherent multilook detection. Compensator used to improve its performance. Criteria established to select p norm plus compensator detector s parameters. Enhance detector performance achieved regardless of the number of looks. Application to detection in compound Gaussian clutter with inverse Gamma texture shows excellent results.,,,
S105120041500024X," We consider the problem of joint tracking and classification using the information from radar and electronic support measure . For each target class a separate filter is operated in parallel and each class dependent filter is implemented by interacting multiple model regularized particle filter . The speed likelihood for each class is defined using a priori information about speed constraint and combined with the likelihoods from two sensors to improve tracking and classification . Moreover the output of classifier is also used for particle reassignment of different classes which might lead to better performance . Simulations show that our proposed method can provide reliable tracking and correct classification . joint tracking and classification point target motion model based JTC rigid target motion model based JTC electronic support measure transferable belief model particle filter regularized particle filter interacting multiple model interacting multiple model regularized particle filter IMMRPF based JTC probability density mass function root mean squared errors posterior joint state class probability density mass function target state vector ith target type known number of target classes measurement sequences from radar and ESM measurement at time k from radar and ESM flight envelope constraints for class i set of maneuver models for class i target Markovian maneuver transition matrix for class i target Gaussian white process noise vector with zero mean and variance Q state transition matrix gain matrix additive Gaussian white noise with zero mean and variance R set of all possible emitters where N is the total number of emitter types set of on board emitters of class i target number of the on board emitters of class i target ESM measurement space where ESM confusion matrix usage transition probability matrix for emitter j overall emitter usage transition matrix for class i target with emitter set posterior target class probability using ESM data only emitter usage status vector of class i target posterior target class probability using radar data only jth model probability of class i target at time k likelihood for jth model probability of class i target at time k speed likelihood functions speed feature measurement initial class probabilities number of particles for class i target at time k initial number of particles for class i indicator function effective sample size for class i target preset re sampling threshold parameter hybrid particle weights of particles 
",It uses information from radar and ESM to avoid the kinematic only classification. For each class a separate filter is operated in parallel and implemented by IMMRPF. Speed likelihood for each class is calculated and combined with likelihoods from two sensors. Output of classifier is also used for particle reassignment of different classes.,,,
S105120041500336X," Acoustic echo canceller is used in communication and teleconferencing systems to reduce undesirable echoes resulting from the coupling between the loudspeaker and the microphone . In this paper we propose an improved variable step size normalized least mean square algorithm for acoustic echo cancellation applications based on adaptive filtering . The steady state error of the NLMS algorithm with a fixed step size is very large for a non stationary input . Variable step size algorithms can be used to decrease this error . The proposed algorithm named MESVSS NLMS combines the generalized sigmoid variable step size NLMS with the ratio of the estimation error to the mean history of the estimation error values . It is shown from single talk and double talk scenarios using speech signals from TIMIT database that the proposed algorithm achieves a better performance more than 3 dB of attenuation in the misalignment evaluation compared to GSVSS NLMS non parametric VSS NLMS and standard NLMS algorithms for a non stationary input in noisy environments . 
",A new technique for acoustic echo cancellation. An improved variable step size normalized least mean square adaptive filtering algorithm. A comparative study using some variable step size algorithms and the proposed algorithm. Experiments in single talk and double talk scenarios using TIMIT database. Standardized performances measures Mean Square Error and Normalized misalignment.,,,
S105120041500281X," The anisotropic diffusion is an efficient smoothing process . It is widely used in noise removing and edges preserving via different schemes . In this paper based on a mathematical background and the existing efficient anisotropic function in the literature we developed a new mathematical anisotropic diffusion function which is able to overcome the drawbacks of the traditional process such as the details loss and the image blur . The simulations results and the comparative study with other recent techniques are conducted and showed that the proposed schema generates a wide improvement in the quality of the restored images . This improvement has been shown subjectively in terms of visual quality and objectively with reference to the computation of some criteria . The simulated images are well de noised but the most important is that details and structural information are kept intact . In addition to that the proposed new function was found very interesting and presents numerous advantages like its similarity to the conventional model and the importance of the speed hence it converges faster which allows an opportunity to be well implemented in our de noising process . 
",Some shortcomings of some anisotropic filtering process were cited. A new filtering process based on new adaptive diffusion function is proposed. The efficiency of the new anisotropic function in the filtering process was shown. The proposed new function was found very interesting and generates good results. The time of the filtering process was highly decreased.,,,
S107731421300221X," Two of the main ingredients of topological persistence for shape comparison are persistence diagrams and the matching distance . Persistence diagrams are signatures capturing meaningful properties of shapes while the matching distance can be used to stably compare them . From the application viewpoint one drawback of these tools is the computational cost for evaluating the matching distance . In this paper we introduce a new framework for the matching distance estimation It preserves the reliability of the entire approach in comparing shapes extremely reducing the computational cost . Theoretical results are supported by experiments on 3D models . 
",This paper deals with the concepts of persistence diagram and matching distance. We present multi scale approaches to approximate the matching distance. Experiments show the capability of the proposed methodologies for shape retrieval.,,,
S107731421300088X," In this paper we address the problem of 2D 3D pose estimation . Specifically we propose an approach to jointly track a rigid object in a 2D image sequence and to estimate its pose in 3D space . We revisit a joint 2D segmentation 3D pose estimation technique and then extend the framework by incorporating a particle filter to robustly track the object in a challenging environment and by developing an occlusion detection and handling scheme to continuously track the object in the presence of occlusions . In particular we focus on partial occlusions that prevent the tracker from extracting an exact region properties of the object which plays a pivotal role for region based tracking methods in maintaining the track . To this end a dynamical choice of how to invoke the objective functional is performed online based on the degree of dependencies between predictions and measurements of the system in accordance with the degree of occlusion and the variation of the object s pose . This scheme provides the robustness to deal with occlusions of an obstacle with different statistical properties from that of the object of interest . Experimental results demonstrate the practical applicability and robustness of the proposed method in several challenging scenarios . 
",A robust algorithm for 2D visual tracking and 3D pose estimation is proposed. We focus on partial occlusions that distort the region properties of an object. Optimal pose of an object is estimated via particle filters in a decoupled manner. The degree of trust between system s predictions and measurements is controlled. The resulting methodology improves tracking performance in clutter and occlusion.,,,
S105120041600021X," First order Riesz transform based monogenic signal representation has been widely used in image processing and computer vision however it only characterizes image intrinsic one dimensional structure and is incapable of describing intrinsic two dimensional structure . To this end a novel feature extraction approach named Riesz Binary Pattern is proposed for face recognition based on image multi scale analysis and multi order Riesz transform . RBP consists of two complementary components i.e . local Riesz binary pattern and global Riesz binary pattern . LRBP is obtained by performing local binary coding operator on each Riesz transform response to extract image intrinsic two dimensional structure features . While GRBP is the global binary coding of joint information of image pixel multi scale analysis and multi order Riesz transform . Histogram of LRBP and GRBP are concatenated to form face image RBP description . Experimental results on three databases demonstrate that our proposed RBP descriptor is more discriminant in extracting image information and can provide a higher classification rate compared to some state of the art image representation methods . 
",A new image descriptor RBP is presented for face recognition in this paper. RBP is based on image multi scale analysis and multi order Riesz transform. RBP consists of two complementary components i.e. local Riesz binary pattern LRBP and global Riesz binary pattern GRBP . Experimental results on four databases demonstrate the superiority of our RBP compared with other image representation methods.,,,
S107731421300132X," Saliency detection has been researched a lot in recent years . Traditional methods are mostly conducted and evaluated on conventional RGB images . Few work has considered the incorporation of multi spectral clues . Considering the success of including near infrared spectrum in applications such as face recognition and scene categorization this paper presents a multi spectral dataset and applies it in saliency detection . Experiments demonstrate that the incorporation of near infrared band is effective in the saliency detection procedure . We also test the combinational models for integrating visible and near infrared bands . Results show that there is no single model to effect on every saliency detection method . Models should be selected according to the specific employed method . 
",A multi spectral dataset is constructed containing RGB and near infrared images. The incorporation of near infrared is proved to be valuable for saliency detection. The best model for integrating RGB and near infrared clues is analyzed.,,,
S107731421300129X," We address the problem of predicting category labels for unlabeled videos in a large video dataset by using a ground truth set of objectively labeled videos that we have created . Large video databases like YouTube require that a user uploading a new video assign to it a category label from a prescribed set of labels . Such category labeling is likely to be corrupted by the subjective biases of the uploader . Despite their noisy nature these subjective labels are frequently used as gold standard in algorithms for multimedia classification and retrieval . Our goal in this paper is NOT to propose yet another algorithm that predicts labels for unseen videos based on the subjective ground truth . On the other hand our goal is to demonstrate that the video classification performance can be improved if instead of using subjective labels we first create an objectively labeled ground truth set of videos and then train a classifier based on such a ground truth so as to predict objective labels for the set of unlabeled videos . With regard to how we generate the objectively labeled ground truth dataset we base it on the notion that when a video is labeled by a panel of diverse individuals the majority opinion rendered by the panel may be taken to be the objective opinion . In this manner using judgments provided by multiple human annotators we have collected objective labels for a ground truth dataset consisting of randomly selected 1000 videos from the TinyVideos database that contains roughly 52 000 videos from YouTube . Through a fourfold cross validation experiment on the ground truth set we demonstrate that the objective labels have a superior consistency compared to the subjective labels when used for video classification . We show that this claim is valid for several different kinds of feature sets that one can use to compare videos and with two different types of classifiers that one can use for label prediction . Subsequently we use the ground truth dataset of 1000 videos to predict the objective category labels of the remaining 51 000 videos . We compare the objective labels thus determined with the subjective labels provided by the video uploaders and qualitatively argue for the more informative nature of the objective labels . 
",We present a strategy to create objectively labeled ground truth set of videos. Such a strategy mitigates the subjective biases of the annotation process. Objective labels show superior consistency than subjective labels. A classifier is trained to predict objective labels for 51K unlabeled videos.,,,
S105120041600035X," In order to realize the patient privacy protection in medical image opposite to traditional reversible data hiding methods which prior to embed message into the smooth area for pursuing high PSNR value the proposed method priors to embed message into the texture area of the medical images for improving the quality of the details information and helping accurate diagnosis . Furthermore in order to decrease the embedding distortion while enhancing the contrast of the texture area this paper also proposes a message sparse representation method . Experiments implemented on medical images showed that the proposed method enhances the contrast of texture area when compared with previous methods . 
",Explain the reason for PSNR metric can t consistent with human visual system. Embed the message into texture area preferentially for helping accurate diagnosis. Propose message sparse representation method for decreasing embedding distortion.,,,
S107731421300074X," Organ shape plays an important role in clinical diagnosis surgical planning and treatment evaluation . Shape modeling is a critical factor affecting the performance of deformable model based segmentation methods for organ shape extraction . In most existing works shape modeling is completed in the original shape space with the presence of outliers . In addition the specificity of the patient was not taken into account . This paper proposes a novel target oriented shape prior model to deal with these two problems in a unified framework . The proposed method measures the intrinsic similarity between the target shape and the training shapes on an embedded manifold by manifold learning techniques . With this approach shapes in the training set can be selected according to their intrinsic similarity to the target image . With more accurate shape guidance an optimized search is performed by a deformable model to minimize an energy functional for image segmentation which is efficiently achieved by using dynamic programming . Our method has been validated on 2D prostate localization and 3D prostate segmentation in MRI scans . Compared to other existing methods our proposed method exhibits better performance in both studies . 
",Propose a novel target oriented shape prior modeling method. Measure the intrinsic similarity between the target shape and the training shapes. Incorporate the shape model into an optimized search based segmentation method. Exhibit better performance than other existing methods.,,,
S105120041500072X," Baseline correction is an important pre processing technique used to separate true spectra from interference effects or remove baseline effects . In this paper an adaptive iteratively reweighted genetic programming based on excellent community information is proposed to model baselines from spectra . Excellent community information which is abstracted from the present excellent community includes an automatic common threshold normal global and local slope information . Significant peaks can be firstly detected by an automatic common threshold . Then based on the characteristic that a baseline varies slowly with respect to wavelength normal global and local slope information are used to further confirm whether a point is in peak regions . Moreover the slope information is also used to determine the range of baseline curve fluctuation in peak regions . The proposed algorithm is more robust for different kinds of baselines and its curvature and slope can be automatically adjusted without prior knowledge . Experimental results in both simulated data and real data demonstrate the effectiveness of the algorithm . 
",Excellent community provides threshold global and local slope information. Global and local slope information further confirm peak distribution information. An adaptive iteratively reweighted genetic programming model the baseline.,,,
S107731421300235X," We present a novel method that evaluates the geometric consistency of putative point matches in weakly calibrated settings i.e . when the epipolar geometry but not the camera calibration is known using only the point coordinates as information . The main idea behind our approach is the fact that each point correspondence in our data belongs to one of two classes . The classification of each point match relies on the histogram of a quantity representing the difference between cross ratios derived from a construction involving 6 tuples of point matches . Neither constraints nor scenario dependent parameters thresholds are needed . Even for few candidate point matches the ensemble of 6 tuples containing each of them turns to provide statistically reliable histograms that prove to discriminate between inliers and outliers . In fact in most cases a random sampling among this population is sufficient . Nevertheless the accuracy of the method is positively correlated to its sampling density leading to an accuracy versus resulting computational complexity trade off . Theoretical analysis and experiments are given that show the consistent performance of the proposed classification method when applied in inlier outlier discrimination . The achieved accuracy is favourably evaluated against established methods that employ geometric only information i.e . those relying on the Sampson the algebraic and the symmetric epipolar distances . Finally we also present an application of our scheme in uncalibrated stereo inside a RANSAC framework and compare it to the same as above methods . 
",We propose a method for the indexing of point matches based on cross ratios. Matches are characterised by the distribution of a scalar measure derived from multiple point relations. Our consistency measure is processed statistically using random sampling. We propose applications both in weekly calibrated and uncalibrated scenarios. Our method exhibits low false positive rates without the use of thresholds.,,,
S107731421300194X," This article presents a unified framework for detecting segmenting and tracking unknown objects in everyday scenes allowing for inspection of object hypotheses during interaction over time . A heterogeneous scene representation is proposed with background regions modeled as a combinations of planar surfaces and uniform clutter and foreground objects as 3D ellipsoids . Recent energy minimization methods based on loopy belief propagation tree reweighted message passing and graph cuts are studied for the purpose of multi object segmentation and benchmarked in terms of segmentation quality as well as computational speed and how easily methods can be adapted for parallel processing . One conclusion is that the choice of energy minimization method is less important than the way scenes are modeled . Proximities are more valuable for segmentation than similarity in colors while the benefit of 3D information is limited . It is also shown through practical experiments that with implementations on GPUs multi object segmentation and tracking using state of art MRF inference methods is feasible despite the computational costs typically associated with such methods . 
",We present a framework for detection segmentation and tracking of multiple objects. The framework has minimal requirements on input for initialization. The choice of MRF inference method is less important than how scenes are modeled. Proximities are more important than colors as cues for segmentation. For real time application message passing is more feasible than graph cuts.,,,
S107731421400023X," Recently new high level features have been proposed to describe the semantic content of images . These features that we call supervised are obtained by exploiting the information provided by an additional set of labeled images . Supervised features were successfully used in the context of image classification and retrieval where they showed excellent results . In this paper we will demonstrate that they can be effectively used also for unsupervised image categorization that is for grouping semantically similar images . We have experimented different state of the art clustering algorithms on various standard data sets commonly used for supervised image classification evaluations . We have compared the results obtained by using four supervised features against those obtained by using low level features . The results show that supervised features exhibit a remarkable expressiveness which allows to effectively group images into the categories defined by the data sets authors . 
",We compared high and low level features for unsupervised image categorization. We verified that high level features significantly outperform low level features. We assessed how much the performance depends on the dimensionality of the feature vectors. We verified that a simple clustering on supervised features outperform strategies specifically designed for this task.,,,
S107731421400126X," In this paper we present the reconstructed residual error which evaluates the quality of a given segmentation of a reconstructed image in tomography . This novel evaluation method which is independent of the methods that were used to reconstruct and segment the image is applicable to segmentations that are based on the density of the scanned object . It provides a spatial map of the errors in the segmented image based on the projection data . The reconstructed residual error is a reconstruction of the difference between the recorded data and the forward projection of that segmented image . The properties and applications of the algorithm are verified experimentally through simulations and experimental micro CT data . The experiments show that the reconstructed residual error is close to the true error that it can improve gray level estimates and that it can help discriminating between different segmentations . 
",We introduce the reconstructed residual error a new segmentation evaluation measure. The method provides a spatial map of the errors in a segmented image in tomography. The original projection images are exploited in an unsupervised approach. Validation is performed through simulations and experimental micro CT data. The method can improve gray level estimates and discriminate between segmentations.,,,
S107731421500212X," In this paper a framework is proposed to localize both Farsi Arabic and Latin scene texts with different sizes fonts and orientations . First candidate text regions are extracted via an MSER detector enhanced by weighted median filtering to adopt the low resolution texts . At the same time based on fuzzy inference system the input image is classified into images with a focused text content and incidental scene text images which the image does not focus on the text content . For the focused scene text images the non text candidates are filtered via an FIS . On the other hand for the incidental scene text images apart from the FIS an extra filtering algorithm based on low rank matrix recovery is proposed . Finally a new approach based on the clustering minimum area rectangle and radon transform techniques is proposed to create the single arbitrarily oriented text lines from the remaining text regions . To evaluate the proposed algorithm we created a collection of natural images containing both Farsi Arabic and Latin texts . Compared with the state of the art methods the proposed method achieves the best performance on our and Epshtein datasets and competitive performances on the ICDAR dataset . 
",Focused and incidental scene text images are processed in a separate manner. Low rank matrix recovery is exploited to process the incidental scene text images. A text confidence map was designed via fuzzy inference system. The proposed algorithm handles both Latin and Farsi Arabic scripts. Farsi Arabic scene texts at arbitrary orientations are localized for the first time.,,,
S107731421500096X," In this paper a new pipeline of structure from motion for ground view images is proposed that uses feature points on an aerial image as references for removing accumulative errors . The challenge here is to design a method for discriminating correct matches from unreliable matches between ground view images and an aerial image . If we depend on only local image features it is not possible in principle to remove all the incorrect matches because there frequently exist repetitive and or similar patterns such as road signs . In order to overcome this difficulty we employ geometric consistency verification of matches using the RANSAC scheme that comprises two stages sampling based local verification focusing on the orientation and scale information extracted by a feature descriptor and global verification using camera poses estimated by the bundle adjustment using sampled matches . 
",A new SfM pipeline that uses aerial images as external references is proposed. Good matches between ground and aerial images are found by two stage verification. Consistency of orientation and scale from a feature descriptor is locally verified. Outliers are removed by global verification with sampling based bundle adjustment.,,,
S107731421500140X," This paper proposes methods for people re identification across non overlapping cameras . We improve the robustness of re identification by using additional group features acquired from the groups of people detected by each camera . People are grouped by discriminatively classifying the spatio temporal features of their trajectories into those of grouped people and non grouped people . Thereafter three group features are obtained in each group and utilized with other general features of each person for people re identification . Our experimental results have demonstrated improvements in people grouping and people re identification when our proposed methods have been applied to a public dataset . 
",People is grouped based on the spatio temporal features of their trajectories. From people groups new group features for people re identification are extracted. The group features can be employed with any kind of existing appearance features. Experiments demonstrated that the group features improve people re identification.,,,
S107731421400157X," In the object recognition community much effort has been spent on devising expressive object representations and powerful learning strategies for designing effective classifiers capable of achieving high accuracy and generalization . In this scenario the focus on the training sets has been historically weak by and large training sets have been generated with a substantial human intervention requiring considerable time . In this paper we present a strategy for automatic training set generation . The strategy uses semantic knowledge coming from WordNet coupled with the statistical power provided by Google Ngram to select a set of meaningful text strings related to the text class label that are subsequently fed into the Google Images search engine producing sets of images with high training value . Focusing on the classes of different object recognition benchmarks our approach collects novel training images compared to the ones obtained by exploiting Google Images with the simple text class label . In particular we show that the gathered images are better able to capture the different visual facets of a concept thus encoding in a more successful manner the intra class variance . As a consequence training standard classifiers with this data produces performances not too distant from those obtained from the classical hand crafted training sets . In addition our datasets generalize well and are stable that is they provide similar performances on diverse test datasets . This process does not require manual intervention and is completed in a few hours . 
",We present a strategy for automatic image training set generation. We use semantic and statistics to generate strings related to a target label. Our gathered images are good in capturing different visual facets of a concept.,,,
S107731421400054X," In query by semantic example image retrieval images are ranked by similarity of semantic descriptors . These descriptors are obtained by classifying each image with respect to a pre defined vocabulary of semantic concepts . In this work we consider the problem of improving the accuracy of semantic descriptors through cross modal regularization based on auxiliary text . A cross modal regularizer composed of three steps is proposed . Training images and text are first mapped to a common semantic space . A regularization operator is then learned for each concept in the semantic vocabulary . This is an operator which maps the semantic descriptors of images labeled with that concept to the descriptors of the associated texts . A convex formulation of the learning problem is introduced enabling the efficient computation of concept specific regularization operators . The third step is the selection of the most suitable operator for the image to regularize . This is implemented through a quantization of the semantic space where a regularization operator is associated with each quantization cell . Overall the proposed regularizer is a non linear mapping implemented as a piecewise linear transformation of the semantic image descriptors to regularize . This transformation is a form of cross modal domain adaptation . It is shown to achieve better performance than recent proposals in the domain adaptation literature while requiring much simpler optimization . 
",In depth literature review on related subjects. Two regularization strategies interpolation and classification. Regularization on images lacking auxiliar information. Extensive evaluation shows astonishing results on CBIR.,,,
S107731421500051X," Recently head pose estimation in real world environments has been receiving attention in the computer vision community due to its applicability to a wide range of contexts . However this task still remains as an open problem because of the challenges presented by real world environments . The focus of most of the approaches to this problem has been on estimation from single images or video frames without leveraging the temporal information available in the entire video sequence . Other approaches frame the problem in terms of classification into a set of very coarse pose bins . In this paper we propose a hierarchical graphical model that probabilistically estimates continuous head pose angles from real world videos by leveraging the temporal pose information over frames . The proposed graphical model is a general framework which is able to use any type of feature and can be adapted to any facial classification task . Furthermore the framework outputs the entire pose distribution for a given video frame . This permits robust temporal probabilistic fusion of pose information over the video sequence and also probabilistically embedding the head pose information into other inference tasks . Experiments on large real world video sequences reveal that our approach significantly outperforms alternative state of the art pose estimation methods . The proposed framework is also evaluated on gender and facial hair estimation . By incorporating pose information into the proposed hierarchical temporal graphical mode superior results are achieved for attribute classification tasks . 
",A hierarchical temporal model is used to estimate head pose in real world videos. Head pose classification in un constrained databases shows superior performance. Proposed model is used to classify facial traits in real world videos. Trait classification with and without using the estimated pose angle is performed. Facial trait classification using the proposed model show superior performance.,,,
S107731421500003X," We present a method for efficiently generating dense relative depth estimates from video without requiring any knowledge of the imaging system either a priori or by estimating it during processing . Instead we only require that the epipolar constraint between any two frames is satisfied and that the fundamental matrix can be estimated . By tracking sparse features across many frames and aggregating the multiple depth estimates together we are able to improve the overall estimate for any given frame . Once the depth estimates are available we treat the generation of the depth maps as a label propagation problem . This allows us to combine the automatically generated depth maps with any user corrections and modifications . 
",A method for generating sparse relative depth estimates from video. Depth estimates can be generated without access to past or future frames. Depth maps are created through efficient filter based propagation. Results are favourable when compared to more expensive SfM MVS techniques.,,,
S107731421400037X," Tagging is nowadays the most prevalent and practical way to make images searchable . However in reality many manually assigned tags are irrelevant to image content and hence are not reliable for applications . A lot of recent efforts have been conducted to refine image tags . In this paper we propose to do tag refinement from the angle of topic modeling and present a novel graphical model regularized latent Dirichlet allocation . In the proposed approach tag similarity and tag relevance are jointly estimated in an iterative manner so that they can benefit from each other and the multi wise relationships among tags are explored . Moreover both the statistics of tags and visual affinities of images in the corpus are explored to help topic modeling . We also analyze the superiority of our approach from the deep structure perspective . The experiments on tag ranking and image retrieval demonstrate the advantages of the proposed method . 
",A novel regularized latent Dirichlet allocation approach to tag refinement is proposed. The proposed approach can explore the multiple wise tag relations and visual relations. The tag relevance estimation using the proposed approach is interpreted in the form of the deep structure.,,,
S107731421500199X," The interdigital palm region represents about 30 of the palm area and is inherently acquired during palmprint imaging nevertheless it has not yet attracted any noticeable attention in biometrics research . This paper investigates the ridge pattern characteristics of the interdigital palm region for its usage in biometric identification . An anatomical study of the interdigital area is initially carried out leading to the establishment of five categories according to the distribution of the singularities and three regions of interest for biometrics . With the identified regions our study analyzes the matching performance of the interdigital palm biometrics and its combination with the conventional palmprint matching approaches and presents comparative experimental results using four competing feature extraction methods . This study has been carried out with two publicly available databases . The first one consists of 2 080 images of 416 subjects acquired with a touchless low cost imaging device focused on acquiring the interdigital palm area . The second database is the publicly available BiosecurID hand database which consists of 3 200 images from 400 users . The experimental results presented in this paper suggest that features from the interdigital palm region can be used to achieve competitive performances as well as offer significant improvements for conventional palmprint recognition . 
",First experimental study on the ridge pattern distribution in the interdigital palm region for biometrics. Novel classification methodology of palms according to five classes. Study of complementarity of the interdigital and traditional palm regions. Evaluation of personal recognition using interdigital region reaching EER 0.01 on database on 416 subjects . Interdigital palm region image database from 416 subjects in this paper is made publicly available.,,,
S107731421500123X," Recent developments in low cost CMOS cameras have created the opportunity of bringing imaging capabilities to sensor networks and a new field called visual sensor networks has emerged . VSNs consist of image sensors embedded processors and wireless transceivers which are powered by batteries . Since energy and bandwidth resources are limited setting up a tracking system in VSNs is a challenging problem . In this paper we present a framework for human tracking in VSN environments . The traditional approach of sending compressed images to a central node has certain disadvantages such as decreasing the performance of further processing because of low quality images . Instead in our decentralized tracking framework each camera node performs feature extraction and obtains likelihood functions . We propose a sparsity driven method that can obtain bandwidth efficient representation of likelihoods extracted by the camera nodes . Our approach involves the design of special overcomplete dictionaries that match the structure of the likelihoods and the transmission of likelihood information in the network through sparse representation in such dictionaries . We have applied our method for indoor and outdoor people tracking scenarios and have shown that it can provide major savings in communication bandwidth without significant degradation in tracking performance . We have compared the tracking results and communication loads with a block based likelihood compression scheme a decentralized tracking method and a distributed tracking method . Experimental results show that our sparse representation framework is an effective approach that can be used together with any probabilistic tracker in VSNs . 
",A decentralized approach for multi view multi person tracking in VSNs. Design overcomplete dictionaries matched to the structure of likelihoods functions. Obtain sparse representation of likelihoods to reduce communication in the network. Comparison on well known L1 solvers to choose a fast and reliable method. Qualitatively and quantitatively our framework outperforms previous approaches.,,,
S107731421500048X," Computer vision is hard because of a large variability in lighting shape and texture in addition the image signal is non additive due to occlusion . Generative models promised to account for this variability by accurately modelling the image formation process as a function of latent variables with prior beliefs . Bayesian posterior inference could then in principle explain the observation . While intuitively appealing generative models for computer vision have largely failed to deliver on that promise due to the difficulty of posterior inference . As a result the community has favoured efficient discriminative approaches . We still believe in the usefulness of generative models in computer vision but argue that we need to leverage existing discriminative or even heuristic computer vision methods . We implement this idea in a principled way with an informed sampler and in careful experiments demonstrate it on challenging generative models which contain renderer programs as their components . We concentrate on the problem of inverting an existing graphics rendering engine an approach that can be understood as Inverse Graphics . The informed sampler using simple discriminative proposals based on existing computer vision technology achieves significant improvements of inference . 
",The informed sampler a general inference technique for Bayesian posterior inference in generative models. This method leverages discriminative computer vision models for faster probabilistic inference in generative models. Three different applications that highlight common challenges of posterior inference. Detailed comparisons and analysis with respect to different baseline sampling based methods. Informed sampling is found to converge faster than all baseline samplers across diverse problems.,,,
S107731421400191X," Classical image segmentation techniques in computer vision exploit visual cues such as image edges lines color and texture . Due to the complexity of real scenarios the main challenge is achieving meaningful segmentation of the imaged scene since real objects have substantial discontinuities in these visual cues . In this paper a new focus based perceptual cue is introduced the focus signal . The focus signal captures the variations of the focus level of every image pixel as a function of time and is directly related to the geometry of the scene . In a practical application a sequence of images corresponding to an autofocus sequence is processed in order to infer geometric information of the imaged scene using the focus signal . This information is integrated with the segmentation obtained using classical cues such as color and texture in order to yield an improved scene segmentation . Experiments have been performed using different off the shelf cameras including a webcam a compact digital photography camera and a surveillance camera . Obtained results using Dice s similarity coefficient and the pixel labeling error show that a significant improvement in the final segmentation can be achieved by incorporating the information obtained from the focus signal in the segmentation process . 
",Focus has been typically exploited in computer vision as depth cue. A new focus based perceptual cue is introduced the focus signal. The focus signal corresponds to the change in focus level as a function of time. The focus signal can be integrated with classical cues for image segmentation. The proposed focus aided methodology yields improved scene segmentation.,,,
S107731421500185X,"With the rapidly increasing demands from surveillance and security industries crowd behaviour analysis has become one of the hotly pursued video event detection frontiers within the computer vision arena in recent years. This research has investigated innovative crowd behaviour detection approaches based on statistical crowd features extracted from video footages. In this paper a new crowd video anomaly detection algorithm has been developed based on analysing the extracted spatio temporal textures. The algorithm has been designed for real time applications by deploying low level statistical features and alleviating complicated machine learning and recognition processes. In the experiments the system has been proven a valid solution for detecting anomaly behaviours without strong assumptions on the nature of crowds for example subjects and density. The developed prototype shows improved adaptability and efficiency against chosen benchmark systems. 
",A real time crowd anomaly detection algorithm for video surveillance is proposed. The research has developed a spatio temporal texture model for feature extraction. A redundant texture feature space has been composed by using wavelet transform. Detection algorithm is based on 3 sigma rule which is fast and robust. The system shows improved accuracy and efficiency against many benchmark systems.,,,
S107731421400188X," With the advent of the digital camera a common popular imaging processing is high dynamic range that aims to overcome the technological limitations of the irradiance sensor dynamic range . In this paper we will present a new method to combine low dynamic range images for HDR processing . This method is based on the theory of evidence . Without a prior knowledge of the sensor intrinsic parameters and no extra data it allows to locally maximizing the signal to noise ratio over the entire acquisition dynamic . In addition our method is less sensitive to object or people in motion into the scene that are causing ghost like artifacts with the conventional methods . This technique require that the camera be absolutely still between exposures or need a translational alignment . Simulation and experimental results are presented to demonstrate both the accuracy and efficiency of our algorithm . 
",We propose a new unbiased method for HDR reconstruction based on evidence theory. Our method allows to locally minimizing the acquisition noise in the HDR image. Our method is adapted for very high dynamic range acquisition. For average user no setting and no additional information are needed. Our method is less sensitive to object or people in motion into the scene.,,,
S107731421400160X," We present a passive forensics method to distinguish photorealistic computer graphics from natural images . The goals of our work are to improve the detection accuracy and the robustness to content preserving image manipulations . In the proposed method Homomorphic filtering is used to highlight the detail information of image . We find that the texture changes are different between photographs and PRCG images under same Homomorphic filtering transformation and then we use the difference matrixes to describe the differences of texture changes . We define a customized statistical feature named texture similarity and combine it with the statistical features extracted from the co occurrence matrixes of differential matrixes to construct forensics features . Then we develop a statistical model and use SVM as classifier to distinguish PRCG from photographs . Experimental results show that the proposed method enjoys following advantages Proposed method reaches higher detection accuracy synchronously it is robust to tolerate content preserving manipulations such as JPEG compression adding noise histogram equalization and filtering . Proposed method is provided with satisfactory generalization capability it will be available when the training samples and the testing samples come from different sources . 
",A method to distinguish photographic images and photorealistic computer graphics is proposed. A customized statistical feature named texture similarity is defined. Proposed method used Homomorphic filtering to highlight the image detail information. Proposed method is robust for content preserving manipulations. Proposed method provides satisfactory detection accuracy and generalization capability.,,,
S107731421400068X," The minimum barrier distance MBD introduced recently in is a pseudo metric defined on a compact subset D of the Euclidean space and whose values depend on a fixed map f from D into . The MBD is defined as the minimal value of the barrier strength of a path between the points which constitutes the length of the smallest interval containing all values of f along the path . In this paper we present a polynomial time algorithm that provably calculates the exact values of MBD for the digital images . We compare this new algorithm theoretically and experimentally with the algorithm presented in which computes the approximate values of the MBD . Moreover we notice that every generalized distance function can be naturally translated to an image segmentation algorithm . The algorithms that fall under such category include Relative Fuzzy Connectedness and those associated with the minimum barrier fuzzy distance and geodesic distance functions . In particular we compare experimentally these four algorithms on the 2D and 3D natural and medical images with known ground truth and at varying level of noise blur and inhomogeneity . 
",Introduction of fast algorithm for the exact values of minimum barrier distance MBD. Comparison of the novel algorithm with its approximate versions. Experimental comparison MBD induced segmentations with other segmentation algorithms.,,,
S107731421500017X," Appearance model is a key part of tracking algorithms . To attain robustness many complex appearance models are proposed to capture discriminative information of object . However such models are difficult to maintain accurately and efficiently . In this paper we observe that hashing techniques can be used to represent object by compact binary code which is efficient for processing . However during tracking online updating hash functions is still inefficient with large number of samples . To deal with this bottleneck a novel hashing method called two dimensional hashing is proposed . In our tracker samples and templates are hashed to binary matrices and the hamming distance is used to measure confidence of candidate samples . In addition the designed incremental learning model is applied to update hash functions for both adapting situation change and saving training time . Experiments on our tracker and other eight state of the art trackers demonstrate that the proposed algorithm is more robust in dealing with various types of scenarios . 
",In this paper we propose a 2D based hashing method which could fast extract the binary feature of samples. We successfully apply the hashing method into tracking model by some details. We design an effective and suitable learning model to update hash functions at every frame. Comparison experiments are done to demonstrate the effectiveness and efficiency of our tracker.,,,
S107731421400112X," This paper describes a method of gait recognition by suppressing and using gait fluctuations . Inconsistent phasing between a matching pair of gait image sequences because of temporal fluctuations degrades the performance of gait recognition . We remove the temporal fluctuations by generating a phase normalized gait image sequence with equal phase intervals . If inter period gait fluctuations within a gait image sequence are repeatedly observed for the same subject they can be regarded as a useful distinguishing gait feature . We extract phase fluctuations as temporal fluctuations as well as gait fluctuation image and trajectory fluctuations as spatial fluctuations . We combine them with the matching score using the phase normalized image sequence as additional matching scores in the score level fusion framework or as quality measures in the score normalization framework . We evaluated the methods in experiments using large scale publicly available databases and showed the effectiveness of the proposed methods . 
",Introduction of the phase trajectory fluctuations and gait fluctuation image. Using gait fluctuation as a quality measure or an additional matching score. Evaluation using large scale publicly available gait databases. Suppressing and utilizing gait fluctuations improve the gait recognition performance.,,,
S107731421400040X," This paper presents a disparity calculation algorithm based on stereo vision for obstacle detection and free space calculation . This algorithm incorporates line segmentation multi pass aggregation and efficient local optimisation in order to produce accurate disparity values . It is specifically designed for traffic scenes where most of the objects can be represented by planes in the disparity domain . The accurate horizontal disparity gradient for the side planes are also extracted during the disparity optimisation stage . Then an obstacle detection algorithm based on the U V disparity is introduced . Instead of using the Hough transform for line detection which is extremely sensitive to the parameter settings the G disparity image is proposed for the detection of side planes . Then the vertical planes are detected separately after removing all the side planes . Faster detection speed lower parameter sensitivity and improved performance are achieved comparing with the Hough transform based detection . After the obstacles are located and removed from the disparity map most of the remaining pixels are projections from the road surface . Using a spline as the road model the vertical profile of the road surface is estimated . Finally the free space is calculated based on the vertical road profile which is not restricted by the planar road surface assumption . 
",We propose a disparity calculation algorithm based on multi pass aggregation and local optimisation. Disparity calculation is fast and accurate in real world scenarios. We propose the G disparity image which can be used with U V disparity for obstacle detection. Obstacle detection is more efficient and accurate. Free space calculation is simplified after obstacle detection.,,,
S107731421500137X," Would it be possible to automatically associate ancient pictures to modern ones and create fancy cultural heritage city maps We introduce here the task of recognizing the location depicted in an old photo given modern annotated images collected from the Internet . We present an extensive analysis on different features looking for the most discriminative and most robust to the image variability induced by large time lags . Moreover we show that the described task benefits from domain adaptation . 
",We introduce a new dataset where the objective is to recognize a location of an old photograph using modern digital images. We evaluate a large number of existing features and encoding methods for this task. We show that existing domain adaptation methods can help to improve results for this specific task. We further show that there is the need for further research in cross domain image retrieval task.,,,
S107731421400099X," This paper presents a multiview model of object categories generally applicable to virtually any type of image features and methods to efficiently perform in a unified manner detection localization and continuous pose estimation in novel scenes . We represent appearance as distributions of low level fine grained image features . Multiview models encode the appearance of objects at discrete viewpoints and in addition how these viewpoints deform into one another as the viewpoint continuously varies . Using a measure of similarity between an arbitrary test image and such a model at chosen viewpoints we perform all tasks mentioned above with a common method . We leverage the simplicity of low level image features such as points extracted along edges or coarse scale gradients extracted densely over the images by building probabilistic templates i.e . distributions of features learned from one or several training examples . We efficiently handle these distributions with probabilistic techniques such as kernel density estimation Monte Carlo integration and importance sampling . We provide an extensive evaluation on a wide variety of benchmark datasets . We demonstrate performance on the ETHZ Shape dataset with single and multiple training examples well above baseline methods on par with a number of more task specific methods . We obtain remarkable performance on the recognition of more complex objects notably the cars of the 3D Object dataset of Savarese et al . with detection rates of 92.5 and an accuracy in pose estimation of 91 . We perform better than the state of the art on continuous pose estimation with the rotating cars dataset of Ozuysal et al . We also demonstrate particular capabilities with a novel dataset featuring non textured objects of undistinctive shapes the pose of which can only be determined from shading captured here by coarse scale intensity gradients . This paper is concerned with the joint recognition and pose estimation of object categories in 2D images . Recognizing that these two tasks represent two sides of a same problem we tackle them in a unified approach . In general the pose of objects can not be inferred from just one type of image information e.g . silhouette and edges to cite a common example . Additional visual cues may be necessary such as the shading onto the object surface . A key point of our contributions is thus to provide techniques generally applicable in this regard even to low level dense and or non descriptive image features . To perform continuous pose estimation our object model captures in addition to the appearance at discrete training viewpoints the deformations between these detected from the optical flow between training examples . A measure of similarity between generated views of the object and a test image allows us to perform detection recognition and pose estimation in a unified manner . The following paragraphs present the principal motivations and key points of the method comparing them to existing related work . Parts of these contributions were introduced in earlier publications . The. 
",Multi view model of object categories. Suitable to any type of image features e.g. edges and coarse scale gradients here. Performs detection localization and continuous pose estimation in unified manner. Encode appearance at discrete training viewpoints and in between. Competitive with best task specific methods with framework generally applicable.,,,
S147403461500004X," Radio frequency identification technology has been used in manufacturing industries to create a RFID enabled ubiquitous environment in where ultimate real time advanced production planning and scheduling will be achieved with the goal of collective intelligence . A particular focus has been placed upon using the vast amount of RFID production shop floor data to obtain more precise and reasonable estimates of APPS parameters such as the arrival of customer orders and standard operation times . The resulting APPS model is based on hierarchical production decision making principle to formulate planning and scheduling levels . A RFID event driven mechanism is adopted to integrate these two levels for collective intelligence . A heuristic approach using a set of rules is utilized to solve the problem . The model is tested through four dimensions including the impact of rule sequences on decisions evaluation of released strategy to control the amount of production order from planning to scheduling comparison with another model and practical operations as well as model robustness . Two key findings are observed . First release strategy based on the RFID enabled real time information is efficient and effective to reduce the total tardiness by 44.46 averagely . Second it is observed that the model has the immune ability on disturbances like defects . However as the increasing of the problem size the model robustness against emergency orders becomes weak while the resistance to machine breakdown is strong oppositely . Findings and observations are summarized into a number of managerial implications for guiding associated end users for purchasing collective intelligence in practice . 
",An RFID event driven mechanism is used to integrate planning and scheduling. RFID production shopfloor data was used to obtain APPS parameters. Release strategy is efficient to reduce the total tardiness by 44.46 averagely.,,,
S107731421600014X," Recognizing scene text is a challenging problem even more so than the recognition of scanned documents . This problem has gained significant attention from the computer vision community in recent years and several methods based on energy minimization frameworks and deep learning approaches have been proposed . In this work we focus on the energy minimization framework and propose a model that exploits both bottom up and top down cues for recognizing cropped words extracted from street images . The bottom up cues are derived from individual character detections from an image . We build a conditional random field model on these detections to jointly model the strength of the detections and the interactions between them . These interactions are top down cues obtained from a lexicon based prior i.e . language statistics . The optimal word represented by the text image is obtained by minimizing the energy function corresponding to the random field model . We evaluate our proposed algorithm extensively on a number of cropped scene text benchmark datasets namely Street View Text ICDAR 2003 2011 and 2013 datasets and IIIT 5K word and show better performance than comparable methods . We perform a rigorous analysis of all the steps in our approach and analyze the results . We also show that state of the art convolutional neural network features can be integrated in our framework to further improve the recognition performance . 
",An energy minimization based approach for scene text recognition with seamless integration of multiple cues. Applied also to the challenging open vocabulary setting where a word specific lexicon is unavailable. Comprehensive experimental evaluation on several state of the art benchmarks.,,,
S136184151400190X," We propose an automated framework for predicting gestational age and neurodevelopmental maturation of a fetus based on 3D ultrasound brain image appearance . Our method capitalizes on age related sonographic image patterns in conjunction with clinical measurements to develop for the first time a predictive age model which improves on the GA prediction potential of US images . The framework benefits from a manifold surface representation of the fetal head which delineates the inner skull boundary and serves as a common coordinate system based on cranial position . This allows for fast and efficient sampling of anatomically corresponding brain regions to achieve like for like structural comparison of different developmental stages . We develop bespoke features which capture neurosonographic patterns in 3D images and using a regression forest classifier we characterize structural brain development both spatially and temporally to capture the natural variation existing in a healthy population 447 over an age range of active brain maturation . On a routine clinical dataset 187 our age prediction results strongly correlate with true GA 0.98 accurate within 6.10 days confirming the link between maturational progression and neurosonographic activity observable across gestation . Our model also outperforms current clinical methods by 4.57 days in the third trimester a period complicated by biological variations in the fetal population . Through feature selection the model successfully identified the most age discriminating anatomies over this age range as being the Sylvian fissure cingulate and callosal sulci . 
",We present a model to predict gestational age from 3D fetal brain ultrasound images. A feature based model characterizes spatial and temporal brain development. We capitalize on sonographic image patterns and clinical measures to predict age. Use of clinical measurements and neuroimage information improves age predictions. We identify the most age discriminating brain anatomies in early brain development.,,,
S107731421630025X," Common to much work on land cover classification in multispectral imagery is the use of single satellite images for training the classifiers for the different land types . Unfortunately more often than not decision boundaries derived in this manner do not extrapolate well from one image to another . This happens for several reasons most having to do with the fact that different satellite images correspond to different view angles on the earth s surface different sun angles different seasons and so on . In this paper we get around these limitations of the current state of the art by first proposing a new integrated representation for all of the images overlapping and non overlapping that cover a large geographic ROI . In addition to helping understand the data variability in the images this representation also makes it possible to create the ground truth that can be used for ROI based wide area learning of the classifiers . We use this integrated representation in a new Bayesian framework for data classification that is characterized by learning of the decision boundaries from a sampling of all the satellite data available for an entire geographic ROI probabilistic modeling of within class and between class variations as opposed to the more traditional probabilistic modeling of the feature vectors extracted from the measurement data and using variance based ML and MAP classifiers whose decision boundary calculations incorporate all of the multi view data for a geographic point if that point is selected for learning and testing . We show results with the new classification framework for an ROI in Chile whose size is roughly 10 000 square kilometers . This ROI is covered by 189 satellite images with varying degrees of overlap . We compare the classification performance of the proposed ROI based framework with the results obtained by extrapolating the decision boundaries learned from a single image to the entire ROI . Using a 10 fold cross validation test we demonstrate significant increases in the classification accuracy for five of the six land cover classes . In addition we show that our variance based Bayesian classifier outperforms a traditional Support Vector Machine based approach to classification for four out of six classes . 
",A wide area learner. Efficient sampling for training. Classify with variances.,,,
S107731421500257X," In this paper we cast multi target tracking as a dense subgraph discovering problem on the undirected relation graph of all given target hypotheses . We aim to extract multiple clusters in which each cluster contains a set of hypotheses of one particular target . In the presence of occlusion or similar moving targets or when there is no reliable evidence for the target s presence each target trajectory is expected to be fragmented into multiple tracklets . The proposed tracking framework can efficiently link such fragmented target trajectories to build a longer trajectory specifying the true states of the target . In particular a discriminative scheme is devised via learning the targets appearance models . Moreover the smoothness characteristic of the target trajectory is utilised by suggesting a smoothness tracklet affinity model to increase the power of the proposed tracker to produce persistent target trajectories revealing different targets moving paths . The performance of the proposed approach has been extensively evaluated on challenging public datasets and also in the context of team sports where team players tend to exhibit quick and unpredictable movements . Systematic experimental results conducted on a large set of sequences show that the proposed approach performs better than the state of the art trackers in particular when dealing with occlusion and fragmented target trajectory . 
",A multi target tracking is formulated as a dense subgraph discovering problem. Both local and global cues are exploited to represent the tracklet affinity model. The distinguishable appearance based models are learned for the targets.,,,
S147403461500018X," The performance of physical assets has become a major determinant success factor for urban flood control . However managing these assets is always challenging as there are a huge number of diverse assets involved which are distributed throughout the city and owned by different agencies . Aiming at improving the management efficiency of these assets and ensuring their performance this paper proposes the concept of cloud asset based on cloud computing mobile agent and various smart devices . Through hardware integration and software encapsulation cloud asset could sense its real time status adapt to varied working scenarios be controlled remotely and shared among agencies . It enables accurate real time control of every asset and thus improves the management efficiency and effectiveness . This paper first presents the concept of cloud asset with its technical architecture and then analyses the software agent model for cloud asset which is the key enabler to realize UPnP management of assets and provides mobility and intelligence for them . After that the framework of cloud asset enabled workflow management is built in which cloud asset could be easily found and dynamically invoked by different workflows . Finally a demonstrative case is provided to verify the effectiveness of cloud asset . 
",The concept of cloud asset is first proposed for urban flood control. The framework of cloud asset is worked out from both hardware and software aspects. Cloud asset enabled workflow management is presented. A case is given to verify the effectiveness of cloud asset.,,,
S138904171300034X," A growing conceptual and empirical literature is advancing the idea that language extends our cognitive skills . One of the most influential positions holds that language qua material symbols facilitates individual thought processes by virtue of its material properties . Extending upon this model we argue that language enhances our cognitive capabilities in a much more radical way the skilful engagement of public material symbols facilitates evolutionarily unprecedented modes of collective perception action and reasoning creating dialogically extended minds . We relate our approach to other ideas about collective minds and review a number of empirical studies to identify the mechanisms enabling the constitution of interpersonal cognitive systems . 
",Language is conceived as an intersubjective engagement enabling shared cognition. Experimental studies highlight ways in which language enables intersubjective informational and behavioural synergies. We therefore argue for language as skilful joint activity leading to dialogically extended minds.,,,
S107731421500274X," Monocular plenoptic cameras are slightly modified off the shelf cameras that have novel capabilities as they allow for truly passive high resolution range sensing through a single camera lens . Commercial plenoptic cameras however are presently delivering range data in non metric units which is a barrier to novel applications e.g . in the realm of robotics . In this work we revisit the calibration of focused plenoptic cameras and bring forward a novel approach that leverages traditional methods for camera calibration in order to deskill the calibration procedure and to increase accuracy . First we detach the estimation of parameters related to either brightness images or depth data . Second we present novel initialization methods for the parameters of the thin lens camera model the only information required for calibration is now the size of the pixel element and the geometry of the calibration plate . The accuracy of the calibration results corroborates our belief that monocular plenoptic imaging is a disruptive technology that is capable of conquering new markets as well as traditional imaging domains . 
",A novel method for the calibration of focused plenoptic monocular cameras is proposed. In this way the camera will deliver metric depth information instead of disparities. We detach the intrinsic camera parameters related to either brightness or depth data. We present novel initialization methods for all parameters. The accuracy is demonstrated on independent ground truth validation data.,,,
S136481521500167X," Sensors are becoming ubiquitous in everyday life generating data at an unprecedented rate and scale . However models that assess impacts of human activities on environmental and human health have typically been developed in contexts where data scarcity is the norm . Models are essential tools to understand processes identify relationships associations and causality formalize stakeholder mental models and to quantify the effects of prevention and interventions . They can help to explain data as well as inform the deployment and location of sensors by identifying hotspots and areas of interest where data collection may achieve the best results . We identify a paradigm shift in how the integration of models and sensors can contribute to harnessing Big Data and more importantly make the vital step from Big Data to Big Information . In this paper we illustrate current developments and identify key research needs using human and environmental health challenges as an example . 
",Sensors and models play vital roles in harnessing Big Data to extract information. Data analytics can help to diminish monitoring burden and support locating sensors. Exploring Big Data is essential to detect universal associations across space and time. Ethical challenges and issues of standards and harmonisation need to be addressed. Citizen science needs robust sensors and models to crowd source and interpret data.,,,
S147403461300075X," This paper proposes a new extended Process to Product Modeling method for integrated and seamless information delivery manual and model view definition development . Current IDM development typically uses Business Process Modeling Notation to represent a process map . Exchange requirements and functional parts specify the information required when information is exchanged between different activities . A set of information requirements specifically defined as a subset of Industry Foundation Classes is called an MVD . Currently however PMs ERs FPs and MVDs are developed as separate documents through independent development steps . Moreover even though ERs and FPs are designed to be reused tracking and reusing the ERs and FPs developed by others is practically impossible . The xPPM method is proposed to provide a tight connection between PMs ERs FPs and MVDs and to improve the reusability of predefined ERs and FPs . The theoretical framework is based on the approach of the Georgia Tech Process to Product Modeling to suit the IDM development process . An xPPM tool is developed and the validity of xPPM is analyzed through the reproduction of existing IDMs and MVDs . The benefits and limitations of xPPM and lessons from the applicability tests are discussed . 
",An extended Process to Product Modeling xPPM method has been proposed. xPPM is for efficient integrated and seamless IDM and MVD development. The applicability of xPPM is analyzed through reproduction of existing IDMs. The problems of the current IDM MVD development are discussed. The benefits and limitations of xPPM and lessons are discussed.,,,
S147403461300027X," Modeling the energy performance of existing buildings enables quick identification and reporting of potential areas for building retrofit . However current modeling practices of using energy simulation tools do not model the energy performance of buildings at their element level . As a result potential retrofit candidates caused by construction defects and degradations are not represented . Furthermore due to manual modeling and calibration processes their application is often time consuming . Current application of 2D thermography for building diagnostics is also facing several challenges due to a large number of unordered and non geo tagged images . To address these limitations this paper presents a new computer vision based method for automated 3D energy performance modeling of existing buildings using thermal and digital imagery captured by a single thermal camera . First using a new image based 3D reconstruction pipeline which consists of Graphic Processing Unit based Structure from Motion and Multi View Stereo algorithms the geometrical conditions of an existing building is reconstructed in 3D . Next a 3D thermal point cloud model of the building is generated by using a new 3D thermal modeling algorithm . This algorithm involves a one time thermal camera calibration deriving the relative transformation by forming the Epipolar geometry between thermal and digital images and the MVS algorithm for dense reconstruction . By automatically superimposing the 3D building and thermal point cloud models 3D spatio thermal models are formed which enable the users to visualize query and analyze temperatures at the level of 3D points . The underlying algorithms for generating and visualizing the 3D spatio thermal models and the 3D registered digital and thermal images are presented in detail . The proposed method is validated for several interior and exterior locations of a typical residential building and an instructional facility . The experimental results show that inexpensive digital and thermal imagery can be converted into ubiquitous reporters of the actual energy performance of existing buildings . The proposed method expedites the modeling process and has the potential to be used as a rapid and robust building diagnostic tool . 
",Image based 3D modeling method of actual building energy performance is presented. Building environments and their energy performances are jointly visualized in 3D. Automatically 3D registered thermal images assist with localizing energy problems. 3D spatio thermal models can facilitate energy building diagnostics and retrofit analysis.,,,
S136184151400187X," We propose a framework for the robust and fully automatic segmentation of magnetic resonance brain images called Multi Atlas Label Propagation with Expectation Maximisation based refinement . The presented approach is based on a robust registration approach highly performant label fusion and intensity based label refinement using EM . We further adapt this framework to be applicable for the segmentation of brain images with gross changes in anatomy . We propose to account for consistent registration errors by relaxing anatomical priors obtained by multi atlas propagation and a weighting scheme to locally combine anatomical atlas priors and intensity refined posterior probabilities . The method is evaluated on a benchmark dataset used in a recent MICCAI segmentation challenge . In this context we show that MALP EM is competitive for the segmentation of MR brain scans of healthy adults when compared to state of the art automatic labelling techniques . To demonstrate the versatility of the proposed approach we employed MALP EM to segment 125 MR brain images into 134 regions from subjects who had sustained traumatic brain injury . We employ a protocol to assess segmentation quality if no manual reference labels are available . Based on this protocol three independent blinded raters confirmed on 13 MR brain scans with pathology that MALP EM is superior to established label fusion techniques . We visually confirm the robustness of our segmentation approach on the full cohort and investigate the potential of derived symmetry based imaging biomarkers that correlate with and predict clinically relevant variables in TBI such as the Marshall Classification or Glasgow Outcome Score . Specifically we show that we are able to stratify TBI patients with favourable outcomes from non favourable outcomes with 64.7 accuracy using acute phase MR images and 66.8 accuracy using follow up MR images . Furthermore we are able to differentiate subjects with the presence of a mass lesion or midline shift from those with diffuse brain injury with 76.0 accuracy . The thalamus putamen pallidum and hippocampus are particularly affected . Their involvement predicts TBI disease progression . 
",A robust and fully automatic segmentation framework for MR brain scans is proposed. A heterogeneous cohort of 125 scans of patients who had sustained TBI is segmented. The approach compares favourably to the state of the art on benchmark and TBI data. MRI based biomarkers correlate with outcome relevant clinical variables in TBI. Evidence that subcortical structures are particularly affected in TBI is presented.,,,
S107731421600076X," Natural and intuitive human interaction with robotic systems is a key point to develop robots assisting people in an easy and effective way . In this paper a Human Robot Interaction system able to recognize gestures usually employed in human non verbal communication is introduced and an in depth study of its usability is performed . The system deals with dynamic gestures such as waving or nodding which are recognized using a Dynamic Time Warping approach based on gesture specific features computed from depth maps . A static gesture consisting in pointing at an object is also recognized . The pointed location is then estimated in order to detect candidate objects the user may refer to . When the pointed object is unclear for the robot a disambiguation procedure by means of either a verbal or gestural dialogue is performed . This skill would lead to the robot picking an object in behalf of the user which could present difficulties to do it by itself . The overall system which is composed by a NAO and Wifibot robots a KinectTM v2 sensor and two laptops is firstly evaluated in a structured lab setup . Then a broad set of user tests has been completed which allows to assess correct performance in terms of recognition rates easiness of use and response times . 
",We present a multi robot human interaction system with two robots and a depth sensor. It includes a static and dynamic gestures recognition module. The set of gestures is described using arm body and facial head features. Interactive disambiguation for floor and object detection based on pointed location. Tested with several real users as well as with an offline test setting.,,,
S107731421630008X," Background estimation in video consists in extracting a foreground free image from a set of training frames . Moving and stationary objects may affect the background visibility thus invalidating the assumption of many related literature where background is the temporal dominant data . In this paper we present a temporal spatial block level approach for background estimation in video to cope with moving and stationary objects . First a Temporal Analysis module obtains a compact representation of the training data by motion filtering and dimensionality reduction . Then a threshold free hierarchical clustering determines a set of candidates to represent the background for each spatial location . Second a Spatial Analysis module iteratively reconstructs the background using these candidates . For each spatial location multiple reconstruction hypotheses are explored to obtain its neighboring locations by enforcing inter block similarities and intra block homogeneity constraints in terms of color discontinuity color dissimilarity and variability . The experimental results show that the proposed approach outperforms the related state of the art over challenging video sequences in presence of moving and stationary objects . 
",We propose a new temporal spatial block based Background estimation approach to compute a foreground free image for video sequences. Threshold free clustering is proposed to discover similar blocks over time which contain the background data. An iterative spatial reconstruction selects blocks to obtain the final background. The performance improvement is validated in two datasets 36 sequences using 13 state of the art algorithms.,,,
S136481521400139X," Graphical abstract Situation 1 An engineer has an incomplete dataset about the quantity and quality of the influent wastewater . She has some ideas about how the influent of the plant should be . She might also have some data about the catchment area . Situation 2 An engineer has information enough about the quantity and quality of the influent wastewater . She would like to translate that into some ASM family model state variables . This is the so called characterisation problem . Situation 3 The engineer has information enough about the quantity and quality of the influent wastewater . However that is only a single realisation of the problem . She would like to feature the uncertainty around these data and to generate other similar influent profiles . a Is there any method to interpolate values so as to increase the frequency of some given WWTP influent data profiles Yes the method of Devisscher et al . based on the use of databases or the ones of Langergraber et al . or Manina et al . based on the use of harmonic functions can help to complete some scarce datasets . Also the phenomenological model of Gernaey et al . has been used with this propose . b Is there any tool to derive influent WWTP data given some properties about the catchment area or the distribution of emission sources Yes the generator of Gernaey er al . provides influent data for urban WWTPs given the population and catchment area characteristics . The generator of De Keyser et al . provides emission profiles expected from a given urban population . The former describes the effect of soil first flush sewer system etc . while the later is only concerned about the very source emission patterns providing profiles that can be used as inputs for sewer WWTP or surface water models . c Are there some benchmark influent data profiles or simple models to generate influent data for urban WWTPs Influent profiles for a WWTP of 100 000 PE are available at http www.benchmarkwwtp.org including dry rain and storm weather conditions . The models of Langergraber et al . or Mannina et al . provide influent profiles for dry weather conditions . The former is accompanied by a set of parameters among which the modeller can select those corresponding to the desired size of tin WWTP under study . d Which would be the main references or tools to improve the understanding of the generating mechanisms in a given catchment area and their relationship with the WWTP influent data Interesting results can be found in the research work conducted by Butler et al . Friedler et al . or Almeida et al . about domestic wastewater generation . This research has been followed by the EU project ScorePP and the work of Ort et al . analysing the most general patterns of daily weekly and yearly emissions . Another good manner of proceed is to use the phenomenological model of. 
",The literature provides a wide range of solutions for WWTP influent data generation. The challenge consists of selecting the most suitable solution for each situation. QRAs requires of generating influent data under hypothetical uncertain conditions. The phenomenological modelling integrates knowledge about the generating mechanisms. More detailed descriptions of the components in the catchment area are required.,,,
S136481521630113X," Graphical abstract MODSIM MODFLOW MODSIM MODFLOW is the integration of two widely used and freely available codes used within the field of water resource planning and management . The nature of the coupling in MODSIM MODFLOW sets it apart from other river operations hydrologic model couplings in that the codes iterate at the time step level and share information via computer memory . Users are required to construct working MODSIM and MODFLOW models prior to using the integrated code and must adhere to each code s input file formatting requirements . Using the custom code interface available with MODSIM the models are integrated by mapping surface water features represented in both models to one another . Eric Morway with MODFLOW contributions from Rich Niswonger and MODSIM contributions from Enrique Triana Research supported by grant from the Water Sustainability Climate Program jointly funded by the National Science Foundation and U.S. Department of Agriculture National Institute of Food Agriculture and by the U.S. Geological Survey s Groundwater Resources Program . C C Fortran Windows with.NET Framework 3.5 installed Download MODSIM from http modsim.engr.colostate.edu . A version of MODFLOW compiled as dynamic link library and required by the integrated code is available upon request from the corresponding author . Finally users will need to download and install the C redistributable package for Microsoft Visual Studio 2012 available from Microsoft . Inevitably as each generation must learn the land and the waters will instruct us in the ways of community . Justice Gregory J. Hobbs Jr . 
",The river reservoir operations model MODSIM is coupled to MODFLOW. Models simulate physics based hydrology and priority rule based water allocation. Coupling uses a Picard iteration and an implicit mass conservative solution. Model simulates conjunctive use of surface water and groundwater. Helps evaluate groundwater sustainability resulting from alternative management.,,,
S109332631530067X," Humic substances are ubiquitous in the environment and have manifold functions . While their composition is well known information on the chemical structure and three dimensional conformation is scarce . Here we describe the Vienna Soil Organic Matter Modeler which is an online tool to generate condensed phase computer models of humic substances . Many different models can be created that reflect the diversity in composition and conformations of the constituting molecules . To exemplify the modeler 18 different models are generated based on two experimentally determined compositions to explicitly study the effect of varying e.g . the amount of water molecules in the models or the pH . Molecular dynamics simulations were performed on the models which were subsequently analyzed in terms of structure interactions and dynamics linking macroscopic observables to the microscopic composition of the systems . We are convinced that this new tool opens the way for a wide range of in silico studies on soil organic matter . 
",We developed an online tool to generate models of Soil Organic Matter SOM . Molecular dynamics simulations of SOM models are now possible. Influence of composition on structure interactions and dynamics is studied. Our work opens the way to new and exciting research in soil sciences.,,,
S136481521400108X," Simulation modelling in ecology is a field that is becoming increasingly compartmentalized . Here we propose a Database Approach To Modelling to create unity in dynamical ecosystem modelling with differential equations . In this approach the storage of ecological knowledge is independent of the language and platform in which the model will be run . To create an instance of the model the information in the database is translated and augmented with the language and platform specifics . This process is automated so that a new instance can be created each time the database is updated . We describe the approach using the simple Lotka Volterra model and the complex ecosystem model for shallow lakes PCLake which we automatically implement in the frameworks OSIRIS GRIND for MATLAB ACSL R DUFLOW and DELWAQ . A clear advantage of working in a database is the overview it provides . The simplicity of the approach only adds to its elegance . 
",Scientific and educational experience with the proposed Database Approach To Modelling DATM shows the following It facilitated overview of and insight in the model by developers and users. Allowed for a much more dynamic scientific development of the model. Allowed for a direct implementation of these developments in multiple platforms.,,,
S136481521500047X," Predicting the probability of wind damage in both natural and managed forests is important for understanding forest ecosystem functioning the environmental impact of storms and for forest risk management . We undertook a thorough validation of three versions of the hybrid mechanistic wind risk model ForestGALES and a statistical logistic regression model against observed damage in a Scottish upland conifer forest following a major storm . Statistical analysis demonstrated that increasing tree height and local wind speed during the storm were the main factors associated with increased damage levels . All models provided acceptable discrimination between damaged and undamaged forest stands but there were trade offs between the accuracy of the mechanistic models and model bias . The two versions of the mechanistic model with the lowest bias gave very comparable overall results at the forest scale and could form part of a decision support system for managing forest wind damage risk . Maximum width of canopy Length of the live crown Drag coefficient scale parameter Drag coefficient Regression between stem weight and resistance to overturning Critical wind speed for damage Zero plane displacement Stem diameter at base of tree Stem diameter at breast height Average spacing between trees Windiness score from Quine and White Dimensionless factor to account for additional turning moment due to crown and stem weight Dimensionless factor to account for reduction in clear wood MOR due to knots Dimensionless factor to account for gustiness of wind Tree height von Karman constant 0.4 Maximum turning moment due to wind loading only and not including additional moment due to overhanging crown and stem Modulus of rupture on wood for species of interest Parameter controlling reduction in drag coefficient with wind speed Density of air Forestry Commission sub compartment database Ratio of average tree spacing after and before a thinning Stem weight Turning moment coefficient from Hale et al . Ratio of turning moment coefficient after and before thinning Wind speed at 10 m above the zero plane displacement height Wind speed at tree height Friction velocity Weibull scale parameter Weibull shape parameter Wind speed calculated from DAMS score Wind speed calculated from WAsP airflow model Wind speed at meteorological station Distance from forest edge Yield class Aerodynamic roughness Name of software ForestGALES Developers Forest Research and INRA Contact address Forest Research Northern Research Station Roslin Midlothian EH25 9SY United Kingdom Email forestgales.support @ forestry.gsi.gov.uk Availability and Online Documentation The software along with supporting material is freely available . Go to http www.forestresearch.gov.uk forestgales to find out how to obtain the software or email forestgales.support @ forestry.gsi.gov.uk Year first available 2000 Hardware required IBM compatible PC Software required MS Windows Programming language Borland Delphi 5.0 . Versions have also been written in Python Fortran R and Java . Contact the corresponding author for further details . Program size 10 MB . With all additional support files and manuals 25 MB . 
",Comprehensive description and validation of three versions of a mechanistic wind risk model for even aged forests. Analysis of the performance of mechanistic and logistic regression models against measured damage in a conifer forest. All models provided acceptable discrimination between damaged and undamaged forest stands. The two version of the mechanistic model with the lowest bias gave very comparable overall results at the forest scale. Statistical analysis showed that increasing tree height and local wind speed were the main factors associated with damage.,,,
S147403461400038X," The design process of mechatronic devices which involves experts from different disciplines working together has limited time and resource constraints . These experts normally have their own domain specific designing methods and tools which can lead to incompatibilities when one needs to work together using these those methods and tools . Having a proper framework which integrates different design tools is of interest as such a framework can prevent incompatibilities between parts during the design process . In this paper we propose our co modelling methodology and co simulation tools integration framework which helps to maintain the domain specific properties of the model components during the co design process of various mechatronic devices . To avoid expensive rework later in the design phase and even possible system failure fault modelling and a layered structure with fault tolerance mechanisms for the controller software are introduced . In the end a practical mechatronic device is discussed to illustrate the methods and tools which are presented in this paper in details . 
",A co modelling methodology supported by a co simulation tool frame work is proposed. Fault modelling and a layered structure with fault tolerance mechanisms for the controller software are introduced. The introduced methods and tools are demonstrated on a practical mechatronic device.,,,
S136481521400022X," This systematic review considers how water quality and aquatic ecology models represent the phosphorus cycle. Although the focus is on phosphorus many of the observations and discussion points here relate to aquatic ecosystem models in general. The review considers how models compare across domains of application the degree to which current models are fit for purpose how to choose between multiple alternative formulations and how models might be improved. Lake and marine models have been gradually increasing in complexity with increasing emphasis on inorganic processes and ecosystems. River models have remained simpler but have been more rigorously assessed. Processes important in less eutrophic systems have often been neglected these include the biogeochemistry of organic phosphorus transformations associated with fluxes through soils and sediments transfer rate limited phosphorus uptake and responses of plants to pulsed nutrient inputs. Arguments for and against increasing model complexity physical and physiological realism are reviewed. 
",The treatment of phosphorus in aquatic models is systematically reviewed. The complexity of lake river and marine models is increasing over time. Catchment river models tend to be simpler than lake and marine models. Performance assessment of lake and marine models is generally inadequate. Processes not included in models are discussed.,,,
S136481521400379X," Graphical abstract The noise model was implemented in R to call functions from PostgreSQL and GRASS GIS packages and can be obtained from the corresponding author or the following website http www.sahsu.org content data download first available in July 2014 TRANEX requires at least one standard desktop PC . 
",Adaptation of the Calculation of Road Traffic Noise method for exposure assessment. Freely available open source software R with PostgreSQL and GRASS GIS . Model estimates compared well to noise measurements r 0.85 0.95 . Noise level exposures modelled for 8.61 million London residents 2003 2010 . Over 1 million residents exposed to high daytime and night time noise levels.,,,
S147403461500052X," The purpose of this research is to develop a formal knowledge e discovery methodology using advanced information technology and decision support analysis to define legal case evolution based on Collective Litigation Intelligence . In this research a decade of Australia s retail franchise and trademark litigation cases are used as the corpus to analyze and synthesize the evolution of modern retail franchise law in Australia . The formal processes used in the legal e discovery research include a LexisNexis search strategy to collect legal documents text mining to find key concepts and their representing key phrases in the documents clustering algorithms to associate the legal cases into groups and concept lattice analysis to trace the evolutionary trends of the main groups . The case analysis discovers the fundamental issues for retail modernization advantages and disadvantages of retail franchising systems and the potential litigation hazards to be avoided in the Australian market . Given the growing number of legal documents in global court systems this research provides a systematic and generalized CLI methodology to improve the efficiency and efficacy of research across international legal systems . In the context of the case study the results demonstrate the critical importance of quickly processing and interpreting existing legal knowledge using the CLI approach . For example a brand management company which purchases a successful franchise in one market is under limited time constraints to evaluate the legal environment across global markets of interest . The proposed CLI methodology can be applied to derive market entry strategies to secure growth and brand expansion of a global franchise . 
",E discovery to define legal evolution using Collective Litigation Intelligence. A scientific approach to improve legal practice litigation research. Derives market entry strategies for global franchise brand expansion.,,,
S147692711500033X," In order to elucidate some basic principles for protein ligand interactions a subset of 87 structures of human proteins with their ligands was obtained from the PDB databank . After a short molecular dynamics simulation a variety of interaction energies and structural parameters were extracted . Linear regression was performed to determine which of these parameters have a potentially significant contribution to the protein ligand interaction . The parameters exhibiting relatively high correlation coefficients were selected . Important factors seem to be the number of ligand atoms the ratio of N O and S atoms to total ligand atoms the hydrophobic polar aminoacid ratio and the ratio of cavity size to the sum of ligand plus water atoms in the cavity . An important factor also seems to be the immobile water molecules in the cavity . Nine of these parameters were used as known inputs to train a neural network in the prediction of seven other . Eight structures were left out of the training to test the quality of the predictions . After optimization of the neural network the predictions were fairly accurate given the relatively small number of structures especially in the prediction of the number of nitrogen and sulfur atoms of the ligand . 
",Molecular dynamics simulations were run on PDB structures containing protein and ligand. Interaction and structural parameters were extracted from the structures. Linear regression was used to check for correlation between these parameters. A neural network NN was used to predict ligand design parameters based on the protein. The NN performance improved when tweaking the protein structural parameters used.,,,
S147692711530089X," Bacteria are increasingly resistant to existing antibiotics which target a narrow range of pathways . New methods are needed to identify targets including repositioning targets among distantly related species . We developed a novel combination of systems and structural modeling and bioinformatics to reposition known antibiotics and targets to new species . We applied this approach to Mycoplasma genitalium a common cause of urethritis . First we used quantitative metabolic modeling to identify enzymes whose expression affects the cellular growth rate . Second we searched the literature for inhibitors of homologs of the most fragile enzymes . Next we used sequence alignment to assess that the binding site is shared by M. genitalium but not by humans . Lastly we used molecular docking to verify that the reported inhibitors preferentially interact with M. genitalium proteins over their human homologs . Thymidylate kinase was the top predicted target and piperidinylthymines were the top compounds . Further work is needed to experimentally validate piperidinylthymines . In summary combined systems and structural modeling is a powerful tool for drug repositioning . 
",We combined systems and structural modeling to repurpose antibiotics for new hosts. We applied our novel approach to the infectious bacterium Mycoplasma genitalium. Our method suggests that thymidylate kinase is a good potential drug target. Our method suggests that piperidinylthymines are good potential lead compounds. Combined systems and structural modeling is a powerful tool for drug repositioning.,,,
S147403461500110X," Kansei evaluation plays a vital role in the implementation of Kansei engineering however it is difficult to quantitatively evaluate customer preferences of a product s Kansei attributes as such preferences involve human perceptual interpretation with certain subjectivity uncertainty and imprecision . An effective Kansei evaluation requires justifying the classification of Kansei attributes extracted from a set of collected Kansei words establishing priorities for customer preferences of product alternatives with respect to each attribute and synthesizing the priorities for the evaluated alternatives . Moreover psychometric Kansei evaluation systems essentially require dealing with Kansei words . This paper presents a Kansei evaluation approach based on the technique of computing with words . The aims of this study were to classify collected Kansei words into a set of Kansei attributes by using cluster analysis based on fuzzy relations to model Kansei preferences based on semantic labels for the priority analysis and to synthesize priority information and rank the order of decision alternatives by means of the linguistic aggregation operation . An empirical study is presented to demonstrate the implementation process and applicability of the proposed Kansei evaluation approach . The theoretical and practical implications of the proposed approach are also discussed . 
",This paper presents a Kansei evaluation approach based on the technique of computing with words. Kansei preferences are modeled by positively worded items with 7 levels of semantic labels. Fuzzy relation based clustering is used to extract a set of Kansei attributes from collected Kansei words. A cluster validation index is proposed to assist evaluators in determining the best number of clusters. Linguistic aggregation is used to synthesize Kansei priority information and rank the order of product alternatives.,,,
S147692711530061X," As a pivotal domain within envelope protein fusion peptide plays a crucial role in pathogenicity and therapeutic intervention . Taken into account the limited FP annotations in NCBI database and absence of FP prediction software it is urgent and desirable to develop a bioinformatics tool to predict new putative FPs in retroviruses . In this work a sequence based FP model was proposed by combining Hidden Markov Method with similarity comparison . The classification accuracies are 91.97 and 92.31 corresponding to 10 fold and leave one out cross validation . After scanning sequences without FP annotations this model discovered 53 946 np FPs . The statistical results on FPs or np FPs reveal that FP is a conserved and hydrophobic domain . The FP software programmed for windows environment is available at https sourceforge.net projects fptool files source navbar . 
",A novel computational model for predicting fusion peptide of retroviruses was proposed. A software tool named FP predict.exe has been developed. A large number of new putative FPs of five typical retroviruses were predicted. Property motif and evolutionary relationship about FP were computed and discussed.,,,
S147692711530092X," Motivation Protein fold space is a conceptual framework where all possible protein folds exist and ideas about protein structure function and evolution may be analyzed . Classification of protein folds in this space is commonly achieved by using similarity indexes and or machine learning approaches each with different limitations . Results We propose a method for constructing a compact vector space model of protein fold space by representing each protein structure by its residues local contacts . We developed an efficient method to statistically test for the separability of points in a space and showed that our protein fold space representation is learnable by any machine learning algorithm . Availability An API is freely available at https code.google.com p pyrcc . 
",We implemented a vectorial representation of residues contacts We implemented an efficient statistical test for machine learnable data Our vectorial model reproduces protein packing A predictor is trained to effectively reproduce CATH and SCOP classifications Our predictor automatically identified inconsistent classification in CATH and SCOP,,,
S147403461500021X," The collection and analysis of data on the three dimensional as built status of large scale civil infrastructure whether under construction newly put into service or in operation has been receiving increasing attention on the part of researchers and practitioners in the civil engineering field . Such collection and analysis of data is essential for the active monitoring of production during the construction phase of a project and for the automatic 3D layout of built assets during their service lives . This review outlines recent research efforts in this field and technological developments that aim to facilitate the analysis of 3D data acquired from as built civil infrastructure and applications of such data not only to the construction process per se but also to facility management in particular to production monitoring and automated layout . This review also considers prospects for improvement and addresses challenges that can be expected in future research and development . It is hoped that the suggestions and recommendations made in this review will serve as a basis for future work and as motivation for ongoing research and development . 
",Access to reliable 3D as built data is a critical issue in civil infrastructure. Applications to production monitoring and automated layout are discussed. Research on applications of as built data in the civil engineering field is reviewed. State of the art and other developments in as built data analysis are surveyed. Unsolved problems and challenges for future improvements in this field are discussed.,,,
S147692711300087X," In this paper the physical and chemical characteristics biological structure and function of a non specific nuclease from Yersinia enterocolitica subsp . palearctica found in our group were studied using multiple bioinformatics approaches . The results showed that Y. NSN had 283 amino acids a weight of 30 692.5ku and a certain hydrophilic property . Y. NSN had a signal peptide no transmembrane domains and disulphide bonds . Cleavage site in Y. NSN was between pos . 23 and 24 . The prediction result of the secondary structure showed Y. NSN was a coil structure based protein . The ratio of helix folded and random coil were 18.73 16.96 and 64.31 respectively . Active sites were pos . 124 125 127 157 165 and 169 . Mg2 binding site was pos . 157 . Substrate binding sites were pos . 124 125 and 169 . The analysis of multisequencing alignment and phylogenetic tree indicated that Y. NSN shared high similarity with the nuclease from Y. enterocolitica subsp . enterocolitica 8081 . The enzyme activity results showed that Y. NSN was a nuclease with good thermostability . 
",It is the first time to study the biological structures and function of Y. NSN using bioinformatics methods. Y. NSN shared high similarity with the nuclease from Yersinia enterocolitica subsp. enterocolitica 8081. Y. NSN showed good thermostability.,,,
S147692711300056X," Reptin functions in a wide range of biological processes including chromatin remodelling nucleolar organization and transcriptional regulation of WNT signalling . As catenin dependent transcriptional repression and activation events involve binding of Reptin and histone deacetylase 1 to APPL endocytic proteins this complex has become an important target to identify molecules governing endocytic processes and WNT signalling . Here we describe the structural basis of APPL binding to Reptin to explore their mode of binding in context with APPL1 APPL2 dimerization . There is an evidence that both PH and BAR domains of APPL proteins exhibit alternately conserved regions involved in hetero dimerization process and our in silico data also corroborate this fact . Moreover APPL2PH domain binds to the BAR domain region encompassing a nuclear localization signal . We conclude that APPLPH binding to BAR domain and Reptin is mutually exclusive which regulates the nucleocytoplasmic shuttling of Reptin . Furthermore Reptin is unable to bind with membrane associated APPL proteins . These observations were further expanded by experimental approaches where we identified a novel point mutation D316N lying in the APPL1PH domain which resulted in a significantly reduced binding with Reptin . By luciferase assays we observed that overexpression of APPL1D316N and APPL1WT stimulated catenin TCF dependent transcriptional activity in a similar manner which suggested that binding of Reptin to APPL1 is not necessary for catenin dependent target gene expression . Overall our data attempt to highlight a comparative role of APPL proteins in controlling catenin dependent transcription mechanism which may improve our understanding of gene regulation . 
",We study the structural basis of APPL and Reptin binding in context with APPL1 APPL2 dimerization. We perform a detailed in silico analysis using the knowledge of APPL1 2 heterodimerization and infer that binding of APPLPH to BAR domain and Reptin is mutually exclusive which may facilitate Reptin in nucleocytoplasmic shuttling. We report new insights into the distinctive characterization of APPL catenin Reptin HDACs ternary complex in terms of target gene regulation and transcriptional mechanisms.,,,
S147692711400108X," Background Bacillus anthracis is a gram positive spore forming rod shaped bacteria which is the etiologic agent of anthrax cutaneous pulmonary and gastrointestinal . A recent outbreak of anthrax in a tropical region uncovered natural and in vitro resistance against penicillin ciprofloxacin quinolone due to over exposure of the pathogen to these antibiotics . This fact combined with the ongoing threat of using B. anthracis as a biological weapon proves that the identification of new therapeutic targets is urgently needed . Methods In this computational approach various databases and online based servers were used to detect essential proteins of B. anthracis A0248 . Protein sequences of B. anthracis A0248 strain were retrieved from the NCBI database which was then run in CD hit suite for clustering . NCBI BlastP against the human proteome and similarity search against DEG were done to find out essential human non homologous proteins . Proteins involved in unique pathways were analyzed using KEGG genome database and PSORTb CELLO v.2.5 ngLOC these three tools were used to deduce putative cell surface proteins . Results Successive analysis revealed 116 proteins to be essential human non homologs among which 17 were involved in unique metabolic pathways and 28 were predicted as membrane associated proteins . Both types of proteins can be exploited as they are unlikely to have homologous counterparts in the human host . Conclusion Being human non homologous these proteins can be targeted for potential therapeutic drug development in future . Targets on unique metabolic and membrane bound proteins can block cell wall synthesis bacterial replication and signal transduction respectively . 
",We identified potential drug target for Bacillus anthracis A0248 strain. We selected non human homolog essential proteins involved in metabolic pathways. PSORTb ngLOC and CELLO are used to predict membrane bound proteins. 17 proteins in unique pathways and 28 membrane bound proteins are identified. These findings from this current study will pave the way for further extensive in wet lab experiments and in that way assisting to drug design against anthrax.,,,
S147692711400053X," Inferring transcriptional regulatory interactions between transcription factors and their targets has utmost importance for understanding the complex regulatory mechanisms in cellular system . In this paper we introduced a computational method to predict regulatory interactions in Arabidopsis based on gene expression data and sequence information . Support vector machine and Jackknife cross validation test were employed to perform our method on a collected dataset including 178 positive samples and 1068 negative samples . Results showed that our method achieved an overall accuracy of 98.39 with the sensitivity of 94.88 and the specificity of 93.82 which suggested that our method can serve as a potential and cost effective tool for predicting regulatory interactions in Arabidopsis . 
",SVM is explored to predict regulatory interactions in Arabidopsis. Experimentally validated regulatory relationships were collected as the positive samples. Negative training samples were randomly selected TF target pairs under some strategies. Each gene pair was represented by incorporating the expression data and sequence information. Through the jackknife test our method reached an overall accuracy of 98.39 with the sensitivity of 94.88 and the specificity of 93.82 .,,,
S147692711500050X," A short partial sequence of 28 amino acids is all the information we have so far about the putative allergen 2S albumin from almond . The aim of this work was to analyze this information using mainly bioinformatics tools in order to verify its rightness . Based on the results reported in the paper describing this allergen from almond we analyzed the original data of amino acids sequencing through available software . The degree of homology of the almond 12kDa protein with any other known 2S albumin appears to be much lower than the one reported in the paper that firstly described it . In a publicly available cDNA library we discovered an expressed sequence tag which translation generates a protein that perfectly matches both of the sequencing outputs described in the same paper . A further analysis indicated that the latter protein seems to belong to the vicilin superfamily rather than to the prolamin one . The fact that also vicilins are seed storage proteins known to be highly allergenic would explain the IgE reactivity originally observed . Based on our observations we suggest that the IgE reactive 12kDa protein from almond currently known as Pru du 2S albumin is in reality the cleaved N terminal region of a 7S vicilin like protein . 
",The current reported sequence of the almond 2S albumin is a partial 28 aa peptide. The translation into protein of an almond EST sequence matches the partial peptide. The in silico generated aa sequence is a member of the vicilin superfamily. The currently known almond 2S albumin is rather a part of a 7S vicilin like protein.,,,
S147692711530075X," Although image based phenotypic assays are considered a powerful tool for siRNA library screening the reproducibility and biological implications of various image based assays are not well characterized in a systematic manner . Here we compared the resolution of high throughput assays of image based cell count and typical cell viability measures for cancer samples . It was found that the optimal plating density of cells was important to obtain maximal resolution in both types of assays . In general cell counting provided better resolution than the cell viability measure in diverse batches of siRNAs . In addition to cell count diverse image based measures were simultaneously collected from a single screening and showed good reproducibility in repetitions . They were classified into a few functional categories according to biological process based on the differential patterns of hit prioritization from the same screening data . The presented systematic analyses of image based parameters provide new insight to a multitude of applications and better biological interpretation of high content cell based assays . 
",Image based multiplexing assay was attempted for siRNA library screening. Diverse orthogonal phenotypic parameters were retrieved from a single screening. Image based cell count provided better resolution than the viability measure. Geneset analysis revealed biological implications of various image based parameters.,,,
S147692711500002X," Mammalian target of rapamycin a key mediator of PI3K Akt mTOR signaling pathway has recently emerged as a compelling molecular target in glioblastoma . The mTOR is a member of serine threonine protein kinase family that functions as a central controller of growth proliferation metabolism and angiogenesis but its signaling is dysregulated in various human diseases especially in certain solid tumors including the glioblastoma . Here considering that there are various kinase inhibitors being approved or under clinical or preclinical development it is expected that some of them can be re exploited as new potent agents to target mTOR for glioblastoma therapy . To achieve this a synthetic pipeline that integrated molecular grafting consensus scoring virtual screening kinase assay and structure analysis was described to systematically profile the binding potency of various small molecule inhibitors deposited in the protein kinase inhibitor database against the kinase domain of mTOR . Consequently a number of structurally diverse compounds were successfully identified to exhibit satisfactory inhibition profile against mTOR with IC50 values at nanomolar level . In particular few sophisticated kinase inhibitors as well as a flavonoid myricetin showed high inhibitory activities which could thus be considered as potential lead compounds to develop new potent selective mTOR inhibitors . Structural examination revealed diverse nonbonded interactions such as hydrogen bonds hydrophobic forces and van der Waals contacts across the complex interface of mTOR with myricetin conferring both stability and specificity for the mTOR inhibitor binding . 
",A protocol is described to graft inhibitors from their cognate kinases to non cognate mTOR. The grafted inhibitor mTOR affinity is virtually evaluated using a consensus scoring strategy. A number of identified inhibitors are assayed to determine their inhibition against mTOR. Diverse nonbonded interactions are found at mTOR inhibitor complex interface.,,,
S147692711300090X," To analyze the evolutionary dynamics of a mutant population in an evolutionary experiment it is necessary to sequence a vast number of mutants by high throughput sequencing technologies which enable rapid and parallel analysis of multikilobase sequences . However the observed sequences include many errors of base call . Therefore if next generation sequencing is applied to analysis of a heterogeneous population of various mutant sequences it is necessary to discriminate between true bases as point mutations and errors of base call in the observed sequences and to subject the sequences to error correction processes . To address this issue we have developed a novel method of error correction based on the Potts model and a maximum a posteriori probability estimate of its parameters corresponding to the true sequences . Our method of error correction utilizes the quality scores which are assigned to individual bases in the observed sequences and the neighborhood relationship among the observed sequences mapped in sequence space . The computer experiments of error correction of artificially generated sequences supported the effectiveness of our method showing that 50 90 of errors were removed . Interestingly this method is analogous to a probabilistic model based method of image restoration developed in the field of information engineering . 
",Sequences analyzed by next generation sequencing contain many errors. We proposed a method of error correction of heterogeneous sequences. We performed a computer experiment of error correction. We confirmed the effectiveness of our method.,,,
S147692711300039X," The modulation of the properties and function of cell membranes by small volatile substances is important for many biomedical applications . Despite available experimental results molecular mechanisms of action of inhalants and organic solvents such as acetone on lipid membranes remain not well understood . To gain a better understanding of how acetone interacts with membranes we have performed a series of molecular dynamics simulations of a POPC bilayer in aqueous solution in the presence of acetone whose concentration was varied from 2.8 to 11.2mol . The MD simulations of passive distribution of acetone between a bulk water phase and a lipid bilayer show that acetone favors partitioning into the water free region of the bilayer located near the carbonyl groups of the phospholipids and at the beginning of the hydrocarbon core of the lipid membrane . Using MD umbrella sampling we found that the permeability barrier of 0.5kcal mol exists for acetone partitioning into the membrane . In addition a Gibbs free energy profile of the acetone penetration across a bilayer demonstrates a favorable potential energy well of 3.6kcal mol located at 15 16 from the bilayer center . The analysis of the structural and dynamics properties of the model membrane revealed that the POPC bilayer can tolerate the presence of acetone in the concentration range of 2.8 5.6mol . The accumulation of the higher acetone concentration of 11.2mol results however in drastic disordering of phospholipid packing and the increase in the membrane fluidity . The acetone molecules push the lipid heads apart and hence act as spacers in the headgroup region . This effect leads to the increase in the average headgroup area per molecule . In addition the acyl tail region of the membrane also becomes less dense . We suggest therefore that the molecular mechanism of acetone action on the phospholipid bilayer has many common features with the effects of short chain alcohols DMSO and chloroform . 
",Acetone favors partitioning into the water free region of the bilayer. The bilayer can tolerate the presence of acetone in the concentration range 2.8 5.6mol . Drastic disordering of phospholipid packing was observed above the critical acetone concentration.,,,
S147692711300042X," An attempt was made to develop a computational model based on artificial neural network and ant colony optimization to estimate the composition of medium components for maximizing the productivity of Penicillin G Acylase enzyme from Escherichia coli DH5 strain harboring the plasmid pPROPAC . As a first step an artificial neural network model was developed to predict the PGA activity by considering the concentrations of seven important components of the medium . Design of experiments employing central composite design technique was used to obtain the training samples . In the second step ant colony optimization technique for continuous domain was employed to maximize the PGA activity by finding the optimal inputs for the developed ANN model . Further the effect of a combination of ant colony optimization for continuous domain with a preferential local search strategy was studied to analyze the performance . For a comparative study the training samples were fed into the response surface methodology optimization software to maximize the PGA production . The obtained PGA activity by the proposed approach was found to be higher than that of the obtained value with the response surface methodology . The optimum solution obtained computationally was experimentally verified . The observed PGA activity exhibited a close agreement with the model predictions . 
",A computational model to maximize Penicillin G Acylase production is proposed. Bioprocess is modeled using ANN and optimized using ACO. Effect of applying local search techniques within global ACO process is analyzed. Computationally obtained optimized solution was successfully verified in the lab. Proposed method can be employed for modeling and optimization of other bioprocesses.,,,
S147692711530058X," Gene silencing is an important function as it keeps newly acquired foreign DNA repressed thereby avoiding possible deleterious effects in the host organism . Known transcriptional regulators associated with this process are called xenogeneic silencers and belong to either the H NS Lsr2 MvaT or Rok families . In the work described here we looked for XS like regulators and their distribution in prokaryotic organisms was evaluated . Our analysis showed that putative XS regulators similar to H NS Lsr2 MvaT or Rok are present only in bacteria . This does not exclude the existence of alternative XS in the rest of the organisms analyzed . Additionally of the four XS groups evaluated in this work those from the H NS family have diversified more than the other groups . In order to compare the distribution of these putative XS regulators we also searched for other nucleoid associated proteins not included in this group such as Fis EbfC YbaB HU IHF and Alba . Results showed that NAPs from the Fis EbfC YbaB HU IHF and Alba families are widely distributed among prokaryotes . These NAPs were found in multiple combinations with or without XS like proteins . In regard with XS regulators results showed that only XS proteins from one family were found in those organisms containing them . This suggests specificity for this type of regulators and their corresponding genomes . 
",Xenogeneic silencer like regulators were searched in fully sequenced prokaryote genomes. Putative xenogeneic silencer regulators are limited to some bacteria and are specific for each division. Xenogeneic silencer regulators from the H NS family are the most diversified. Other nucleoid associated proteins were also searched and their distribution was also assessed. Our search updates and extends xenogeneic silencer regulators database. Results showed that xenogeneic silencer like regulators are specific for the bacteria they are found in and exclusive as members of the other families are excluded.,,,
S147692711400111X," Reconstructions of genome scale metabolic networks from different organisms have become popular in recent years . Metabolic engineering can simulate the reconstruction process to obtain desirable phenotypes . In previous studies optimization algorithms have been implemented to identify the near optimal sets of knockout genes for improving metabolite production . However previous works contained premature convergence and the stop criteria were not clear for each case . Therefore this study proposes an algorithm that is a hybrid of the ant colony optimization algorithm and flux balance analysis to predict near optimal sets of gene knockouts in an effort to maximize growth rates and the production of certain metabolites . Here we present a case study that uses Baker s yeast also known as Saccharomyces cerevisiae as the model organism and target the rate of vanillin production for optimization . The results of this study are the growth rate of the model organism after gene deletion and a list of knockout genes . The ACOFBA algorithm was found to improve the yield of vanillin in terms of growth rate and production compared with the previous algorithms . 
",We proposed a hybrid algorithm to increase metabolites production. The hybrid algorithm select best combination of gene knockout. Using 2 microorganisms shows increment in metabolites production.,,,
S147692711500047X," Naturally inspired evolutionary algorithms prove effectiveness when used for solving feature selection and classification problems . Artificial Bee Colony is a relatively new swarm intelligence method . In this paper we propose a new hybrid gene selection method namely Genetic Bee Colony algorithm . The proposed algorithm combines the used of a Genetic Algorithm along with Artificial Bee Colony algorithm . The goal is to integrate the advantages of both algorithms . The proposed algorithm is applied to a microarray gene expression profile in order to select the most predictive and informative genes for cancer classification . In order to test the accuracy performance of the proposed algorithm extensive experiments were conducted . Three binary microarray datasets are use which include colon leukemia and lung . In addition another three multi class microarray datasets are used which are SRBCT lymphoma and leukemia . Results of the GBC algorithm are compared with our recently proposed technique mRMR when combined with the Artificial Bee Colony algorithm . We also compared the combination of mRMR with GA and Particle Swarm Optimization algorithms . In addition we compared the GBC algorithm with other related algorithms that have been recently published in the literature using all benchmark datasets . The GBC algorithm shows superior performance as it achieved the highest classification accuracy along with the lowest average number of selected genes . This proves that the GBC algorithm is a promising approach for solving the gene selection problem in both binary and multi class cancer classification . 
",We improved the ABC algorithm by adding a uniform crossover operation in the onlooker phase. We increased the number of scout bees to two. We adopted a mutation operation during the replacement process at the scout bee phase.,,,
S147692711300114X," Cse1p and Xpot are two karyopherin proteins that transport the corresponding cargos during the nucleocytoplasmic transport . We utilized Elastic Network Model and Finite Element Analysis to study their conformational dynamics . These dynamics were interpreted by their intrinsic modes that played key roles in the flexibility of karyopherins which further affected the binding affinities . The findings included that it was the karyopherin s versatile conformations composed of the same superhelices of HEAT repeats that produced different degrees of functional flexibilities . We presented evidence that these coarse grained methods could help to elucidate the biological function behind the structures of the two karyopherins . 
",The intrinsic dynamics of exportins were inherent from their cargo bound and cargo free conformations. Dominant modes were found to be relevant to the functional flexibilities and the binding affinities. ENM and FEA were complementary methods to study conformational dynamics of proteins.,,,
S147692711400125X," Organisms thriving at extreme cold surroundings are called as psychrophiles and they present a wealth of knowledge about sequence adjustments in proteins that had occurred during the adaptation to low temperatures . In this paper we propose a new cascading model to investigate the basis for psychrophilicity . In this model a superior classifier was used to discriminate psychrophilic from mesophilic protein sequences and then the PART rule generating algorithm was applied on the input instances that are correctly classified by the classifier to generate human interpretable rules . These derived rules were further validated on a structural dataset and finally analyzed to discover the underlying biological basis about the psychrophilicity . In this study we have used one of the key features of psychrophilic proteins accountable for remaining functional in extreme cold temperature surroundings i.e . global patterns of amino acid composition as the input features . The rotation forest classifier outperformed all the other classifiers with maximum accuracy of 70.5 and maximum AUC of 0.78 . The effect of sequence length on the classification accuracy was also investigated . The analysis of the derived rules and interpretation of the analyzed results had revealed some interesting phenomena such as the amino acids A D G F and S are over represented and T is under represented in psychrophilic proteins . These findings augment the existing domain knowledge for psychrophilic sequence features . 
",Enhanced classification of psychrophilic proteins by rotation forest. Rule extraction from correctly classified sequences. Validation of generated rules on structural data. Biological interpretation of rules. Ranking of amino acids according to their discriminative ability.,,,
S147692711400067X," Receptor like kinase is an important member in protein kinase family which is widely involved in plant growth development and defense responses . It is significant to analyze the kinase structure and evolution of pollen RLKs in order to study their mechanisms . In our study 64 and 73 putative pollen RLKs were chosen from maize and Arabidopsis . Phylogenetic analysis showed that the pollen RLKs were conservative and might had existed before divergence between monocot and dicot which were mainly concentrated in RLCK VII and LRR III two subfamilies . Chromosomal localization and gene duplication analysis showed the expansion of pollen RLKs were mainly caused by segmental duplication . By calculating Ka Ks value of extracellular domain intracellular domain and kinase domain in pollen RLKs we found that the pollen RLKs duplicated genes had mainly experienced the purifying selection while maize might have experienced weaker purifying selection . Meanwhile extracellular domain might have experienced stronger diversifying selection than intracellular domain in both species . Estimation of duplication time showed that the duplication events of Arabidopsis have occurred approximately between 18 and 69 million years ago compared to 0.67 170 million years ago of maize . 
",64 and 73 putative pollen RLKs chosen from maize and Arabidopsis can be divided into 8 subfamilies. Expansion of pollen RLKs were mainly caused by segmental duplication. Maize might have experienced weaker purifying selection as compare to Arabidopsis. Duplication events of Arabidopsis and maize are between 18 69 million years and 0.67 170 million years ago respectively.,,,
S152407031500020X," Continuous collision detection is a key technique to meet non penetration requirements in many applications . Even though it is possible to perform efficient culling operations in the broad stage of a continuous collision detection algorithm such as bounding volume hierarchies a huge number of potentially colliding triangles still survive and go to the succeeding narrow stage . This heavily burdens the elementary collision tests in a collision detection algorithm and affects the performance of the entire pipeline especially for fast moving or deforming objects . This paper presents a low cost filtering algorithm using algebraic analysis techniques . It can significantly reduce the number of elementary collision tests that occur in the narrow stage . We analyze the root existence during the time interval for a standard cubic equation defining an elementary collision test . We demonstrate the efficiency of the algebraic filter in our experiments . Cubic solvers augmented by our filtering algorithm are able to achieve up to 99 filtering ratios and more than 10 performance improvement against the standard cubic solver without any filters . 
",We present a low cost filtering algorithm using algebraic analysis techniques. Our algorithm can significantly reduce the number of elementary collision tests that occur in the narrow stage of continuous collision detection. We demonstrated that cubic solvers augmented by our filtering algorithm are able to achieve up to 99. We observed more than ten times performance improvement against the standard cubic solver without any filters.,,,
S152407031400040X," We present algorithms for computing the differential geometry properties of lines of curvature of parametric surfaces . We derive a unit tangent vector curvature vector binormal vector torsion and algorithms to evaluate their higher order derivatives of lines of curvature of parametric surfaces . Among these quantities it is shown that the curvature and its first derivative of the lines of curvature lend a hand for the formation of curved plates in shipbuilding . We also visualize the twist of lines of curvature which enables us to observe how much the osculating plane of the line of curvature turns about the tangent vector . 
",We examine the differential geometry properties of the lines of curvature. We visualize the twist of line of curvature using its Frenet frame. We apply our technique to flatten doubly curved plates used in shipbuilding. We apply our technique to thin plate freeform fabrication.,,,
S147692711630072X," In recent years computer aided redesigning methods based on genome scale metabolic network models have played important roles in metabolic engineering studies however most of these methods are hindered by intractable computing times . In particular methods that predict knockout strategies leading to overproduction of desired biochemical are generally unable to do high level prediction because the computational time will increase exponentially . In this study we propose a new framework named IdealKnock which is able to efficiently evaluate potentials of the production for different biochemical in a system by merely knocking out pathways . In addition it is also capable of searching knockout strategies when combined with the OptKnock or OptGene framework . Furthermore unlike other methods IdealKnock suggests a series of mutants with targeted overproduction which enables researchers to select the one of greatest interest for experimental validation . By testing the overproduction of a large number of native metabolites IdealKnock showed its advantage in successfully breaking through the limitation of maximum knockout number in reasonable time and suggesting knockout strategies with better performance than other methods . In addition gene reaction relationship is well considered in the proposed framework . 
",IdealKnock can be employed to efficiently identify excellent knockout strategies for the overproduction of various products. IdealKnock breaks through the common bottleneck of knockout number limitation and gene reaction relationship is well considered. The knockout strategies given by IdealKnock are generally robust which means the maximum and minimum production rate are close.,,,
S147692711530133X," Genome wide association studies and other genetic analyses have identified a large number of genes and variants implicating a variety of disease etiological mechanisms . It is imperative for the study of human diseases to put these genetic findings into a coherent functional context . Here we use system biology tools to examine disease connections of five master genes for CD4 T cell subtypes . We compiled a list of genes functionally interacting with the master genes then we surveyed the disease connections either by experimental evidence or by genetic association . Embryonic lethal genes are over represented in master genes and their interacting genes . Transcription factors are significantly enriched among genes interacting with the master genes . Predicted haploinsufficiency is a feature of most these genes . Disease connected genes are enriched in this list of genes 42 of these genes have a disease connection according to Online Mendelian Inheritance in Man and 74 are associated with some diseases or phenotype in a Genome Wide Association Study . Seemingly not all of the diseases connected to genes surveyed were immune related which may indicate pleiotropic functions of the master regulator genes and associated genes . 
",CD4 T cell subtype master genes and their connected genes are more likely to be associated with a disease or a phenotype. Genes connected to the CD4 T cell subtype master genes are more likely to be transcription factors. CD4 T cell subtype master genes and their connected genes are more likely to be haploinsufficient. CD4 T cell subtype master genes and their connected genes are more likely to be embryonic lethal gene essential genes .,,,
S153204641300110X," Efficient identification of patient intervention comparison and outcome components in medical articles is helpful in evidence based medicine . The purpose of this study is to clarify whether first sentences of these components are good enough to train naive Bayes classifiers for sentence level PICO element detection . We extracted 19 854 structured abstracts of randomized controlled trials with any P I O label from PubMed for naive Bayes classifiers training . Performances of classifiers trained by first sentences of each section and those trained by all sentences were compared using all sentences by ten fold cross validation . The results measured by recall precision and F measures show that there are no significant differences in performance between CF and CA for detection of O element . However CA perform better for I elements in terms of recall and F measures . For P elements CF have higher precision but lower recall . CF are not always better than CA in sentence level PICO element detection . Their performance varies in detecting different elements . 
",Two sets of naive Bayes classifiers were developed for PICO detection. We trained one set with first sentences and the other with all sentences. The first sentence classifier performs slightly better for patient P elements. The all sentence classifier performs better for intervention I elements. The performances are about the same for outcome O elements.,,,
S147784241500041X," In a reconfigurable system the response to contextual or internal change may trigger reconfiguration events which on their turn activate scripts that change the system s architecture at runtime . To be safe however such reconfigurations are expected to obey the fundamental principles originally specified by its architect . This paper introduces an approach to ensure that such principles are observed along reconfigurations by verifying them against concrete specifications in a suitable logic . Architectures reconfiguration scripts and principles are specified in Archery an architectural description language with formal semantics . Principles are encoded as constraints which become formulas of a two layer graded hybrid logic where the upper layer restricts reconfigurations and the lower layer constrains the resulting configurations . Constraints are verified by translating them into logic formulas which are interpreted over models derived from Archery specifications of architectures and reconfigurations . Suitable notions of bisimulation and refinement to which the architect may resort to compare configurations are given and their relationship with modal validity is discussed . 
",Describes approach to ensure fundamental principles of a system in reconfigurations. Specifies principles as constraints in an architectural description language. Translates constraints into a two layer graded hybrid logic. Derives interpretation models from specifications of architectures and reconfigurations. Provides equivalence and refinement notions to compare reconfigurations.,,,
S152407031500003X," Given a set of symmetric antisymmetric filter vectors containing only regular multiresolution filters the method we present in this article can establish a balanced multiresolution scheme for images allowing their balanced decomposition and subsequent perfect reconstruction without the use of any extraordinary boundary filters . We define balanced multiresolution such that it allows balanced decomposition i.e . decomposition of a high resolution image into a low resolution image and corresponding details of equal size . Such a balanced decomposition makes on demand reconstruction of regions of interest efficient in both computational load and implementation aspects . We find this balanced decomposition and perfect reconstruction based on an appropriate combination of symmetric antisymmetric extensions near the image and detail boundaries . In our method exploiting such extensions correlates to performing sample split operations . Our general approach is demonstrated for some commonly used symmetric antisymmetric multiresolution filters . We also show the application of such a balanced multiresolution scheme in real time focus context visualization . 
",Devised balanced multiresolution BMR schemes allow balanced decomposition. Constructed balanced wavelet transform BWT allows perfect reconstruction. BWT provides efficient access to previously extracted details on demand. We eliminate the need for using extraordinary boundary filters. BMR schemes use symmetric antisymmetric extensions at image and detail boundaries.,,,
S153204641300049X," Purpose Despite years of effort and millions of dollars spent to create unified electronic communicable disease reporting systems the goal remains elusive . A major barrier has been a lack of understanding by system designers of communicable disease work and the public health workers who perform this work . This study reports on the application of user centered design representations traditionally used for improving interface design to translate the complex CD work identified through ethnographic studies to guide designers and developers of CD systems . The purpose of this work is to better understand public health practitioners and their information workflow with respect to CD monitoring and control at a local health agency and to develop evidence based design representations that model this CD work to inform the design of future disease surveillance systems . Methods We performed extensive onsite semi structured interviews targeted work shadowing and a focus group to characterize local health agency CD workflow . Informed by principles of design ethnography and user centered design we created persona scenarios and user stories to accurately represent the user to system designers . Results We sought to convey to designers the key findings from ethnographic studies public health CD work is mobile and episodic in contrast to current CD reporting systems which are stationary and fixed health agency efforts are focused on CD investigation and response rather than reporting and current CD information systems must conform to public health workflow to ensure their usefulness . In an effort to illustrate our findings to designers we developed three contemporary design support representations persona scenario and user story . Conclusions Through application of user centered design principles we were able to create design representations that illustrate complex public health communicable disease workflow and key user characteristics to inform the design of CD information systems for public health . 
",We modeled communicable disease CD workflow at health department using user centered design. Public health CD work is mobile and episodic yet CD reporting systems are stationary and fixed. Health department efforts are focused on CD investigation and response rather than reporting. Current CD information systems do not conform to PH workflow thus affecting their usefulness. Personas scenarios and user stories provide evidence based representations for designers.,,,
S147784241300016X," Polymorphic programming languages have been adapted for constructing distributed access control systems where a program represents a proof of eligibility according to a given policy . As a security requirement it is typically stated that the programs of such languages should satisfy noninterference . However this property has not been defined and proven semantically . In this paper we first propose a semantics based on Henkin models for a predicative polymorphic access control language based on lambda calculus . A formal semantic definition of noninterference is then proposed through logical relations . We prove a type soundness theorem which states that any well typed program of our language meets the noninterference property defined in this paper . In this way it is guaranteed that access requests from an entity do not interfere with those from unrelated or more trusted entities . 
",A denotational semantics based on Henkin models is given for a predicative polymorphic calculus of access control. Noninterference as a basic semantic notion of security is semantically defined. It is proven that the language is type sound in the sense that every welltyped program of the language satisfies noninterference.,,,
S147784241500055X," We propose a language independent symbolic execution framework for languages endowed with a formal operational semantics based on term rewriting . Starting from a given definition of a language a new language definition is generated with the same syntax as the original one but whose semantical rules are transformed in order to rewrite over logical formulas denoting possibly infinite sets of program states . Then the symbolic execution of concrete programs is by definition the execution of the same programs with the symbolic semantics . We prove that the symbolic execution thus defined has the properties naturally expected from it . A prototype implementation of our approach was developed in the framework . We demonstrate the tool s genericity by instantiating it on several languages and illustrate it on the reachability analysis and model checking of several programs . 
",We propose a language independent symbolic execution framework for languages. The proposed language independent approach is based on language transformations. We prove that the expected formal properties of symbolic execution. We present a prototype and we use it on several languages.,,,
S152407031400023X," In this paper we present an efficient approach for parameterizing a genus zero triangular mesh onto the sphere with an optimal radius in an as rigid as possible manner which is an extension of planar ARAP parametrization approach to spherical domain . We analyze the smooth and discrete ARAP energy and formulate our spherical parametrization energy from the discrete ARAP energy . The solution is non trivial as the energy involves a large system of non linear equations with additional spherical constraints . To this end we propose a two step iterative algorithm . In the first step we adopt a local global iterative scheme to calculate the parametrization coordinates . In the second step we optimize a best approximate sphere on which parametrization triangles can be embedded in a rigidity preserving manner . Our algorithm is simple robust and efficient . Experimental results show that our approach provides almost isometric spherical parametrizations with lowest rigidity distortion over state of the art approaches . 
",We develop model of spherical parameterization in a rigidity preserving manner. We analyze ARAP energy from smooth description to its corresponding discrete case. We propose a iterative algorithm to efficiently solve the non linear model.,,,
S152407031300060X," Intrinsic shape matching has become the standard approach for pose invariant correspondence estimation among deformable shapes . Most existing approaches assume global consistency . While global isometric matching is well understood only a few heuristic solutions are known for partial matching . Partial matching is particularly important for robustness to topological noise which is a common problem in real world scanner data . We introduce a new approach to partial isometric matching based on the observation that isometries are fully determined by local information a map of a single point and its tangent space fixes an isometry . We develop a new representation for partial isometric maps based on equivalence classes of correspondences between pairs of points and their tangent spaces . We apply our approach to register partial point clouds and compare it to the state of the art methods where we obtain significant improvements over global methods for real world data and stronger guarantees than previous partial matching algorithms . 
",We characterize partial isometries of shapes by single point maps up to first order. A novel matching algorithm for partial intrinsic matching. We use redundancy in our representation for approximate partial iso metric matching. Robustness to strong topological noise geometric noise and missing data.,,,
S153204641300052X," In this paper we discuss the design and development of TRAK an ontology that formally models information relevant for the rehabilitation of knee conditions . TRAK provides the framework that can be used to collect coded data in sufficient detail to support epidemiologic studies so that the most effective treatment components can be identified new interventions developed and the quality of future randomized control trials improved to incorporate a control intervention that is well defined and reflects clinical practice . TRAK follows design principles recommended by the Open Biomedical Ontologies Foundry . TRAK uses the Basic Formal Ontology as the upper level ontology and refers to other relevant ontologies such as Information Artifact Ontology Ontology for General Medical Science and Phenotype And Trait Ontology . TRAK is orthogonal to other bio ontologies and represents domain specific knowledge about treatments and modalities used in rehabilitation of knee conditions . Definitions of typical exercises used as treatment modalities are supported with appropriate illustrations which can be viewed in the OBO Edit ontology editor . The vast majority of other classes in TRAK are cross referenced to the Unified Medical Language System to facilitate future integration with other terminological sources . TRAK is implemented in OBO a format widely used by the OBO community . TRAK is available for download from http www.cs.cf.ac.uk trak . In addition its public release can be accessed through BioPortal where it can be browsed searched and visualized . 
",TRAK ontology models domain specific knowledge about the rehabilitation of knee conditions. TRAK provides terminology definitions and graphical illustrations of knee treatments. TRAK supports shared understanding of standard care in knee rehabilitation. TRAK provides the framework that can be used to collect data for epidemiologic studies of knee conditions. TRAK can support randomized control with a control intervention that is well defined and reflects clinical practice.,,,
S147784241500007X," In this paper we compose six different Python and Prolog VMs into 4 pairwise compositions one using C interpreters one running on the JVM one using meta tracing interpreters and one using a C interpreter and a meta tracing interpreter . We show that programs that cross the language barrier frequently execute faster in a meta tracing composition and that meta tracing imposes a significantly lower overhead on composed programs relative to mono language programs . 
",We present a viable Python Prolog composition and show four different implementations each using a different composition style. We present the first experiment designed to help understand the effects of different composition styles upon performance. We thoroughly analyse and discuss the results of the experiment breaking down the impact that each composition style has on performance.,,,
S147784241600004X," The actor model is a message passing concurrency model that avoids deadlocks and low level data races by construction . This facilitates concurrent programming especially in the context of complex interactive applications where modularity security and fault tolerance are required . The tradeoff is that the actor model sacrifices expressiveness and safety guarantees with respect to parallel access to shared state . In this paper we present domains as a set of novel language abstractions for safely encapsulating and sharing state within the actor model . We introduce four types of domains namely immutable isolated observable and shared domains that each is tailored to a certain access pattern on that shared state . The domains are characterized with an operational semantics . For each we discuss how the actor model s safety guarantees are upheld even in the presence of conceptually shared state . Furthermore the proposed language abstractions are evaluated with a case study in Scala comparing them to other synchronization mechanisms to demonstrate their benefits in deadlock freedom parallel reads and enforced isolation . 
",The domain model is an extension to the communicating event loop actor model. The domain model retains the safety and liveness properties of the actor model. We provide an operational semantics and validation of the domain model.,,,
S147784241500024X," The Trapezoid Step Functions domain is introduced in order to approximate continuous functions by a finite sequence of trapezoids adopting linear functions to abstract the upper and the lower bounds of a continuous variable in each time slot . The lattice structure of TSF is studied showing how to build and compute a sound abstraction of a given continuous function . Experimental results underline the effectiveness of the approach in terms of both precision and efficiency with respect to the domain of Interval Valued Step Functions . 
",The domain of Trapezoid Step Functions is introduced for the static analysis on continuous functions values. The domain is a proper refinement of the Interval Valued Step Function Domain. A constructive abstraction procedure is provided that deals with floating point precision issues.,,,
S153204641300083X," Objective To report on the results of a review concerning the use of mobile phones for health with older adults . Methods PubMed and CINAHL were searched for articles using older adults and mobile phones along with related terms and synonyms between 1965 and June 2012 . Identified articles were filtered by the following inclusion criteria original research project utilizing a mobile phone as an intervention involve target adults 60years of age or older and have an aim emphasizing the mobile phone s use in health . Results Twenty one different articles were found and categorized into ten different clinical domains including diabetes activities of daily life and dementia care among others . The largest group of articles focused on diabetes care followed by COPD Alzheimer s dementia Care and osteoarthritis . Areas of interest studied included feasibility acceptability and effectiveness . While there were many different clinical domains the majority of studies were pilot studies that needed more work to establish a stronger base of evidence . Conclusions Current work in using mobile phones for older adult use are spread across a variety of clinical domains . While this work is promising current studies are generally smaller feasibility studies and thus future work is needed to establish more generalizable stronger base of evidence for effectiveness of these interventions . 
",We review the use of mobile phones for health applied to older adults. The field emerging field contains many feasibility studies with smaller samples. A variety of clinical domains are appropriate for mobile phone interventions. Future work should address generalizability and establish a stronger evidence base.,,,
S153204641300107X," Although biomedical information available in articles and patents is increasing exponentially we continue to rely on the same information retrieval methods and use very few keywords to search millions of documents . We are developing a fundamentally different approach for finding much more precise and complete information with a single query using predicates instead of keywords for both query and document representation . Predicates are triples that are more complex datastructures than keywords and contain more structured information . To make optimal use of them we developed a new predicate based vector space model and query document similarity function with adjusted tf idf and boost function . Using a test bed of 107 367 PubMed abstracts we evaluated the first essential function retrieving information . Cancer researchers provided 20 realistic queries for which the top 15 abstracts were retrieved using a predicate based and keyword based approach . Each abstract was evaluated double blind by cancer researchers on a 0 5 point scale to calculate precision and relevance . Precision was significantly higher for the predicate based than for the keyword based approach . Relevance was almost doubled with the predicate based approach 2.1 versus 1.6 without rank order adjustment and 1.34 versus 0.98 with rank order adjustment for predicate versus keyword based approach respectively . Predicates can support more precise searching than keywords laying the foundation for rich and sophisticated information search . 
",Predicates called triples contain more structured information than keywords. We propose a new predicate based search engine approach for biomedical texts. We develop a new vector space model and query document similarity function. It achieves higher precision and relevance score than traditional keyword approach. Predicates can support more precise information search than keywords.,,,
S147784241530004X," Most of today s embedded systems are very complex . These systems controlled by computer programs continuously interact with their physical environments through network of sensory input and output devices . Consequently the operations of such embedded systems are highly reactive and concurrent . Since embedded systems are deployed in many safety critical applications where failures can lead to catastrophic events an approach that combines mathematical logic and formal verification is employed in order to ensure correct behavior of the control algorithm . This paper presents What You Prove Is What You Execute compilation strategy for a Globally Asynchronous Locally Synchronous programming language called Safey Critical SystemJ . SC SystemJ is a safety critical subset of the SystemJ language . A formal big step transition semantics of SC SystemJ is developed for compiling SC SystemJ programs into propositional Linear Temporal Logic formulas . These LTL formulas are then converted into a network of Mealy automata using a novel and efficient compilation algorithm . The resultant Mealy automata have a straightforward syntactic translation into Promela code . The resultant Promela models can be used for verifying correctness properties via the SPIN model checker . Finally there is a single translation procedure to compile both Promela and C Java code for execution which satisfies the De Bruijn index i.e . this final translation step is simple enough that is can be manually verified . 
",Introduction of safety critical subset of the SystemJ language called Safety Critical SC SystemJ. Automata based compilation approach for the SC SystemJ language. A tool chain for verifying correctness properties e.g. liveness and safety of the SC SystemJ programs and generating executable from the verified code for deployment. The new compiler generates both faster and smaller executable compared to the original SystemJ compiler.,,,
S147784241530021X," This paper presents the design implementation and applications of a software testing tool TAO which allows users to specify and generate test cases and oracles in a declarative way . Extended from its previous grammar based test generation tool TAO provides a declarative notation for defining denotational semantics on each productive grammar rule such that when a test case is generated its expected semantics will be evaluated automatically as well serving as its test oracle . TAO further provides a simple tagging mechanism to embed oracles into test cases for bridging the automation between test case generation and software testing . Two practical case studies are used to illustrate how automated oracle generation can be effectively integrated with grammar based test generation in different testing scenarios locating fault inducing input patterns on Java applications and Selenium based automated web testing . 
",TAO provides a declarative framework for automated test and oracle generation. TAO integrates test and oracle generation as a whole to promote better automated software testing. TAO is the first attempt to apply the methodology of denotational semantics in test and oracle generation.,,,
S147692711530236X," The chaetognaths constitute a small and enigmatic phylum of little marine invertebrates . Both nuclear and mitochondrial genomes have numerous originalities some phylum specific . Until recently their mitogenomes seemed containing only one tRNA gene but a recent study found in two chaetognath mitogenomes two and four tRNA genes . Moreover apparently two conspecific mitogenomes have different tRNA gene numbers . Reanalyses by tRNAscan SE and ARWEN softwares of the five available complete chaetognath mitogenomes suggest numerous additional tRNA genes from different types . Their total number never reaches the 22 found in most other invertebrates using that genetic code . Predicted error compensation between codon anticodon mismatch and tRNA misacylation suggests translational activity by tRNAs predicted solely according to secondary structure for tRNAs predicted by tRNAscan SE not ARWEN . Numbers of predicted stop suppressor tRNAs coevolve with predicted overlapping frameshifted protein coding genes including stop codons . Sequence alignments in secondary structure prediction with non chaetognath tRNAs suggest that the most likely functional tRNAs are in intergenic regions as regular mt tRNAs . Due to usually short intergenic regions generally tRNA sequences partially overlap with flanking genes . Some tRNA pairs seem templated by sense antisense strands . Moreover 16S rRNA genes but not 12S rRNAs appear as tRNA nurseries as previously suggested for multifunctional ribosomal like protogenomes . 
",Chaetognath mitogenomes exhibit more than one tRNA like sequence as previously suggested. 16S rRNA genes appear as chaetognath tRNA nurseries. TRNA pairs seem templated by sense antisense strands as previously suggested for multifunctional ribosomal like protogenomes.,,,
S153204641400094X," Background and purpose Poor device design that fails to adequately account for user needs cognition and behavior is often responsible for use errors resulting in adverse events . This poor device design is also often latent and could be responsible for No Fault Found reporting in which medical devices sent for repair by clinical users are found to be operating as intended . Unresolved NFF reports may contribute to incident under reporting clinical user frustration and biomedical engineering technologist inefficacy . This study uses human factors engineering methods to investigate the relationship between NFF reporting frequency and device usability . Material and methods An analysis of medical equipment maintenance data was conducted to identify devices with a high NFF reporting frequency . Subsequently semi structured interviews and heuristic evaluations were performed in order to identify potential usability issues . Finally usability testing was conducted in order to validate that latent usability related design faults result in a higher frequency of NFF reporting . Results The analysis of medical equipment maintenance data identified six devices with a high NFF reporting frequency . Semi structured interviews heuristic evaluations and usability testing revealed that usability issues caused a significant portion of the NFF reports . Other factors suspected to contribute to increased NFF reporting include accessory issues intermittent faults and environmental issues . Usability testing conducted on three of the devices revealed 23 latent usability related design faults . Conclusions These findings demonstrate that latent usability related design faults manifest themselves as an increase in NFF reporting and that devices containing usability related design faults can be identified through an analysis of medical equipment maintenance data . 
",Human factors methods were used to investigate No Fault Found NFF incidents. Medical equipment maintenance data was used to identify devices with high NFF rates. Interviews and heuristic analyses revealed usability issues cause NFF incidents. Usability testing validated the results. Our methodology can be used to identify devices with usability related design flaws.,,,
S153204641400080X," Objective Publications are a key data source for investigator profiles and research networking systems . We developed ReCiter an algorithm that automatically extracts bibliographies from PubMed using institutional information about the target investigators . Methods ReCiter executes a broad query against PubMed groups the results into clusters that appear to constitute distinct author identities and selects the cluster that best matches the target investigator . Using information about investigators from one of our institutions we compared ReCiter results to queries based on author name and institution and to citations extracted manually from the Scopus database . Five judges created a gold standard using citations of a random sample of 200 investigators . Results About half of the 10 471 potential investigators had no matching citations in PubMed and about 45 had fewer than 70 citations . Interrater agreement for the gold standard was 0.81 . Scopus achieved the best recall of 0.81 while name based queries had 0.78 and ReCiter had 0.69 . ReCiter attained the best precision of 0.93 while Scopus had 0.85 and name based queries had 0.31 . Discussion ReCiter accesses the most current citation data uses limited computational resources and minimizes manual entry by investigators . Generation of bibliographies using named based queries will not yield high accuracy . Proprietary databases can perform well but requite manual effort . Automated generation with higher recall is possible but requires additional knowledge about investigators . 
",Publications are a key data source for research networking systems. ReCiter extracts bibliographies from PubMed given information about investigators. ReCiter has better precision but worse recall than Scopus a proprietary database. It is challenging to link publications to investigators at a given institution. It is difficult to identify research staff at an institution using publication data.,,,
S153204641400152X," Objective Electronic medical records data is increasingly incorporated into genome phenome association studies . Investigators hope to share data but there are concerns it may be re identified through the exploitation of various features such as combinations of standardized clinical codes . Formal anonymization algorithms can prevent such violations but prior studies suggest that the size of the population available for anonymization may influence the utility of the resulting data . We systematically investigate this issue using a large scale biorepository and EMR system through which we evaluate the ability of researchers to learn from anonymized data for genome phenome association studies under various conditions . Methods We use a k anonymization strategy to simulate a data protection process for resources of similar size to those found at nine academic medical institutions within the United States . Following the protection process we replicate an existing genome phenome association study and compare the discoveries using the protected data and the original data through the correlation of the p values of association significance . Results Our investigation shows that anonymizing an entire dataset with respect to the population from which it is derived yields significantly more utility than small study specific datasets anonymized unto themselves . When evaluated using the correlation of genome phenome association strengths on anonymized data versus original data all nine simulated sites results from largest scale anonymizations retained better utility to those on smaller sizes . We observed a general trend of increasing for larger data set sizes 0.9481 for small sized datasets 0.9493 for moderately sized datasets 0.9934 for large sized datasets . Conclusions This research implies that regardless of the overall size of an institution s data there may be significant benefits to anonymization of the entire EMR even if the institution is planning on releasing only data about a specific cohort of patients . 
",Anonymization of large scale clinical codes allows for reliable genome phenome analysis. Across various repository sizes full EMR most reliable. Preserves utility for finding genome phenome associations.,,,
S153204641400197X," Background Gene name recognition and normalization is together with detection of other named entities a crucial step in biomedical text mining and the underlying basis for development of more advanced techniques like extraction of complex events . While the current state of the art solutions achieve highly promising results on average performance can drop significantly for specific genes with highly ambiguous synonyms . Depending on the topic of interest this can cause the need for extensive manual curation of such text mining results . Our goal was to enhance this curation step based on tools widely used in pharmaceutical industry utilizing the text processing and classification capabilities of the Konstanz Information Miner along with publicly available sources . Results F score achieved on gene specific test corpora for highly ambiguous genes could be improved from values close to zero due to very low precision to values 0.9 for several cases . Interestingly the presented approach even resulted in an increased F score for genes showing already good results in initial gene name normalization . For most test cases we could significantly improve precision while retaining a high recall . Conclusions We could show that KNIME can be used to assist in manual curation of text mining results containing high numbers of false positive hits . Our results also indicate that it could be beneficial for future development in the field of gene name normalization to create gene specific training corpora based on incorrectly identified genes common to current state of the art algorithms . 
",False positive removal after gene recognition and normalization. Integration of existing annotations acronym resolution and classification. Based on well established technologies in pharmaceutical industry. Improvement in precision while keeping recall high. We propose to use existing gene taggers to create corpora for common problematic cases.,,,
S153204641400063X," The ubiquity of Online Social Networks is creating new sources for healthcare information particularly in the context of pharmaceutical drugs . We aimed to examine the impact of a given OSN s characteristics on the content of pharmaceutical drug discussions from that OSN . We compared the effect of four distinguishing characteristics from ten different OSNs on the content of their pharmaceutical drug discussions General versus Health OSN OSN moderation OSN registration requirements and OSNs with a question and answer format . The effects of these characteristics were measured both quantitatively and qualitatively . Our results show that an OSN s characteristics indeed affect the content of its discussions . Based on their information needs healthcare providers may use our findings to pick the right OSNs or to advise patients regarding their needs . Our results may also guide the creation of new and more effective domain specific health OSNs . Further future researchers of online healthcare content in OSNs may find our results informative while choosing OSNs as data sources . We reported several findings about the impact of OSN characteristics on the content of pharmaceutical drug discussion and synthesized these findings into actionable items for both healthcare providers and future researchers of healthcare discussions on OSNs . Future research on the impact of OSN characteristics could include user demographics quality and safety of information and efficacy of OSN usage . 
",Psychotherapeutic drugs dominate health OSNs especially non moderated health OSNs. Genitourinary tract agents and nutritional drugs chatter dominates general OSNs. Respiratory and hormonal drugs appear more often in OSNs that require registration. OSNs with a question and answer format are less subjective. Users ask more questions about gastrointestinal and metabolic drugs.,,,
S153204641400269X," The workflow models of the patient journey in a Pediatric Emergency Department seems to be an effective approach to develop an accurate and complete representation of the PED processes . This model can drive the collection of comprehensive quantitative and qualitative service delivery and patient treatment data as an evidence base for the PED service planning . Our objective in this study is to identify crowded situation indicators and bottlenecks that contribute to over crowding . The greatest source of delay in patient flow is the waiting time from the health care request and especially the bed request to exit from the PED for hospital admission . It represented 70 of the time that these patients occupied in the PED waiting rooms . The use of real data to construct the workflow model of the patient path is effective in identifying sources of delay in patient flow and aspects of the PED activity that could be improved . The development of this model was based on accurate visits made in the PED of the Regional University Hospital Center of Lille . This modeling which has to represent most faithfully possible the reality of the PED of CHRU of Lille is necessary . It must be detailed enough to produce an analysis allowing to identify the dysfunctions of the PED and also to propose and to estimate prevention indicators of crowded situations . Our survey is integrated into the French National Research Agency project titled Hospital Optimization Simulation and avoidance of strain . H pital Optimisation Simulation et vitement des Tensions . 
",Proposal a model workflow to identify crowding indicators and bottlenecks. Approach based on the optimization of the patient path in the PED. The use of real data to construct the workflow model of the patient path. Build BPMN models to represent the patient journey through the PED.,,,
S153204641400224X," Risk stratification is instrumental to modern clinical decision support systems . Comprehensive risk stratification should be able to provide the clinicians with not only the accurate assessment of a patient s risk but also the clinical context to be acted upon . However existing risk stratification techniques mainly focus on predicting the risk score for individual patients at the cohort level they offer little insight beyond a flat score based segmentation . This essentially reduces a patient to a score and thus removes him her from his her clinical context . To address this limitation in this paper we propose a bilinear model for risk stratification that simultaneously captures the three key aspects of risk stratification it predicts the risk of each individual patient it stratifies the patient cohort based on not only the risk score but also the clinical characteristics and it embeds all patients into clinical contexts with clear interpretation . We apply our model to a cohort of 4977 patients 1127 among which were diagnosed with Congestive Heart Failure . We demonstrate that our model can not only accurately predict the onset risk of CHF but also provide rich and actionable clinical insights into the patient cohort . 
",Bilinear risk prediction model for EHR data. Low dimensional embedding of patients in a learned risk space. Identification of meaningful clinical contexts. Significant improvement in accuracy over existing risk prediction model.,,,
S153204641400238X," The focus of this paper is on the challenges and opportunities presented by developing scenarios of use for interactive medical devices . Scenarios are integral to the international standard for usability engineering of medical devices and are also applied to the development of health software . The 62366 standard lays out a process for mitigating risk during normal use . However this begs the question of whether real use matches normal use . In this paper we present an overview of the product lifecycle and how it impacts on the type of scenario that can be practically applied . We report on the development and testing of a set of scenarios intended to inform the design of infusion pumps based on real use . The scenarios were validated by researchers and practitioners experienced in clinical practice and their utility was assessed by developers and practitioners representing different stages of the product lifecycle . These evaluations highlighted previously unreported challenges and opportunities for the use of scenarios in this context . Challenges include integrating scenario based design with usability engineering practice covering the breadth of uses of infusion devices and managing contradictory evidence . Opportunities included scenario use beyond design to guide marketing to inform purchasing and as resources for training staff . This study exemplifies one empirically grounded approach to communicating and negotiating the realities of practice . 
",Scenarios are day in the life descriptions designed to represent user needs. Their use for medical device certification differs from user centred design practice. We investigated this discrepancy and provide recommendations for their application. Scenarios elicit variations in use and difference between actual and intended practice.,,,
S153204641300155X," The task of recognizing and normalizing protein name mentions in biomedical literature is a challenging task and important for text mining applications such as protein protein interactions pathway reconstruction and many more . In this paper we present ProNormz an integrated approach for human proteins tagging and normalization . In Homo sapiens a greater number of biological processes are regulated by a large human gene family called protein kinases by post translational phosphorylation . Recognition and normalization of human protein kinases is considered to be important for the extraction of the underlying information on its regulatory mechanism from biomedical literature . ProNormz distinguishes HPKs from other HPs besides tagging and normalization . To our knowledge ProNormz is the first normalization system available to distinguish HPKs from other HPs in addition to gene normalization task . ProNormz incorporates a specialized synonyms dictionary for human proteins and protein kinases a set of 15 string matching rules and a disambiguation module to achieve the normalization . Experimental results on benchmark BioCreative II training and test datasets show that our integrated approach achieve a fairly good performance and outperforms more sophisticated semantic similarity and disambiguation systems presented in BioCreative II GN task . As a freely available web tool ProNormz is useful to developers as extensible gene normalization implementation to researchers as a standard for comparing their innovative techniques and to biologists for normalization and categorization of HPs and HPKs mentions in biomedical literature . URL http www.biominingbu.org pronormz . 
",We present ProNormz as a freely available web based tool for human protein normalization. Besides normalization ProNormz distinguishes human protein kinases from other proteins. ProNormz is the first system to distinguish human protein kinases from other proteins. The methodology can be useful to developers for extensible gene normalization implementation.,,,
S153204641300141X," Objective In medical information retrieval research semantic resources have been mostly used by expanding the original query terms or estimating the concept importance weight . However implicit term dependency information contained in semantic concept terms has been overlooked or at least underused in most previous studies . In this study we incorporate a semantic concept based term dependence feature into a formal retrieval model to improve its ranking performance . Design Standardized medical concept terms used by medical professionals were assumed to have implicit dependency within the same concept . We hypothesized that by elaborately revising the ranking algorithms to favor documents that preserve those implicit dependencies the ranking performance could be improved . The implicit dependence features are harvested from the original query using MetaMap . These semantic concept based dependence features were incorporated into a semantic concept enriched dependence model . We designed four different variants of the model with each variant having distinct characteristics in the feature formulation method . Measurements We performed leave one out cross validations on both a clinical document corpus and a medical literature corpus which are representative test collections in medical information retrieval research . Results Our semantic concept enriched dependence model consistently outperformed other state of the art retrieval methods . Analysis shows that the performance gain has occurred independently of the concept s explicit importance in the query . Conclusion By capturing implicit knowledge with regard to the query term relationships and incorporating them into a ranking model we could build a more robust and effective retrieval model independent of the concept importance . 
",We incorporated semantic concept enriched dependence into the medical IR algorithm. The semantic concept enriched dependence model showed robust performance. Performance gain was achieved independently from other previous approaches.,,,
S153204641400015X," An ever increasing amount of medical data such as electronic health records is being collected stored shared and managed in large online health information systems and electronic medical record systems . From such rich collections data is often published in the form of census and statistical data sets for the purpose of knowledge sharing and enabling medical research . This brings with it an increasing need for protecting individual people privacy and it becomes an issue of great importance especially when information about patients is exposed to the public . While the concept of data privacy has been comprehensively studied for relational data models and algorithms addressing the distinct differences and complex structure of XML data are yet to be explored . Currently the common compromise method is to convert private XML data into relational data for publication . This ad hoc approach results in significant loss of useful semantic information previously carried in the private XML data . Health data often has very complex structure which is best expressed in XML . In fact XML is the standard format for exchanging and publishing health information . Lack of means to deal directly with data in XML format is inevitably a serious drawback . In this paper we propose a novel privacy protection model for XML and an algorithm for implementing this model . We provide general rules both for transforming a private XML schema into a published XML schema and for mapping private XML data to the new privacy protected published XML data . In addition we propose a new privacy property dependency which can be applied to both relational and XML data and that takes into consideration the hierarchical nature of sensitive data . Lastly we provide an implementation of our model algorithm and privacy property and perform an experimental analysis to demonstrate the proposed privacy scheme in practical application . 
",Semantic relationships in XML make the data vulnerable to privacy attacks. We propose a privacy approach for XML that considers semantic relationships. We compare our model and algorithm against other common privacy approaches. Diversification and dissection techniques can be used to protect privacy in XML.,,,
S153204641400121X," Objectives New DNA sequencing technologies have revolutionized the search for genetic disruptions . Targeted sequencing of all protein coding regions of the genome called exome analysis is actively used in research oriented genetics clinics with the transition to exomes as a standard procedure underway . This transition is challenging identification of potentially causal mutation amongst 106 variants requires specialized computation in combination with expert assessment . This study analyzes the usability of user interfaces for clinical exome analysis software . There are two study objectives To ascertain the key features of successful user interfaces for clinical exome analysis software based on the perspective of expert clinical geneticists To assess user system interactions in order to reveal strengths and weaknesses of existing software inform future design and accelerate the clinical uptake of exome analysis . Methods Surveys interviews and cognitive task analysis were performed for the assessment of two next generation exome sequence analysis software packages . The subjects included ten clinical geneticists who interacted with the software packages using the think aloud method . Subjects interactions with the software were recorded in their clinical office within an urban research and teaching hospital . All major user interface events were time stamped and annotated with coding categories to identify usability issues in order to characterize desired features and deficiencies in the user experience . Results We detected 193 usability issues the majority of which concern interface layout and navigation and the resolution of reports . Our study highlights gaps in specific software features typical within exome analysis . The clinicians perform best when the flow of the system is structured into well defined yet customizable layers for incorporation within the clinical workflow . The results highlight opportunities to dramatically accelerate clinician analysis and interpretation of patient genomic data . Conclusion We present the first application of usability methods to evaluate software interfaces in the context of exome analysis . Our results highlight how the study of user responses can lead to identification of usability issues and challenges and reveal software reengineering opportunities for improving clinical next generation sequencing analysis . While the evaluation focused on two distinctive software tools the results are general and should inform active and future software development for genome analysis software . As large scale genome analysis becomes increasingly common in healthcare it is critical that efficient and effective software interfaces are provided to accelerate clinical adoption of the technology . Implications for improved design of such applications are discussed . 
",First application of usability methods to evaluate exome analysis software. Key list of user desiderata on exome software from clinical geneticists is compiled. Identified usability challenges and design features for reengineering opportunities.,,,
S153204641400183X," This paper proposes the all IP WSNs for real time patient monitoring . In this paper the all IP WSN architecture based on gateway trees is proposed and the hierarchical address structure is presented . Based on this architecture the all IP WSN can perform routing without route discovery . Moreover a mobile node is always identified by a home address and it does not need to be configured with a care of address during the mobility process so the communication disruption caused by the address change is avoided . Through the proposed scheme a physician can monitor the vital signs of a patient at any time and at any places and according to the IPv6 address he can also obtain the location information of the patient in order to perform effective and timely treatment . Finally the proposed scheme is evaluated based on the simulation and the simulation data indicate that the proposed scheme might effectively reduce the communication delay and control cost and lower the packet loss rate . 
",The all IP WSN architecture based on gateway trees is proposed. The proposed routing algorithm is achieved in the link layer without route discovery. A mobile node does not need to be configured with a care of address. A physician can monitor the vital signs of a patient at any time and at any places.,,,
S153204641400149X," One of the main reasons that leads to a low adoption rate of telemedicine systems is poor usability . An aspect that influences usability during the reporting of findings is the input mode e.g . if a free text or a structured report interface is employed . The objective of our study is to compare the usability of FT and ST telemedicine systems specifically in terms of user satisfaction efficiency and general usability . We comparatively evaluate the usability of these two input modes in a telecardiology system for issuing electrocardiography reports in the context of a statewide telemedicine system in Brazil with more than 350.000 performed tele electrocardiography examinations . We adopted a multiple method research strategy applying three different kinds of usability evaluations user satisfaction was evaluated through interviews with seven medical professionals using the System Usability Scale questionnaire and specific questions related to adequacy and user experience . Efficiency was evaluated by estimating execution time using the Keystroke Level Model . General usability was assessed based on the conformity of the systems to a set of e health specific usability heuristics . The results of this comparison provide a first indication that a structured report input mode for such a system is more satisfactory and efficient with a larger conformity to usability heuristics than free text input . User satisfaction using the SUS questionnaire has been scored in average with 58.8 and 77.5 points for the FT and SR system respectively which means that the SR system was rated 18.65 points higher than the FT system . In terms of efficiency the completion of a findings report using the SR mode is estimated to take 8.5s 3.74 times faster than using the FT system . The SR system also demonstrated less violations to usability heuristics in comparison to 14 points observed in the FT system . These results provide a first indication that the usage of structured reporting as an input mode in telecardiology systems may enhance usability . This also seems to confirm the advantages of the usage of structured reporting as already described in the literature for other areas such as teleradiology . 
",Poor usability leads to a low adoption rate of telemedicine systems. Mode of input free text or structured report influences usability. Usability and user satisfaction are higher for structured report interfaces in telecardiology.,,,
S156742231400009X," To date the utilitarian benefits of online consumption have only been partially investigated . This study undertakes an exhaustive approach to fully delimit the dimensional structure related to the utilitarian motivations for online consumption . First an in depth literature review is carried out in order to allow the proposal of an aprioristic base structure of eleven categories of utilitarian motivations . Next qualitative analyses are applied to assess and eventually refine the structure of utilitarian motivations proposed after the literature review their labels and respective measurement scales . Finally this qualitative phase concludes with ten motivational categories and 46 items . Then quantitative analyses are applied based on a questionnaire administered to a sample of 667 Internet users to keep refining and to eventually validate both the dimensional structure of motivations and the related measurement scales . Finally a structure of 9 utilitarian motivations is established with the following labels assortment economy convenience availability of information adaptability customization desire for control payment services anonymity and absence of social interaction . The nomological validity of this structure is satisfactorily tested using a second order factor model . The article finishes by discussing some implications for practitioners . 
",We delimit a full structure for the utilitarian motivations in online consumption. Measurement scales are proposed for every utilitarian motivation. Qualitative analyses are applied to purify a full structure of motivations. Confirmatory analyses are applied to validate motivational structure and scales. We conclude a nine dimension utilitarian motivational structure for online consumption.,,,
S156742231300077X," Researchers have found that price dispersion and market inefficiency exists in electronic marketplaces . Little attention has been bestowed to explore difference in market efficiency between traditional and electronic marketplaces . This study integrates both product and channel preference factors to analyze differences in market efficiency between electronic and traditional shopping environments . Data Envelopment Analysis is applied to calculate market efficiency for single channel and multi channel shoppers . Results show that market efficiencies vary across consumer segments and products . In summary this paper enhances understanding of market efficiency by incorporating behavioral segment and product characteristics into the explanatory framework . 
",Integrating product and channel preference factors to analyze differences of efficiency in electronic and traditional markets. Applying Data Envelopment Analysis DEA to calculate market efficiency for single channel and multi channel shoppers. Price dispersion and market inefficiency exist in electronic marketplaces. Market efficiencies vary across consumer segments and products. Incorporation of behavioral segmentation and product characteristics enhances the understanding of market efficiency.,,,
S153204641500012X," Objective Structured data on mammographic findings are difficult to obtain without manual review . We developed and evaluated a rule based natural language processing system to extract mammographic findings from free text mammography reports . Materials and Methods The NLP system extracted four mammographic findings mass calcification asymmetry and architectural distortion using a dictionary look up method on 93 705 mammography reports from Group Health . Status annotations and anatomical location annotation were associated to each NLP detected finding through association rules . After excluding negated uncertain and historical findings affirmative mentions of detected findings were summarized . Confidence flags were developed to denote reports with highly confident NLP results and reports with possible NLP errors . A random sample of 100 reports was manually abstracted to evaluate the accuracy of the system . Results The NLP system correctly coded 96 99 out of our sample of 100 reports depending on findings . Measures of sensitivity specificity and negative predictive values exceeded 0.92 for all findings . Positive predictive values were relatively low for some findings due to their low prevalence . Discussion Our NLP system was implemented entirely in SAS Base which makes it portable and easy to implement . It performed reasonably well with multiple applications such as using confidence flags as a filter to improve the efficiency of manual review . Refinements of library and association rules and testing on more diverse samples may further improve its performance . Conclusion Our NLP system successfully extracts clinically useful information from mammography reports . Moreover SAS is a feasible platform for implementing NLP algorithms . 
",We developed and evaluated a rule based natural language processing system. The NLP system extracts mammographic findings from free text mammography reports. Manual review showed that the NLP system performs reasonably well. We developed confidence flags to facilitate further manual review. The NLP system was implemented entirely in SAS Base with SAS code available.,,,
S156742231500054X," This article aims at assessing the progress of mobile payment research over the last 8years . A previous literature review covering articles published between 1999 and 2006 showed that the majority of research had only focused on a few topics . In order to address this issue a research agenda was formulated to encourage researchers to explore new topics . Almost a decade later our review reveals that researchers have continued to focus on the same topics with a limited accumulation of new knowledge and similar findings . In addition to reviewing the literature we discuss the possible reasons for the lack of research diversity and propose new recommendations to enhance future mobile payment research . 
",This study reviews the mobile payment research literature from around 2006 to 2015. The authors use a multi perspective framework to classify and analyze the literature. Despite the complexity of the issues that have arisen around mobile payments during the past 10years the related research still lacks diversity. There have been far too many adoption studies that cover the same ground with respect to theory and practice and fail to deliver anything more than incremental knowledge if any. A new agenda for research is proposed based on the analysis and results of the authors critical review that is intended to enhance the quality and relevance of future mobile payment research.,,,
S156742231400074X," In 1982 Betamax the world s first personal recording service was ruled as a fair use in court . Although the copyright holders of TV content claimed that Betamax was an infringement of copyright the court determined that the benefits of personal recording services were significant and that the copyright holder s profits could be protected because the original service was of better quality and had a better cost structure . It also ruled that the loss from manual advertisement skip was minimal . However recent advancements in information technology have allowed new kinds of personal recording services such as a cloud DVR that provides unlimited storage and flawless quality and an Auto hop feature that automatically removes embedded advertisements . This paper introduces a microeconomic model for reviewing the copyright holder s business model and social welfare under the court s decision in relation to newer personal recording services powered by information technologies . Before cloud DVR existed applying fair use to personal recording services increased social welfare while protecting the copyright holder s profits however after the introduction of cloud DVR it may no longer do so . 
",We model personal recording services of TV contents under the fair use doctrine. We examine changes in IT about the copyright holder s profit and the social welfare. Before cloud DVR the copyright holder s profit and the social welfare increased. After cloud DVR the copyright holder s profit and the social welfare decrease. The court is advised to consider the recent IT when applying the fair use doctrine.,,,
S153204641500180X," For the purpose of post marketing drug safety surveillance which has traditionally relied on the voluntary reporting of individual cases of adverse drug events other sources of information are now being explored including electronic health records which give us access to enormous amounts of longitudinal observations of the treatment of patients and their drug use . Adverse drug events which can be encoded in EHRs with certain diagnosis codes are however heavily underreported . It is therefore important to develop capabilities to process by means of computational methods the more unstructured EHR data in the form of clinical notes where clinicians may describe and reason around suspected ADEs . In this study we report on the creation of an annotated corpus of Swedish health records for the purpose of learning to identify information pertaining to ADEs present in clinical notes . To this end three key tasks are tackled recognizing relevant named entities labeling attributes of the recognized entities and relationships between them . For each of the three tasks leveraging models of distributional semantics i.e . unsupervised methods that exploit co occurrence information to model typically in vector space the meaning of words and in particular combinations of such models is shown to improve the predictive performance . The ability to make use of such unsupervised methods is critical when faced with large amounts of sparse and high dimensional data especially in domains where annotated resources are scarce . 
",A corpus of Swedish clinical notes was annotated for adverse drug event information. Detecting adverse drug events in clinical notes can support pharmacovigilance. Modeling context with distributional semantics yielded better predictive models. Distributed word representations allowed more context information to be incorporated. Inter sentential relations between drugs and disorders findings are hard to detect.,,,
S156742231400091X," This study uses eye tracking to explore the Elaboration Likelihood Model in online shopping . The results show that the peripheral cue did not have moderating effect on purchase intention but had moderating effect on eye movement . Regarding purchase intention the high elaboration had higher purchase intention than the low elaboration with a positive peripheral cue but there was no difference in purchase intention between the high and low elaboration with a negative peripheral cue . Regarding eye movement with a positive peripheral cue the high elaboration group was observed to have longer fixation duration than the low elaboration group in two areas of interest however with a negative peripheral cue the low elaboration group had longer fixation on the whole page and two AOIs . In addition the relationship between purchase intention and eye movement of the AOIs is more significant in the high elaboration group when given a negative peripheral cue and in the low elaboration group when given a positive peripheral cue . This study not only examines the postulates of the ELM but also contributes to a better understanding of the cognitive processes of the ELM . These findings have practical implications for e sellers to identify characteristics of consumers elaboration in eye movement and designing customization and persuasive context for different elaboration groups in e commerce . 
",Peripheral cue moderates elaboration on eye movements but not purchase intention. Purchase intention is different between high and low elaborations under positive cue but not negative cue. Under positive cue high elaboration is longer fixation duration than low elaboration. Under negative cue low elaboration is longer fixation duration than high elaboration. The relationship between purchase intention and eye movement is more significant in high elaboration with negative cue and in low elaboration with positive cue.,,,
S153204641500194X," Despite recent progress in prediction and prevention heart disease remains a leading cause of death . One preliminary step in heart disease prediction and prevention is risk factor identification . Many studies have been proposed to identify risk factors associated with heart disease however none have attempted to identify all risk factors . In 2014 the National Center of Informatics for Integrating Biology and Beside issued a clinical natural language processing challenge that involved a track for identifying heart disease risk factors in clinical texts over time . This track aimed to identify medically relevant information related to heart disease risk and track the progression over sets of longitudinal patient medical records . Identification of tags and attributes associated with disease presence and progression risk factors and medications in patient medical history were required . Our participation led to development of a hybrid pipeline system based on both machine learning based and rule based approaches . Evaluation using the challenge corpus revealed that our system achieved an F1 score of 92.68 making it the top ranked system of the 2014 i2b2 clinical NLP challenge . 
",We proposed a hybrid system to automatically identify heart disease risk factors. We divided different types of risk factors into three categories according to their descriptions. Our system achieves an F score of 92.86 on 2014 i2b2 corpus which is top ranked.,,,
S156742231500040X," Advertisement options are a recent development in online advertising . Simply an ad option is a first look contract in which a publisher or search engine grants an advertiser a right but not obligation to enter into transactions to purchase impressions or clicks from a specific ad slot at a pre specified price on a specific delivery date . Such a structure provides advertisers with more flexibility of their guaranteed deliveries . The valuation of ad options is an important topic and previous studies on ad options pricing have been mostly restricted to the situations where the underlying prices follow a geometric Brownian motion . This assumption is reasonable for sponsored search however some studies have also indicated that it is not valid for display advertising . In this paper we address this issue by employing a stochastic volatility model and discuss a lattice framework to approximate the proposed SV model in option pricing . Our developments are validated by experiments with real advertising data we find that the SV model has a better fitness over the GBM model we validate the proposed lattice model via two sequential Monte Carlo simulation methods we demonstrate that advertisers are able to flexibly manage their guaranteed deliveries by using the proposed options and publishers can have an increased revenue when some of their inventories are sold via ad options . 
",A new advertisement option that allows an advertiser to pay a fixed CPM CPC to purchase impressions or clicks. The fixed payment can be different to the underlying ad format. The proposed option can be priced under the lattice framework for both SV and GBM underlying models. The studied model is validated by two advertising datasets.,,,
S156742231300015X," Online reviews as one kind of quality indicator of products or service are becoming increasingly important in influencing purchase decisions of prospective consumers on electronic commerce websites . With the fast growth of the Chinese e commerce industry it is thus indispensable to design effective online review systems for e commerce websites in the Chinese context by taking into account cultural factors . In this paper we conduct two empirical studies on online reviews . Firstly we study how culture differences across countries impact the way in which consumers provide online reviews . Secondly we investigate the impact of online reviews on product sales in the Chinese context and show that directly copying the ideas of successful online review systems in the USA will deteriorate the effectiveness of the systems in China . Finally we propose several suggestions for the development of effective online review systems in the Chinese context based on the results of our two empirical studies and the findings in previous studies . 
",Consumers reviewing behavior is affected by culture differences across countries. Chinese are less engaged less negative and value negation more in review systems. Review volume emotional tendency and spotlight review positively affect book sales. Length and Reviewer rank negatively affect sales. Helpful votes have no impact. Effective review systems in China need care about review format and voting system.,,,
S153204641500283X," Background Drug repositioning is the process of finding new indications for existing drugs . Its importance has been dramatically increasing recently due to the enormous increase in new drug discovery cost . However most of the previous molecular centered drug repositioning work is not able to reflect the end point physiological activities of drugs because of the inherent complexity of human physiological systems . Methods Here we suggest a novel computational framework to make inferences for alternative indications of marketed drugs by using electronic clinical information which reflects the end point physiological results of drug s effects on the biological activities of humans . In this work we use the concept of complementarity between clinical disease signatures and clinical drug effects . With this framework we establish disease related clinical variable vectors and drug related clinical variable vectors by applying two methodologies . Finally we assign a repositioning possibility score to each disease drug pair by the calculation of complementarity and association between clinical states of disease signatures and clinical effects of drugs . A total of 717 clinical variables in the electronic clinical dataset are considered in this study . Results The statistical significance of our prediction results is supported through two benchmark datasets . We discovered not only lots of known relationships between diseases and drugs but also many hidden disease drug relationships . For example glutathione and edetic acid may be investigated as candidate drugs for asthma treatment . We examined prediction results by using statistical experiments and presented evidences for those with already published literature . Conclusion The results show that electronic clinical information is a feasible data resource and utilizing the complementarity between clinical signatures of disease and clinical effects of drugs is a potentially predictive concept in drug repositioning research . It makes the proposed approach useful to identity novel relationships between diseases and drugs that have a high probability of being biologically valid . 
",We suggest a computational framework to find new uses of existing drugs. We use the complementarity between clinical disease signatures and clinical drug effects. The statistical significance of prediction results is supported through two benchmark datasets.,,,
S156742231400088X," Studies have shown that perceptual maps derived from online consumer generated data are effective for depicting market structure such as demonstrating positioning of competitive brands . However most text mining algorithms would require manual reading to merge extracted product features with synonyms . In response Topic modeling is introduced to group synonyms together under a topic automatically leading to convenient and accurate evaluation of brands based on consumers online reviews . To ensure the feasibility of employing Topic modeling in assessing competitive brands we developed a unique and novel framework named WVAP based on Scree plot technique . WVAP can filter the noises in posterior distribution obtained from Topic modeling and improve accuracy in brand evaluation . A case study exploring online reviews of mobile phones is conducted . We extract topics to reflect the features of the cell phones with a qualified validity . In addition to perceptual maps derived by multi dimensional scaling for product positioning we also rank these products by TOPSIS so as to visualize the market structure from different perspectives . Our case study of cell phones shows that the proposed framework is effective in mining online reviews and providing insights into the competitive landscape . 
",Apply Topic modeling to group synonyms under a topic to avoid human intervention and improve automatic market structure generation. Develop the WVAP method to filter noises in Topic modeling results to elicit market structure. Besides perceptual maps of product positioning the proposed framework can provide rankings of products.,,,
S156742231500023X," Owing to the limited information possessed by patients there exists significant information asymmetry between patients and physicians . In addition as services are intangible inseparable and heterogeneous patients are difficult to judge the physicians service quality . With the development of online healthcare services healthcare websites currently provide more information for patients such as patient generated and system generated information . Those kinds of information can reflect the quality of physicians service outcome and delivery process to help patients to select physicians . However there is scant research on the role of patient generated and system generated information in patients online behavior . Collecting data from a healthcare website this paper develops a two equation model to verify the effects of two kinds of information on patients search evaluation and decision making on healthcare websites . The results of our empirical research show that positive patient generated and system generated information on physicians service quality positively impact patients reactions at different stages . Moreover we also find that synergies between patient generated and system generated information are positively associated with patients decisions to consult a physician . These findings provide valuable contributions to the online healthcare research . 
",Patients can access information on healthcare websites to determinate physicians quality. We verify the effects of physicians information on patients online reactions. Physicians information can be divided into patient generated information and system generated information. We find that the physicians information positively impact patients reactions at different stages. Moreover synergies between two kinds of information are positively associated with patients decisions.,,,
S156742231300032X," In e commerce applications vendors can construct detailed profiles about customers preferences which is known as buyer profiling . These profiles can then be used by vendors in order to perform practices such as price discrimination poor judgment etc . The use of pseudonyms and specially changing pseudonyms from time to time are known to minimize profiling minimizing the capacity of vendors to perform such practices in turn . Although there are some frameworks and tools that support pseudonym change there are few proposals that suggest or directly change the pseudonym in an automated fashion . Instead users are usually provided with the mechanisms to change pseudonyms but without any advise on when they should actually use these mechanisms . In this paper we present an approach to control buyer profiling by means of automated pseudonym changes performed according to human privacy attitudes . We also present an application scenario and an evaluation of our proposal . 
",We present a novel automated buyer profiling control mechanism. It is based on pseudonym changes performed according to human privacy attitudes. Agents automatically decide whether reusing or changing their pseudonyms. We present the results of an extensive experimental evaluation of the mechanism based on an application scenario.,,,
S153204641400272X," Cognitive Informatics is a burgeoning interdisciplinary domain comprising of the cognitive and information sciences that focuses on human information processing mechanisms and processes within the context of computing and computer applications . Based on a review of articles published in the Journal of Biomedical Informatics between January 2001 and March 2014 we identified 57 articles that focused on topics related to cognitive informatics . We found that while the acceptance of CI into the mainstream informatics research literature is relatively recent its impact has been significant from characterizing the limits of clinician problem solving and reasoning behavior to describing coordination and communication patterns of distributed clinical teams to developing sustainable and cognitively plausible interventions for supporting clinician activities . Additionally we found that most research contributions fell under the topics of decision making usability and distributed team activities with a focus on studying behavioral and cognitive aspects of clinical personnel as they performed their activities or interacted with health information systems . We summarize our findings within the context of the current areas of CI research future research directions and current and future challenges for CI researchers . We are at a turbulent yet exciting phase in healthcare turbulent as the transformations in healthcare practice have been driven by paradigmatic shift toward the use of health information technology both as a result of necessity and federal mandates exciting as such transformations have highlighted the central role of cognitive and behavioral sciences in developing usable systems that can provide high quality patient care . While there is a bright future in terms of opportunities for researchers and practitioners who seek to engage in cognitive science research it is also important to reflect on past research to understand the historical context and foundations of the development of cognitive research in biomedical informatics the theories constructs and frameworks that drive the current research and the potential directions for future research . Within this focus this special communication provides a broader context of the cognitive and behavioral research on HIT in biomedical informatics . In addition we have also created a virtual issue of the Journal of Biomedical Informatics that will provide a snapshot of the research that has been published in JBI pertaining to cognitive and social science research . Cognitive science is an interdisciplinary field that draws from psychology computer science linguistics philosophy and anthropology to understand human activities including reasoning decision making and problem solving . Principles from cognitive science have been applied for studying the usability of medical devices and interfaces developing training educational interventions and guidelines streamlining and improving workflow and clinical processes and for understanding the process of clinical judgment reasoning and decision making . In summary cognitive science provides a viable mechanism to inform our understanding in technology rich clinical environments and represents an important component of biomedical informatics . Additionally cognitive research has been a key to shaping and structuring the use of HIT adapting to the various needs of the clinical environment . Cognitive informatics by. 
",Cognitive informatics CI research has its foundations in cognitive science. Transformations seen in CI in JBI reflect the changes seen broadly in the field of CI. Key topics include decision making usability comprehension workflow and errors. Recent developments toward use of applied cognition for usability and HCI studies. Future trends point toward consumer health tools and the use of mobile technology.,,,
S153204641500009X," Introduction While mammography notably contributes to earlier detection of breast cancer it has its limitations including a large number of false positive exams . Improved radiology education could potentially contribute to alleviating this issue . Toward this goal in this paper we propose an algorithm for modeling of false positive error making among radiology trainees . Identifying troublesome locations for the trainees could focus their training and in turn improve their performance . Methods The algorithm proposed in this paper predicts locations that are likely to result in a false positive error for each trainee based on the previous annotations made by the trainee . The algorithm consists of three steps . First the suspicious false positive locations are identified in mammograms by Difference of Gaussian filter and suspicious regions are segmented by computer vision based segmentation algorithms . Second 133 features are extracted for each suspicious region to describe its distinctive characteristics . Third a random forest classifier is applied to predict the likelihood of the trainee making a false positive error using the extracted features . The random forest classifier is trained using previous annotations made by the trainee . We evaluated the algorithm using data from a reader study in which 3 experts and 10 trainees interpreted 100 mammographic cases . Results The algorithm was able to identify locations where the trainee will commit a false positive error with accuracy higher than an algorithm that selects such locations randomly . Specifically our algorithm found false positive locations with 40 accuracy when only 1 location was selected for all cases for each trainee and 12 accuracy when 10 locations were selected . The accuracies for randomly identified locations were both 0 for these two scenarios . Conclusions In this first study on the topic we were able to build computer models that were able to find locations for which a trainee will make a false positive error in images that were not previously seen by the trainee . Presenting the trainees with such locations rather than randomly selected ones may improve their educational outcomes . 
",A method for predicting trainees false positive locations in mammography is proposed. This is the first exploratory study on the topic using computer algorithms. Predictions are made using 133 imaging features and a random forest classifier. The predicted locations are more accurate than the locations selected randomly. The method can select educational material with more challenging locations.,,,
S156742231500037X," Concept drift is a common phenomenon in stock market that can cause the devaluation of the knowledge learned from cross sectional analysis as the market changes over time in unforeseen ways . The widely used cross sectional regression analysis based on expert knowledge has obvious limitations in handling problems that involve concept drift and high dimensional data . To discover causal relations between portfolio selection factors and stock returns and identify concept drifts of these relations we apply a novel causal discovery technique called modified Additive Noise Model with Conditional Probability Table . In evaluation experiments we compares ANMCPT to the conventional cross sectional analysis approach in mining relationships between portfolio selection factors and stock returns . Results indicate that the factors selected by ANMCPT outperform the factors adopted in most previous cross sectional researches that followed the Fama French framework . To the best of our knowledge this paper is the first to compare causal inference technique with Fama French framework in concept drift mining of stock portfolio selection factors . Our causal inference based concept drift mining method provides a new approach to accurate knowledge discovery in stock market . 
",Provide a model based on causal discovery technique ANMCPT for concept drift mining in cross sectional analysis. AVMCPT can discover causal in high dimensional and dynamic environment. ANMCPT outperform the classical Fama French framework. Concept drift phenomenon in China stock market is observed and exhibited clearly.,,,
S153204641500043X," In Electronic Health Records much of valuable information regarding patients conditions is embedded in free text format . Natural language processing techniques have been developed to extract clinical information from free text . One challenge faced in clinical NLP is that the meaning of clinical entities is heavily affected by modifiers such as negation . A negation detection algorithm NegEx applies a simplistic approach that has been shown to be powerful in clinical NLP . However due to the failure to consider the contextual relationship between words within a sentence NegEx fails to correctly capture the negation status of concepts in complex sentences . Incorrect negation assignment could cause inaccurate diagnosis of patients condition or contaminated study cohorts . We developed a negation algorithm called DEEPEN to decrease NegEx s false positives by taking into account the dependency relationship between negation words and concepts within a sentence using Stanford dependency parser . The system was developed and tested using EHR data from Indiana University and it was further evaluated on Mayo Clinic dataset to assess its generalizability . The evaluation results demonstrate DEEPEN which incorporates dependency parsing into NegEx can reduce the number of incorrect negation assignment for patients with positive findings and therefore improve the identification of patients with the target clinical findings in EHRs . 
",Utilizing Stanford dependency relation to further analyze the negation status of clinical concepts negated by NegEx. Improvement of NegEx algorithm by decreasing the number of false positives. Comparison of NegEx and DEEPEN on clinical reports from two different clinical settings.,,,
S156742231600003X," Despite the huge growth potential that has been predicted for in app purchases and the mobile game market little is known about what motivates game players to make such purchases . The purpose of this paper is to build a research model based on the loyalty literature and studies of value theory to identify the antecedents of in app purchase intention in the context of mobile games . The proposed model was empirically evaluated using a web survey of 3309 mobile game players 813 nonpaying players and 2496 paying players . Structural equation modeling was used to assess the research model . The results reveal that loyalty to the mobile game has significant influence on a player s intention to make an in app purchase . The perceived values of the game have direct influence on the loyalty of all players but appear to have relatively little impact on the purchase intentions of nonpaying players . Two values were found to have a direct impact on a player s intention to make an in app purchase . Specifically our study revealed differences between paying users and nonpaying users . This study provides a better understanding of how the values influence loyalty among all players of the game and the purchase intentions of paying and nonpaying players . Further insights into mobile game app marketing strategies are provided as well . 
",This study investigates the in app purchase intention and mobile game loyalty. The paying players intention is determined by playfulness good price and reward. The nonpaying players is only determined by good price.,,,
S153204641500129X," Objective Some phase 1 clinical trials offer strong financial incentives for healthy individuals to participate in their studies . There is evidence that some individuals enroll in multiple trials concurrently . This creates safety risks and introduces data quality problems into the trials . Our objective was to construct a privacy preserving protocol to track phase 1 participants to detect concurrent enrollment . Design A protocol using secure probabilistic querying against a database of trial participants that allows for screening during telephone interviews and on site enrollment was developed . The match variables consisted of demographic information . Measurement The accuracy of the matching and its computational performance in seconds were measured under simulated environments . Accuracy was also compared to non secure matching methods . Results The protocol performance scales linearly with the database size . At the largest database size of 20 000 participants a query takes under 20s on a 64 cores machine . Sensitivity precision and negative predictive value of the queries were consistently at or above 0.9 and were very similar to non secure versions of the protocol . Conclusion The protocol provides a reasonable solution to the concurrent enrollment problems in phase 1 clinical trials and is able to ensure that personal information about participants is kept secure . 
",We present a privacy preserving protocol to detect concurrent trial participants. We present a name representation scheme resilient to frequency attacks. The accuracy of the protocol is similar to standard non secure methods. For a database size of 20 000 the private query time is under 40s on 32 cores.,,,
S156849461300166X," In this study a novel approach via the composite of fuzzy controllers and dithers is presented . According to this approach we can synthesize a set of fuzzy controllers and find appropriate dithers to stabilize nonlinear multiple time delay interconnected systems . A robustness design of model based fuzzy control is first proposed to overcome the effect of modeling errors between the NMTD interconnected subsystems and Takagi Sugeno fuzzy models . In terms of Lyapunov s direct method a delay dependent stability criterion is then derived to guarantee the asymptotic stability of NMTD interconnected systems . Based on this criterion and the decentralized control scheme a set of model based fuzzy controllers is synthesized via the technique of parallel distributed compensation to stabilize the NMTD interconnected system . When the designed fuzzy controllers can not stabilize the NMTD interconnected systems a batch of high frequency signals is simultaneously introduced to stabilize it . If the frequencies of dithers are high enough the outputs of the dithered interconnected system and those of its corresponding mathematical model the relaxed interconnected system can be made as close as desired . This makes it possible to obtain a rigorous prediction of the stability of the dithered interconnected system based on the one of the relaxed interconnected system . Finally a numerical example is given to illustrate the feasibility of our approach . 
",A novel approach via the composite of fuzzy controllers and dithers is presented. We can synthesize a set of fuzzy controllers and find appropriate dithers to stabilize nonlinear multiple time delay NMTD interconnected systems. When the designed fuzzy controllers cannot stabilize the NMTD interconnected systems a batch of high frequency signals commonly referred to as dithers is simultaneously introduced to stabilize it.,,,
S156849461300238X," Analytic Hierarchy Process is increasingly applied to healthcare and medical research and applications . However knowledge representation of pairwise reciprocal matrix is still dubious . This research discusses the related drawbacks and recommends pairwise opposite matrix as the ideal alternative . Pairwise opposite matrix is the key foundation of Primitive Cognitive Network Process which revises the AHP approach with practical changes . A medical decision treatment evaluation using AHP is revised by P CNP with a step by step tutorial . Comparisons with AHP have been discussed . The proposed method could be a promising decision tool to replace AHP to share information among patients or and doctors and to evaluate therapies medical treatments health care technologies medical resources and healthcare policies . 
",Indicating the rating scale problems of the Analytic Hierarchy Process AHP and proposing the paired interval scale addressing the limitations. Introducing Primitive Cognitive Network Process P CNP to medical treatment decision making and showing how to use it from laymen perspective. Demonstrating how the current AHP data to medical decision can be converted to P CNP data which is further processed by the P CNP. Applications with the AHP data can be revised by P CNP to explore the more reliable research findings or make more reliable decisions. P CNP can be a promising decision making approach to evaluate medical and healthcare decisions.,,,
S156849461300286X," This paper proposes a novel multi objective model for an unrelated parallel machine scheduling problem considering inherent uncertainty in processing times and due dates . The problem is characterized by non zero ready times sequence and machine dependent setup times and secondary resource constraints for jobs . Each job can be processed only if its required machine and secondary resource are available at the same time . Finding optimal solution for this complex problem in a reasonable time using exact optimization tools is prohibitive . This paper presents an effective multi objective particle swarm optimization algorithm to find a good approximation of Pareto frontier where total weighted flow time total weighted tardiness and total machine load variation are to be minimized simultaneously . The proposed MOPSO exploits new selection regimes for preserving global as well as personal best solutions . Moreover a generalized dominance concept in a fuzzy environment is employed to find locally Pareto optimal frontier . Performance of the proposed MOPSO is compared against a conventional multi objective particle swarm optimization algorithm over a number of randomly generated test problems . Statistical analyses based on the effect of each algorithm on each objective space show that the proposed MOPSO outperforms the CMOPSO in terms of quality diversity and spacing metrics . 
",Proposing a fuzzy multi objective model for unrelated parallel machine scheduling. Presenting an effective multi objective particle swarm optimization solution method. Comparing the proposed MOPSO algorithm against a conventional MOPSO algorithm.,,,
S156849461300121X," Merging sustainable development with the business and taking goals into account from its three dimensions which are derived from customer and stakeholder requirements have been a potential source of competitive differentiation for firms . Academic and corporate interest in sustainable supply chain management has also risen considerably in recent years . This paper examines the components and elements of SSC management and how they serve as a foundation for an evaluation framework . By using quality function deployment as a product system planning and improvement tool an effective SSC structure can be obtained . QFD uses a matrix called the House of Quality and constructing the HoQ is a critical step in the application of QFD as it translates customer requirements into engineering characteristics . However participants of HoQ construction sessions tend to provide information about their individual judgments in multiple formats such as numerically or linguistically depending on their different knowledge experience culture and circumstance . Furthermore they can generate incomplete preferences which are challenging to assess in a consistent way . Therefore the objective of this study is to apply an extended QFD methodology in SSC by introducing a new group decision making approach that takes multiple preference formats and incomplete information into account and fusions different formats of expressions into one uniform group decision by means of the fuzzy set theory . To assess the validity of the proposed approach a case study conducted at HAVI Logistics Turkey is also presented in the paper . 
",The objective of the paper is to apply an extended QFD methodology in sustainable supply chain SSC . The paper examines the main elements of SSC and how they serve as a foundation for an evaluation framework. A new GDM approach that takes multiple preference formats and incomplete information into account is introduced. To assess the validity of the proposed approach a case study conducted at HAVI Logistics Turkey is presented.,,,
S156849461300269X," Missing data in large insurance datasets affects the learning and classification accuracies in predictive modelling . Insurance datasets will continue to increase in size as more variables are added to aid in managing client risk and will therefore be even more vulnerable to missing data . This paper proposes a hybrid multi layered artificial immune system and genetic algorithm for partial imputation of missing data in datasets with numerous variables . The multi layered artificial immune system creates and stores antibodies that bind to and annihilate an antigen . The genetic algorithm optimises the learning process of a stimulated antibody . The evaluation of the imputation is performed using the RIPPER k nearest neighbour na ve Bayes and logistic discriminant classifiers . The effect of the imputation on the classifiers is compared with that of the mean mode and hot deck imputation methods . The results demonstrate that when missing data imputation is performed using the proposed hybrid method the classification improves and the robustness to the amount of missing data is increased relative to the mean mode method for data missing completely at random missing at random and not missing at random .The imputation performance is similar to or marginally better than that of the hot deck imputation . 
",Genetic algorithm optimization is effective for partial imputation using MAIS. The hybrid MAIS and genetic algorithm improves performance of classifiers. Increased strength and resilience in the presence of escalating missing data.,,,
S156849461300416X," This paper presents a novel soft computing based solution to a complex optimal control or dynamic optimization problem that requires the solution to be available in real time . The complexities in this problem of optimal guidance of interceptors launched with high initial heading errors include the more involved physics of a three dimensional missile target engagement and those posed by the assumption of a realistic dynamic model such as time varying missile speed thrust drag and mass besides gravity and upper bound on the lateral acceleration . The classic pure proportional navigation law is augmented with a polynomial function of the heading error and the values of the coefficients of the polynomial are determined using differential evolution . The performance of the proposed DE enhanced guidance law is compared against the existing conventional laws in the literature on the criteria of time and energy optimality peak lateral acceleration demanded terminal speed and robustness to unanticipated target maneuvers to illustrate the superiority of the proposed law . 
",This paper solves the real world problem of tactical missile guidance in which optimality is sought to be achieved using the evolutionary computing method of differential evolution. A valid criticism against optimization by evolutionary computing methods is that they are computationally intensive as compared to gradient based methods. By posing the problem as that of finding the coefficients of a third order polynomial the dimensionality of the problem is so greatly reduced that online implementation in real time is shown to be possible. The results so obtained are compared against conventional methods in the guidance literature.,,,
S156849461300135X," A methodology for designing semi physical fuzzy models is proposed . Prior physical knowledge about the dynamics of the system is modeled with continuous time differential equations . Fuzzy knowledge bases are embedded in these equations as nonlinear constructive blocks . Rules comprising the knowledge bases are fitted to interval valued data with metaheuristics . A possibilistic filter is proposed that is able to gradually evolve an initial estimation of the latent variables of the model on the basis of successive prediction errors . This methodology has been applied to the prediction of voltage and state of charge of LiFePO4 batteries . An empirical study has been carried over data gathered in experiments at the Battery Laboratory at Oviedo University . Fitting between the proposed model and actual measurements is studied for four different manufacturers and different charge discharge patterns . Predictions of the evolution of the voltage during charge discharge and inactivity compare favorably to different models in the literature . The possibilistic filter allows to estimate the state of charge of batteries after an arbitrary path that may include partial charges and discharges . It is shown that the accuracy of the open loop model improves that of other approaches in the literature and at the same time the observer based online model is able to approximate the effective remnant charge of the battery after a reasonably short time . 
",Methodology for obtaining fuzzy rule based semi physical models. Possibilistic maximally specific observer of the state of a nonlinear system. Mathematical model of open circuit voltage and state of charge of LiFePO4 batteries.,,,
S156849461300207X," This paper proposes a method for finding solutions of arbitrarily nonlinear systems of functional equations through stochastic global optimization . The original problem is transformed into a global optimization one by synthesizing objective functions whose global minima if they exist are also solutions to the original system . The global minimization task is carried out by the stochastic method known as fuzzy adaptive simulated annealing triggered from different starting points aiming at finding as many solutions as possible . To demonstrate the efficiency of the proposed method solutions for several examples of nonlinear systems are presented and compared with results obtained by other approaches . We consider systems composed of n equations on Euclidean spaces . 
",Efficient and accurate approach to solve nonlinear systems of functional equation. Optimization is carried out by stochastic fuzzy adaptive simulated annealing. Several examples presented and compared with results obtained by other approaches.,,,
S156849461300272X," In the bacteria foraging optimization algorithm the chemotactic process is randomly set imposing that the bacteria swarm together and keep a safe distance from each other . In hybrid bacteria foraging optimization algorithm and particle swarm optimization algorithm the principle of swarming is introduced in the framework of BFAO . The hBFOA PSO algorithm is based on the adjustment of each bacterium position according to the neighborhood environment . In this paper the effectiveness of the hBFOA PSO algorithm has been tested for automatic generation control of an interconnected power system . A widely used linear model of two area non reheat thermal system equipped with proportional integral controller is considered initially for the design and analysis purpose . At first a conventional integral time multiply absolute error based objective function is considered and the performance of hBFOA PSO algorithm is compared with PSO BFOA and GA. Further a modified objective function using ITAE damping ratio of dominant eigenvalues and settling time with appropriate weight coefficients is proposed to increase the performance of the controller . Further robustness analysis is carried out by varying the operating load condition and time constants of speed governor turbine tie line power in the range of 50 to 50 as well as size and position of step load perturbation to demonstrate the robustness of the proposed hBFOA PSO optimized PI controller . The proposed approach is also extended to a non linear power system model by considering the effect of governor dead band non linearity and the superiority of the proposed approach is shown by comparing the results of craziness based particle swarm optimization approach for the identical interconnected power system . Finally the study is extended to a three area system considering both thermal and hydro units with different PI coefficients and comparison between ANFIS and proposed approach has been provided . 
",Suitable objective function selection is very important for controller design. An objective function using ITAE damping ratio and settling times is proposed. The concept is applied to design an hBFOA PSO based PI controller for AGC system. Linear and nonlinear interconnected power system models are considered. Simulation results show better performance than PSO BFOA GA CRAZYPSO and ANFIS approaches.,,,
S156849461300183X," The ability of artificial neural networks to model the rainfall discharge relationships of karstic aquifers has been studied in the Terminio massif which supplies the Naples area with a yearly mean discharge of approximately 1 3.5m3 s. The Mediterranean climate causes a rapid increase in evapotranspiration and a decrease in rainfall towards spring summer . Especially during drought and in combination with highly sensitive climatic parameters there are dramatic changes in the discharge amount especially during the July and August months . A neural network model was developed based on MLP network to forecast of water resources three and six month before the main stress months of July and August . Example data were extracted on an ultra centenarian hydrological serial . The training and validation phases confirmed by a ten fold cross validation methodology led to a very satisfactory calibration of the ANN model with errors in forecasting discharge values of just 5 and 10 . 
",Ability of artificial neural networks to model the rainfall discharge relationships of karstic aquifers. Three month before forecast of water resources. Six month before forecast of water resources. Error in forecasting discharge values of just 5 three months before . Error in forecasting discharge values of just 10 six months before .,,,
S156849461300313X," Stress is a major health problem in our world today . For this reason it is important to gain an objective understanding of how average individuals respond to real life events they observe in environments they encounter . Our aim is to estimate an objective stress signal for an observer of a real world environment stimulated by meditation . A computational stress signal predictor system is proposed which was developed based on a support vector machine genetic algorithm and an artificial neural network to predict the stress signal from a real world data set . The data set comprised of physiological and physical sensor response signals for stress over the time of the meditation activity . A support vector machine based individual independent classification model was developed to determine the overall shape of the stress signal and results suggested that it matched the curves formed by a linear function a symmetric saturating linear function and a hyperbolic tangent function . Using this information of the shape of the stress signal an artificial neural network based stress signal predictor was developed . Compared to the curves formed from a linear function symmetric saturating linear function and hyperbolic tangent function the stress signal produced by the stress signal predictor for the observers was the most similar to the curve formed by a hyperbolic tangent function with p 0.01 according to statistical analysis . The research presented in this paper is a new dimension in stress research it investigates developing an objective stress measure that is dependent on time . 
",Proposed a computational stress signal predictor system to estimate a stress signal. System based on support vector machine genetic algorithm and neural network. An experiment was conducted to acquire real world stress data. Features extracted from the stress data were provided as input to the system. The stress signal was most similar to a hyperbolic tangent curve.,,,
S156849461300358X," Models based on data mining and machine learning techniques have been developed to detect the disease early or assist in clinical breast cancer diagnoses . Feature selection is commonly applied to improve the performance of models . There are numerous studies on feature selection in the literature and most of the studies focus on feature selection in supervised learning . When class labels are absent feature selection methods in unsupervised learning are required . However there are few studies on these methods in the literature . Our paper aims to present a hybrid intelligence model that uses the cluster analysis techniques with feature selection for analyzing clinical breast cancer diagnoses . Our model provides an option of selecting a subset of salient features for performing clustering and comprehensively considers the use of most existing models that use all the features to perform clustering . In particular we study the methods by selecting salient features to identify clusters using a comparison of coincident quantitative measurements . When applied to benchmark breast cancer datasets experimental results indicate that our method outperforms several benchmark filter and wrapper based methods in selecting features used to discover natural clusters maximizing the between cluster scatter and minimizing the within cluster scatter toward a satisfactory clustering quality . 
",Our hybrid intelligent model considers the use of filter and wrapper based feature selection methods. Three qualitative principles are highlighted. The usefulness of our model is demonstrated using relative cluster validities. Better use a subset of salient features for analyzing clinical diagnoses in performing clustering.,,,
S156849461300402X," Diabetes mellitus is a disease that affects to hundreds of millions of people worldwide . Maintaining a good control of the disease is critical to avoid severe long term complications . In recent years several artificial pancreas systems have been proposed and developed which are increasingly advanced . However there is still a lot of research to do . One of the main problems that arises in the automatic control of diabetes is to get a model explaining how glycemia varies with insulin food intakes and other factors fitting the characteristics of each individual or patient . This paper proposes the application of evolutionary computation techniques to obtain customized models of patients unlike most of previous approaches which obtain averaged models . The proposal is based on a kind of genetic programming based on grammars known as Grammatical Evolution . The proposal has been tested with in silico patient data and results are clearly positive . We present also a study of four different grammars and five objective functions . In the test phase the models characterized the glucose with a mean percentage average error of 13.69 modeling well also both hyper and hypoglycemic situations . 
",We propose a method based on Grammatical Evolution to obtain individualized and customized glycemia models in humans. We have tested this proposal with five in silico patients taken from AIDA simulator. We present a study of four different grammars and five objective functions. In the test phase GE models characterized glucose levels with a mean percentage average error of 13.69 . Models obtained with our method reflect also a good representation of both hyper and hypoglycemic situations.,,,
S156849461300210X," Control of power electronics converters used in PV system is very much essential for the efficient operation of the solar system . In this paper a modified incremental conduction maximum power point tracking algorithm in conjunction with an adaptive fuzzy controller is proposed to control the DC DC boost converter in the PV system under rapidly varying atmospheric and partial shading conditions . An adaptive hysteresis current controller is proposed to control the inverter . The proposed current controller provides constant switching frequency with less harmonic content compared with fixed hysteresis current control algorithm and sinusoidal PWM controller . The modeling and simulation of PV system along with the proposed controllers are done using MATLAB SIMSCAPE software . Simulation results show that the proposed MPPT algorithm is faster in transient state and presents smoother signal with less fluctuations in steady state . The hardware implementation of proposed MPPT algorithm and inverter current control algorithms using Xilinx spartran 3 FPGA is also presented . The experimental results show satisfactory performance of the proposed approaches . 
",A modified incremental conduction MPPT algorithm is proposed for solar PV system under partial shading conditions. An adaptive fuzzy modulator is developed to provide PWM pluses to the DC DC converter. An adaptive hysteresis current control algorithm is proposed for DC AC inverter in the PV system. The hardware implementation of proposed algorithms using Xilinx spartran 3 FPGA is presented.,,,
S156849461300344X," Brushless DC machines are found increasing use in applications that demand high and rugged performance . In some critical circumstance such as aerospace the motor must be highly reliable . In this context a novel model based fault diagnosis system is developed for brushless DC motor speed control system . Under the consideration of the complexity of characterizing the dynamic of BLDC motor control system with analytic expression a LRGF neural network with pole assignment technique is carried out for modeling the system . During the diagnosis process fault signal of the motor is isolated with LRGFNN online . Meanwhile adaptive lifting scheme and adaptive threshold method are presented for detecting the faults from the isolated fault signal under the existence of mechanical error and electrical error . The effectiveness of the diagnosis system is demonstrated in the simulation of electrical and mechanical fault in the motor . The detection of the incipient fault is also given . 
",An LRGF neural network with pole assignment technique is proposed to model the dynamic system. An adaptive lifting scheme is used for the improvement of the mechanical fault detection. An adaptive threshold scheme is proposed for the detection of several kinds of faults.,,,
S156849461300197X," A semi integrated system for driver assistance and pedestrian safety is presented . This system is composed of a single camera which focuses on the driver for picking up visual cues and a stereo rig that focus on the road ahead for the detection of road obstructions and pedestrians . While the car is in motion the driver s viewing direction is obtained and analyzed along with information of road condition and any moving vehicle ahead in order to determine if the current driving condition is safe . In addition when the vehicle is moving slowly the system can also detect the existence of a pedestrian ahead and warns the driver if the pedestrian moves in front of the car . This system contains algorithm based safety analysis as well as fuzzy rules based analysis for interaction between variables . Our experimental results show that the condition for driver safety can be accurately classified in 94.5 of the tested driving conditions and the pedestrians can be identified in 93.18 of the tested cases . These were compared to the results of similar systems and shown to be superior . 
",A driver assistance system that correlates obstacles with driver view is proposed. Fuzzy rules based subsystems are used for analysis under different road conditions. One set of fuzzy rules for when a vehicle or pedestrian ahead is detected. One set of fuzzy rules for when no immediate obstacle is detected. Lab experiments and comparisons using real world data show good performance.,,,
S156849461400146X," Large number of population based Differential Evolution algorithms has been proposed in the literature . Their good performance is often reported for benchmark problems . However when applied to Neural Networks training for regression these methods usually perform poorer than classical Levenberg Marquardt algorithm . The major aim of the present paper is to clarify why In this research in which Neural Networks are used for a real world regression problem it is empirically shown that various Differential Evolution algorithms are falling into stagnation during Neural Network training . It means that after some time the individuals stop improving or improve very occasionally although the population diversity remains high . Similar behavior of Differential Evolution algorithms is observed for some but not the majority of benchmark problems . In the paper the impact of Differential Evolution population size the initialization range and bounds on Neural Networks performance is also discussed . Among tested algorithms only the Differential Evolution with Global and Local neighborhood based mutation operators performs better than the Levenberg Marquardt algorithm for Neural Networks training . This version of Differential Evolution also shows the symptoms of stagnation but much weaker than the other tested variants . To enhance exploitation in the final stage of Neural Networks training it is proposed to merge the Differential Evolution with Global and Local neighborhood based mutation operators algorithm with the Trigonometric mutation operator . This method does not rule out the stagnation problem but slightly improves the performance of trained Neural Networks . 
",Differential Evolution algorithms applied to ANN training suffer from stagnation. The lack of difference vectors of small magnitude is noted during ANN training by Differential Evolution methods. In case of benchmark problems the lack of difference vectors of small magnitude is only occasionally observed. DEGL algorithm outperforms other Differential Evolution variants for ANN training. Best algorithms found for benchmark problems do not perform well for ANN training.,,,
S156849461400088X," It is undeniably crucial for a firm to be able to make a forecast regarding the sales volume of new products . However the current economic environments invariably have uncertain factors and rapid fluctuations where decision makers must draw conclusions from minimal data . Previous studies combine scenario analysis and technology substitution models to forecast the market share of multigenerational technologies . However a technology substitution model based on a logistic curve will not always fit the S curve well . Therefore based on historical data and the data forecast by both the Scenario and Delphi methods a two stage fuzzy piecewise logistic growth model with multiple objective programming is proposed herein . The piecewise concept is adopted in order to reflect the market impact of a new product such that it can be possible to determine the effective length of sales forecasting intervals even when handling a large variation in data or small size data . In order to demonstrate the model s performance two cases in the Television and Telecommunication industries are treated using the proposed method and the technology substitution model or the Norton and Bass diffusion model . A comparison of the results shows that the proposed model outperforms the technology substitution model and the Norton and Bass diffusion model . 
",Use historical data and the data investigated by the Scenario and Delphi methods. Propose a two stage fuzzy piecewise logistic growth model for sale forecasting. Forecast the market shares of the optimistic pessimistic and most possible scenarios. Demonstrate two cases in the Television and Telecommunication industries of the global market. Outperform the technology substitution model or the Norton and Bass diffusion model according to MAE MSE and MAPE.,,,
S156849461400194X," This paper presents a detailed study about a biomedical image retrieval framework by extracting Phase Congruency features from L a b triplets of images and representing them in fuzzy feature space . These features correspond to an edge corner map of the given image . The resulting map is then processed by Scale Invariant Feature Transform to derive keypoints that are invariant to affine transformations . The ensuing features were vector quantized to build a codebook of keypoints . The codebook was produced using a Spherical Self Organizing Map built with a geodesic data structure termed as GeoSOM . Then keypoints of the query image are mapped with the codebook and their occurrences are counted to formulate a histogram termed as Phase Congruency based Bag of Keypoints . This histogram is generated offline for target images and a similarity measure was performed with the query image to yield the nearest match based on a global fuzzy membership function . Exhaustive experiments of the proposed framework named as BIRS were performed on a diverse medical image collection . Finally performance of BIRS demonstrates the advantage of the proposed image representation approach in terms of Precision Recall parameters . Furthermore relative comparison of the proposed scheme with existing feature descriptors depicts improved P R values . The proposed feature extraction and representation scheme was also robust against quantization errors . 
",BIRS Biomedical Image Retrieval System framework for diverse medical image collection. Phase Congruency based features for acute image characterizations. Building codebook using GeoSOM for image encoding thereby achieving dimensionality reduction. SIFT features combined with fuzzy function for significant improvement in information retrieval. Performance analysis indicates reduction in retrieval time with efficient indexing and effective retrieval.,,,
S156849461400310X," This research addresses system reliability analysis using weakest t norm based approximate intuitionistic fuzzy arithmetic operations where failure probabilities of all components are represented by different types of intuitionistic fuzzy numbers . Due to the incomplete imprecise vague and conflicting information about the component of system the present study evaluates the reliability of system in terms of membership function and non membership function by using weakest t norm based approximate intuitionistic fuzzy arithmetic operations on different types of intuitionistic fuzzy numbers . In general interval arithmetic operations have been used to analyze the fuzzy system reliability . In complicated systems interval arithmetic operations may occur the accumulating phenomenon of fuzziness . In order to overcome the accumulating phenomenon of fuzziness this research adopts approximate intuitionistic fuzzy arithmetic operations under the weakest t norm arithmetic operations to analyze fuzzy system reliability . The approximate intuitionistic fuzzy arithmetic operations employ principle of interval arithmetic under the weakest t norm arithmetic operations . The proposed novel fuzzy arithmetic operations may obtain fitter decision values which have smaller fuzziness accumulating and successfully analyze the system reliability . Also weakest t norm arithmetic operations provide more exact fuzzy results and effectively reduce fuzzy spreads . Using proposed approach fuzzy reliability of series system and parallel system are also constructed . For numerical verification of proposed approach a malfunction of printed circuit board assembly is presented as a numerical example . The result of the proposed method is compared with the listing approaches of reliability analysis methods . 
",Weakest t norm T based approximate intuitionistic fuzzy arithmetic operations on different types of intuitionistic fuzzy numbers to evaluate fault interval and reliability interval. The proposed novel fuzzy arithmetic operations may obtain fitter decision values which have smaller fuzziness accumulating and successfully analyze the system reliability. Also weakest t norm arithmetic operations provide more exact fuzzy results and effectively reduce fuzzy spreads fuzzy intervals . Fuzzy reliability of PCBA fault has been analyzed using the proposed approach.,,,
S156849461400129X," The present paper proposes a multidimensional coupled chaotic map as a pseudo random number generator . Based on an introduced dynamical systems a watermark scheme is presented . By modifying the original image and embedding a watermark in the difference values within the original image the proposed scheme overcomes the problem of embedding a watermark in the spatial domain . As the watermark extraction does not require the original image the introduced model can be employed for practical applications . This algorithm tries to improve the problem of failure of embedding in small key space embedding speed and level of security . 
",A multidimensional coupled chaotic map as a pseudo random number generator is proposed. A watermark scheme is presented. The proposed scheme is subjected to some security analysis as well as statistical tests suites. Watermark extraction does not require the original image.,,,
S156849461400221X," Inspired by the ideas of multi swarm information sharing and elitist perturbation guiding a novel multi swarm cooperative multistage perturbation guiding particle swarm optimizer is proposed in this paper . The multi swarm information sharing idea is to harmoniously improve the evolving efficiency via information communicating and sharing among different sub swarms with different evolution mechanisms . It is possible to drive a stagnated sub swarm to revitalize once again with the beneficial information obtained from other sub swarms . Multistage elitist perturbation guiding strategy aims to slow down the learning speed and intensity in a certain extent from the global best individual while keeping the elitist learning mechanism . It effectively enlarges the exploration domain and diversifies the flying tracks of particles . Extensive experiments indicate that the proposed strategies are necessary and cooperative both of which construct a promising algorithm MCpPSO when comparing with other particle swarm optimizers and state of the art algorithms . The ideas of central position perturbation along the global best particle different computing approaches for central position and important parameters influence analysis are presented and analyzed . 
",Propose a novel multi swarm cooperative multistage perturbation guiding particle swarm optimizer. Multi swarm information sharing idea aims to improve the evolving efficiency via information communicating and sharing among different sub swarms. Multistage perturbation guiding strategy aims to slow down the learning speed and intensity. Comprehensive studies on algorithm are presented.,,,
S156849461400060X," In actuality for example the review of the National Science Foundation and the blind peer review of doctoral dissertation in China the evaluation experts are requested to provide two types of information such as the performance of the evaluation objects and the familiarity with the evaluation areas . However existing information aggregation research achievements can not be used to fusion the two types information described above effectively . In this paper we focus on the information aggregation issue in the situation where there are confidence levels of the aggregated arguments under intuitionistic fuzzy environment . Firstly we develop some confidence intuitionistic fuzzy weighted aggregation operators such as the confidence intuitionistic fuzzy weighted averaging operator and the confidence intuitionistic fuzzy weighted geometric operator . Then based on the Einstein operations we proposed the confidence intuitionistic fuzzy Einstein weighted averaging operator and the confidence intuitionistic fuzzy Einstein weighted geometric operator . Finally a practical example about the review of the doctoral dissertation in Chinese universities is provided to illustrate the developed intuitionistic fuzzy information aggregation operators . 
",The confidence intuitionistic fuzzy weighted averaging CIFWA operator and the confidence intuitionistic fuzzy weighted geometric CIFWG operator are proposed. The confidence intuitionistic fuzzy Einstein weighted averaging CIFEWA operator and the confidence intuitionistic fuzzy Einstein weighted geometric CIFEWG operator are proposed. The properties of the CIFWA and the CIFWG operators are studied in detail. The application of the proposed method to the review of the doctoral dissertation in Chinese universities is provided.,,,
S156849461400249X," Autonomy and adaptability are key features of intelligent agents . Many applications of intelligent agents such as the control of ambient intelligence environments and autonomous intelligent robotic systems require the processing of information coming in from many available sensors to produce adequate output responses in changing scenarios . Autonomy in these cases applies not only to the ability of the agent to produce correct outputs without human guidance but also to its ubiquity and or portability low power consumption and integrability . In this sense an embedded electronic system implementation paradigm can be applied to the design of autonomous intelligent agents in order to satisfy the above mentioned characteristics . However processing complex computational intelligence algorithms with tight delay constraints in resource constrained and low power embedded systems is a challenging engineering problem . In this paper a single chip intelligent agent based on a computationally efficient neuro fuzzy information processing core is described . The system has been endowed with an information preprocessing module based on Principal Component Analysis that permits a substantial reduction of the input space dimensionality with little loss of modeling capability . Moreover the PCA module has been tested as a means to achieve deep adaptability in changing environment dynamics and to endow the agent with fault tolerance in the presence of sensor failures . For data driven trials and research a data set obtained from an experimental intelligent inhabited environment has been used as a benchmark system . 
",Single chip embedded MIMO autonomous intelligent agent with on line learning capability small footprint low power transparent device. Dynamic deep adaptability through an integrated PCA analyzer. Dynamic on chip significant feature space reduction for adaptation speed up and enhanced generalization capability. Fault tolerance in the presence of sensor failures by minimum input feature space re computing in the integrated PCA module. Real implementation figures and real operation verification.,,,
S156849461400177X," Traditional parametric software reliability growth models are based on some assumptions or distributions and none such single model can produce accurate prediction results in all circumstances . Non parametric models like the artificial neural network based models can predict software reliability based on only fault history data without any assumptions . In this paper initially we propose a robust feedforward neural network based dynamic weighted combination model for software reliability prediction . Four well known traditional SRGMs are combined based on the dynamically evaluated weights determined by the learning algorithm of the proposed FFNN . Based on this proposed FFNN architecture we also propose a robust recurrent neural network based dynamic weighted combination model to predict the software reliability more justifiably . A real coded genetic algorithm is proposed to train the ANNs . Predictability of the proposed models are compared with the existing ANN based software reliability models through three real software failure data sets . We also compare the performances of the proposed models with the models that can be developed by combining three or two of the four SRGMs . Comparative studies demonstrate that the PFFNNDWCM and PRNNDWCM present fairly accurate fitting and predictive capability than the other existing ANN based models . Numerical and graphical explanations show that PRNNDWCM is promising for software reliability prediction since its fitting and prediction error is much less relative to the PFFNNDWCM . 
",We propose robust feedforward and recurrent neural network based dynamic weighted combination models. We combine four software reliability growth models by dynamically evaluated weights. We propose genetic algorithm based learning algorithm to train the proposed ANNs. Experimental results demonstrate that proposed models have fairly accurate predictability. PRNNDWCM has best software reliability prediction capability.,,,
S156849461400074X," This paper proposes a new evolutionary algorithm for continuous non linear optimization problems . This optimization algorithm is inspired by the procedure of trading the shares on stock market and it is called exchange market algorithm . Evaluation of how the stocks are traded on the stock market by elites has formed this evolutionary as an optimization algorithm . In the proposed method there are two different modes in EMA . In the first mode there is no oscillation in the market whereas in the second mode the market has oscillation . It is noticeable that at the end of each mode the individuals finesses are evaluated . For the first mode the algorithm s duty is to recruit people toward successful individuals while in the second case the algorithm seeks optimal points . In this algorithm the generation and organization of random numbers are performed in the best way due to the existence of two absorbent operators and two searching operators leading to high capability in global optimum point extraction . To evaluate the performance of the proposed algorithm this algorithm has been implemented on 12 different benchmark functions with 10 20 30 and 50 dimension variables . The results obtained by 30 dimension variables are compared with the results obtained by the eight new and efficient algorithms . The results indicate the ability of the proposed algorithm in finding the global optimum point of the functions for each run of the program . 
",This paper proposes a new heuristic algorithm for solving optimization problems. This optimization algorithm is inspired of trading the shares on stock market. The proposed algorithm is successfully implemented on 12 benchmark functions. Result shows the high ability of proposed algorithm in global optimum extraction.,,,
S156849461400235X," Mechanical and physical properties of sandstone are interesting scientifically and have great practical significance as well as their relations to the mineralogy and pore features . These relations are however highly nonlinear and can not be easily formulated by conventional methods . This paper investigates the potential of the technique named as the relevance vector machine for prediction of the elastic compressibility of sandstone based on its characteristics of physical properties . Based on the fact that the hyper parameters may have effects on the RVM performance an iteration method is proposed in this paper to search for optimal hyper parameter value so that it can produce best predictions . Also the qualitative sensitivity of the physical properties is investigated by the backward regression analysis . Meanwhile the hyper parameter effect of the RVM approach is discussed in the prediction of the elastic compressibility of sandstone . The predicted results of the RVM demonstrate that hyper parameter values have evident effects on the RVM performance . Comparisons on the results of the RVM the artificial neural network and the support vector machine prove that the proposed strategy is feasible and reliable for prediction of the elastic compressibility of sandstone based on its physical properties . 
",The relevance vector machine is evaluated for estimation of rock compressibility based on physical properties. An iteration strategy is proposed to optimize the hyper parameters of the relevance vector machine. The parameter effect is demonstrated on the performance of the relevance vector machine. The adaptive relevance vector machine is compared to the artificial neural networks and the support vector machine in the estimation.,,,
S156849461400132X," Human performance evaluation is one of the most important fields to analyze for the continuity of an organization . Evaluation files filled by the managers generally end up in dusty folders where no one looks . This decreases the credibility of the evaluators and the process itself . Whereas the management thinks that they are taking the valuable time from the people who can do better things instead of these evaluations . In this paper we add an engineering point of view to this process by giving a Hybrid Multicriteria Decision Making approach to evaluate employees performances working for a same task and explain an efficient way of handling the qualitative and quantitative data simultaneously . The real life situations where performance criteria show interaction will be possible to solve and the different types of interactions will be handled with the proposed hybrid method using Analytical Network Process and Choquet Integral simultaneously . We also give a numerical illustration at the end of the study with the appropriate concluding remarks including the advantages of the proposed method . 
",We propose a performance evaluation method for human resources. We use a Hybrid Multicriteria Decision Making Model fort his purpose. All possible interaction types are considered and handled with the proposed method. Ignoring the interactions may lead to erroneous decisions.,,,
S156849461400163X," Scalar and vector drives have been the cornerstones of control of industrial motors for decades . In both the elimination of mechanical speed sensor consists in a trend of modern drives . This work proposes the development of an adaptive neuro fuzzy inference system angular rotor speed estimator applied to vector and scalar drives . A multi frequency training of ANFIS is proposed initially for a V f scheme and after that a vector drive with magnetizing flux oriented control is proposed . In the literature ANFIS has been commonly proposed as a speed controller in substitution of the classical PI controller of the speed control loop . This paper investigates the ANFIS as an open loop speed estimator instead . The subtractive clustering technique was used as strategy for generating the membership functions for all the incoming signal inputs of ANFIS . This provided a better analysis of the training data set improving the comprehension of the estimator . Additionally the subtractive cluster technique allowed the training with experimental data corrupted by noise improving the estimator robustness . Simulations to evaluate the performance of the estimator considering the V f and vector drive system were realized using the Matlab Simulink software . Finally experimental results are presented to validate the ANFIS open loop estimator . angular speed of the magnetizing flux oriented reference frame angular slip speed direct sum overall function implemented by the adaptive network arbitrary function instantaneous value of magnetizing current instantaneous value of direct component stator current instantaneous value of quadrature component stator current magnetizing inductance stator inductance predicted network output for the pattern k differential operator positive constant positive constant stator resistance set of parameters desired network output for the pattern k rotor leakage time constant rotor time constant instantaneous value of direct component stator voltage instantaneous value of quadrature component stator voltage vector of input variables moment of inertia number of training pattern number of pole pairs 
",It was modeled a ANFIS speed estimator applied to both vector and scalar induction motor drives. Subtractive clustering was used to generate the membership functions. Subtractive clustering allowed training the ANFIS with experimental data with noise. In the scalar drive the ANFIS estimator used the RMS values of voltages and currents as incoming signals. Magnetizing FOC was used in the vector drive instead of Rotor FOC.,,,
S156849461400057X," Most of the real world problems have dynamic characteristics where one or more elements of the underlying model for a given problem including the objective constraints or even environmental parameters may change over time . Hyper heuristics are problem independent meta heuristic techniques that are automating the process of selecting and generating multiple low level heuristics to solve static combinatorial optimization problems . In this paper we present a novel hybrid strategy for applicability of hyper heuristic techniques on dynamic environments by integrating them with the memory search algorithm . The memory search algorithm is an important evolutionary technique that have applied on various dynamic optimization problems . We validate performance of our method by considering both the dynamic generalized assignment problem and the moving peaks benchmark . The former problem is extended from the generalized assignment problem by changing resource consumptions capacity constraints and costs of jobs over time and the latter one is a well known synthetic problem that generates and updates a multidimensional landscape consisting of several peaks . Experimental evaluation performed on various instances of the given two problems validates that our hyper heuristic integrated framework significantly outperforms the memory search algorithm . 
",We propose a novel hybrid strategy for applicability of hyper heuristic techniques on dynamic environments. Performance of our method is validated with the dynamic generalized assignment problem and the moving peaks benchmark. Our approach outperforms the related work for various problem instances with respect to quality of solutions.,,,
S156849461300450X," Artificial chromosomes with genetic algorithm is one of the latest versions of the estimation of distribution algorithms . This algorithm has already been applied successfully to solve different kinds of scheduling problems . However due to the fact that its probabilistic model does not consider variable interactions ACGA may not perform well in some scheduling problems particularly if sequence dependent setup times are considered . This is due to the fact that the previous job will influence the processing time of the next job . Simply capturing ordinal information from the parental distribution is not sufficient for a probabilistic model . As a result this paper proposes a bi variate probabilistic model to add into the ACGA . This new algorithm is called the ACGA2 and is used to solve single machine scheduling problems with sequence dependent setup times in a common due date environment . A theoretical analysis is given in this paper . Some heuristics and local search algorithm variable neighborhood search are also employed in the ACGA2 . The results indicate that the average error ratio of this ACGA2 is half the error ratio of the ACGA . In addition when ACGA2 is applied in combination with other heuristic methods and VNS the hybrid algorithm achieves optimal solution quality in comparison with other algorithms in the literature . Thus the proposed algorithms are effective for solving the scheduling problems . 
",The major motivation of ACGA2 is to take the bi variate probabilistic models into the consideration. We further provide a theoretical analysis of the two models EDAs. This studied conducted extensive experiments on the single machine scheduling problems with sequence dependent setup times in a common due date environment. The experimental result shows the proposed ACGA2 outperforms ACGA significantly because the average error ratio of ACGA2 is half of ACGA.,,,
S156849461400180X," Four wave mixing crosstalk is the dominant nonlinear effect in long haul repeaterless wavelength division multiplexing lightwave fiber optical communication systems . To reduce FWM crosstalk in optical communication systems unequally spaced channel allocation method is used . One of the unequal bandwidth channel allocation techniques is designed by using the concept of Golomb ruler . It allows the gradual computation of an optimally allocated channel set such that degradations caused by inter channel interference and FWM is minimal . This paper applies two soft computing based approaches i.e . Genetic Algorithm and Biogeography Based Optimization to generate near optimal Golomb ruler sequences in reasonable time . The generated sequences have been compared with the two other classical approaches namely Extended Quadratic Congruence and Search Algorithm . It has been observed that BBO GA outperforms the other two approaches . 
",The application of Genetic Algorithm and Biogeography Based Optimization to generate near optimal Golomb ruler sequences is being proposed. Both the approaches produce near optimal Golomb ruler sequences very efficiently in reasonable execution time. The performances have been compared with the two other classical methods namely Extended Quadratic Congruence and Search Algorithm. The preliminary results indicate that BBO and GA appear to be most efficient approach to such NP complete problems. BBO approach outperforms the GA slightly for larger mark values.,,,
S156849461400091X," In this paper some multi item inventory models for deteriorating items are developed in a random planning horizon under inflation and time value money with space and budget constraints . The proposed models allow stock dependent consumption rate and partially backlogged shortages . Here the time horizon is a random variable with exponential distribution . The inventory parameters other than planning horizon are deterministic in one model and in the other the deterioration and net value of the money are fuzzy available budget and space are fuzzy and random fuzzy respectively . Fuzzy and random fuzzy constraints have been defuzzified using possibility and possibility probability chance constraint techniques . The fuzzy objective function also has been defuzzified using possibility chance constraint against a goal . Both deterministic optimization problems are formulated for maximization of profit and solved using genetic algorithm and fuzzy simulation based genetic algorithm . The models are illustrated with some numerical data . Results for different achievement levels are obtained and sensitivity analysis on expected profit function is also presented . Scope and purpose The traditional inventory model considers the ideal case in which depletion of inventory is caused by a constant demand rate . However for more sale inventory should be maintained at a higher level . Of course this would result in higher holding or procurement cost etc . Also in many real situations during a shortage period the longer the waiting time is the smaller the backlogging rate would be . For instance for fashionable commodities and high tech products with short product life cycle the willingness for a customer to wait for backlogging diminishes with the length of the waiting time . Most of the classical inventory models did not take into account the effects of inflation and time value of money . But at present the economic situation of most of the countries has been much deteriorated due to large scale inflation and consequent sharp decline in the purchasing power of money . So it has not been possible to ignore the effects of inflation and time value of money any further . The purpose of this article is to maximize the expected profit of two inventory control systems in the random planning horizon . 
",Multi item inventory models allowing partial backlogging deterioration under random planning horizon. Inventory models with random planning horizon in imprecise environment with the effect of inflation and discounting. A random fuzzy constraint has been successively introduced for the first time in the inventory models. A real life inventory system with limitations on available budget and storing space constraints in both fuzzy and random fuzzy. Maximum profit has been compared by genetic algorithm and fuzzy simulation based genetic algorithm.,,,
S156849461400307X," This study applies the Multiple Criteria Decision Making to evaluate the service quality of some Turkish hospitals . In general the service quality has abstract properties which mean that using the previously known measurement approach is insufficient . It is for this reason that the fuzzy set theory is adopted as a research template . In Istanbul Turkey there are four B class hospitals classed as private hospitals that are covered by the Social Security Institution and for which we propose to represent the service performance measurement using triangular fuzzy numbers . In this study importance weights of performance criteria are found with AHP . Then the Multiple Criteria Decision Making methods TOPSIS and Yager s min max approach are applied to find and rank the crisp performance values . In a second step an aggregation of performance criteria with OWA and Compensatory AND operators are looked at instead of the TOPSIS method and min max approach . Thereby numerical applications are supplied by the four methods and the obtained results are compared . 
",We have applied Multiple Criteria Decision Making MCDM to evaluate the service quality of Turkish hospitals. We adopted fuzzy set theory as a research template. Importance weights of the performance criteria have found with AHP. TOPSIS which is MCDM method has applied to find and rank the crisp performance values. OWA operators Compensatory AND operator and Yager s Min Max method have investigated instead of TOPSIS method.,,,
S156849461400252X," In this paper we suggest a block image encryption algorithm which can give us an efficient scheme to hide and encrypt image data . Only the diffusion function instead of classical permutation plus diffusion operations is adopted . The plain image is firstly divided into two equal parts randomly by vertical horizontal or diagonal directions . Then encryption of one part depends on the other part in which the keystream is generated by the plain image i.e . one of the two parts . An error concept is added in the initial conditions in every round . It means that the keystreams are different in the process of encryption steps . The error may be positive or negative decided by a rule of sign function . Experiment results show that the proposed method can provide a high security of cryptosystem and can reduce the computation redundancy compared with that of the traditional architectures such as Arnold map based method and totally shuffling based method . 
",Self adaptive error is added in the initial conditions in every round of which shows the novelty of this manuscript. A rule is designed to let errors be positive or negative. A new control parameter is generated dependent on errors it is also self adaptive. Only diffusion function is applied but it can reach a high security and less time cost. The keystream depends on the plain image which can resist efficiently the known plaintext and chosen plaintext attacks. UACI unified average changing intensity and NPCR number of pixels change rate results confirm that the proposed method can reach the ideal effect.,,,
S156849461400266X," We develop an orthogonal forward selection approach to construct radial basis function network classifiers for two class problems . Our approach integrates several concepts in probabilistic modelling including cross validation mutual information and Bayesian hyperparameter fitting . At each stage of the OFS procedure one model term is selected by maximising the leave one out mutual information between the classifier s predicted class labels and the true class labels . We derive the formula of LOOMI within the OFS framework so that the LOOMI can be evaluated efficiently for model term selection . Furthermore a Bayesian procedure of hyperparameter fitting is also integrated into the each stage of the OFS to infer the l 2 norm based local regularisation parameter from the data . Since each forward stage is effectively fitting of a one variable model this task is very fast . The classifier construction procedure is automatically terminated without the need of using additional stopping criterion to yield very sparse RBF classifiers with excellent classification generalisation performance which is particular useful for the noisy data sets with highly overlapping class distribution . A number of benchmark examples are employed to demonstrate the effectiveness of our proposed approach . 
",An orthogonal forward selection algorithm is proposed for constructing radial basis function classifiers based on maximises the leave one out mutual information between the classifier s predicted class labels and the true class labels. Integrated within each OFS step a Bayesian procedure of hyperparameter fitting is introduced to infer the l 2 norm local regularisation parameter from the data. The results obtained demonstrate that the proposed algorithm automatically constructs very parsimonious RBF classifiers with excellent classification generalisation performance.,,,
S156849461400324X," In medical system there may be many critical diseases where experts do not have sufficient knowledge to handle those problems . For these cases experts may provide their opinion only about certain aspects of the disease and remain silent for those unknown features . Feeling the need of prioritizing different experts based on their given information this article uses a novel concept for assigning confident weights to different experts which are mainly based on their provided information . Experts provide their opinions about various symptoms using intuitionistic fuzzy soft matrix . In this article we propose an algorithmic approach based on intuitionistic fuzzy soft set which explores a particular disease reflecting the agreement of all experts . This approach is guided by the group decision making model and uses cardinals of IFSS as novel concept . We have used choice matrix as an important parameter which is based on choice parameters of individual expert . This article has also validated the proposed approach using distance measurements and consents of the majority of experts . The effectiveness of the proposed approach is demonstrated using a suitable case study . 
",Group decision making for medical diagnosis. Intuitionistic fuzzy soft set and fuzzy soft matrix. Hamming distance and Euclidean approach. Cardinal of intuitionistic fuzzy soft set to compute the weight. Viral fever related diagnosis.,,,
S156849461400115X," JCSE SPIHT an algorithm of joint compression and selective encryption based on set partitioning in hierarchical trees is proposed to achieve image encryption and compression simultaneously . It can protect SPIHT compressed images by only fast scrambling a tiny portion of crucial data during the coding process while keeping all the virtues of SPIHT intact . Intensive experiments are conducted to validate and evaluate the proposed algorithm the results show that the efficiency and the compression performance of JCSE SPIHT are very close to original SPIHT . In security analysis JCSE SPIHT is proved to be immune to various attacks not only from traditional cryptanalysis but also by utilizing sophisticated image processing techniques . 
",We propose JCSE SPIHT an algorithm of joint compression and selective encryption based on SPIHT. We design a fast random insertion to accelerate the encryption from taking O n 2 time to O n . JCSE SPIHT can generate plaintext dependent keystream by cryptographically secure PRNG. We exam the security of JCSE SPIHT by traditional cryptanalysis and image processing techniques.,,,
S156849461400297X," All dynamic crop models for growth and development have several parameters whose values are usually determined by using measurements coming from the real system . The parameter estimation problem is raised as an optimization problem and optimization algorithms are used to solve it . However because the model generally is nonlinear the optimization problem likely is multimodal and therefore classical local search methods fail in locating the global minimum and as a consequence the model parameters could be inaccurate estimated . This paper presents a comparison of several evolutionary and bio inspired algorithms considered as global optimization methods such as Differential Evolution Covariance Matrix Adaptation Evolution Strategy Particle Swarm Optimization and Artificial Bee Colony on parameter estimation of crop growth SUCROS model . Subsequently the SUCROS model for potential growth was applied to a husk tomato crop using data coming from an experiment carried out in Chapingo Mexico . The objective was to determine which algorithm generates parameter values that give the best prediction of the model . An analysis of variance was carried out to statistically evaluate the efficiency and effectiveness of the studied algorithms . Algorithm s efficiency was evaluated by counting the number of times the objective function was required to approximate an optimum . On the other hand the effectiveness was evaluated by counting the number of times that the algorithm converged to an optimum . Simulation results showed that standard DE rand 1 bin got the best result . 
",An approach based on evolutionary and bio inspired algorithms is proposed for solving the parameter estimation problem in crop growth dynamic models. Differential Evolution algorithm showed the best performance in solving the parameter estimation problem of a dynamic crop growth model. A statistical analysis and ANOVA were applied to quantitatively evaluate the efficiency and effectiveness of Differential Evolution CMA ES PSO ABC and LSE algorithms.,,,
S156849461400595X," This paper presents a novel parameter automation strategy for particle swarm optimization algorithm for solving non convex emission constrained economic dispatch problems . Many evolutionary techniques such as particle swarm optimization differential evolution have been applied to solve these problems and found to perform in a better way in comparison with conventional optimization methods . But often these methods converge to a sub optimal solution prematurely . This paper presents a new improved particle swarm optimization technique called self organizing hierarchical particle swarm optimization technique with time varying acceleration coefficients for non convex emission constrained economic dispatch problems to avoid premature convergence . Generator ramp rate limits and prohibited operating zones are taken into account in problem formulation . Non convex emission constrained economic dispatch problem is obtained by considering both the economy and emission objectives . The performance of the proposed method is demonstrated on two sample test systems . The results of the proposed method are compared with other methods . It is found that the results obtained by the proposed method are superior in terms of fuel cost emission output and losses . 
",Nonconvex emission constrained economic dispatch NECED is a complex optimization problem. Many optimization techniques and algorithm have been applied to solve it. A new improved particle swarm optimization technique is proposed for the same. The effectiveness of the proposed method is tested on two practical systems. The results are compared with classical as well as other heuristic technique.,,,
S156849461400413X," This paper presents a real coded chemical reaction based algorithm to solve the short term hydrothermal scheduling problem . Hydrothermal system is highly complex and related with every problem variables in a nonlinear way . The objective of the hydro thermal scheduling is to determine the optimal hourly schedule of power generation for different hydrothermal power system for certain intervals of time such that cost of power generation is minimum . Chemical reaction optimization mimics the interactions of molecules in term of chemical reaction to reach a low energy stable state . A real coded version of chemical reaction optimization known as real coded chemical reaction optimization is considered here . To check the effectiveness of the RCCRO 3 different test systems are considered and mathematical remodeling of the algorithm is done to make it suitable for solving short term hydrothermal scheduling problem . Simulation results confirm that the proposed approach outperforms several other existing optimization techniques in terms quality of solution obtained and computational efficiency . Results also establish the robustness of the proposed methodology to solve STHS problems . 
",The paper solved different short term hydro thermal scheduling STHS problems. The paper considered both small and large size test systems. Real coded chemical reaction optimization RCCRO is applied to solve the problem. Performance of RCCRO has been compared with different PSO DE modified DE etc. Performance of RCCRO has been found to be more encouraging to solve STHS problems.,,,
S156849461400619X," Although greedy algorithms possess high efficiency they often receive suboptimal solutions of the ensemble pruning problem since their exploration areas are limited in large extent . And another marked defect of almost all the currently existing ensemble pruning algorithms including greedy ones consists in they simply abandon all of the classifiers which fail in the competition of ensemble selection causing a considerable waste of useful resources and information . Inspired by these observations an interesting greedy Reverse Reduce Error pruning algorithm incorporated with the operation of subtraction is proposed in this work . The RRE algorithm makes the best of the defeated candidate networks in a way that the Worst Single Model is chosen and then its votes are subtracted from the votes made by those selected components within the pruned ensemble . The reason is because for most cases the WSM might make mistakes in its estimation for the test samples . And different from the classical RE the near optimal solution is produced based on the pruned error of all the available sequential subensembles . Besides the backfitting step of RE algorithm is replaced with the selection step of a WSM in RRE . Moreover the problem of ties might be solved more naturally with RRE . Finally soft voting approach is employed in the testing to RRE algorithm . The performances of RE and RRE algorithms and two baseline methods i.e . the method which selects the Best Single Model in the initial ensemble and the method which retains all member networks of the initial ensemble are evaluated on seven benchmark classification tasks under different initial ensemble setups . The results of the empirical investigation show the superiority of RRE over the other three ensemble pruning algorithms . 
",An interesting RRE pruning algorithm incorporated with the operation of subtraction is proposed in this work. The WSM is chosen and its votes are subtracted from the votes made by those selected components. The backfitting step of RE algorithm is replaced with the selection step of a WSB in RRE. The problem of ties might be solved more naturally with RRE. Soft voting approach is employed in the testing to RRE algorithm.,,,
S156849461400444X," PieceWise AutoRegressive eXogenous models represent one of the broad classes of the hybrid dynamical systems . Among many classes of HDS PWARX model used as an attractive modeling structure due to its equivalence to other classes . This paper presents a novel fuzzy distance weight matrix based parameter identification method for PWARX model . In the first phase of the proposed method estimation for the number of affine submodels present in the HDS is proposed using fuzzy clustering validation based algorithm . For the given set of input output data points generated by predefined PWARX model fuzzy c means clustering procedure is used to classify the data set according to its affine submodels . The fuzzy distance weight matrix based weighted least squares algorithm is proposed to identify the parameters for each PWARX submodel which minimizes the effect of noise and classification error . In the final phase fuzzy validity function based model selection method is applied to validate the identified PWARX model . The effectiveness of the proposed method is demonstrated using three benchmark examples . Simulation experiments show validation of the proposed method . 
",A novel fuzzy distance weight matrix based parameter identification method. Fuzzy clustering based algorithm used to find sub models of HDS. WLS algorithm is used to identify parameters of sub models. Results validated through simulation experiments.,,,
S156849461400489X," In this research we propose a novel framework referred to as collective game behavior decomposition where complex collective behavior is assumed to be generated by aggregation of several groups of agents following different strategies and complexity emerges from collaboration and competition of individuals . The strategy of an agent is modeled by certain simple game theory models with limited information . Genetic algorithms are used to obtain the optimal collective behavior decomposition based on history data . The trained model can be used for collective behavior prediction . For modeling individual behavior two simple games the minority game and mixed game are investigated in experiments on the real world stock prices and foreign exchange rate . Experimental results are presented to show the effectiveness of the new proposed model . 
",Collective behaviors are modeled as aggregations of individual behaviors. Individual behavior is modeled by the minority game. Parameters of individual behavior can be learned using genetic algorithms. The new model is tested based on real world financial data.,,,
S156849461400386X," Large scale software systems are in general difficult to manage and monitor . In many cases these systems display unexpected behavior especially after being updated or when changes occur in their environment . Therefore to handle a changing environment it is desirable to base fault detection and performance monitoring on self adaptive techniques . Several studies have been carried out in the past which inspired on the immune system aim at solving complex technological problems . Among them anomaly detection pattern recognition system security and data mining are problems that have been addressed in this framework . There are similarities between the software fault detection problem and the identification of the pathogens that are found in natural immune systems . Being inspired by vaccination and negative and clonal selection observed in these systems we developed an effective self adaptive model to monitor software applications analyzing the metrics of system resources . 
",Monitoring applications that only use negative selection feature detect too many false positives. The fault injection reinforcement learning process enables the overall model to become robust and efficient. The clonal selection feature promotes adaptation. The model is very flexible because is based on performance indicators and consumption resources.,,,
S156849461400492X," We propose a new image encryption algorithm which is based on the spatiotemporal non adjacent coupled map lattices . The system of non adjacent coupled map lattices has more outstanding cryptography features in dynamics than the logistic map or coupled map lattices does . In the proposed image encryption we employ a bit level pixel permutation strategy which enables bit planes of pixels permute mutually without any extra storage space . Simulations have been carried out and the results demonstrate the superior security and high efficiency of the proposed algorithm . 
",We propose an image encryption scheme based on a new spatiotemporal chaotic system. The encryption scheme is not the one time pad encryption. The proposed image encryption has a large key space and high security.,,,
S156849461400338X," Multilayer perceptron and support vector machine two popular learning machines are increasingly being used as alternatives to classical statistical models for ground level ozone prediction . However employing learning machines without sufficient awareness about their limitations can lead to unsatisfactory results in modeling the ozone evolving mechanism especially during ozone formation episodes . With the spirit of literature review and justification this paper discusses with respect to the concerning of ozone prediction the recently developed algorithms technologies for treating the most prominent model performance degradation limitations . MLP has the black box property i.e . it hardly provides physical explanation for the trained model overfitting and local minima problems and SVM has parameters identification and class imbalance problems . This commentary article aims to stress that the underlying philosophy of using learning machines is by no means as trivial as simply fitting models to the data because it causes difficulties controversies or unresolved problems . This article also aims to serve as a reference point for further technical readings for experts in relevant fields . 
",Address the potential of learning machine to forecast ground level ozone in urban area. Summarize the existing learning machines used to predict ground level ozone. Compare the performance of commented models via practical case in Hong Kong. Address the underlying philosophy of using learning machine in ozone related prediction.,,,
S156849461400502X," This paper presents fully fuzzy fixed charge multi item solid transportation problems in which direct costs fixed charges supplies demands conveyance capacities and transported quantities are fuzzy in nature . Objective is to minimize the total fuzzy cost under fuzzy decision variables . In this paper some approaches are proposed to find the fully fuzzy transported amounts for a fuzzy solid transportation problem . Proposed approaches are applicable for both balanced and unbalanced FFFCMISTPs . Another fuzzy fixed charge multi item solid transportation problem in which transported amounts are not fuzzy is also presented and solved by some other techniques . The models are illustrated with numerical examples and nature of the solutions is discussed . 
",Fully fuzzy fixed charge multi item solid transportation problem FFFCMISTP is considered. FFFCMISTP with the decision variable are taken as fuzzy. New defuzzification method fuzzy slack and surplus variable is used for FFFCMISTP. Minimization of transportation cost as well as fuzziness of the solution for FFFCMISTP is discussed.,,,
S156849461400622X," The proposed work involves the multiobjective PSO based adaption of optimal neural network topology for the classification of multispectral satellite images . It is per pixel supervised classification using spectral bands . This paper also presents a thorough experimental analysis to investigate the behavior of neural network classifier for given problem . Based on 1050 number of experiments we conclude that following two critical issues needs to be addressed selection of most discriminative spectral bands and determination of optimal number of nodes in hidden layer . We propose new methodology based on multiobjective particle swarm optimization technique to determine discriminative spectral bands and the number of hidden layer node simultaneously . The accuracy with neural network structure thus obtained is compared with that of traditional classifiers like MLC and Euclidean classifier . The performance of proposed classifier is evaluated quantitatively using Xie Beni and indexes . The result shows the superiority of the proposed method to the conventional one . 
",We present the work on efficient classification of multispectral images using soft computing approach. Selection of most discriminative spectral bands and determination of the number of hidden layer neurons are the two most critical issues. We proposed a new multiobjective particle swarm optimization based methodology for adaption of neural network structure for pixel classification of Satellite Imagery. It simultaneously estimates the most discriminative spectral features and the optimal number of nodes in hidden layer. Xie Beni and indexes of proposed algorithm are better than MLC and Euclidean Classifier.,,,
S156849461400578X," In this study two induced generalized hesitant fuzzy hybrid operators called the induced generalized hesitant fuzzy Shapley hybrid weighted averaging operator and the induced generalized hesitant fuzzy Shapley hybrid geometric mean operator are defined . The prominent characteristics of these two operators are that they do not only globally consider the importance of elements and their ordered positions but also overall reflect their correlations . Furthermore when the weight information of the attributes and the ordered positions is partly known using grey relational analysis method and the Shapley function models for the optimal fuzzy measures on an attribute set and on an ordered set are respectively established . Finally an approach to hesitant fuzzy multi attribute decision making with incomplete weight information and interactive conditions is developed and an illustrative example is provided to show its practicality and effectivity . 
",An improvement ranking method for HFEs is proposed. Two induced generalized hesitant fuzzy Shapley hybrid operators are defined. Models for the optimal fuzzy measures are built. An approach to hesitant fuzzy multi attribute decision making is developed.,,,
S156849461400547X," Bibliometrics is a discipline that analyzes bibliographic material from a quantitative perspective . It is very useful for classifying information according to different variables including journals institutions and countries . This paper presents a general overview of research in the fuzzy sciences using bibliometric indicators . The main advantage is that these indicators provide a general picture identifying some of the most influential research in this area . The analysis is divided into key sections focused on relevant journals papers authors institutions and countries . Most of the results are in accordance with our common knowledge although some unexpected results are also found . Note that the aim of this paper is to be informative and these indicators identify most of the fundamental research in this field . However some very influential issues may be omitted if they are not included in the Web of Science database which is used for carrying out the bibliometric analysis . 
",Bibliometrics in fuzzy research. List of most cited papers on fuzzy topics of all time. An overview of influential authors institutions and countries. Journal analysis in fuzzy research.,,,
S156849461400427X," In this paper we investigate the deviation of the priority weights from hesitant multiplicative preference relations in group decision making environments . As basic elements of HMPRs hesitant multiplicative elements usually have different numbers of possible values . To correctly compute or compare HMEs there are two principles to normalize them i.e . the normalization and the normalization . Based on the normalization we develop a new goal programming model to derive the priority weights from HMPRs in group decision making environments . Based on the normalization a consistent HMPR and an acceptably consistent HMPR are defined and their desired properties are studied . A convex combination method is then developed to obtain interval weights from an acceptably consistent HMPR . This approach is further extended to group decision making situations in which the experts evaluate their preferences as several HMPRs . Finally some numerical examples are provided to illustrate the validity and applicability of the proposed models . 
",We investigate the deviation of the priority weights from HMPRs under GDM. Based on the normalization and normalization we develop two models to derive the weights from HMPRs. Some numerical examples are given to illustrate the proposed models.,,,
S156849461400430X," Clustering is an efficient topology control method which balances the traffic load of the sensor nodes and improves the overall scalability and the life time of the wireless sensor networks . However in a cluster based WSN the cluster heads consume more energy due to extra work load of receiving the sensed data data aggregation and transmission of aggregated data to the base station . Moreover improper formation of clusters can make some CHs overloaded with high number of sensor nodes . This overload may lead to quick death of the CHs and thus partitions the network and thereby degrade the overall performance of the WSN . It is worthwhile to note that the computational complexity of finding optimum cluster for a large scale WSN is very high by a brute force approach . In this paper we propose a novel differential evolution based clustering algorithm for WSNs to prolong lifetime of the network by preventing faster death of the highly loaded CHs . We incorporate a local improvement phase to the traditional DE for faster convergence and better performance of our proposed algorithm . We perform extensive simulation of the proposed algorithm . The experimental results demonstrate the efficiency of the proposed algorithm . 
",The proposed work is a novel DE based clustering scheme for WSNs. The algorithm incorporates an additional step to enhance the performance. Experimental results demonstrate the superiority over existing algorithms. The performance is shown in terms of network life energy consumption etc.,,,
S156849461400550X," Evolutionary algorithms start with an initial population vector which is randomly generated when no preliminary knowledge about the solution is available . Recently it has been claimed that in solving continuous domain optimization problems the simultaneous consideration of randomness and opposition is more effective than pure randomness . In this paper it is mathematically proven that this scheme called opposition based learning also does well in binary spaces . The proposed binary opposition based scheme can be embedded inside many binary population based algorithms . We applied it to accelerate the convergence rate of binary gravitational search algorithm as an application . The experimental results and mathematical proofs confirm each other . 
",We introduce the concept of opposition based learning in binary spaces.It is proven that utilizing random numbers and their opposite is beneficial in evolutionary algorithms.Opposite numbers are applied to accelerate the convergence rate of Binary Gravitational Search Algorithm BGSA .The results show that OBGSA possesses superior performance in accuracy as compared to the BGSA.,,,
S156849461400341X," Application of machine learning techniques to the functional Magnetic Resonance Imaging data is recently an active field of research . There is however one area which does not receive due attention in the literature preparation of the fMRI data for subsequent modelling . In this study we focus on the issue of synchronization of the stream of fMRI snapshots with the mental states of the subject which is a form of smart filtering of the input data performed prior to building a predictive model . We demonstrate investigate and thoroughly discuss the negative effects of lack of alignment between the two streams and propose an original data driven approach to efficiently address this problem . Our solution involves casting the issue as a constrained optimization problem in combination with an alternative classification accuracy assessment scheme applicable to both batch and on line scenarios and able to capture information distributed across a number of input samples lifting the common simplifying i.i.d . assumption . The proposed method is tested using real fMRI data and experimentally compared to the state of the art ensemble models reported in the literature outperforming them by a wide margin . 
",We challenge a popular approach to labelling fMRI data for predictive modelling. We propose a new labelling method based on data stream synchronization. We validate the proposed method experimentally on real fMRI data. We observe major classification accuracy improvement and model complexity reduction.,,,
S156849461400475X," Global positioning system is the most widely used military and commercial positioning tool for real time navigation and location . Geometric dilution of precision stands as a relevant measure of positioning accuracy and consequently the performance quality of the GPS positioning algorithm . Since the calculation of GPS GDOP has a time and power burden that involves complicated transformation and inversion of measurement matrices in this paper we propose hybrid intelligent methods namely adaptive neuro fuzzy inference system improved ANFIS and radial basis function for GPS GDOP classification . Through investigation it is verified that the ANFIS is a high performance and valuable classifier . In the ANFIS training the radius vector has very important role for its recognition accuracy . Therefore in the optimization module bee algorithm is proposed for finding the optimum vector of radius . In order to improve the performance of the proposed method a new improvement for the BA is used . In addition to enhance the accuracy of the method principal component analysis is utilized as a pre processing step . Experimental results clearly indicate that the proposed intelligent methods have high classification accuracy rates comparing with conventional ones . 
",GDOP stands as a relevant measure of positioning accuracy. We propose hybrid intelligent methods namely ANFIS improved ANFIS and RBF for GPS GDOP classification. Bee algorithm BA and improved BA are proposed for finding the optimum radius vector of the ANFIS. To enhance the classification accuracy PCA is utilized.,,,
S156849461400581X," Removal of miscible hazardous materials from aqueous solutions is an alarming problem for the environmental scientists . Several linear and nonlinear regression models like Langmuir Freundlich D R Tempkin isotherm models are in vogue for determining the adsorbing capacity of standard adsorbents used for this purpose . In this article we propose a novel quantum inspired backpropagation multilayer perceptron based on quantum gates for the prediction of this adsorption behavior exhibited by calcareous soil oftentimes used in adsorbing miscible iron from aqueous solutions . The backpropagation learning formulae for the proposed QBMLP architecture has also been generalized for multiple number of layers in both field homogeneous and field heterogeneous configurations characterized by three standard activations viz . sigmoid tanh and tan1.5h functions . Applications of the efficiency of the proposed QBMLP over the regression models are demonstrated with regards to the prediction behavior of the adsorption of iron by calcareous soil from an aqueous solution with effect to various characteristic adsorbent parameters . The adsorption process is considered to be a physical one since the activation energy of ferrous ion adsorption is 9.469kJmol 1 due to Arrhenius . Moreover the thermodynamic parameters of Gibb s free energy enthalpy and entropy values indicate it be spontaneous . Results indicate that QBMLP predicts the adsorption behavior of calcareous soil to a very closer extent thereby obviating the need for further regression experimental analysis . Comparison with the performance of a similar classical multilayer perceptron architecture also reveals the prediction and time efficiency of the proposed QBMLP architecture . 
",The ability of calcareous soil to remove Fe II from aqueous solution is investigated. The maximum adsorption of Fe II of 2.475mg g at 6.0mgL 1 initial Fe II concentration was found at pH 6.0 and a temperature of 303K after 120min. As an inhouse alternative an intelligent technique has been envisaged by a conventional MLP and a proposed QBMLP architecture. Both the MLP and QBMLP are found to be efficient in the compared to the batch study process.,,,
S156849461400369X," Traditionally clustering is the task of dividing samples into homogeneous clusters based on their degrees of similarity . As samples are assigned to clusters users need to manually give descriptions for all clusters . In this paper a rapid fuzzy rule clustering method based on granular computing is proposed to give descriptions for all clusters . A new and simple unsupervised feature selection method is employed to endow every sample with a suitable description . Exemplar descriptions are selected from sample s descriptions by relative frequency and data granulation is guided by the selected exemplar fuzzy descriptions . Every cluster is depicted by a single fuzzy rule which make the clusters understandable for humans . The experimental results show that our proposed model is able to discover fuzzy IF THEN rules to obtain the potential clusters . 
",A rapid fuzzy rule clustering method based on granular computing is proposed. Exemplar descriptions are selected from sample s descriptions by relative frequency. Data granulation is guided by the selected exemplar descriptions.,,,
S156849461400533X," Recently a novel probabilistic model building evolutionary algorithm named probabilistic model building genetic network programming has been proposed . PMBGNP uses graph structures for its individual representation which shows higher expression ability than the classical EDAs . Hence it extends EDAs to solve a range of problems such as data mining and agent control . This paper is dedicated to propose a continuous version of PMBGNP for continuous optimization in agent control problems . Different from the other continuous EDAs the proposed algorithm evolves the continuous variables by reinforcement learning . We compare the performance with several state of the art algorithms on a real mobile robot control problem . The results show that the proposed algorithm outperforms the others with statistically significant differences . 
",This paper proposes a novel continuous estimation of distribution algorithm EDA . A recent EDA named PMBGNP is extended from discrete domain to continuous domain. Reinforcement Learning RL is applied to construct the probabilistic model. Experiments on real mobile robot control show the superiority of the proposed algorithm. It bridges the gap between EDA and RL.,,,
S156849461400372X," The evaluation of bone development is a complex and time consuming task for the physicians since it may cause intraobserver and interobserver differences . In this study we present a new training algorithm for support vector machines in order to determine the bone age in young children from newborn to 6 years old . By the new algorithm we aimed to assist the radiologists so as to eliminate the disadvantages of the methods used in bone age determination . To achieve this purpose primarily feature extraction procedure was performed to the left hand wrist X ray images by using image processing techniques and the features related with the carpal bones and distal epiphysis of radius bone were obtained . Then these features were used for the input arguments of the classifier . In the classification process a new training algorithm for support vector machines was proposed by using particle swarm optimization . When training support vector machines particle swarm optimization was used for generating a new training instance which will represent the whole training set of the related class by using the training set . Finally these new instances were used as the support vectors and classification process was carried out by using these new instances . The performance of the proposed method was compared with the naive Bayes k nearest neighborhood support vector machines and C4.5 algorithms . As a result it was determined that the proposed method was found successful than the other methods for bone age determination with a classification performance of 74.87 . 
",This paper proposes a new approach for training support vector machines with a bone age determination system. The proposed approach is a combination of particle swarm optimization PSO and support vector machines SVMs . The performance and accuracy of the proposed PSO SVM algorithm are examined on a bone age data set. The results obtained by PSO SVM show that PSO SVM is more effective than the previous study based on conventional SVM.,,,
S156849461500143X," This paper deals with Hamilton Jacobi Bellman equation based stabilized optimal control of hybrid dynamical systems . This paper presents the fuzzy clustering based event wise multiple linearized modeling approaches for HDS to describe the continuous dynamic in each event . In the present work a fuzzy clustering validation approach is presented for the selection of number of linearized models which span entire HDS . The method also describes how to obtain event wise operating point using fuzzy membership function which is used to find the event wise model bank by linearizing the first principles model . The event wise linearized models are used for the formulation of the optimal control law . The HJB equation is formulated using a suitable quadratic term in the objective function . By use of the direct method of Lyapunov stability the control law is shown to be optimal with respect to objective functional and stabilized the event wise linearized models . The global Lyapunov function is proposed with discrete variables which stabilized the HDS . The proposed modeling and control algorithm have been applied on two HDSs . Necessary theoretical and simulation experiments are presented to demonstrate the performance and validation of the proposed algorithm . 
",Hamilton Jacobi Bellman HJB equation based stabilized optimal control of hybrid dynamical systems HDS is presented. The fuzzy validity based method is used to find the number of linear models present in the HDS. Stability proof for the event wise and generalized HJB solution based optimal control is proposed. The proposed modeling and control algorithm have been applied on two HDSs.,,,
S156849461500277X," We present an optimization based unsupervised approach to automatic document summarization . In the proposed approach text summarization is modeled as a Boolean programming problem . This model generally attempts to optimize three properties namely relevance summary should contain informative textual units that are relevant to the user redundancy summaries should not contain multiple textual units that convey the same information and length summary is bounded in length . The approach proposed in this paper is applicable to both tasks single and multi document summarization . In both tasks documents are split into sentences in preprocessing . We select some salient sentences from document to generate a summary . Finally the summary is generated by threading all the selected sentences in the order that they appear in the original document . We implemented our model on multi document summarization task . When comparing our methods to several existing summarization methods on an open DUC2005 and DUC2007 data sets we found that our method improves the summarization results significantly . This is because first when extracting summary sentences this method not only focuses on the relevance scores of sentences to the whole sentence collection but also the topic representative of sentences . Second when generating a summary this method also deals with the problem of repetition of information . The methods were evaluated using ROUGE 1 ROUGE 2 and ROUGE SU4 metrics . In this paper we also demonstrate that the summarization result depends on the similarity measure . Results of the experiment showed that combination of symmetric and asymmetric similarity measures yields better result than their use separately . 
",We model document summarization as a quadratic Boolean programming problem. We create a modified differential evolution to solve the optimization problem. Experimental study shows that the model improves the summarization results.,,,
S156849461500321X," In this paper a Multitree Genetic Programming based method is developed to learn an INTerpretable and ACcurate Takagi Sugeno Kang fuzzy rule based sYstem for dynamic portfolio trading . The MGP INTACTSKY utilizes a TSK model with a new structure to develop a more interpretable and accurate system for dynamic portfolio trading . In the new structure of TSK disjunctive normal form rules with variable structured consequent parts are developed in which the absence of some input variables is allowed . Input variables are the most influential technical indices which are selected by stepwise regression analysis . The technical indices are computed using wavelet transformed stock price series to eliminate the noise . The proposed system directly induces the preferred portfolio weights from the stock s technical indices through time . Here genetic programming with the multitree structure is applied to learn the TSK fuzzy rule bases with the Pittsburgh approach . With this approach the correlation of different stocks is properly considered during the evolutionary process . To evaluate the performance of the MGP INTACTSKY for portfolio trading the proposed model is implemented on the Tehran Stock Exchange as an emerging market as well as Toronto and Frankfurt Stock Exchanges as two mature markets . The experimental results show that the proposed model outperforms other methods such as the momentum strategy the multitree genetic programming based crisp system the genetic algorithm based first order TSK system the buy and hold approach and the market s main index in terms of accuracy and interpretability . 
",MGP INTACTSKY is a fuzzy rule based system for dynamic portfolio trading. Multitree Genetic Programming MGP is applied to learn the TSK fuzzy rule bases. The new TSK structure leads to a more interpretable and accurate system. Input variables are technical indices selected by stepwise regression analysis. The results are based on testing of the model on one emerging and two mature markets.,,,
S156849461400670X," In this article the dynamic multi swarm particle swarm optimizer and a new cooperative learning strategy are hybridized to obtain DMS PSO CLS . DMS PSO is a recently developed multi swarm optimization algorithm and has strong exploration ability for the use of a novel randomly regrouping schedule . However the frequently regrouping operation of DMS PSO results in the deficiency of the exploitation ability . In order to achieve a good balance between the exploration and exploitation abilities the cooperative learning strategy is hybridized to DMS PSO which makes information be used more effectively to generate better quality solutions . In the proposed strategy for each sub swarm each dimension of the two worst particles learns from the better particle of two randomly selected sub swarms using tournament selection strategy so that particles can have more excellent exemplars to learn and can find the global optimum more easily . Experiments are conducted on some well known benchmarks and the results show that DMS PSO CLS has a superior performance in comparison with DMS PSO and several other popular PSO variants . 
",A new cooperative learning strategy is hybridized with DMS PSO. Information can be exchanged among sub swarms before the regrouping process. Experimental results show that DMS PSO CLS has a superior performance.,,,
S156849461500174X," Designing of scalable routing protocol with prolonged network lifetime for a wireless sensor network is a challenging task . WSN consists of large number of power communication and computational constrained inexpensive nodes . It is difficult to replace or recharge battery of a WSN node when operated in a hostile environment . Cluster based routing is one of the techniques to provide prolonged network lifetime along with scalability . This paper proposes a technique for cluster formation derived from Grid based method . We have also proposed a new decentralized cluster head election method based on Bollinger Bands . Bollinger Bands are based on Upper Bollinger Band and Lower Bollinger Band both of these bands are extremely reactive to any change in the inputs provided to them . We have used this property of Bollinger Bands to elect CH . Simulation result shows significant improvement in the network lifetime in comparison with other decentralized and ant based algorithms . 
",Used Grid Based Method to create clusters. Derived cluster head selection method based on Bollinger Bands BB that is a technical trading tool. Simulation results shows significant improvement in the network life time that is measured using First Node Dies FND Half of the Nodes Alive HNA and Last Node Dies LND . Proposed algorithm is compared with seven different algorithms using three different node deployment pattern and two different positions of Sink.,,,
S156849461400667X," The pyramidal dual tree directional filter bank transform is a new image decomposition which has many advantages such as multiscale and multidirectional transform efficient implementation high angular resolution low redundant ratio and shiftable subbands . In this paper we present a new color image segmentation algorithm based on PDTDFB domain hidden Markov tree model . Firstly the joint statistics and mutual information of the PDTDFB coefficients are studied . Then the PDTDFB coefficients are modeled using a HMT model with Gaussian mixtures which can effectively capture the intra scale inter scale and inter direction dependencies . Finally a color image segmentation using PDTDFB domain HMT model is developed in which expectation maximization parameter estimation Bayesian multiscale raw segmentation context based multiscale fusion and majority vote based color component fusion are used . Experimental evidence shows that the proposed color image segmentation algorithm has very effective segmentation results in comparison with the state of the art segmentation methods recently proposed in the literature . 
",The joint statistics and mutual information of the PDTDFB transform coefficients are studied. The PDTDFB transform coefficients are modeled using a HMT statistical model with Gaussian mixtures. The intra scale inter scale and inter direction dependencies are captured comprehensively. A PDTDFB domain HMT statistical model based color image segmentation scheme is developed.,,,
S156849461500071X," This paper presents a modified version of the water cycle algorithm . The fundamental concepts and ideas which underlie the WCA are inspired based on the observation of water cycle process and how rivers and streams flow to the sea . New concept of evaporation rate for different rivers and streams is defined so called evaporation rate based WCA which offers improvement in search . Furthermore the evaporation condition is also applied for streams that directly flow to sea based on the new approach . The ER WCA shows a better balance between exploration and exploitation phases compared to the standard WCA . It is shown that the ER WCA offers high potential in finding all global optima of multimodal and benchmark functions . The WCA and ER WCA are tested using several multimodal benchmark functions and the obtained optimization results show that in most cases the ER WCA converges to the global solution faster and offers more accurate results than the WCA and other considered optimizers . Based on the performance of ER WCA on a number of well known benchmark functions the efficiency of the proposed method with respect to the number of function evaluations and accuracy of function value are represented . 
",WCA concepts are inspired by nature and based on water cycle process. New concepts of evaporation rate for rivers and streams are applied in the ER WCA. Variable evaporation rate is utilized in the ER WCA to evaporate the water adaptively. ER WCA forces new generated streams to search near sea using the concept of variance. ER WCA outperforms or equals other methods in terms of function evaluations and solution quality.,,,
S156849461500023X," In this paper a hybrid gravitational search algorithm and pattern search technique is proposed for load frequency control of multi area power system . Initially various conventional error criterions are considered the PI controller parameters for a two area power system are optimized employing GSA and the effect of objective function on system performance is analyzed . Then GSA control parameters are tuned by carrying out multiple runs of algorithm for each control parameter variation . After that PS is employed to fine tune the best solution provided by GSA . Further modifications in the objective function and controller structure are introduced and the controller parameters are optimized employing the proposed hybrid GSA and PS approach . The superiority of the proposed approach is demonstrated by comparing the results with some recently published modern heuristic optimization techniques such as firefly algorithm differential evolution bacteria foraging optimization algorithm particle swarm optimization hybrid BFOA PSO NSGA II and genetic algorithm for the same interconnected power system . Additionally sensitivity analysis is performed by varying the system parameters and operating load conditions from their nominal values . Also the proposed approach is extended to two area reheat thermal power system by considering the physical constraints such as reheat turbine generation rate constraint and governor dead band nonlinearity . Finally to demonstrate the ability of the proposed algorithm to cope with nonlinear and unequal interconnected areas with different controller coefficients the study is extended to a nonlinear three unequal area power system and the controller parameters of each area are optimized using proposed hGSA PS technique . 
",Selection of objective function and controller structure is vital for controller design. An objective function using ITAE damping ratio and settling times is proposed. The concept is applied to design an hGSA PS based PI PID controller for LFC. Nonlinear interconnected power system model with GRC GDB and time delay is considered.,,,
S156849461500318X," In this paper we present a clustering framework for type 2 fuzzy clustering which covers all steps of the clustering process including clustering algorithm parameters estimation and validation and verification indices . The proposed clustering algorithm is developed based on dual centers type 2 fuzzy clustering model . In this model the centers of clusters are defined by a pair of objects rather than a single object . The membership values of the objects to the clusters are defined by type 2 fuzzy numbers and there are not any type reduction or defuzzification steps in the proposed clustering algorithm . In addition the relation among the size of the cluster bandwidth distance between dual centers and fuzzifier parameter are indicated and analyzed to facilitate the parameters estimation step . To determine the optimum number of clusters we develop a new validation index which is compatible with the proposed model structure . A new compatible verification index is also defined to compare the results of the proposed model with existing type 1 fuzzy clustering model . Finally the results of computational experiments are presented to show the efficiency of the proposed approach . 
",The clustering model considers dual centers rather than single centers. The dual centers type 2 clustering model and algorithm are proposed. The relations among parameters of the proposed model are explained. The degrees of belonging to the clusters are defined by type 2 fuzzy numbers. The verification and verification indices are developed for model evaluation.,,,
S156849461500201X," Unplanned dilution and ore loss are the most critical challenges in underground stoping operations . These problems are the main cause behind a mine closure and directly influencing the productivity of the underground stope mining and the profitability of the entire operation . Despite being aware of the significance of unplanned dilution and ore loss prediction of these phenomena is still unexplained as they occur through complex mechanisms and causative factors . Current management practices primarily rely on similar stope reconciliation data and the intuition of expert mining engineers . In this study an innovative unplanned dilution and ore loss management system is established using a neuro fuzzy system . The aim of the proposed decision support system is to overcome the UB phenomenon in underground stope blasting which provides quantitative prediction of unplanned dilution and ore loss with practical recommendations simultaneously . To achieve the method proposed an uneven break prediction system was developed by an artificial neural network considering 1076 datasets covering 10 major UB causative factors collected from three underground stoping mines in Western Australia . In succession the UB consultation system was established via a fuzzy expert system in reference to surveyed results of fifteen underground mining experts . The UB prediction and consultation system were combined as one concurrent neuro fuzzy system that is named the uneven break optimiser . Because the current UB prediction systems in investigated mines were highly unsatisfactory with correlation coefficient of 0.088 and limited to only unplanned dilution the performance of the proposed UB prediction system is a remarkable achievement . The uneven break optimiser can be directly employed to improve underground stoping production and this tool will be beneficial not only for underground stope planning and design but also for production management . 
",Focus on a minimisation unplanned dilution and ore loss in underground stope. Comprehensive datasets were collected from three mines. Prediction model was built via artificial neural network ANN . Consultation model was built via Fuzzy expert system FES . ANN and FES are combined as a concurrent neuro fuzzy model.,,,
S156849461500054X," Feature selection is often required as a preliminary step for many pattern recognition problems . However most of the existing algorithms only work in a centralized fashion i.e . using the whole dataset at once . In this research a new method for distributing the feature selection process is proposed . It distributes the data by features i.e . according to a vertical distribution and then performs a merging procedure which updates the feature subset according to improvements in the classification accuracy . The effectiveness of our proposal is tested on microarray data which has brought a difficult challenge for researchers due to the high number of gene expression contained and the small samples size . The results on eight microarray datasets show that the execution time is considerably shortened whereas the performance is maintained or even improved compared to the standard algorithms applied to the non partitioned datasets . 
",Feature selection is indispensable when dealing with microarray data. A new method for distributing the filtering process is proposed. The data is distributed by features and then merged in a final subset. The method is tested on 8 microarray datasets. The classification accuracy is maintained and the time considerably shortened.,,,
S156849461500294X," In this paper we aim at proposing a switching adaptive control scheme using a Hopfield based dynamic neural network for nonlinear systems with external disturbances . In our proposed scheme an auxiliary direct adaptive controller ensures the system stability when the indirect adaptive controller is failed that is approaches to zero where is the denominator of an indirect adaptive control law . The IAC s limitation of then can be solved by simply switching the IAC to the DAC where is a positive desired value . The Hopfield dynamic neural network is used to not only design DAC but also approximate the unknown plant nonlinearities in IAC design . The designed simple structure of HDNN keeps the tracking performance well and also makes the practical implementation much easier because of the use of less and fixed number of neurons . 
",A switching adaptive control scheme using a Hopfield based dynamic neural network SACHNN for nonlinear systems with external disturbances is proposed. The IAC s limitation of can be solved by simply switching the IAC to the DAC where is a positive desired value. The Hopfield dynamic neural network HDNN is used to not only design DAC but also approximate the unknown plant nonlinearities in IAC design.,,,
S156849461500068X," Hill climbing constitutes one of the simplest way to produce approximate solutions of a combinatorial optimization problem and is a central component of most advanced metaheuristics . This paper focuses on evaluating climbing techniques in a context where deteriorating moves are not allowed in order to isolate the intensification aspect of metaheuristics . We aim at providing guidelines to choose the most adequate method for climbing efficiently fitness landscapes with respect to their size and some ruggedness and neutrality measures . To achieve this we compare best and first improvement strategies as well as different neutral move policies on a large set of combinatorial fitness landscapes derived from academic optimization problems including NK landscapes . The conclusions highlight that first improvement is globally more efficient to explore most landscapes while best improvement superiority is observed only on smooth landscapes and on some particular structured landscapes . The empirical analysis realized on neutral move policies shows that a stochastic hill climbing reaches in average better configurations and requires fewer evaluations than other climbing techniques . Results indicate that accepting neutral moves at each step of the search should be useful on all landscapes especially those having a significant rate of neutrality . Last we point out that reducing adequately the precision of a fitness function makes the climbing more efficient and helps to solve combinatorial optimization problems . 
",Comparison of the behavior of classical basic local search techniques and their ability to reach high local optima in combinatorial fitness landscapes. Two particular aspects of local search are considered pivoting rules first and best improvement strategies neutral move policies how neutral moves are considered during the search . Experimental analysis on four different landscape models non neutral and neutral NK landscapes flow shop QAP MAXSAT .,,,
S156849461500126X," The classification of imbalanced data is a major challenge for machine learning . In this paper we presented a fuzzy total margin based support vector machine method to handle the class imbalance learning problem in the presence of outliers and noise . The proposed method incorporates total margin algorithm different cost functions and the proper approach of fuzzification of the penalty into FTM SVM and formulates them in nonlinear case . We considered an excellent type of fuzzy membership functions to assign fuzzy membership values and got six FTM SVM settings . We evaluated the proposed FTM SVM method on two artificial data sets and 16 real world imbalanced data sets . Experimental results show that the proposed FTM SVM method has higher G Mean and F Measure values than some existing CIL methods . Based on the overall results we can conclude that the proposed FTM SVM method is effective for CIL problem especially in the presence of outliers and noise in data sets . 
",A fuzzy total margin based support vector machine FTM SVM method to handle the class imbalance learning CIL problem in the presence of outliers and noise was presented. The proposed method incorporates total margin algorithm different cost functions and the proper approach of fuzzification of the penalty into FTM SVM and formulates them in nonlinear case. We thoroughly evaluated the proposed FTM SVM method on two artificial data sets and sixteen real world imbalanced data sets,,,
S156849461500191X," Support vector machine is sensitive to the outliers which reduces its generalization ability . This paper presents a novel support vector regression together with fuzzification theory inconsistency matrix and neighbors match operator to address this critical issue . Fuzzification method is exploited to assign similarities on the input space and on the output response to each pair of training samples respectively . The inconsistency matrix is used to calculate the weights of input variables followed by searching outliers through a novel neighborhood matching algorithm and then eliminating them . Finally the processed data is sent to the original SVR and the prediction results are acquired . A simulation example and three real world applications demonstrate the proposed method for data set with outliers . 
",Support vector machine is sensitive to the outliers. A novel support vector regression together with fuzzification theory inconsistency matrix and neighbors match operator is presented. The objective of this novel support vector regression is to increase the generalization ability for data set with outliers.,,,
S156849461500188X," In this study a dynamic screening strategy is proposed to discriminate subjects with autistic spectrum disorder from healthy controls . The ASD is defined as a neurodevelopmental disorder that disrupts normal patterns of connectivity between the brain regions . Therefore the potential use of such abnormality for autism screening is investigated . The connectivity patterns are estimated from electroencephalogram data collected from 8 brain regions under various mental states . The EEG data of 12 healthy controls and 6 autistic children were collected during eyes open and eyes close resting states as well as when subjects were exposed to affective faces . Subsequently the subjects were classified as autistic or healthy groups based on their brain connectivity patterns using pattern recognition techniques . Performance of the proposed system in each mental state is separately evaluated . The results present higher recognition rates using functional connectivity features when compared against other existing feature extraction methods . 
",A dynamic strategy for screening of ASD is proposed based on patterns of information flow between 8 brain regions. The EEG data is collected from 12 healthy and 6 autistic children in the age of 7 to10 years old. The subjects are then classified as autistic or healthy based on the connectivity features. The connectivity features are also compared with other established methods. The promising recognition rates of 93 were achieved in this study. This study shows that patterns of functional and effective connectivity in ASD subjects are different from healthy subjects.,,,
S156849461500215X," In this paper a new fuzzy peer assessment methodology that considers vagueness and imprecision of words used throughout the evaluation process in a cooperative learning environment is proposed . Instead of numerals words are used in the evaluation process in order to provide greater flexibility . The proposed methodology is a synthesis of perceptual computing and a fuzzy ranking algorithm . Per C is adopted because it allows uncertainties of words to be considered in the evaluation process . Meanwhile the fuzzy ranking algorithm is deployed to obtain appropriate performance indices that reflect a student s contribution in a group and subsequently rank the student accordingly . A case study to demonstrate the effectiveness of the proposed methodology is described . Implications of the results are analyzed and discussed . The outcomes clearly demonstrate that the proposed fuzzy peer assessment methodology can be deployed as an effective evaluation tool for cooperative learning of students . 
",A new fuzzy peer assessment methodology is proposed. A synthesis of perceptual computing and a fuzzy ranking algorithm. Providing both crisp scores and recommendations. A case study in Universiti Malaysia Sarawak is reported. An illustrative example is adopted to detect possible free riders.,,,
S156849461500085X," Image quality assessment of distorted or decompressed images without any reference to the original image is challenging from computational point of view . Quality of an image is best judged by human observers without any reference image and evaluated using subjective measures . The paper aims at designing a generic no reference image quality assessment method by incorporating human visual perception in assigning quality class labels to the images . Using fuzzy logic approach we consider information theoretic entropies of visually salient regions of images as features and assess quality of the images using linguistic values . The features are transformed into fuzzy feature space by designing an algorithm based on interval type 2 fuzzy sets . The algorithm measures uncertainty present in the input output feature space to predict image quality accurately as close to human observations . We have taken a set of training images belonging to five different pre assigned quality class labels for calculating foot print of uncertainty corresponding to each class . To assess the quality class label of the test images maximum of T conorm applied on the lower and upper membership functions of the test images belonging to different classes is calculated . Our proposed image quality metric is compared with other no reference quality metrics demonstrating more accurate results and compatible with subjective mean opinion score metric . 
",Human visual perception in assessing image quality using type 2 fuzzy sets. Entropy of visually salient regions measure uncertainty in feature space. Transformation of features to interval type 2 fuzzy feature space. Free from type reduction and defuzzification computation. Promising results compared to prominent subjective and objective image quality metrics.,,,
S156849461500304X," The job shop scheduling problem with operators is an extension of the classic job shop problem in which an operation must be assisted by one of a limited set of human operators so it models many real life situations . In this paper we tackle the JSO by means of memetic algorithms with the objective of minimizing the makespan . We define and analyze a neighborhood structure which is then exploited in local search and tabu search algorithms . These algorithms are combined with a conventional genetic algorithm to improve a fraction of the chromosomes in each generation . We also consider two different schedule builders for chromosome decoding . All these elements are combined to obtain memetic algorithms which are evaluated over an extensive set of instances . The results of the experimental study show that they reach high quality solutions in very short time comparing favorably with the state of the art methods . 
",New neighborhood structures for the job shop scheduling problem with operators JSO are proposed and analyzed. A new memetic algorithm is proposed for the JSO that incorporates the neighborhood structures in the local search procedure. An experimental study was conducted showing that the memetic algorithm compares favorably with the state of the art.,,,
S156849461500099X," A new disturbance detection and classification technique based on modified Adaline and adaptive neuro fuzzy information system is proposed for a distributed generation system comprising a wind power generating system and a photovoltaic array . The proposed technique is based on a fast Gauss Newton parameter updating rule rather than the conventional Widrow Hoff delta rule for the Adaline network . The voltage and current signals near the target distributed generation particularly the DFIG whose speed varies from minimum to the maximum cut off speed are processed through the modified Adaline network to yield the features like the negative sequence power harmonic amplification factor total harmonic distortion etc . These features are then used as training sets for the ANFIS which employs a gradient descent algorithm to update its parameters . The proposed technique distinguishes the islanding condition of the distributed generation system with some other disturbances such as switching faults capacitor bank switching voltage swell voltage sag distorted grid voltage unbalanced load switching etc . which are referred to as non islanding cases in this paper . 
",A modified Adaline and ANFIS are used for disturbance detection in distribution generation. Training of the Adaline is done using a robust decoupled Gauss Newton algorithm. Impact of wind velocity of the wind farm on islanding and non islanding cases is studied. Power quality indices are used to classify disturbances. Comparison with other techniques is shown to validate the superiority.,,,
S156849461500263X," Classifying walking patterns helps the diagnosis of health status disease progression and the effect of interventions . In this paper we develop previous research on human gait to extract a meaningful set of parameters that allow us to design a highly interpretable system capable of identifying different gait styles with linguistic fuzzy if then rules . The model easily discriminates among five different walking patterns namely normal walk on tiptoes dragging left limb dragging right limb and dragging both limbs . We have carried out a complete experimentation to test the performance of the extracted parameters to correctly classify these five chosen gait styles . 
",Based on previous research in the field of computing with perceptions. Highly interpretable and efficient linguistic model used to recognize the gait phases. Set of parameters that characterize relevant aspects of the gait. Fuzzy rule based classifier that discriminates among different walking patterns. Parameters capable of recognizing among five walking patterns with an accuracy of 84 .,,,
S156849461500006X," Recent research revealed that model assisted parameter tuning can improve the quality of supervised machine learning models . The tuned models were especially found to generalize better and to be more robust compared to other optimization approaches . However the advantages of the tuning often came along with high computation times meaning a real burden for employing tuning algorithms . While the training with a reduced number of patterns can be a solution to this it is often connected with decreasing model accuracies and increasing instabilities and noise . Hence we propose a novel approach defined by a two criteria optimization task where both the runtime and the quality of ML models are optimized . Because the budgets for this optimization task are usually very restricted in ML the surrogate assisted Efficient Global Optimization algorithm is adapted . In order to cope with noisy experiments we apply two hypervolume indicator based EGO algorithms with smoothing and re interpolation of the surrogate models . The techniques do not need replicates . We find that these EGO techniques can outperform traditional approaches such as latin hypercube sampling as well as EGO variants with replicates . 
",The Kriging based EGO techniques performed better than the baseline LHS approach. The use of re interpolation is crucial to cope with noise. Repeats can be necessary but also decrease the number of possible infill points.,,,
S156849461500229X," In this paper speed control of Brushless DC motor using Bat algorithm optimized online Adaptive Neuro Fuzzy Inference System is presented . Learning parameters of the online ANFIS controller i.e . Learning Rate Forgetting Factor and Steepest Descent Momentum Constant are optimized for different operating conditions of Brushless DC motor using Genetic Algorithm Particle Swarm Optimization and Bat algorithm . In addition tuning of the gains of the Proportional Integral Derivative Fuzzy PID and Adaptive Fuzzy Logic Controller is optimized using Genetic Algorithm Particle Swarm Optimization and Bat Algorithm . Time domain specification of the speed response such as rise time peak overshoot undershoot recovery time settling time and steady state error is obtained and compared for the considered controllers . Also performance indices such as Root Mean Squared Error Integral of Absolute Error Integral of Time Multiplied Absolute Error and Integral of Squared Error are evaluated and compared for the above controllers . In order to validate the effectiveness of the proposed controller simulation is performed under constant load condition varying load condition and varying set speed conditions of the Brushless DC motor . The real time experimental verification of the proposed controller is verified using an advanced DSP processor . The simulation and experimental results confirm that bat algorithm optimized online ANFIS controller outperforms the other controllers under all considered operating conditions . 
",Bat algorithm optimized online ANFIS based speed controller presented for Brushless DC motor. The speed response of Brushless DC motor is analyzed for different operating conditions. The proposed controller eliminates the uncertainty problem due to load disturbance and set speed variations. The proposed controller enhances the time domain specifications and performance indices in all operating conditions.,,,
S156849461500558X," The single machine scheduling problem with sequence dependent setup times with the objective of minimizing the total weighted tardiness is a challenging problem due to its complexity and has a huge number of applications in real production environments . In this paper we propose a memetic algorithm that combines and extends several ideas from the literature including a crossover operator that respects both the absolute and relative position of the tasks a replacement strategy that improves the diversity of the population and an effective but computationally expensive neighborhood structure . We propose a new decomposition of this neighborhood that can be used by a variable neighborhood descent framework and also some speed up methods for evaluating the neighbors . In this way we can obtain competitive running times . We conduct an experimental study to analyze the proposed algorithm and prove that it is significantly better than the state of the art in standard benchmarks . 
",A replacement strategy that improves the diversity of the population is proposed. A decomposition of a computationally expensive neighborhood is defined. Some methods to speed up the evaluation of the neighbors are proposed and extended. The resulting hybrid algorithm is significantly better than the state of the art.,,,
S156849461500513X," The traditional visual and acoustic embolic signal detection methods based on the expert analysis of individual spectral recordings and Doppler shift sounds are the gold standards . However these types of detection methods are high cost subjective and can only be applied by experts . In order to overcome these drawbacks computer based automated embolic detection systems which employ spectral properties of emboli speckle and artifact using Fourier and Wavelet Transforms have been proposed . In this study we propose a fast accurate and robust automated emboli detection system based on the Dual Tree Complex Wavelet Transform . Employing the DTCWT which does not suffer from the lack of shift invariance property of ordinary Discrete Wavelet Transform increases the robustness of the coefficients extracted from the Doppler ultrasound signals . In this study a Doppler ultrasound dataset including 100 samples from each embolic Doppler speckle and artifact signal is used . Each sample obtained from forward and reverse blood flow directions is represented by 1024 points . In our method we first extract the forward and reverse blood flow coefficients separately using DTCWT from the samples . Then dimensionality reduction is applied to each set of coefficients and both of the reduced set of coefficients are fed to classifiers individually . Subsequently in the view that the forward and reverse blood flow coefficients carry different characteristics the individual predictors of these classifiers are combined using ensemble stacking method . We compare the obtained results with Fast Fourier Transform and DWT based emboli detection systems and show that the features extracted using DTCWT give the highest accuracy and emboli detection rate . It is also observed that combining forward and reverse coefficients using stacking ensemble method improves the emboli and artifact detection rates and overall accuracy . 
",Embolic signals are used for the identification of active embolic sources in stroke prone individuals. Dual Tree Complex Wavelet Transform DTCWT is used as a new feature extractor from forward and reverse Doppler ultrasound signals. The features acquired from forward and reverse flow directions of the blood are fed into k NN and SVMs. The individual predictions of classifiers are combined using ensemble stacking method considering that the forward and reverse blood flow coefficients carry different characteristics. The results show that the DTCWT is superior to the DWT and FFT.,,,
S156849461500455X," This paper presents a biologically inspired sequential learning spiking neural classifier for pattern classification problems . It consists of a two layered neural network and a separate decision block which estimates the predicted class label . Inspired by observations in the neuroscience literature the input layer employs a new neuron model which converts real valued stimuli into spikes with varying amplitudes and firing times . The intermediate layer neurons are modeled as integrate and fire spiking neurons . The decision block identifies that intermediate neuron which fires first and returns the class label associated with that neuron as the predicted class label . The sequential learning algorithm for the spiking neural network automatically determines the network structure from the training samples and adapts its synaptic weights by long term potentiation and long term depression . Performance of SLSNC has been evaluated using a number of benchmark classification problems and the results have been compared with other well known spiking neural network classifiers in the literature as well as with the standard support vector machine with a Gaussian kernel and the fast learning Extreme Learning Machine classifiers . The results clearly indicate that the described spiking neural network produces similar or better generalization performance with a smaller network . 
",LSNC automatically evolves the architecture. Real valued data is encoded using a 2 D encoding having spike amplitude and time. Sequential learning algorithm developed for SLSNC. Learning algorithm relies on computationally inexpensive operations.,,,
S156849461500561X," The ability of artificial immune systems to adapt to varying pathogens makes such systems a suitable choice for various robotic applications . Generally immunity based robotic applications map local instantaneous sensory information into either an antigen or a co stimulatory signal according to the choice of representation schema . Algorithms then use relevant immune functions to output either evolved antibodies or maturity of dendritic cells in terms of actuation signals . It is observed that researchers do not try to replicate the biological immunity but select necessary immune functions instead resulting in an ad hoc manner these applications are reported . On the other hand the paradigm shift in robotics research from reactive to probabilistic approaches is also not being reflected in these applications . Authors therefore present a detailed review of immuno inspired robotic applications in an attempt to identify the possible areas to explore . Moreover the literature has been categorized according to the underlying immuno definitions . Implementation details have been critically reviewed in terms of corresponding mathematical expressions and their representation schema that include binary real or hybrid approaches . Limitations of reported applications have also been identified in light of modern immunological interpretations including the danger theory . As a result of this study authors suggest a renewed focus on innate immunity action contextualization prior to B T cell invocation and behavior evolution instead of arbitration . In this context a multi tier immunological framework for robotics research combining innate and adaptive components together is also suggested and skeletonized . 
",Immunity based robotic applications are reviewed according to immunological models old and new. Mathematical details of reported literature are tabulated genealogically. Issues pertaining to validity of immunological models are raised. We have suggested immunological equivalents of various support functions in these applications. Modern trends in robotics are emphasized in conjunction with those in immunology.,,,
S156849461500366X," Genetic algorithm is a branch of evolutionary algorithm has proved its effectiveness in solving constrain based complex real world problems in variety of dimensions . The individual phases of GA are the mimic of the basic biological processes and hence the self adaptability of GA varied in accordance to the adjustable natural processes . In some instances self adaptability in GA fails in identifying adaptable genes to form a solution set after recombination which leads converge toward infeasible solution sometimes this infeasible solution could not be converted into feasible form by means of any of the repairing techniques . In this perspective Gene Suppressor a bio inspired process is being proposed as a new phase after recombination in the classical GA life cycle . This phase works on new individuals generated after recombination to attain self adaptability by adapting best genes in the environment to regulate chromosomes expression for achieving desired phenotype expression . Repairing in this phase converts infeasible solution into feasible solution by suppressing conflicting gene from the environment . Further the solution vector expression is improved by inducing best genes expression in the environment within the set of intended constrains . Multiobjective Multiple Knapsack Problems one of the popular NP hard combinatorial problems is being considered as the test bed for proving the competence of the proposed new phase of GA . The standard MMKP benchmark instances obtained from OR library are used for the experiments reported in this paper . The outcomes of the proposed method is compared with the existing repairing techniques where the analyses proved the proficiency of the proposed GS model in terms of better error and convergence rates for all instances . 
",Gene Suppressor proposed as a new add on phase in GA for attaining self adaption and repairing. This regulates genes dosage and its functional expression with respect to its environment. Identifies suppressor genes to perform suppression activity for attaining specific phenotype. Allows adjustment by gene adaption and repairing to obtain best solution and improving it. Experiment focused on proving single problem but the buildup model can be easily adopted to other problem that uses MKP as a base.,,,
S156849461500441X," In this paper we propose a H.264 AVC compressed domain human action recognition system with projection based metacognitive learning classifier . The features are extracted from the quantization parameters and the motion vectors of the compressed video stream for a time window and used as input to the classifier . Since compressed domain analysis is done with noisy sparse compression parameters it is a huge challenge to achieve performance comparable to pixel domain analysis . On the positive side compressed domain allows rapid analysis of videos compared to pixel level analysis . The classification results are analyzed for different values of Group of Pictures parameter time window including full videos . The functional relationship between the features and action labels are established using PBL McRBFN with a cognitive and meta cognitive component . The cognitive component is a radial basis function while the meta cognitive component employs self regulation to achieve better performance in subject independent action recognition task . The proposed approach is faster and shows comparable performance with respect to the state of the art pixel domain counterparts . It employs partial decoding which rules out the complexity of full decoding and minimizes computational load and memory usage . This results in reduced hardware utilization and increased speed of classification . The results are compared with two benchmark datasets and show more than 90 accuracy using the PBL McRBFN . The performance for various GOP parameters and group of frames are obtained with twenty random trials and compared with other well known classifiers in machine learning literature . 
",Proposed a fast human action recognition system in H.264 compressed domain. Readily available quantization parameters and motion vectors are used for feature extraction. The functional relationship between the features and action labels are classified using PBL McRBFN which has a cognitive and meta cognitive component. Results are analyzed for various GOP parameters. Achieved more than 90 accuracy using PBL McRBFN on two benchmark datasets KTH and Weizmann.,,,
S156849461500383X," In this paper we investigate the reduction in total transmission time and the energy consumption of wireless sensor networks using multi hop data aggregation by forming coordination in hierarchical clustering . Novel algorithm handles wireless sensor network in numerous circumstances as in large extent and high density deployments . One of the major purposes is to collect information from inaccessible areas by using factorization of the area into subareas and appointing cluster head in each of the subarea . Coordination and cooperation among the local nodes via relay nodes in local cluster helped to serve each and every node . Routing is based on the predefined path proposed by new transmission algorithm . Transmission distance is minimized by using cluster coordinators for inter cluster communication and relay nodes within the cluster . We show by extended simulations that Chain Based Cluster Cooperative Protocol performs very well in terms of energy and time . To prove it we compare it with LEACH SEP genetic HCR and ERP and found that new protocol consumes six times less energy than LEACH five times less energy than SEP four time less energy than genetic HCR and three times less energy than ERP which further validate our work . 
",This paper has presented the our new routing protocol with all results in QoS Quality of service metrics of WSN Wireless Sensor Nodes and it is compared with LEACH SEP genetic HCR and ERP routing protocols. This research is useful for densely deployed networks Terrestrial Environmental and Border surveillance applications . This paper presents the routing protocol in which data is transmitted thorough the CHs and CCOs for intra and inter cluster communication. Our research has optimized many parameters of sensor nodes as Energy Time Reliability Throughput and scalability and results calculated in MATLAB validate our work.,,,
S156849461500527X," The analysis of internal connective operators of fuzzy reasoning is very significant and the robustness of fuzzy reasoning has been calling for study . An interesting and important question is that how to choose suitable internal connective operators to guarantee good robustness of rule based fuzzy reasoning This paper is intended to answer it . In this paper Lipschitz aggregation property and copula characteristic of t norms and implications are discussed . The robustness of rule based fuzzy reasoning is investigated and the relationships among input perturbation rule perturbation and output perturbation are presented . The suitable t norm and implication can be chosen to satisfy the need of robustness of fuzzy reasoning . In 1 Lipschitz operators if both t norm and implication are copulas the rule based fuzzy reasoning is much more stable and more reliable . In copulas if both t norm and implication are 1 l Lipschitz they can guarantee good robustness of fuzzy reasoning . The experiments not only illustrate the ideas proposed in the paper but also can be regarded as applications of soft computing . The approach in the paper also provides guidance for choosing suitable fuzzy connective operators and decision making application in rule based fuzzy reasoning . 
",Lipschitz aggregation property and copula characteristic of t norms and implications are discussed and the robustness of rule based fuzzy reasoning is investigated. According to Lipschitz aggregation property and copula characteristic of t norms and implications suitable t norm and implication can be chosen to satisfy the need of robustness of fuzzy reasoning. The approach provides guidance for choosing suitable fuzzy connective operators and decision making application in rule based fuzzy reasoning. The experiments which illustrate the ideas are the applications of intelligent information processing.,,,
S156849461500352X," Time series forecasting has been widely used to determine future prices of stocks and the analysis and modeling of finance time series is an important task for guiding investors decisions and trades . Nonetheless the prediction of prices by means of a time series is not trivial and it requires a thorough analysis of indexes variables and other data . In addition in a dynamic environment such as the stock market the non linearity of the time series is a pronounced characteristic and this immediately affects the efficacy of stock price forecasts . Thus this paper aims at proposing a methodology that forecasts the maximum and minimum day stock prices of three Brazilian power distribution companies which are traded in the S o Paulo Stock Exchange BM FBovespa . When compared to the other papers already published in the literature one of the main contributions and novelty of this paper is the forecast of the range of closing prices of Brazilian power distribution companies stocks . As a result of its application investors may be able to define threshold values for their stock trades . Moreover such a methodology may be of great interest to home brokers who do not possess ample knowledge to invest in such companies . The proposed methodology is based on the calculation of distinct features to be analysed by means of attribute selection defining the most relevant attributes to predict the maximum and minimum day stock prices of each company . Then the actual prediction was carried out by Artificial Neural Networks which had their performances evaluated by means of Mean Absolute Error Mean Absolute Percentage Error and Root Mean Square Error calculations . The proposed methodology for addressing the problem of prediction of maximum and minimum day stock prices for Brazilian distribution companies is effective . In addition these results were only possible to be achieved due to the combined use of attribute selection by correlation analysis and ANNs . 
",We predict maximum and minimum day stock prices of power companies. The methodology is based on attribute selection and time series prediction. The most relevant attributes are determined by correlation analysis. The actual time series prediction is carried out by neural networks. The proposed methodology provides very good results.,,,
S156849461500469X," The main aim in network anomaly detection is effectively spotting hostile events within the traffic pattern associated to network operations by distinguishing them from normal activities . This can be only accomplished by acquiring the a priori knowledge about any kind of hostile behavior that can potentially affect the network or more easily by building a model that is general enough to describe the normal network behavior and detect the violations from it . Earlier detection frameworks were only able to distinguish already known phenomena within traffic data by using pre trained models based on matching specific events on pre classified chains of traffic patterns . Alternatively more recent statistics based approaches were able to detect outliers respect to a statistic idealization of normal network behavior . Clearly while the former approach is not able to detect previously unknown phenomena the latter one has limited effectiveness since it can not be aware of anomalous behaviors that do not generate significant changes in traffic volumes . Machine learning allows the development of adaptive non parametric detection strategies that are based on understanding the network dynamics by acquiring through a proper training phase a more precise knowledge about normal or anomalous phenomena in order to classify and handle in a more effective way any kind of behavior that can be observed on the network . Accordingly we present a new anomaly detection strategy based on supervised machine learning and more precisely on a batch relevance based fuzzyfied learning algorithm known as U BRAIN aiming at understanding through inductive inference the specific laws and rules governing normal or abnormal network traffic in order to reliably model its operating dynamics . The inferred rules can be applied in real time on online network traffic . This proposal appears to be promising both in terms of identification accuracy and robustness flexibility when coping with uncertainty in the detection classification process as verified through extensive evaluation experiments . 
",Adaptive network anomaly detection strategy based on a batch relevance based fuzzified learning algorithm. Couples the capability of inferring decisional structures from incomplete observations with the flexibility of a fuzzy based uncertainty management strategy. Infers the laws and rules governing normal or abnormal network traffic in order to model its operating dynamics. Based on a rule based detection strategy is more effective against previously unknown phenomena and robust against obfuscation mechanisms.,,,
S156849461500410X," In this paper a new approach called evolving principal component clustering is applied to a data stream . Regions of the data described by linear models are identified . The method recursively estimates the data variance and the linear model parameters for each cluster of data . It enables good performance robust operation low computational complexity and simple implementation on embedded computers . The proposed approach is demonstrated on real and simulated examples from laser range finder data measurements . The performance complexity and robustness are validated through a comparison with the popular split and merge algorithm . 
",A novel approach for data stream clustering to linear model prototypes. Good performance robust operation low computational complexity and simple implementation. Validation of results by comparison to well known algorithms.,,,
S156849461500472X," Surface Electromyography is a non invasive easy to record signal of superficial muscles from the skin surface . The sEMG is widely used in evaluating the functional status of the hand to assist in hand gesture recognition prosthetics and rehabilitation applications . Considering the nonlinear and non stationary characteristics of sEMG hand gesture recognition using sEMG signals necessitate designers to use Maximal Lyapunov Exponent or ensemble Empirical Mode Decomposition based MLEs . In this research we propose a hand gesture recognition method of sEMG based on nonlinear multiscale MLE . The aim is to increase the classification accuracy of sEMG features while reducing the complexity of EMD . The nonlinear MLE features are classified using Flexible Neural Tree which can solve highly structured dependent problems of the Artificial Neural Network . The testing has been conducted using several experiments with five participants . The classification performance of nonlinear multiscale MLE method is compared with MLE and EMD based MLE through simulations . Experimental results demonstrate that the former algorithm outperforms the two latter algorithms and can classify six different hand gestures up to 97.6 accuracy . 
",We use nonlinear and non stationary features of sEMG to identify various gestures. In order to reduce the complexity of EMD nonlinear MLE method is used. In order to improve characteristic stability we use a method based on nonlinear MLE. The proposed method classifies six different hand gestures up to 97.6 accuracy. The method can be used for prosthetics and other rehabilitation applications.,,,
S156849461500438X," Despite the wide application of evolutionary computation techniques to rule discovery in stock algorithmic trading a comprehensive literature review on this topic is unavailable . Therefore this paper aims to provide the first systematic literature review on the state of the art application of EC techniques for rule discovery in stock AT . Out of 650 articles published before 2013 51 relevant articles from 24 journals were confirmed . These papers were reviewed and grouped into three analytical method categories and three EC technique categories . A significant bias toward the applications of genetic algorithm based and genetic programming based techniques in technical trading rule discovery is observed . Other EC techniques and fundamental analysis lack sufficient study . Furthermore we summarize the information on the evaluation scheme of selected papers and particularly analyze the researches which compare their models with buy and hold strategy . We observe an interesting phenomenon where most of the existing techniques perform effectively in the downtrend and poorly in the uptrend and considering the distribution of research in the classification framework we suggest that this phenomenon can be attributed to the inclination of factor selections and problem in transaction cost selections . We also observe the significant influence of the transaction cost change on the margins of excess return . Other influenced factors are also presented in detail . The absence of ways for market trend prediction and the selection of transaction cost are two major limitations of the studies reviewed . In addition the combination of trading rule discovery techniques and portfolio selection is a major research gap . Our review reveals the research focus and gaps in applying EC techniques for rule discovery in stock AT and suggests a roadmap for future research . 
",The first systematic literature review on evolutionary rule discovery in stock algorithmic trading. A clear demonstrate of studies in this field based on a classification framework. A precise analysis of gaps and limitations in existing studies based on detail of evaluation scheme. The most important factors influencing profitability of models are presented in detail. Targeted suggestions for future improvements based on the review are proposed.,,,
S156849461500407X," This paper introduces a novel approach for identity authentication system based on metacarpophalangeal joint patterns . A discriminative common vector based method is utilized for feature selection . In the literature there is no study using whole MJP for identity authentication exceptionally a work using the hand knuckle pattern which is some part of the MJP draws the attention as a similar study . The originality of this approach is that whole MJP is firstly used as a biometric identifier and DCV method is firstly applied for extracting the feature set of MJP . The developed system performs some basic tasks like image acquisition image pre processing feature extraction matching and performance evaluation . The feasibility and effectiveness of this approach is rigorously evaluated using the k fold cross validation technique on two different databases a publicly available database and a specially established database . The experimental results indicate that the MJPs are very distinctive biometric identifiers and can be securely used in biometric identification and verification systems DCV method is successfully employed for obtaining the feature set of MJPs and proposed MJP based authentication approach is very successful according to state of the art techniques with a recognition rate of between 95.33 and 100.00 . 
",A new biometric identifier whole MJP pattern is introduced. An effective fast and robust MJP based biometric system is developed and presented. Discriminative common vector based method is firstly applied to obtain the feature sets of MJPs.,,,
S156849461500397X," This paper introduces a novel approach to detect and classify power quality disturbance in the power system using radial basis function neural network . The proposed method requires less number of features as compared to conventional approach for the identification . The feature extracted through the wavelet is trained by a radial basis function neural network for the classification of events . After training the neural network the weight obtained is used to classify the Power Quality problems . For the classification 20 types of disturbances are taken into account . The classification performance of RBFNN is compared with feed forward multilayer network learning vector quantization probabilistic neural network and generalized regressive neural network . The classification accuracy of the RBFNN network is improved just by rewriting the weights and updating the weights with the help of cognitive as well as the social behavior of particles along with fitness value . The simulation results possess significant improvement over existing methods in signal detection and classification . 
",The power quality disturbance detection and classification is important for improving the power quality. Various disturbances were taken in to account over 20 events. Wavelet is used to extract features from a MATLAB simulated disturbance waveforms. Radial Basis Function Neural Network has been used to detect and classify the disturbance and compared with other considered approach. The entire work has been presented with particle swarm optimization.,,,
S156849461500530X," In this paper using the Dempster Shafer theory of evidence a new decision criterion is proposed which can quickly classify airborne objects without any a priori knowledge whose data are laced with environmental noise characteristics within 10seconds from the time it is detected . Kinematic parameters of an airborne object received from radars are used to classify it into one of the six classes which include three levels of ballistic target discrimination aerodynamic satellite and unknown . The DST is chosen as it can suitably handle the element of uncertainty limited a priori data and short observation times that exist with the data acquired for the purpose of classification . The focus of the work is on ballistic targets in a theater of war . The approach is compared with the popularly known k NN and decision tree techniques and is found to perform better with the chosen data sets . This approach is tested using both real flight test data and simulated data . 
",Approach presents classification of ballistic missiles using evidential theory. Airborne objects classified without prior knowledge in few seconds after detection. Decision criterion proposed to categorize airborne objects into six major classes. Approach performs better than k NN and decision tree methods with chosen data sets. Validated with real and simulated data sets from single and multiple radars sources.,,,
S156849461500575X," In this paper a new version of the particle swarm optimization algorithm suitable for discrete optimization problems is presented and applied for the solution of the capacitated location routing problem and for the solution of a new formulation of the location routing problem with stochastic demands . The proposed algorithm combines three different topologies which are incorporated in a constriction particle swarm optimization algorithm and thus a very effective new algorithm the global and local combinatorial expanding neighborhood topology particle swarm optimization was developed . The algorithm was tested initially in the three classic sets of benchmark instances for the capacitated location routing problem with discrete demands and then as there are no benchmark instances for the location routing problem with stochastic demands these instances were transformed appropriately in order to be suitable for the problem with stochastic demands . The algorithm was tested in the problem with the stochastic demands using these transformed sets of benchmark instances . The algorithm was compared with a number of different implementations of the PSO and with metaheuristic evolutionary and nature inspired algorithms from the literature for the location routing problem with discrete and stochastic demands . 
",An improved variant of the particle swarm optimization algorithm is presented. A new formulation of the location routing problem with stochastic demands is given. A new neighborhood topology for PSO suitable for combinatorial optimization problems is proposed. The proposed algorithm is tested in the CLRP and in the LRPSDs. Comparisons with other algorithms from the literature are performed.,,,
S156849461500349X," The Bayesian neural networks are useful tools to estimate the functional structure in the nonlinear systems . However they suffer from some complicated problems such as controlling the model complexity the training time the efficient parameter estimation the random walk and the stuck in the local optima in the high dimensional parameter cases . In this paper to alleviate these mentioned problems a novel hybrid Bayesian learning procedure is proposed . This approach is based on the full Bayesian learning and integrates Markov chain Monte Carlo procedures with genetic algorithms and the fuzzy membership functions . In the application sections to examine the performance of proposed approach nonlinear time series and regression analysis are handled separately and it is compared with the traditional training techniques in terms of their estimation and prediction abilities . 
",A novel Monte Carlo algorithm is proposed to train Bayesian neural networks. This algorithm is based on the full Bayesian approach of artificial neural networks. Monte Carlo methods are integrated with GAs and fuzzy membership functions. Proposed algorithm is applied to time series and regression analysis in the context of BNNs. Proposed approach is superior to traditional training methods in terms of estimation performance.,,,
S156849461500589X," Cognitive radio network enables unlicensed users to sense for and opportunistically operate in underutilized licensed channels which are owned by the licensed users . Cognitive radio network has been regarded as the next generation wireless network centered on the application of artificial intelligence which helps the SUs to learn about as well as to adaptively and dynamically reconfigure its operating parameters including the sensing and transmission channels for network performance enhancement . This motivates the use of artificial intelligence to enhance security schemes for CRNs . Provisioning security in CRNs is challenging since existing techniques such as entity authentication are not feasible in the dynamic environment that CRN presents since they require pre registration . In addition these techniques can not prevent an authenticated node from acting maliciously . In this article we advocate the use of reinforcement learning to achieve optimal or near optimal solutions for security enhancement through the detection of various malicious nodes and their attacks in CRNs . RL which is an artificial intelligence technique has the ability to learn new attacks and to detect previously learned ones . RL has been perceived as a promising approach to enhance the overall security aspect of CRNs . RL which has been applied to address the dynamic aspect of security schemes in other wireless networks such as wireless sensor networks and wireless mesh networks can be leveraged to design security schemes in CRNs . We believe that these RL solutions will complement and enhance existing security solutions applied to CRN To the best of our knowledge this is the first survey article that focuses on the use of RL based techniques for security enhancement in CRNs . 
",Cognitive radio leverages on reinforcement learning RL to enhance network security. There is lack of reviews on the application of RL to based security schemes. We cover the challenges characteristics performance enhancements and others.,,,
S156849461500633X," This paper deals with the problem of parameter estimation in the generalized Mallows model by using both local and global search metaheuristic algorithms . The task we undertake is to learn parameters for defining the GMM from a dataset of complete rankings permutations . Several approaches can be found in the literature some of which are based on greedy search and branch and bound search . The greedy approach has the disadvantage of usually becoming trapped in local optima while the branch and bound approach basically A search usually comes down to approximate search because of memory requirements losing in this way its guaranteed optimality . Here we carry out a comparative study of several MH algorithms methods variable neighborhood search methods genetic algorithms and estimation of distribution algorithms and a tailored algorithm A to address parameter estimation in GMMs . We use 22 real datasets of different complexity all but one of which were created by the authors by preprocessing real raw data . We provide a complete analysis of the experiments in terms of accuracy number of iterations and CPU time requirements . 
",We deal with the problem of parameter estimation in Generalized Mallows model GMM . We deal with 22 real datasets all of them but one created by the authors. We have designed two experiments varying the maximum evaluations allowed. Obtained results significantly improve the previous competing approaches.,,,
S156849461500719X," In this research flexible flow shop scheduling with unrelated parallel machines at each stage are considered . The number of stages and machines vary at each stage and each machine can process specific operations . In other words machines have eligibility and parts have different release times . In addition the blocking restriction is considered for the problem . Parts should pass each stage and process on only one machine at each stage . In the proposed problem transportation of parts loading and unloading parts are done by robots and the objective function is finding an optimal sequence of processing parts and robots movements to minimize the makespan and finding the closest number to the optimal number of robots . The main contribution of this study is to present the mixed integer linear programming model for the problem which considers release times for parts in scheduling area loading and unloading times of parts which transferred by robots . New methodologies are investigated for solving the proposed model . Ant Colony Optimization with double pheromone and genetic algorithm are proposed . Finally two meta heuristic algorithms are compared to each other computational results show that the GA performs better than ACO and the near optimal numbers of robots are determined . 
",Flexible flow shop scheduling problem with robotic transportation is presented. Minimization of maximum completion time of all parts is considered as objective function. A mixed integer linear programming model is proposed for the problem. Two meta heuristic algorithm GA and ACO is presented.,,,
S156849461500664X," With the advent of paralleling and implementation of restructuring in the power market some routine rules and patterns of traditional market should be accomplished in a way different from the past . To this end the unit commitment scheduling that has once been aimed at minimizing operating costs in an integrated power market is metamorphosed to profit based unit commitment by adopting a new schema in which generation companies have a common tendency to maximize their own profit . In this paper a novel optimization technique called imperialist competitive algorithm as well as an improved version of this evolutionary algorithm are employed for solving the PBUC problem . Moreover traditional binary approach of coding of initial solutions is replaced with an improved integer based coding method in order to reduce computational complexity and subsequently ameliorate convergence procedure of the proposed method . Then a sub ICA algorithm is proposed to obtain optimal generation power of thermal units . Simulation results validate effectiveness and applicability of the proposed method on two scenarios a set of unimodal and multimodal standard benchmark functions two GENCOs consist of 10 and 100 generating units . fuel consumption coefficient of unit fuel consumption coefficient of unit fuel consumption coefficient of unit cost of imperialist cost of country cost function of unit i for units with ON status P P normalized cost of nth imperialist cold start up cost of unit i cooling constant of unit i ramp down rate of unit i cost of best obtained solution at last iteration of algorithm hot start up cost of unit i initial state of unit i commitment state of unit i at time t number of decades of main sub algorithm total number of generating units number of colonies number of imperialists initial number of colonies of empire n a chaotic vector produced by a chose map total system demand at time t positions of a colony at the current decade positions of an imperialist at the current decade positions of a colony at the next decade maximum active generation of unit i maximum response rate limited active power output of unit i at time t minimum active generation of unit i maximum response rate limited active power output of unit i at time t generation of unit i at time t a randomly generated number within total revenue start up cost of unit i at time t stopping criterion of the algorithm when it converges into 10 6 tolerance dispatch period total cost of generation total cost of imperialist empire n minimum OFF time of unit i minimum ON time of unit i ramp up rate of unit i a vector that is directed toward the imperialist locations and starts from the previous location of the colony v 1 1 a vector perpendicular to vector V time duration for which unit i has been OFF at time t time duration for which unit i has been ON at time t colonies corporation factor in imperialist power a chaotic. 
",Employing ICA as well as ICA combined with chaos as main solvers of PBUC. Utilizing a novel conformational integer coded algorithm for PUBC problem. Proposing a sub ICA cascaded with main solver to determine optimal power of units. Proposing a novel concept to cut down number of integers representing ON OFF status of units. Presenting a heuristic based constraint handling to overcome complexities of PBUC.,,,
S156849461600003X," The theoretical studies of differential evolution algorithm have gradually attracted the attention of more and more researchers . According to recent researches the classical DE can not guarantee global convergence in probability except for some special functions . Along this perspective a problem aroused is that on which functions DE can not guarantee global convergence . This paper firstly addresses that DE variants are difficult on solving a class of multimodal functions identified by two characteristics . One is that the global optimum of the function is near a boundary of the search space . The other is that the function has a larger deceptive optima set in the search space . By simplifying the class of multimodal functions this paper then constructs a Linear Deceptive function . Finally this paper develops a random drift model of the classical DE algorithm to prove that the algorithm can not guarantee global convergence on the class of functions identified by the two above characteristics . 
",We constructed a Linear Deceptive function as the representative of a class of multimodal functions. DE cannot guarantee convergence in probability on the above class of multimodal functions. A random drift model was firstly used to analyze the convergence of a real coded evolutionary algorithm. DE s mutation operators prefer to search in the aggregating region of the target individuals.,,,
S156849461500705X," Detecting discontinuities in electrical signals from recorded oscillograms makes it possible to segment them . This is the first step in implementing automated methods which will ensure disturbances in electrical power systems are detected classified and stored . In this context this paper presents a way of determining an adaptive threshold based on the decomposition of electrical signals through the Discrete Wavelet Transform using Daubechies family filter banks allowing for the segmentation of signals and as a consequence the analysis of disturbances related to Power Quality . Considering this the proposed approach was initially evaluated for signals originating from mathematical models representing short term voltage fluctuations transients and harmonic distortions . In the synthetic signal database either single or combined occurrences of more than one disturbance were considered . By applying the DWT the amount of energy and entropy of energy were then calculated for the leaves of the second level of decomposition . Based on these calculations a unique adaptive threshold could be determined for each analyzed signal . Afterwards the amount of existing intersections between the threshold and the curve of details obtained for the second level of decomposition was then defined . Thus the intersections determine the beginning and end of the segments . In order to validate the approach the performance of the proposed methodology was analyzed considering the signals obtained from oscillograms provided by IEEE 1159.3 Task Force as well as real oscillograms obtained from a regional distribution utility . After these analyses it was observed that the proposed approach is efficient and applicable to automatic segmentation of events related to PQ . 
",An adaptive threshold is determined for segmentation of power quality signals. The power quality signals are based on mathematical models and acquired in field. Wavelet transforms are used to decompose the signals. The intersections between the adaptive threshold and the wavelet transform detail curves determine the start and the end of the segments. The adaptive threshold was accurate in more than 96 percent of the signals.,,,
S156849461630014X," Time series forecasting is an important and widely popular topic in the research of system modeling and stock index forecasting is an important issue in time series forecasting . Accurate stock price forecasting is a challenging task in predicting financial time series . Time series methods have been applied successfully to forecasting models in many domains including the stock market . Unfortunately there are 3 major drawbacks of using time series methods for the stock market some models can not be applied to datasets that do not follow statistical assumptions most time series models that use stock data with a significant amount of noise involutedly have worse forecasting performance and the rules that are mined from artificial neural networks are not easily understandable . To address these problems and improve the forecasting performance of time series models this paper proposes a hybrid time series adaptive network based fuzzy inference system model that is centered around empirical mode decomposition to forecast stock prices in the Taiwan Stock Exchange Capitalization Weighted Stock Index and Hang Seng Stock Index . To measure its forecasting performance the proposed model is compared with Chen s model Yu s model the autoregressive model the ANFIS model and the support vector regression model . The results show that our model is superior to the other models based on root mean squared error values . 
",This paper proposes a hybrid time series ANFIS model based on EMD to forecast stock price. In order to evaluate the forecasting performances the proposed model is compared with other models. The experimental results show that proposed model is superior to the listing models.,,,
S156849461500784X," We consider the problem faced by a company that must outsource reverse logistics activities to third party providers . Addressing RL outsourcing problems has become increasingly relevant issue in the management science and decision making literatures . The correct evaluation and ranking of the decision criteria priorities determining the selection of the best third party RL providers is essential for the competitive performance of the outsourcing company . The method proposed in this study allows to identify and classify these decision criteria . First the relevant criteria and sub criteria are identified using a SWOT analysis . Then Intuitionistic Fuzzy AHP is used to evaluate the relative importance weights among the criteria and the corresponding sub criteria . These relative weights are implemented in a novel extension of Mikhailov s fuzzy preference programming method to produce local weights for all criteria and sub criteria . Finally these local weights are used to assign a global weight to each sub criterion and create a ranking . We discuss the results obtained by applying the proposed model to a case study of a real company . In particular these results show that the most important priority for the company when delegating RL activities to 3PRLPs is to focus on the core business while reducing costs constitutes one of its least important priorities . 
",We study a criteria evaluation system for outsourcing reverse logistics ORL . A hybrid SWOT and intuitionistic fuzzy AHP model evaluates strategic factors in ORL. Triangular intuitionistic fuzzy numbers are used to model ambiguity and uncertainty. The proposed model is validated through a case study. Focusing on core business is shown to be the organization s strategic priority.,,,
S156849461500681X," This paper presents a new algorithm designed to find the optimal parameters of PID controller . The proposed algorithm is based on hybridizing between differential evolution and Particle Swarm Optimization with an aging leader and challengers algorithms . The proposed algorithm is tested on twelve benchmark functions to confirm its performance . It is found that it can get better solution quality higher success rate in finding the solution and yields in avoiding unstable convergence . Also ALC PSODE is used to tune PID controller in three tanks liquid level system which is a typical nonlinear control system . Compared to different PSO variants genetic algorithm differential evolution and Ziegler Nichols method the proposed algorithm achieve the best results with least standard deviation for different swarm size . These results show that ALC PSODE is more robust and efficient while keeping fast convergence . 
",Propose a modified PSO DV algorithm using aging mechanism. The algorithm is tested on 12 benchmark functions. The algorithm is used for determining the optimal parameters of PID controller. Comparisons with different evolutionary algorithms show that the algorithm is efficient and robust.,,,
S156849461500602X," Many variants of particle swarm optimization both enhance the performance of the original method and greatly increase its complexity . Motivated by this fact we investigate factors that influence the convergence speed and stability of basic PSO without increasing its complexity from which we develop an evaluation index called Control Strategy PSO . The evaluation index is based on the oscillation properties of the transition process in a control system . It provides a method of selection parameters that promote system convergence to the optimal value and thus helps manage the optimization process . In addition it can be applied to the characteristic analyses and parameter confirmation processes associated with other intelligent algorithms . We present a detailed theoretical and empirical analysis in which we compare the performance of CSPSO with published results on a suite of well known benchmark optimization functions including rotated and shifted functions . We used the convergence rates and iteration numbers as metrics to compare simulation data and thereby demonstrate the effectiveness of our proposed evaluation index . We applied CSPSO to antenna array synthesis and our experimental results show that it offers high performance in pattern synthesis . 
",An evaluation index called Control Strategy PSO is developed. It can be applied to other intelligent algorithms. We present a detailed theoretical and empirical analysis.,,,
S156849461500592X," Advanced sensing technologies have produced a significant amount of discrete point data in the past decade . Measurement uncertainty frequently occurs at the geometric discontinuity of mechanical parts . In this paper a genetic search algorithm is developed for optimally constrained multiple line fitting of discrete data points . It contains two important technical components constrained least squares fitting of multiple lines and genetic search for optimal corner edge points . The algorithm is designed for both two dimensional and three dimensional cases . Numerical experiments demonstrate the effectiveness of the proposed approach compared to the conventional least squares fitting method as well as exhaustive search method . A comparative study with a particle swarm method indicates that both the genetic search and particle swarm search produce similar results in terms of minimum fitting errors . It can be used for the effective determination of sharp edges or corners based on discrete data points measured for high precision industrial inspection and manufacturing . direction numbers of a line in a three dimensional space binary numbers for a chromosome cognitive and social parameters decimal value corresponding to binary bits the shortest distance of the ith data point and the fitting line fitness function in two dimensional cases fitness function in three dimensional cases the fitness value of the ith candidate corner mean and maximum fitness the probability of crossover the probability of mutation the position of the globally best individual the best previous position of the ith particle random numbers that are uniformly distributed within a range the kth position of the ith particle in 2D The kth position of the ith particle in 3D the coordination of the ith data point in a three dimension space The coordination of start point for genetic search orthogonal projection of data points to fitted lines the real increment in a given genetic search the coordination of candidate constrained corner the size of genetic search in three coordinate directions the coordinates of the point passed by a line the coordinates of any point in a three dimension space the kth velocity of the ith particle inertial weight the coefficients of a line equation in two and three dimensions the sum of squared coordinate difference in x coordinate between data points and their corresponding points on the fitted line the sum of squared coordinate difference in y coordinate between data points and their corresponding points on the fitted line the gradient of E the gradient of E standard deviation of data points 
",Numerical computations were conducted under many conditions including different chromosome lengths and search sizes. The good experimental performance of two schemes indicates that the genetic search proposed in this paper is a better way to locate exactly a constrained point at geometric discontinuity. Our algorithm is invariant of chromosome length and search space size to a certain degree.,,,
S156849461500616X," As social media and e commerce on the Internet continue to grow opinions have become one of the most important sources of information for users to base their future decisions on . Unfortunately the large quantities of opinions make it difficult for an individual to comprehend and evaluate them all in a reasonable amount of time . The users have to read a large number of opinions of different entities before making any decision . Recently a new retrieval task in information retrieval known as Opinion Based Entity Ranking has emerged . OpER directly ranks relevant entities based on how well opinions on them are matched with a user s preferences that are given in the form of queries . With such a capability users do not need to read a large number of opinions available for the entities . Previous research on OpER does not take into account the importance and subjectivity of query keywords in individual opinions of an entity . Entity relevance scores are computed primarily on the basis of occurrences of query keywords match by assuming all opinions of an entity as a single field of text . Intuitively entities that have positive judgments and strong relevance with query keywords should be ranked higher than those entities that have poor relevance and negative judgments . This paper outlines several ranking features and develops an intuitive framework for OpER in which entities are ranked according to how well individual opinions of entities are matched with the user s query keywords . As a useful ranking model may be constructed from many ranking features we apply learning to rank approach based on genetic programming to combine features in order to develop an effective retrieval model for OpER task . The proposed approach is evaluated on two collections and is found to be significantly more effective than the standard OpER approach . 
",In this paper we address Opinion Based Entity Rank OpER task. OpER ranks entities based on how well opinion of entities match with users queries. We have outlined an extensive list of ranking features that can be used to capture the notion of query keyword relevance with individual opinions. Our experiments indicate that these ranking features have significantly high effectiveness for OpER task than standard retrieval models. For further improving the effectiveness we combine these ranking feature using genetic programming GP and learning to rank approach.,,,
S156849461600017X," Mining frequent itemsets is an essential problem in data mining and plays an important role in many data mining applications . In recent years some itemset representations based on node sets have been proposed which have shown to be very efficient for mining frequent itemsets . In this paper we propose DiffNodeset a novel and more efficient itemset representation for mining frequent itemsets . Based on the DiffNodeset structure we present an efficient algorithm named dFIN to mining frequent itemsets . To achieve high efficiency dFIN finds frequent itemsets using a set enumeration tree with a hybrid search strategy and directly enumerates frequent itemsets without candidate generation under some case . For evaluating the performance of dFIN we have conduct extensive experiments to compare it against with existing leading algorithms on a variety of real and synthetic datasets . The experimental results show that dFIN is significantly faster than these leading algorithms . 
",In this paper we propose an algorithm named dFIN for frequent itemset mining. dFIN achieves high performance by employing DiffNodesets to represent itemsets. Experiment results on real datasets show that dFIN is effective and outperforms state of the art algorithms.,,,
S156849461630031X," Cluster ensemble is a powerful method for improving both the robustness and the stability of unsupervised classification solutions . This paper introduced group method of data handling to cluster ensemble and proposed a new cluster ensemble framework which named cluster ensemble framework based on the group method of data handling . CE GMDH consists of three components an initial solution a transfer function and an external criterion . Several CE GMDH models can be built according to different types of transfer functions and external criteria . In this study three novel models were proposed based on different transfer functions least squares approach cluster based similarity partitioning algorithm and semidefinite programming . The performance of CE GMDH was compared among different transfer functions and with some state of the art cluster ensemble algorithms and cluster ensemble frameworks on synthetic and real datasets . Experimental results demonstrate that CE GMDH can improve the performance of cluster ensemble algorithms which used as the transfer functions through its unique modelling process . It also indicates that CE GMDH achieves a better or comparable result than the other cluster ensemble algorithms and cluster ensemble frameworks . 
",A cluster ensemble framework based on the group method of data handling was proposed. The components of the CE GMDH can be chosen according to the target of the application. Three novel transfer functions in CE GMDH were proposed. CE GMDH outperforms the other cluster ensemble algorithms and frameworks.,,,
S156849461500695X," Minimum class variance support vector machine and large margin linear projection classifier in contrast with traditional support vector machine take the distribution information of the data into consideration and can obtain better performance . However in the case of the singularity of the within class scatter matrix both MCVSVM and LMLP only exploit the discriminant information in a single subspace of the within class scatter matrix and discard the discriminant information in the other subspace . In this paper a so called twin space support vector machine algorithm is proposed to deal with the high dimensional data classification task where the within class scatter matrix is singular . TSSVM is rooted in both the non null space and the null space of the within class scatter matrix takes full advantage of the discriminant information in the two subspaces and so can achieve better classification accuracy . In the paper we first discuss the linear case of TSSVM and then develop the nonlinear TSSVM . Experimental results on real datasets validate the effectiveness of TSSVM and indicate its superior performance over MCVSVM and LMLP . 
",In the case of the singularity of the within class scatter matrix the drawbacks of both MCVSVM and LMLP are analyzed. A novel algorithm TSSVM is proposed to deal with the high dimensional data classification task where the within class scatter matrix is singular. An alternative version of the nonlinear MCVSVM and the nonlinear LMLP are proposed. The nonlinear TSSVM is developed.,,,
S156849461500678X," Power quality issues have become more important than before due to increased use of sensitive electrical loads . In this paper a new hybrid algorithm is presented for PQ disturbances detection in electrical power systems . The proposed method is constructed based on four main steps simulation of PQ events extraction of features selection of dominant features and classification of selected features . By using two powerful signal processing tools i.e . variational mode decomposition and S transform some potential features are extracted from different PQ events . VMD as a new tool decomposes signals into different modes and ST also analyzes signals in both time and frequency domains . In order to avoid large dimension of feature vector and obtain a detection scheme with optimum structure sequential forward selection and sequential backward selection as wrapper based methods and Gram Schmidt orthogonalization based feature selection method as filter based method are used for elimination of redundant features . In the next step PQ events are discriminated by support vector machines as classifier core . Obtained results of the extensive tests prove the satisfactory performance of the proposed method in terms of speed and accuracy even in noisy conditions . Moreover the start and end points of PQ events can be detected with high precision . 
",Different types of power quality disturbances are discriminated from each other. VMD and ST are used for extraction of dominant features. SVM with simple structure and few adjustable parameters is used as the classifier core. The generalization capability and detection accuracy of the proposed method are increased by elimination of redundant features by using different feature selection methods. The start and end points of PQ events can be detected accurately.,,,
S156849461500722X," Fabrication of three dimensional structures has gained increasing importance in the bone tissue engineering field . Mechanical properties and permeability are two important requirement for BTE scaffolds . The mechanical properties of the scaffolds are highly dependent on the processing parameters . Layer thickness delay time between spreading each powder layer and printing orientation are the major factors that determine the porosity and compression strength of the 3D printed scaffold . In this study the aggregated artificial neural network was used to investigate the simultaneous effects of layer thickness delay time between spreading each layer and print orientation of porous structures on the compressive strength and porosity of scaffolds . Two optimization methods were applied to obtain the optimal 3D parameter settings for printing tiny porous structures as a real BTE problem . First particle swarm optimization algorithm was implemented to obtain the optimum topology of the AANN . Then Pareto front optimization was used to determine the optimal setting parameters for the fabrication of the scaffolds with required compressive strength and porosity . The results indicate the acceptable potential of the evolutionary strategies for the controlling and optimization of the 3DP process as a complicated engineering problem . 
",The aggregated artificial neural network was used to investigate the simultaneous effects of printing parameters on the compressive strength and porosity of scaffolds. Particle swarm optimization algorithm was implemented to obtain the optimum topology of the AANN. Pareto front optimization was used to determine the optimal setting parameters. The presented results and discussion can give informative information to practitioners who want to design a porous structure and need to know the impact of influential design parameters.,,,
S156849461500647X," In this paper novel interval and general type 2 self organizing fuzzy logic controllers are proposed for the automatic control of anesthesia during surgical procedures . The type 2 SOFLC is a hierarchical adaptive fuzzy controller able to generate and modify its rule base in response to the controller s performance . The type 2 SOFLC uses type 2 fuzzy sets derived from real surgical data capturing patient variability in monitored physiological parameters during anesthetic sedation which are used to define the footprint of uncertainty of the type 2 fuzzy sets . Experimental simulations were carried out to evaluate the performance of the type 2 SOFLCs in their ability to control anesthetic delivery rates for maintaining desired physiological set points for anesthesia under signal and patient noise . Results show that the type 2 SOFLCs can perform well and outperform previous type 1 SOFLC and comparative approaches for anesthesia control producing lower performance errors while using better defined rules in regulating anesthesia set points while handling the control uncertainties . The results are further supported by statistical analysis which also show that zSlices general type 2 SOFLCs are able to outperform interval type 2 SOFLC in terms of their steady state performance . 
",Type 2 self organizing fuzzy logic controllers for automatic anesthesia control. Type 2 SOFLC use type 2 fuzzy sets to handle anesthesia control uncertainties. Data capturing inter and intra patient variability used to define type 2 fuzzy sets. Simulations show effectiveness of type 2 SOFLC in control of anesthetic infusion under noisy and uncertain surgical conditions. Type 2 SOFLC are able to outperform the existing type 1 SOFLC.,,,
S156849461500798X," This paper proposes relaxed conditions for control synthesis of discrete time Takagi Sugeno fuzzy control systems under unreliable communication links . To widen the applicability of the fuzzy control approach under network environments a novel fuzzy controller which is homogenous polynomially parameter dependent on both the current time normalized fuzzy weighting functions and the multi steps past normalized fuzzy weighting functions is provided to make much more use of the information of the underlying system . Moreover a new kind of slack variable approach is also developed and thus the algebraic properties of these multi instant normalized fuzzy weighting functions are collected into some augmented matrices . As a result the conservatism of control synthesis of discrete time Takagi Sugeno fuzzy control systems under unreliable communication links can be significantly reduced . Two illustrative examples are presented to demonstrate the effectiveness of the theoretical development . 
",A novel fuzzy controller and a new kind of slack variable approach are developed. The criterion takes the form of an LMI which is computationally tractable. The obtained stabilization conditions are less conservative.,,,
S156849461500753X," An accurate contour estimation plays a significant role in classification and estimation of shape size and position of thyroid nodule . This helps to reduce the number of false positives improves the accurate detection and efficient diagnosis of thyroid nodules . This paper introduces an automated delineation method that integrates spatial information with neutrosophic clustering and level sets for accurate and effective segmentation of thyroid nodules in ultrasound images . The proposed delineation method named as Spatial Neutrosophic Distance Regularized Level Set is based on Neutrosophic L Means clustering which incorporates spatial information for Level Set evolution . The SNDRLS takes rough estimation of region of interest as input provided by Spatial NLM clustering for precise delineation of one or more nodules . The performance of the proposed method is compared with level set NLM clustering Active Contour Without Edges Fuzzy C Means clustering and Neutrosophic based Watershed segmentation methods using the same image dataset . To validate the SNDRLS method the manual demarcations from three expert radiologists are employed as ground truth . The SNDRLS yields the closest boundaries to the ground truth compared to other methods as revealed by six assessment measures . The experimental results show that the SNDRLS is able to delineate multiple nodules in thyroid ultrasound images accurately and effectively . The proposed method achieves the automated nodule boundary even for low contrast blurred and noisy thyroid ultrasound images without any human intervention . Additionally the SNDRLS has the ability to determine the controlling parameters adaptively from SNLM clustering . 
",The proposed method integrates SNLM clustering and level set. Able to delineate thyroid nodules in ultrasound images accurately and automatically. Can be applied without preprocessing due to its indeterminacy handling capability. The parameters of SNDRLS are determined adaptively from SNLM clustering. Experimental results show the effectiveness of the proposed method.,,,
S156849461500808X," Two soft computing techniques are implemented to model and optimize the compressive strength of carbon polymer composites . Artificial neural network is used to establish a relationship between the uniaxial compressive strength of fabricated materials and the most significant processing parameters . To put together a database three different types of wood are carbonized at various heat treatment temperatures in specific pyrolysis time periods . Compression tests are then conducted at room temperature on the composites at a constant strain rate . The collected data of compressive strength and the related fabrication parameters are used as sets of data for training a neural network . A nested cross validation scheme is used to ensure the efficiency of the network . Results are indicative of a very good network which generalizes very well . Next an attempt is made to optimize the compressive behavior of the composites by controlling carbonization temperature time and also starting material type with the aid of a genetic algorithm coupled with the trained network . The optimization system yields promising results significantly enhancing the compressive strength . The validity of the optimal experiment as proposed by the soft computing system is verified by subsequent laboratory testing . 
",Neural network is trained to predict the compressive strength of composites from experimental variables. Input variables are optimized to maximize compressive strength by genetic algorithm. Experimental constraints are considered during optimization. The consistency of the analysis is verified by laboratory experiments.,,,
S156849461500650X," The present study applies a Hybrid method for identification of unknown parameters in a semi empirical tire model the so called Magic Formula . Firstly the Hybrid method used a Genetic Algorithm as a global search methodology with high exploration power . Secondly the results of the Genetic Algorithm were used as starting values for the Levenberg Marquardt algorithm as a gradient based method with high exploitation power . In this way the beneficial aspects of both methods are simultaneously exploited and their shortcomings are avoided . In order to establish the effectiveness of the proposed method performance of the Hybrid method has been compared with other methods available in the literature . In addition the use of GA as a Heuristic method for tire parameters identification has been discussed . Moreover the extrapolation power of Magic Formula identified with Hybrid method has been properly investigated . Finally the performance of the Hybrid method has been examined through tire parameter identification with priori known model . The results indicated that the Hybrid method has outstanding benefits such as high convergence speed high accuracy and null sensitivity to the starting values of unknown parameters . 
",A Hybrid method for identifying tire parameters in Magic Formula has been applied. Firstly the Hybrid method uses Genetic Algorithm GA with a high exploration power. Then the best result of GA is used as starting values SVs for Levenberg Marquardt. The method shows high convergence speed and accuracy and null sensitivity to SVs.,,,
S156849461630120X," In this paper we studied fuzzy linear fractional differential equations under Riemann Liouville H differentiability as the fuzzy initial value problems . Some of the previous results on solutions of these equations are concreted . We obtained new solutions by using the fractional hyperbolic functions and their properties in details . Finally an application and two examples are given to illustrate our results . 
",Fuzzy linear fractional differential equations in form under Riemann Liouville H differentiability are studied. Some of the previous results on solutions of these equations are concreted. New solutions are obtained using the fractional hyperbolic functions and their properties. An application and two examples are given to illustrate our results.,,,
S156849461630093X," In this paper an exchange market algorithm approach is applied to solve highly non linear power system optimal reactive power dispatch problems . ORPD is most vital optimization problems in power system study and are usually devised as optimal power flow problem . The problem is formulated as nonlinear non convex constrained optimization problem with the presence of both continuous and discrete control variables . The EMA searches for optimal solution via two main phases namely balanced market and oscillation market . Each of the phases comprises of both exploration and exploitation which makes the algorithm unique . This uniqueness of EMA is exploited in this paper to solve various vital objectives associated with ORPD problems . Programs are developed in MATLAB and tested on standard IEEE 30 and IEEE 118 bus systems . The results obtained using EMA are compared with other contemporary methods in the literature . Simulation results demonstrate the superiority of EMA in terms of its computational efficiency and robustness . Consumed function evaluation for each case study is mentioned in the convergence plot itself for better clarity . Parametric study is also performed on different case studies to obtain the suitable values of tuneable parameters . susceptance of the lines connecting between bus and bus objective function conductance of transmission line between bus and bus stability index of th load bus number of generator busses or PV busses number of load busses or PQ busses number of tap changing transformers number of transmission lines number of shunt capacitors active power loss in MW minimum limit of active power output of slack generator of slack generator of slack generator of slack generator G1 active power generation of th generator active power load demand at th bus minimum limit of reactive power output of th generator maximum limit of reactive power output of ith generator reactive power generation for th generator reactive power load demand at th bus limiting value of reactive power generation of th generator reactive power output of slack generator G1 reactive power output of Shunt capacitor minimum value of reactive power output of th shunt capacitor maximum value of reactive power output of th shunt capacitor reactive power output of st shunt capacitor reactive power output of th shunt capacitor apparent power flow through th transmission line apparent power flow limit of th transmission line max . limit of apparent power flow through th transmission line shares of shareholders lower limit of thshare of th shareholder upper limit of th share of th shareholder variation in the share of th shareholder of the third group tap positions of tap changing transformers minimum position of th tap changing transformer maximum position of th tap changing transformer tap position of the first tap changing transformer tap position of the th tap changing transformer voltage of first generator G . magnitude of the load voltage voltage magnitude of the first load bus voltage magnitude of the th bus voltage magnitude of the th bus minimum voltage of. 
",This paper presents a maiden application of EMA to solve power system ORPD problems. Steps of implementation of EMA to solve ORPD are elaborately discussed. The performance of EMA is tested on standard IEEE test systems. The selection of control parameters of EMA is done through exhaustive parametric study.,,,
S174680941300133X," In this paper we have made a humble attempt to automate an ophthalmologic diagnostic system based on signal processing using wavelets . Electroretinographic signals indicate the activity of the retinal cells from different layers of the inner retina and therefore these signals are used to predict various dreadful diseases . In this work we have analyzed 95 subjects from four different classes viz . Controls Congenital Stationery Night Blindness rod cone dystrophy and Central Retinal Vein Occlusion . The signal features extracted by wavelets are used for morphological and statistical analysis and for getting the subtle parameters like entropy . The results found comprises of difference in the values of wavelet coefficients a wave and b wave amplitude in the case of normal and pathological signals . The colour intensity distribution of scalograms shows highlighting variations in the case of maximum response and oscillatory potentials of the ERG signals for specific type of diseases . Furthermore we propose an Electroretinographic Index from different entropy parameters which can be used to distinguish between the normal and abnormal classes . This new method based on ERG signal analysis can be reliable enough to build a solution for the constraints in the field of ophthalmology . 
",To automate an ophthalmologic diagnostic system based on signal processing. Wavelet transform is used to extract the features. Entropy analysis is used and we have formulated an Electroretinographic Index to discriminate between controls and suspects. It will assist the physicians to cross check their diagnosis.,,,
S174680941400038X," Complex biological systems such as the human brain can be expected to be inherently nonlinear and hence difficult to model . Most of the previous studies on investigations of brain function have either used linear models or parametric nonlinear models . In this paper we propose a novel application of a nonlinear measure of phase synchronization based on recurrences correlation between probabilities of recurrence to study seizures in the brain . The advantage of this nonparametric method is that it makes very few assumptions thus making it possible to investigate brain functioning in a data driven way . We have demonstrated the utility of CPR measure for the study of phase synchronization in multichannel seizure EEG recorded from patients with global as well as focal epilepsy . For the case of global epilepsy brain synchronization using thresholded CPR matrix of multichannel EEG signals showed clear differences in results obtained for epileptic seizure and pre seizure . Brain headmaps obtained for seizure and pre seizure cases provide meaningful insights about synchronization in the brain in those states . The headmap in the case of focal epilepsy clearly enables us to identify the focus of the epilepsy which provides certain diagnostic value . Comparative studies with linear correlation have shown that the nonlinear measure CPR outperforms the linear correlation measure . 
",Phase synchronization in brain using recurrence plot based technique is studied. A novel application of the technique to multichannel seizure EEG data is made. Contour plots of Correlation between Probability of Recurrence CPR matrix show clear contrast between seizure and pre seizure signals and a classification accuracy of 100 has been achieved. CPR could identify the focus of epilepsy and the identification is much better than that obtained using linear correlation. The paper has shown the utility of a nonparametric nonlinear model in studying synchronization in the brain.,,,
S174680941400041X," A novel quadrature clutter rejection approach based on multivariate empirical mode decomposition which is an extension of empirical mode decomposition to multivariate for processing multichannel signals is proposed in this paper to suppress the quadrature clutter signals induced by the vascular wall and the surrounding stationary or slowly moving tissues in composite Doppler ultrasound signals and extract more blood flow components with low velocities . In this approach the MEMD algorithms which include the bivariate empirical mode decomposition with a nonuniform sampling scheme for adaptive selection of projection directions and the trivariate empirical mode decomposition with noise assistance are directly employed to adaptively decompose the complex valued quadrature composite signals echoed from both bidirectional blood flow and moving wall into a small number of zero mean rotation components which are defined as complex intrinsic mode functions . Then the relevant CIMFs contributed to blood flow components are automatically distinguished in terms of the break of the CIMFs power and then directly added up to give the quadrature blood flow signal . Specific simulation and human subject experiments are taken up to demonstrate the advantages and limitations of this novel method for quadrature clutter rejection in bidirectional Doppler ultrasound signals . Due to eliminating the extra errors induced by the Hilbert transform or complex FIR filter algorithms used in the traditional clutter rejection approaches based on the directional separation process the proposed method provides improved accuracy for clutter rejection and preserve more slow blood blow components which could be helpful to early diagnose arterial diseases . 
",We proposed a novel quadrature clutter rejection approach to suppress the quadrature clutter signals induced by the slowly moving vessel walls and tissues in composite Doppler ultrasound signals. The proposed clutter rejection approach is based on multivariate empirical mode decomposition MEMD . The phase information between the quadrature demodulated Doppler ultrasound signals is keeping well during the decomposition by MEMD. The extra errors induced by the traditional clutter rejection approaches could be removed effectively. The NA TEMD provides the best performance for conserving more bidirectional blood flow components with low velocities.,,,
S174680941300181X," The analysis of heart rate variability is central for cardiac diagnostics but its essential nonstationarity has started to gain attention only recently . The aim of this work is to develop a method for finding mathematical indicators of HRV spectral properties considering frequency nonstationarity . The analysis is done both for the new model of rhythmogram taking into account frequency modulation and for the true rhythmogram record during head up tilt test . Continuous wavelet transformation of the frequency modulated signal has been derived in analytical form . The local frequency of heart rhythm giving the maximum of CWT has been determined . Treated as another non stationary signal this frequency has been subjected to CWT following double CWT procedure . The transient periods for local frequency the frequencies of local frequency fluctuation against the main trend and the periods of emergence and attenuation of such fluctuations have been defined by estimating the spectral integrals in the ranges ULF VLF LF HF . The presented technique allows to use HRV control in the cases of arrhythmia ectopic beats heart turbulence and other non stationary violence of heart rhythm and also while studying long term cardiac records . 
",We propose a new model of non stationary rhythmogram as a frequency modulated signal. Double continuous wavelet transformation is performed to analyze non stationary HRV. Various scenarios of oscillation rearrangement are studied for frequency modulated rhythmogram signal. The transient periods and other spectral characteristics are calculated basing on head up tilt test real records.,,,
S156849461630165X," Materials informatics is a growing field in materials science . Materials scientists have begun to use soft computing techniques to discover novel materials . In order to apply these techniques the descriptors of a material must be selected thereby deciding the resulting performance . As a way of describing a material the properties of each element in the material are used directly as the features of the input variable . Depending on the number of elements in the material the dimensionality of the input may differ . Hence it is not possible to apply the same model to materials with different numbers of elements for tasks such as regression or discrimination . In the present paper we present a novel method of uniforming the dimensionality of the input that allows regression or discriminative tasks to be performed using soft computing techniques . The main contribution of the proposed method is to provide a solution for uniforming the dimensionality among input vectors of different size . The proposed method is a variant of the denoising autoencoder Vincent et al . using neural networks and gives a latent representation with uniformed dimensionality of the input . In the experiments of the present study we consider compounds with ionic conductivity and hydrogen storage materials . The results of the experiments indicate that the regression tasks can be performed using the uniformed latent data learned by the proposed method . Moreover in the clustering task using these latent data we observed distance preservation in data space which is also the case for the denoising autoencoder . This result may enable the proposed method to be used in a broad range of applications . 
",We introduced two representation in respect to materials naive representation and expert representation. We proposed a uniforming method of dimensionality of data with different size input vectors using a neural network. The proposed method outperformed the conventional methods the multi layer autoencoder the denoising autoencoder and kernel PCA for the linear regression task on synthetic data. The experimental results showed the robustness for data size and number of constituent elements. In the linear regression task on ion conductivity data of bulk materials and hydrogen storage materials the good fitting performance was obtained in terms of the latent data uniformed by the proposed method.,,,
S174680941300116X," We study the theoretical performance of using Electrical Impedance Tomography to measure the conductivity of the main tissues of the head . The governing equations are solved using the Finite Element Method for realistically shaped head models with isotropic and anisotropic electrical conductivities . We focus on the Electroencephalography signal frequency range since EEG source localization is the assumed application . We obtain the Cram r Rao Lower Bound to find the minimum conductivity estimation error expected with EIT measurements . The more convenient electrode pairs selected for current injection from a typical EEG array are determined from the CRLB . Moreover using simulated data the Maximum Likelihood Estimator of the conductivity parameters is shown to be close to the CRLB for a relatively low number of measurements . The results support the idea of using EIT as a low cost and practical tool for individually measure the conductivity of the head tissues and to use them when solving the EEG source localization . Even when the conductivity of the soft tissues of the head is available from Diffusion Tensor Imaging EIT can complement the electrical model with the estimation of the skull and scalp conductivities . 
",Parametric estimation of scalp skull and brain conductivities is analyzed. Realistic head models including anisotropy are adopted. Convenient electrode pairs for the current injection are determined. The Maximum Likelihood Estimator is advisable in practical situations.,,,
S156849461630117X," Conventionally optimal reactive power dispatch is described as the minimization of active power transmission losses and or total voltage deviation by controlling a number of control variables while satisfying certain equality and inequality constraints . This article presents a newly developed meta heuristic approach chaotic krill herd algorithm for the solution of the ORPD problem of power system incorporating flexible AC transmission systems devices . The proposed CKHA is implemented and its performance is tested successfully on standard IEEE 30 bus test power system . The considered power system models are equipped with two types of FACTS controllers . Simulation results indicate that the proposed approach yields superior solution over other popular methods surfaced in the recent state of the art literature including chaos embedded few newly developed optimization techniques . The obtained results indicate the effectiveness for the solution of ORPD problem of power system considering FACTS devices . Finally simulation is extended to some large scale power system models like IEEE 57 bus and IEEE 118 bus test power systems for the same objectives to emphasis on the scalability of the proposed CKHA technique . The scalability the robustness and the superiority of the proposed CKHA are established in this paper . 
",CKHA algorithm is applied for ORPD problems considering FACTS devices. CKHA is implemented on three IEEE standard test systems. Two different objective functions are considered. The results of CKHA are compared to other algorithms surfaced recently. Effectiveness of CKHA is established for ORPD problem with FACTS devices.,,,
S174680941400007X," This work proposes a comparative study of a pair of electrocardiographic 2D representations the frontal plane and a preferential plane obtained from ECG data . During depolarization and repolarization main electrical vectors were analyzed and compared between healthy subjects and patients referred for percutaneous transluminal coronary angioplasty . Recordings were obtained at rest . Many patients from the latter group presented normal ECGs thus the hypothesis to prove was that electrical axis in any of the studied planes would effectively discriminate silent ischemia records from healthy ones . The FP was constructed with I and aVF leads while the PP used the two first eigenvectors of the spatial correlation matrix of the ECG . Although the depolarization and repolarization vectors from both groups resulted normal those from the silent ischemia group appeared strongly biased to the left closer to the limit of the normality range . This slight change originated a significant separation between health and disease in the FP . Here most of the parameters resulted highly informative even those related to the depolarization phase . The cardiac vector integrating both depolarization and repolarization information presented the highest performance . Parameters in the PP however did not produce an acceptable discrimination except for the amplitude of the T wave . Additionally the repolarization orientation in the FP was the only marker that simultaneously discriminated three different groups of patients according to their occlusion sites . In conclusion the FP offered a 2D representation general enough to enable the separation of silent ischemia versus health populations while the PP did not due mainly to its individually optimized nature failing to provide a unique referencial frame for all the subjects . 
",We compared cardiac vectors of ischemic and healthy patients in two different planes. Planes under study the Einthoven s plane FP and a PCA transformed plane PP . We compared the ability to discriminate ischemia from health of both representations. The FP enabled the discrimination of ischemic patients while the PP did not.,,,
S174680941300102X," Due to noises speckles etc . automatic prostate segmentation is rather challenging and using only low level information such as intensity gradient is insufficient and unable to tackle the problem . In this paper we propose an automatic prostate segmentation method combining intrinsic properties of TRUS images with the high level shape prior information . First intrinsic properties of TRUS images such as the intensity transition near the prostate boundary as well as the speckle induced texture features obtained by Gabor filter banks are integrated to deform the model to the target contour . These properties make our method insensitive to high gradient regions introduced by noises and speckles . Then the preliminary segmentation is fine tuned by the non parametric shape prior which is optimally distilled by non parametric kernel density estimation as it can approximate arbitrary distributions . The refinement is along the direction of mean shift vector and considerably strengthens the robustness of the method . The performance of our method is validated by experimental results . Compared with the state of the art the accuracy and robustness of the method is quite promising and the mean absolute distance is only 1.21 0.85mm . 
",An explicit prostate segmentation model utilizing non parametric shape prior is proposed. The method can model the shape that is not statistically significant in the shape space. The method can escape from local minima by using a fairly large detection range during freely deforming. Compared with recently proposed sparse shape composition method training data set with a large number of instances is not necessary. The intrinsic properties of TRUS images are integrated into the model.,,,
S174680941300092X," A spectral angle based feature extraction method Spectral Clustering Independent Component Analysis is proposed in this work to improve the brain tissue classification from Magnetic Resonance Images . SC ICA provides equal priority to global and local features thereby it tries to resolve the inefficiency of conventional approaches in abnormal tissue extraction . First input multispectral MRI is divided into different clusters by a spectral distance based clustering . Then Independent Component Analysis is applied on the clustered data in conjunction with Support Vector Machines for brain tissue analysis . Normal and abnormal datasets consisting of real and synthetic T1 weighted T2 weighted and proton density fluid attenuated inversion recovery images were used to evaluate the performance of the new method . Comparative analysis with ICA based SVM and other conventional classifiers established the stability and efficiency of SC ICA based classification especially in reproduction of small abnormalities . Clinical abnormal case analysis demonstrated it through the highest Tanimoto Index accuracy values 0.75 98.8 observed against ICA based SVM results 0.17 96.1 for reproduced lesions . Experimental results recommend the proposed method as a promising approach in clinical and pathological studies of brain diseases . 
",Spectral clustering extension to independent component analysis has been proposed. Synthetic and clinical MRI data analyzed to verify the potential of the algorithm. High performance tissue classification observed in multispectral brain MRI study. The proposed method shows 98.8 accuracy in clinical abnormality analysis.,,,
S174680941300089X," Automatic analysis of biomedical time series such as electroencephalogram and electrocardiographic signals has attracted great interest in the community of biomedical engineering due to its important applications in medicine . In this work a simple yet effective bag of words representation that is originally developed for text document analysis is extended for biomedical time series representation . In particular similar to the bag of words model used in text document domain the proposed method treats a time series as a text document and extracts local segments from the time series as words . The biomedical time series is then represented as a histogram of codewords each entry of which is the count of a codeword appeared in the time series . Although the temporal order of the local segments is ignored the bag of words representation is able to capture high level structural information because both local and global structural information are well utilized . The performance of the bag of words model is validated on three datasets extracted from real EEG and ECG signals . The experimental results demonstrate that the proposed method is not only insensitive to parameters of the bag of words model such as local segment length and codebook size but also robust to noise . 
",A novel feature representation is proposed for biomedical time series classification. The representation is extended from the bag of words model in text document analysis. The proposed bag of words representation is effective and discriminative. The representation is able to capture both global and local structural information.,,,
S156849461630076X," Technology credit scoring models have been used to screen loan applicant firms based on their technology . Typically a logistic regression model is employed to relate the probability of a loan default of the firms with several evaluation attributes associated with technology . However these attributes are evaluated in linguistic expressions represented by fuzzy number . Besides the possibility of loan default can be described in verbal terms as well . To handle these fuzzy input and output data we proposed a fuzzy credit scoring model that can be applied to predict the default possibility of loan for a firm that is approved based on its technology . The method of fuzzy logistic regression as an appropriate prediction approach for credit scoring with fuzzy input and output was presented in this study . The performance of the model is improved compared to that of typical logistic regression . This study is expected to contribute to practical utilization of the technology credit scoring with linguistic evaluation attributes . 
",We propose a technology credit scoring model based on fuzzy logistic regression. Fuzzy predictor fuzzy binary responses with crisp coefficients are considered. Fuzzy least square method is used to estimate parameters. The performance of proposed fuzzy logistic regression model is improved compared to the logistic regression.,,,
S156849461630103X," Diaphragmatic electromyogram signal plays an important role in the diagnosis and analysis of respiratory diseases . However EMGdi recordings are often contaminated by electrocardiographic interference which posing serious obstacle to traditional denoising approaches due to overlapped spectra of these signals . In this paper a novel method based on wavelet transform and independent component analysis is proposed to remove the ECG interference from noisy EMGdi signals . With the proposed method the original independent components of contaminated EMGdi signal were first obtained with ICA . Then the ECG components contained were removed by a specially designed wavelet domain filter . After that the purified independent components were reconstructed back to the original signal space by ICA to obtain clean EMGdi signals . Experimental results achieved on practical clinical data show that the proposed approach is better than several traditional methods include wavelet transform ICA digital filter and adaptive filter in ECG interference removing . 
",A novel de noising method is proposed to remove the ECG interference from noisy EMGdi signals. Experimental results achieved on practical clinical data show that the proposed approach is better than traditional methods include wavelet transform WT ICA digital filter and adaptive filter in ECG interference removing. The original independent components of contaminated EMGdi signal were firstly obtained with independent component analysis ICA . Then the ECG components contained were removed by a specially designed wavelet domain filter. Finally the purified independent components were reconstructed back to the original signal space by ICA to obtain the clean EMGdi signals.,,,
S156849461630045X," The minimum vertex cover problem is a classical combinatorial optimization problem . This paper studies this problem based on rough sets . We show that finding the minimal vertex cover of a graph can be translated into finding the attribute reduction of a decision information table . At the same time finding a minimum vertex cover of graphs is equivalent to finding an optimal reduct of a decision information table . As an application of the theoretical framework a new algorithm for the minimum vertex cover problem based on rough sets is constructed . Experiments show that the proposed algorithm gets better performance in terms of the ratio value when compared with some other algorithms . 
",We study the NP hard problem of finding a minimum vertex cover of graphs based on rough sets. The problem of finding a minimum vertex cover of graphs can be translated into the problem of finding an optimal reduct of a decision information table in rough sets. A new method based on rough sets is proposed to compute the minimum vertex cover of a given graph.,,,
S156849461630148X," In this paper I introduce a new method for feature extraction to classify digital mammograms using fast finite shearlet transform . Initially fast finite shearlet transform was performed over mammogram images and feature vectors were built using coefficients of the transform . In subsequent calculations features were ranked according to t test statistics and capabilities were distinguished between different classes . To maximize differences between class representatives a thresholding process was implemented as a final stage of feature extraction and classifications were calculated over the optimal feature set using 5 fold cross validation and a support vector machine classifier . The present results show that the proposed method provides satisfactory classification accuracy . 
",This paper presents an approach for breast cancer diagnosis in digital mammograms using shearlet transform. A new approach is represented for feature extraction using shearlet transform t test statistic and dynamic thresholding. The system uses two datasets ROIs obtained from two different database MIAS and DDSM database. The datasets include all abnormalites for breast cancer. The classification section was performed to distinguish the ROIs as normal abnormal and benign malignant.,,,
S174680941400144X," Eye activity has larger electrical potential than the average electroencephalogram recording thus making it one of the major sources of artefacts . Ocular artefacts must be removed as completely as possible with little or no loss of EEG to obtain a higher quality EEG . Using independent component analysis the EEG is separated into independent components and the contaminated component is removed thus removing the OA . However ICA does not separate the sources completely and some of the meaningful EEG is lost . In this paper a new method combining ICA and wavelet neural networking is proposed . In this method WNN is applied to the contaminated ICs correcting the OA and thus lowering the data lost . The method was evaluated using simulated and real datasets and the results show that the OA are successfully removed with very little data loss . 
",A combination of ICA and WNN is introduced to eliminate EOG artefacts. This method corrects only EOG artefacts within a contaminated component. Minimal low frequency and underlying cerebral activity is lost. EEG quality is increased.,,,
S174680941500066X," Pulse transit time and pulse wave velocity are the markers most widely used to evaluate the vascular effects of aging hypertension arterial stiffness and atherosclerosis . To calculate these markers it is necessary to determine the location of the onset and systolic peak of the arterial pulse wave . In this paper a method employed for electrocardiography R peak detection with a slight modification is applied for both the onset and systolic peak detections in APW . The method employs Shannon energy envelope estimator Hilbert transform and moving average filter . The minimum value and the positive zero crossing points of the odd symmetry function of the HT correspond to the locations of the onset and systolic peak respectively . The algorithm was evaluated using expert s annotations with 10 records of 5min length and different signal to noise ratios and achieved a good performance and precision . When compared to expert s annotation the algorithm detected these fiducial points with average sensitivity positive predictivity and accuracy of 100 and presented errors less than 10ms . In APW signals contaminated with noise in both cases the relative error is less than 2 respect to pulse wave periods of 800ms . The performance of algorithm was compared with both foot approximation and adaptive threshold methods and the results show that the algorithm outperforms theses reported methods with respect to manuals annotation . The results are promising suggesting that the method provides a simple but accurate onset and systolic peak detection and can be used in the measurement of pulse transit time pulse wave velocity and pulse rate variability . 
",Hilbert transform method can be used for detecting both the onset and systolic peak in pulse wave signals. It achieves good performance and precision when compared to expert annotation. It is robust when noise and interference are present in pulse wave and also. Shows a low detection error rate. It is computationally simple.,,,
S174680941400113X," We propose a methodological study for the optimization of surface EMG based hand gesture classification effective to implement a human computer interaction device for both healthy subjects and transradial amputees . The widely commonly used unsupervised Principal Component Analysis approach was compared to the promising supervised common spatial pattern methodology to identify the best classification strategy and the related tuning parameters . A low density array of sEMG sensors was built to record the muscular activity of the forearm and classify five different hand gestures . Twenty healthy subjects were recruited to compute optimized parameters for and to compare between the two strategies . The system was also tested on a transradial amputee subject in order to assess the robustness of the optimization in recognizing disabled users gestures . Results show that RMS WA ANN is the best feature vector classifier pair for the PCA approach whereas M RMS WA ANN is the best pair for the CSP methodology . Statistical analysis on classification results shows no significant differences between the two strategies . Moreover we found out that the optimization computed for healthy subjects was proven to be sufficiently robust to be used on the amputee subject . This motivates further investigation of the proposed methodology on a larger sample of amputees . Our results are useful to boost EMG based hand gesture recognition and constitute a step toward the definition of an efficient EMG controlled system for amputees . 
",An optimization study on EMG based hand gesture recognition is proposed. We compare unsupervised PCA vs. supervised CSP preprocessing technique. Artificial neural network achieves best classification accuracies in both cases. The study identifies the best parameters to choose in pattern recognition stages. The optimization algorithm is robust enough to be used on amputees as well.,,,
S174680941500049X," In this paper we propose a novel approach for the compression of multichannel electroencephalograph signals . The method assumes that EEG signals are the linear mixture of several independent components . To retain the ICs the proposed scheme first applies an independent component analysis with a preprocessing step of principal component analysis to EEG signals . Then the compression scheme is composed of two parts the ICs compression part and the residue compression part . Each IC is arranged in the form of matrix and then compressed with the algorithm of set partitioning in hierarchical trees . The residue signals are compressed in the same way as ICs but with a higher compression ratio . The appropriate combination of compression ratios of the ICs and the residue is explored to achieve desired performance . The compression scheme is tested with eight datasets sampled at two different frequencies . The experimental results demonstrate the high compression performance of the proposed approach and its potential usage in the EEG related telemedicine applications . 
",PCA based ICA is proposed to exploit the inter channel correlations. 1 D signal is arranged in the form of matrix to exploit the intra channel correlations. SPIHT algorithm is used to compress the signals in matrix form.,,,
S174680941400158X," Features greatly influence the results of speech emotion recognition among which Mel frequency Cepstral Coefficients is the most commonly used in speech emotion . However MFCC does not consider both the relationship among neighbor coefficients of Mel filters of a frame and the relationship among coefficients of Mel filters of neighbor frames which possibly leads to lose many useful features from spectrogram . This paper presents novel weighted spectral features based on Local Hu moments . The idea is motivated by that the energy on spectrogram would drastically vary with some emotion types such as angry and happy while it would slightly change with other emotion types such as sadness and fear . This phenomenon would affect the local energy distribution of spectrogram in both time axis and frequency axis of spectrogram . To describe local energy distribution of spectrogram Hu moments computed from local regions of spectrogram are used as Hu moments can evaluate the degree how the energy is concentrated to the center of energy gravity of local region of spectrogram and can significantly vary with the speech emotion types . The conducted experiments validate the proposed features in terms of the effectiveness of the speech emotion recognition . 
",HuWSF is computed from local regions of a spectrogram by Hu moments. HuWSF can evaluates how the energy aggregate to some frequencies in a spectrogram. HuWSF extracts the features among neighbor coefficients of Mel filters of a frame. HuWSF extracts the features among coefficients of Mel filters of neighbor frames. HuWSF can reduce the changes brought by sentences speakers and speaking styles.,,,
S174680941400202X," The feature extraction of event related potentials is a significant prerequisite for many types of P300 BCIs . In this paper we proposed a multi ganglion artificial neural network based feature learning method to extract a deep feature structure of single trial multi channel ERP signals and improve classification accuracy . Five subjects took part in the Imitating Reading ERP experiments . We recorded the target electroencephalography samples and non target samples for each subjects . Then we applied ANNFL method to extract the feature vectors and classified them by using support vector machine . The ANNFL method outperforms the principal component analysis method and conventional three layer auto encoder and then leads to higher classification accuracies of five subjects BCI signals than using the single channel temporal features . ANNFL is an unsupervised feature learning method which can automatically learn feature vector from EEG data and provide more effective feature representation than PCA method and single channel temporal feature extraction method . 
",The multi ganglion ANN based feature learning ANNFL method is an unsupervised feature extraction method. This method can find an effective feature representation automatically for single trial P300 signal. The ANNFL method reduces the training time of conventional three layer auto encoder and leads to better classification results in the P300 BCI paradigm of our study.,,,
S174680941500018X," One of the most common signs of tiredness or fatigue is yawning . Naturally identification of fatigued individuals would be helped if yawning is detected . Existing techniques for yawn detection are centred on measuring the mouth opening . This approach however may fail if the mouth is occluded by the hand as it is frequently the case . The work presented in this paper focuses on a technique to detect yawning whilst also allowing for cases of occlusion . For measuring the mouth opening a new technique which applies adaptive colour region is introduced . For detecting yawning whilst the mouth is occluded local binary pattern features are used to also identify facial distortions during yawning . In this research the Strathclyde Facial Fatigue database which contains genuine video footage of fatigued individuals is used for training testing and evaluation of the system . 
",Yawning is detected upon mouth opening mouth covering and facial feature distortion. Mouth covered detection is based on classification of LBP features extracted from mouth covering hand. Proposed approach has been tested on real video sequences of sleep deprived volunteers.,,,
S174680941500083X," Feature extraction and automatic classification of mental states is an interesting and open area of research in the field of brain computer interfacing . A well trained classifier would allow the BCI system to control an external assistive device in real world problems . Sometimes standard existing classifiers fail to generalize the components of a non stationary signal like Electroencephalography which may pose one or more problems during real time usage of the BCI system . In this paper we aim to tackle this issue by designing an interval type 2 fuzzy classifier which deals with the uncertainties of the EEG signal over various sessions . Our designed classifier is used to decode various movements concerning the wrist and finger . For this purpose we have employed extreme energy ratio to construct the feature vector . The average classification accuracy achieved during offline training and online testing over eight subjects are 86.45 and 78.44 respectively . On comparison with other related works it is shown that our designed IT2FS classifier presents a better performance . 
",Session to session EEG variation is addressed and tackled by the proposed method. Interval type 2 fuzzy classification approach adopted for the purpose. Studied on discrimination of wrist and fingers motor imagery EEG signals. Offline and online experiments establish the efficacy of the proposed method. Using Extreme Energy Ratio as features 86.45 and 78.44 accuracies are obtained.,,,
S174680941500004X," The signal preprocessing is prerequisite for reduction of noise and for better estimation of sources from the measured field distribution of multichannel data since different measurement channels may be contaminated by different types of artifacts and noise . Toward this we use a combination of independent component analysis and ensemble empirical mode decomposition to denoise the multichannel magnetocardiography data . In this technique MCG time series data is first subjected to ICA to obtain the statistically independent components and subsequently the EEMD interval threshold based denoising is applied to the ICs prior to the reconstruction of the signal . We compare the results obtained from EEMD ICA with those obtained using the conventional ICA alone and also using the wavelet enhanced ICA . We illustrate the effect of these denoising techniques on the pseudo current density maps which aid in visualizing the source location . The results obtained from the EEMD ICA are seen to be decidedly superior compared to those obtained by ICA alone and wICA methods . 
",The combination of EEMD and ICA proposed and used for multichannel MCG signal denoising. Reduces the computational complexity associated with EEMD alone and effectively enhances the signal quality. The effect of ICA alone wICA and EEMD ICA on the magnetic field map and pseudo current density maps is investigated. The EEMD ICA combination yield robust source estimate on the PCD map. The EEMD ICA can be applied to beat to beat analysis of MCG signals.,,,
S174680941500052X," A novel feedback controlled hydrodynamic human circulatory system simulator well suited for in vitro validation of cardiac assist devices is presented in this paper . The cardiovascular system simulator consists of high bandwidth actuators allowing a high precision hardware in the loop hydrodynamic interface in connection with physiological circulatory models calculated in real time . The hydrodynamically coupled process dynamics consist of several actuator loops and demand a multivariable control design approach in the face of system nonlinearities and uncertainties . Based on a detailed model employing the Lagrange formalism a robust decentralised controller is designed . Fixed structural constraints and the minimisation of the norm necessitate the application of nonsmooth optimisation techniques . The robust decentralised norm optimal controller is tested in extensive in vitro experiments and shows good performance with regard to reference tracking and system coupling . In vitro experiments include multivariable reference step tests and frequency analysis tests of the vascular impedance transfer function . 
",We develop a novel decentralised feedback controller for a hydrodynamic human circulatory system simulator. We present a detailed model of the human circulatory system simulator. Nonlinear simulation and experimental results underline the performance of the proposed controller. In vitro experiments with a blood pump show sufficiently fast reference tracking to realise aortic and ventricle pressures.,,,
S174680941500107X," The response of the brain to a sensory stimulus may present itself in the electroencephalogram as evoked and or induced activity . While the evoked response is given by peaks and troughs in the signal time locked and phase locked to the stimuli the induced response is time but not phase locked and can be considered as an increase or a decrease in the power of EEG in a specific frequency band at a specific time range with regard to the stimulus onset . The induced response does not have the same phase following successive stimuli . It is believed that cognition and perception of a stimulus present themselves primarily as the induced response in the EEG . In this paper the induced response of the brain to auditory speech stimuli is investigated and different approaches to detect induced activity are compared . It is shown that there is an increase in theta and delta power in response to words compared to the baseline starting around 500ms after their onset . During this time there is also an increase in pairwise coherence between the posterior electrodes . In response to tone bursts a change in pairwise coherence was observed in the beta band starting around 200ms . To the best of our knowledge this is the first time such responses have been described using simple protocols without complex stimulus manipulations being involved . Responses in the EEG to speech rather than the more conventional tone bursts or clicks suggests that it may be feasible to use the EEG as an objective means to demonstrate brain activation to salient real world stimuli . This would be of particular benefit in investigating access to speech in patients who are unable or unwilling to reliably respond to conventional subjective experimental protocols such as infants . 
",Brain s induced response was measured in response to auditory tones and words. Power and coherence analyses were used for this purpose. Power analyses detected induced activity in response to words. Coherence analyses showed significant induced activity in response to both stimuli. The results may be of interest in studies involving hearing impaired patients.,,,
S174680941400161X," Bipolar disorders are characterized by a mood swing ranging from mania to depression . A system that could monitor and eventually predict these changes would be useful to improve therapy and avoid dangerous events . Speech might convey relevant information about subjects mood and there is a growing interest to study its changes in presence of mood disorders . In this work we present an automatic method to characterize fundamental frequency dynamics in voiced part of syllables . The method performs a segmentation of voiced sounds from running speech samples and estimates two categories of features . The first category is borrowed from Taylor s Tilt intonational model . However the meaning of the proposed features is different from the meaning of Taylor s ones since the former are estimated from all voiced segments without performing any analysis of intonation . A second category of features takes into account the speed of change of F0 . In this work the proposed features are first estimated from an emotional speech database . Then an analysis on speech samples acquired from eleven psychiatric patients experiencing different mood states and eighteen healthy control subjects is introduced . Subjects had to perform a text reading task and a picture commenting task . The results of the analysis on the emotional speech database indicate that the proposed features can discriminate between high and low arousal emotions . This was verified both at single subject and group level . An intra subject analysis was performed on bipolar patients and it highlighted significant changes of the features with different mood states although this was not observed for all the subjects . The directions of the changes estimated for different patients experiencing the same mood swing were not coherent and were task dependent . Interestingly a single subject analysis performed on healthy controls and on bipolar patients recorded twice with the same mood label resulted in a very small number of significant differences . In particular a very good specificity was highlighted for the Taylor inspired features and for a subset of the second category of features thus strengthening the significance of the results obtained with patients . Even if the number of enrolled patients is small this work suggests that the proposed features might give a relevant contribution to the demanding research field of speech based mood classifiers . Moreover the results here presented indicate that a model of speech changes in bipolar patients might be subject specific and that a richer characterization of subject status could be necessary to explain the observed variability . 
",We present an automatic method for estimating features describing speech F0 contour. Analysis on acted emotional speeches distinguished high from low arousal emotions. Intra subject analysis on bipolar patients found out differences between mood states. The results on bipolar patients are subject specific and task dependent. Results on controls confirmed a good specificity of the proposed features.,,,
S174680941500110X," Muscle fiber conduction velocity can be measured by estimating the time delay between surface EMG signals recorded by electrodes aligned with the fiber direction . In the case of dynamic contractions the EMG signal is highly non stationary and the time delay between recording sites may vary rapidly over time . Thus the processing methods usually applied in the case of static contractions do not hold anymore and the delay estimation requires processing techniques that are adapted to non stationary conditions . The current paper investigates several methods based on time frequency approaches or adaptive filtering in order to solve the time varying delay estimation problem . These approaches are theoretically analyzed and compared by Monte Carlo simulations in order to determine if their performance is sufficient for practical applications . Moreover results obtained on experimental signals recorded during cycling from the vastus medialis muscle are also shown . The study presents for the first time a set of approaches for instantaneous delay estimation from two channels EMG signals . 
",Muscle fiber conduction velocity estimators during dynamic exercises are proposed. Time varying delay estimators are compared by Monte Carlo simulations. TFTS methods are robust to noise and accurate for the conduction velocity tracking.,,,
S174680941400130X," Fatty liver or steatosis is a pathology characterized by fat accumulation in the liver cells . Ultrasound is the most common technique used for its evaluation however the diagnosis is strongly dependent on the physician s expertise and system settings . These drawbacks have motivated the development of procedures for the quantitative analysis of ultrasound images to help the steatosis diagnosis . In this work three approaches are presented and tested with human liver images . The first one addresses textural analysis of the hepatic parenchyma using five classifiers 357 features a feature selector and classifiers fusion . Its performance is measured by two parameters accuracy and area under the ROC curve . The second makes use of the hepatorenal coefficient followed by a statistical analysis to discriminate echogenicity differences between liver and kidney . The third is based on the acoustical attenuation coefficient evaluated over a line traced in the images with parallel orientation to the acoustical beam . The use of classifiers fusion has provided better results when compared with the performance of the best one considered alone . The hepatorenal coefficient proved to be a good parameter for steatosis detection with calculated sensitivity and specificity of 0.90 and 0.88 respectively . It was observed the hepatorenal coefficient is not influenced by the ultrasound machine parameters . The attenuation coefficient provided lower sensitivity and specificity values than the ones from the hepatorenal coefficient . 
",Identification of steatosis through three distinct approaches. We show that it is possible to detect fatty liver using classifiers with a reduced number of features. We prove that the hepatorenal coefficient and the attenuation coefficient are capable to successfully discriminate steatotic from normal livers. Our methodologies could be also used to detect the degrees of steatosis useful for analyzing its evolution over time.,,,
S174680941500035X," When multiple acquisition systems are used to simultaneously acquire signals synchronization issues may arise potentially causing errors in the determination of acquisition starting points and continuous clock offsets and shifts on each device . This paper introduces a processing method to efficiently synchronize these signals in the presence of white noise sources without the requirement of clock sharing or any other digital line exchange . The use of a signal source such as white noise with a very wide frequency band is of great interest for synchronization purposes due to its aperiodic nature . This high bandwidth signal is simultaneously acquired by all the acquisition channels on distinct systems and synchronized afterwards using cross correlation methods . Two different correlation methods were tested a global method used when clock system frequencies are exactly known and a local method used when independent clocks evidence shifts over time that cumulatively account for long term acquisition errors in the synchronization process . In a computational simulation with known clock frequencies the results show a synchronization error of 1 10 of the time resolution for both methods . For unknown clock frequencies the global method achieved an error of 24 10 the time resolution indicating a much poorer performance . In the experimental set up only the local method was tested . The best result shows a synchronization error of 4 10 of the time resolution . All the signal conditioning and acquisition parameters were chosen taking into account potential biomedical applications . 
",Motivation Synchronize biomedical signals from several acquisition systems is a difficult task. Methodology Use of white noise and correlation methods to synchronize biomedical signals. Results Synchronization with an error of 0.2ms in an experimental case. Conclusions This is a reliable and flexible method for several biomedical acquisition systems.,,,
S174680941500021X," This review discusses the critical issues and recommended practices from the perspective of myoelectric interfaces . The major benefits and challenges of myoelectric interfaces are evaluated . The article aims to fill gaps left by previous reviews and identify avenues for future research . Recommendations are given for example for electrode placement sampling rate segmentation and classifiers . Four groups of applications where myoelectric interfaces have been adopted are identified assistive technology rehabilitation technology input devices and silent speech interfaces . The state of the art applications in each of these groups are presented . 
",Critical issues and practices for myoelectric interfaces are discussed. Robust classification in real use is the main challenge of sEMG interfaces. Expected trend is toward regression and factorization methods and sensor fusion. Simplified sEMG setup may be adequate in real life environment. Potential uses of sEMG interfaces are rapidly increasing.,,,
S174680941400086X," A minimal recruitment model can be used to guide mechanical ventilation PEEP selection for acute respiratory distress syndrome patients . However implementation of this model requires a specific clinical protocol and is computationally expensive and thus not suitable for bedside application . This work aims to improve the performance and bedside utility of the minimal recruitment model through simplifying the model and improving the identification algorithm without compromising the model s physiological relevance to the disease . Identifying the model requires fitting of 8 unique parameters to pressure volume data at multiple levels of positive end expiratory pressure . A minimal algorithm is proposed to improve the model s performance . The algorithm utilises a non linear least squares solver to estimate a global set of the parameters to a pressure volume curve at one PEEP level . These global parameters were then subsequently used to estimate other parameters at different pressure volume curves . The accuracy and computational performance of the minimal algorithm is compared to the grid search algorithm for 2 ARDS patient cohorts . The median absolute percentage curve fitting error over all patients for grid search algorithm is 1.40 and for the minimal algorithm is 2.43 . The median computational time for all patients for the grid search algorithm is 394.51s and for the minimal algorithm is 0.72s where a 600 of reduction of computational time was found for the minimal algorithm . The estimated parameters using the minimal algorithm are correlated with the grid search algorithm with median person s correlation of R 0.9 . The model fitting error for the minimal algorithm is higher than the grid search algorithm . However the model was able capture similar trends in physiologically relevant parameters without the loss of important clinical information . The minimal algorithm is less computationally intensive than the grid search algorithm whilst still providing a means of selecting PEEP with only a small increase in model fitting error . The minimal algorithm is able to improve computational performance while maintaining the ability to capture physiological parameters as the grid search algorithm . The significant reduction in computational time encourages its clinical application at the bedside for decision making . 
",A minimal recruitment model which captures alveolar opening and closing pressures is presented. The model is computationally intense and not suitable for bedside application. The model is simplified while maintaining its physiological relevance. A new algorithm is used to improve computational performance.,,,
S231471721630006X," Multiple watermarking is used to share the copyright of multiple users increase robustness and high security . The proposed method is comparison of multiple watermarking techniques based on Discrete Wavelet Transform and Singular Value Decomposition using Genetic algorithm . This research elaborates the three main categories of multiple watermarking techniques such as successive segmented and composite watermarking . The experimental results show that the DWT based watermarking algorithms possess multi resolution description characteristics achieving imperceptibility . The SVD based watermarking algorithms add the watermark information to the singular values of the diagonal matrix achieving robustness requirements . The optimization is to maximize the performance of peak signal to noise ratio and normalized correlation in multiple watermarking techniques using genetic algorithms . 
",The DWT SVD based algorithm achieves imperceptibility and robustness. The multiple watermarking techniques achieve high security of medical images more robustness and to preserve the privacy. The transparency and robustness are considered as an optimization problem and solved by applying Genetic algorithms.,,,
S175115771500005X," There are two versions in the literature of counting co author pairs . Whereas the first version leads to a two dimensional power function distribution the other version shows three dimensional graphs totally rotatable around and their shapes are visible in space from all possible points of view . As a result these new 3 D computer graphs called Social Gestalts deliver more comprehensive information about social network structures than simple 2 D power function distributions . The mathematical model of Social Gestalts and the corresponding methods for the 3 D visualization and animation of collaboration networks are presented in Part I of this paper . Fundamental findings in psychology sociology and physics are used as a basis for the development of this model . The application of these new methods to male and to female networks is shown in Part II . After regression analysis the visualized Social Gestalts are rather identical with the corresponding empirical distributions . The structures of female co authorship networks differ markedly from the structures of the male co authorship networks . For female co author pairs networks accentuation of productivity dissimilarities of the pairs is becoming visible but on the contrary for male co author pairs networks accentuation of productivity similarities of the pairs is expressed . 
",A mathematical model for 3 D computer graphs is presented. Structures of preference who is collaborating with whom are shown in 3 D version. The methods are applied on male and female networks.,,,
S221205481500017X,"3D modeling in rock art studies involves different techniques according to the size and morphology of the subject. It has mainly been used for reconstructing the volume of caves morphology of walls and as a substitute to graphic and photographic recording of the prehistoric pictures. Little work has been done at macroscopic and microscopic scale partly because lasergrammetry which is the most common technique is poorly adequate under the centimetric scale while for patrimonial purposes recording at high resolution was of little interest. Thanks to the increasing performance of personal computers new modeling techniques are becoming available based on photographic recording and no longer depending on costly and cumbersome equipments. We have tested in open air and underground sites in France Portugal and Russia the potential of photogrammetry and focus stacking for 3D recording of millimetric and submillimetric details of prehistoric petroglyphs and paintings along with original simple optical solutions. 
",Photogrammetry is suitable for 3D modeling at macroscopic scale. Most relevant criteria for the traceology of petroglyphs are in the scope of photogrammetry. Focus stacking enlarges the depth of field of photomicrography and gives data for relief elevation. Consumers devices and software provide effective solutions for macro and microscopic recording.,,,
S174680941500155X," Recent psychoacoustic studies found that across band envelope correlation carried important information for speech intelligibility . This motivated the present work to propose an ABEC based intelligibility measure that could be used to non intrusively predict speech intelligibility in noise using only temporal envelope waveforms extracted from the noise corrupted speech . The proposed ABEC based metric was computed by averaging the correlation coefficients of mean removed envelope waveforms from adjacent frequency bands of the noise corrupted speech . The ABECm measures were evaluated with intelligibility scores obtained from normal hearing listeners presented with sentences corrupted by four types of maskers in a total of 22 conditions . High correlation was obtained between ABECm values and listeners sentence recognition scores and this correlation was comparable to those computed with existing intrusive and non intrusive intelligibility measures . This suggests that across band envelope correlation may work as a simple but efficient predictor of speech intelligibility in noise whose computation does not need access to the clean reference signal . 
",ABECm may efficiently and non intrusively predict speech intelligibility in noise. High correlation was obtained between ABECm values and intelligibility scores. The computation of ABECm does not need access to the clean reference signal.,,,
S228843001530035X,"A designer is mainly supported by two essential factors in design decisions. These two factors are intelligence and experience aiding the designer by predicting the interconnection between the required design parameters. Through classification of product data and similarity recognition between new and existing designs it is partially possible to replace the required experience for an inexperienced designer. Given this context the current paper addresses a framework for recognition and flexible retrieval of similar models in product design. The idea is to establish an infrastructure for transferring design as well as the required PLM Product Lifecycle Management know how to the design phase of product development in order to reduce the design time. Furthermore such a method can be applied as a brainstorming method for a new and creative product development as well. The proposed framework has been tested and benchmarked while showing promising results. 
",Developing a knowledge based framework to assist the designer in design decisions. Opitz feature recognition and code generation from STEP for data standardization. An efficient similarity recognition algorithm to retrieve models from database.,,,
S228843001500055X," To injection mould polymers designing mould is a key task involving several critical decisions with direct implications to yield quality productivity and frugality . One prominent decision among them is specifying sprue bush conduit expansion as it significantly influences overall injection moulding abstruseness anguish in its design criteria deceives direct determination . Intuitively designers decide it wisely and then exasperate by optimising or manipulating processing parameters . To overwhelm that anomaly this research aims at proposing an ideal design criteria holistically for all polymeric materials also tend as a functional assessment metric towards perfection i.e . criteria to specify sprue conduit size before mould development . Accordingly a priori analytical criterion was deduced quantitatively as expansion ratio from ubiquitous empirical relationships specifically a.k.a an exclusive expansion angle imperatively configured for injectant properties . Its computational intelligence advantage was leveraged to augment functionality of perfectly injecting into an impression gap while synchronising both injector capacity and desired moulding features . For comprehensiveness it was continuously sensitised over infinite scale as an explicit factor dependent on in situ spatio temporal injectant state perplexity with discrete slope and altitude for each polymeric character . In which congregant ranges of apparent viscosity and shear thinning index were conceived to characteristically assort most thermoplastics . Thereon results accorded aggressive conduit expansion widening for viscous incrust while a very aggressive narrowing for shear thinning encrust among them apparent viscosity had relative dominance . This important rationale would certainly form a priori design basis as well diagnose filling issues causing several defects . Like this the proposed generic design criteria being simple would immensely benefit mould designers besides serve as an inexpensive preventive clich to moulders . Its adaption ease to practice manifests a hope of injection moulding extremely alluring polymers . Therefore we concluded that appreciating injectant s polymeric character to design exclusive sprue bush offers a definite a priori advantage . 
",Features a specific quantitative a priori analytical criterion as expansion ratio.. Injector capacity and desired moulding features are synchronised to augment functionality. In situ spatio temporal injectant state perplexity described as explicit function. Comprehensive sensitisation showed discrete slope and altitude for each polymeric character. All thermoplastic characteristics were assorted across congruent range of apparent viscosity and shear thinning index. Congruent ranges of apprent viscosity and shear thinning index were conceived to characteristically assort most thermoplastics.,,,
S221313371300022X," We study the benefits and limits of parallelised Markov chain Monte Carlo sampling in cosmology . MCMC methods are widely used for the estimation of cosmological parameters from a given set of observations and are typically based on the Metropolis Hastings algorithm . Some of the required calculations can however be computationally intensive meaning that a single long chain can take several hours or days to calculate . In practice this can be limiting since the MCMC process needs to be performed many times to test the impact of possible systematics and to understand the robustness of the measurements being made . To achieve greater speed through parallelisation MCMC algorithms need to have short autocorrelation times and minimal overheads caused by tuning and burn in . The resulting scalability is hence influenced by two factors the MCMC overheads and the parallelisation costs . In order to efficiently distribute the MCMC sampling over thousands of cores on modern cloud computing infrastructure we developed a Python framework called CosmoHammer which embeds emcee an implementation by Foreman Mackey et al . of the affine invariant ensemble sampler by Goodman and Weare . We test the performance of CosmoHammer for cosmological parameter estimation from cosmic microwave background data . While Metropolis Hastings is dominated by overheads CosmoHammer is able to accelerate the sampling process from a wall time of 30 h on a dual core notebook to 16 min by scaling out to 2048 cores . Such short wall times for complex datasets open possibilities for extensive model testing and control of systematics . 
",We analyse MCMC methods for cosmology regarding parallelisability and efficiency. We present the Python framework CosmoHammer for parallelised MCMC sampling. It enables us to estimate cosmological parameters on high performance clusters. To test the efficiency of CosmoHammer we use an elastic cloud computing environment.,,,
S221478291500024X," Internet interventions constitute a promising and cost effective treatment alternative for a wide range of psychiatric disorders and somatic conditions . Several clinical trials have provided evidence for its efficacy and effectiveness and recent research also indicate that it can be helpful in the treatment of conditions that are debilitating but do not necessarily warrant more immediate care for instance procrastination a self regulatory failure that is associated with decreased well being and mental health . However providing treatment interventions for procrastination via the Internet is a novel approach making it unclear how the participants themselves perceive their experiences . The current study thus investigated participants own apprehension of undergoing Internet based cognitive behavior therapy for procrastination by distributing open ended questions at the post treatment assessment for instance What did you think about the readability of the texts How valuable do you believe that this treatment has been for you and The thing that I am most displeased with is . In total 75 participants responded and the material was examined using thematic analysis . The results indicate that there exist both positive and negative aspects of the treatment program . Many participants increased their self efficacy and were able to gain momentum on many tasks and assignments that had been deferred in their everyday life . Meanwhile several participants lacked motivation to complete the exercises had too many conflicting commitments and were unable to keep up with the tight treatment schedule . Hence the results suggest that Internet interventions for procrastination could profit from individual tailoring shorter and more manageable modules and that the content need to be adapted to the reading comprehension and motivational level of the participant . 
",Internet based cognitive behavior therapy for procrastination was considered credible by a majority of the participants. Many participants were able to gain momentum and raise their self efficacy as a result of the treatment interventions. Conflicting commitments prevented a number of the participants from completing the treatment program. Feedback might have to be adapted to the individual needs of the participants. Short and manageable modules could be of particular importance in the treatment of procrastination.,,,
S228843001530018X,"Normally all manufacturing and fabrication processes introduce residual stresses in a component. These stresses exist even after all service or external loads have been removed. Residual stresses have been studied elaborately in the past and even in depth research have been done to determine their magnitude and distribution during different manufacturing processes. But very few works have dealt with the study of residual stresses formation during the casting process. Even though these stresses are less in magnitude they still result in crack formation and subsequent failure in later phases of the component usage. In this work the residual stresses developed in a shifter during casting process are first determined by finite element analysis using ANSYS Mechanical APDL Release 12.0 software. Initially the analysis was done on a simple block to determine the optimum element size and boundary conditions. With these values the actual shifter component was analyzed. All these simulations are done in an uncoupled thermal and structural environment. The results showed the areas of maximum residual stress. This was followed by the geometrical optimization of the cast part for minimum residual stresses. The resulting shape gave lesser and more evenly distributed residual stresses. Crack compliance method was used to experimentally determine the residual stresses in the modified cast part. The results obtained from the measurements are verified by finite element analysis findings. 
",This paper focus on analytical numerical and experimental design optimization of shifter. Performed design optimization by finite element analysis and experimental of live industrial problem. The results can applicable as a basis of design and optimization of new type of the automotive parts. The results of the current work present the actual behavior of induced stresses.,,,
S228843001530049X,"In this paper we present an efficient technique for sketch based 3D modeling using automatically extracted image features. Creating a 3D model often requires a drawing of irregular shapes composed of curved lines as a starting point but it is difficult to hand draw such lines without introducing awkward bumps and edges along the lines. We propose an automatic alignment of a user s hand drawn sketch lines to the contour lines of an image facilitating a considerable level of ease with which the user can carelessly continue sketching while the system intelligently snaps the sketch lines to a background image contour no longer requiring the strenuous effort and stress of trying to make a perfect line during the modeling task. This interactive technique seamlessly combines the efficiency and perception of the human user with the accuracy of computational power applied to the domain of 3D modeling where the utmost precision of on screen drawing has been one of the hurdles of the task hitherto considered a job requiring a highly skilled and careful manipulation by the user. We provide several examples to demonstrate the accuracy and efficiency of the method with which complex shapes were achieved easily and quickly in the interactive outline drawing task. 
",We present an efficient technique for sketch based 3D modeling using automatically extracted image features. An automatic and real time method is proposed to align a user s hand drawn sketch line to the contour lines of an image facilitating a considerable level of ease for 3D modeling. We use a geometric method to align a sketch line to the outlines of an image using the features of the sketch line and contour lines of an image and some operations are proposed to refine the result of alignment. In the sketch based 3D modeling method the sketch line is represented by a editable spline therefore the aligned sketch line can be further adjusted interactively.,,,
S235222081500111X," A classical result in algebraic specification states that a total function defined on an initial algebra is a homomorphism if and only if the kernel of that function is a congruence . We expand on the discussion of that result from an earlier paper extending it from total to partial functions simplifying the proofs using relational calculus and generalising the setting to regular categories . 
",A total function on an initial algebra is a homomorphism iff its kernel is a congruence. An earlier paper CMCS 2001 elaborates on that classical result. We extend the result from total to partial functions. We simplify the proofs using the relational calculus. We generalise the setting to regular categories.,,,
S221478291400027X," Sustainable online peer to peer support groups require engaged members . A metric commonly used to identify these members is the number of posts they have made . The 90 9 1 principle has been proposed as a rule of thumb for classifying members using this metric with a recent study demonstrating the applicability of the principal to digital health social networks . Using data from a depression Internet support group the current study sought to replicate this finding and to investigate in more detail the model of best fit for classifying participant contributions . Our findings replicate previous results and also find the fit of a power curve to account for 98.6 of the variance . The Zipf distribution provides a more nuanced image of the data and may have practical application in assessing the coherence of the sample . 
",The 90 9 1 principle provides a model for participation in an Internet support group. Alternately a power curve accounts for 98.6 of variance in frequency of posts. We confirm the application of Zipf s Law for the Internet support group.,,,
S174680941500138X," Permutation entropy is a well known and fast method extensively used in many physiological signal processing applications to measure the irregularity of time series . Multiscale PE is based on assessing the PE for a number of coarse grained sequences representing temporal scales . However the stability of the conventional MPE may be compromised for short time series . Here we propose an improved MPE to reduce the variability of entropy measures over long temporal scales leading to more reliable and stable results . We gain insight into the dependency of MPE and IMPE on several straightforward signal processing concepts which appear in biomedical activity via a set of synthetic signals . We also apply these techniques to real biomedical signals via publicly available electroencephalogram recordings acquired with eyes open and closed and to ictal and non ictal intracranial EEGs . We conclude that IMPE improves the reliability of the entropy estimations in comparison with the traditional MPE and that it is a promising technique to characterize physiological changes affecting several temporal scales . We provide the source codes of IMPE and the synthetic data in the public domain . 
",Permutation entropy PE is a broadly used algorithm to measure the complexity of signals. Multiscale PE MPE is based on assessing the PE for a number of coarse grained sequences representing temporal scales. To increase the stability and reliability of MPE improved MPE IMPE is proposed. Several signal processing concepts are used to show the ability of IMPE. We also apply MPE and IMPE for real publicly available electroencephalogram EEG signals.,,,
S221205481500003X," Contemporary rock art researchers have a wide choice of 3D laser scanners available to them for recording stone surfaces and this is complimented by numerous software packages that are able to process point cloud data . Though ESRI s ArcGIS primarily handles geographical data it also offers the ability to visualise XYZ data from a stone surface . In this article the potential of ArcGIS for rock art research is explored by focusing on 3D data obtained from two panels of cup and ring marks found at Loups s Hill County Durham England . A selection of methods commonly utilised in LiDAR studies which enhance the identification of landscape features are also conducted upon the rock panels including DSM normalisation and raster data Principle Component Analysis . Collectively the visualisations produced from these techniques facilitate the identification of the rock art motifs but there are limitations to these enhancements that are also discussed . 
",Problems of traditional rock art recordings were discussed Laser scans of rock art carvings were processed in ArcGIS Detailed DSMs of rock art surfaces were generated from TIN interpolation Rasters with different surface illuminations were summarised with PCA ArcGIS offers the possibility to visualise and enhance rock art carvings,,,
S221478291600004X," Psychotherapy process research examines the content of treatment sessions and their association with outcomes in an attempt to better understand the interactions between therapists and clients and to elucidate mechanisms of behavior change . A similar approach is possible in technology delivered interventions which have an interaction process that is always perfectly preserved and rigorously definable . The present study sought to examine the process of participants interactions with a computer delivered brief intervention for drug use from a study comparing computer and therapist delivered brief interventions among adults at two primary health care centers in New Mexico . Specifically we sought to describe the pattern of participants choices and reactions throughout the computer delivered brief intervention and to examine associations between that process and intervention response at 3 month follow up . Participants were most likely to choose marijuana as the first substance they wished to discuss . Most participants indicated that they had not experienced any problems as a result of their drug use but nearly a third of these nevertheless indicated a desire to stop or reduce its use participants who did report negative consequences were most likely to endorse financial or relationship concerns . However participant ratings of the importance of change or of the helpfulness of personalized normed feedback were unrelated to changes in substance use frequency . Design of future e interventions should consider emphasizing possible benefits of quitting rather than the negative consequences of drug use and when addressing consequences should consider focusing on the impacts of substance use on relationship and financial aspects . These findings are an early but important step toward using process evaluation to optimize e intervention content . 
",Process variables in eHealth interventions such as choices made during an interactive intervention are readily available. Such variables can provide insight into how participants use eHealth interventions. Correlations between choices and outcomes may also guide intervention optimization and future research. Among screen positive adults in primary care 108 60.7 denied any problems as a result of their drug use. Participant ratings of the importance of change were unrelated to changes in substance use frequency.,,,
S221478291500007X," Growth models have the capability of studying change at the group as well as the individual level . In addition these methods have documented advantages over traditional data analytic approaches in the analysis of repeated measures data . These advantages include but are not limited to the ability to incorporate time varying predictors handle dependence among repeated observations in a very flexible manner and to provide accurate estimates with missing data under fairly unrestrictive missing data assumptions . The flexibility of the growth curve modeling approach to the analysis of change makes it the preferred choice in the evaluation of direct indirect and moderated intervention effects . Although offering many benefits growth models present challenges in terms of design analysis and reporting of results . This paper provides a nontechnical overview of growth models in the analysis of change in randomized experiments and advocates for their use in the field of internet interventions . Practical recommendations for design analysis and reporting of results from growth models are provided . 
",This paper provides a nontechnical overview of growth models. I advocate for these methods use in the field of internet interventions. Recommendations for design analysis and reporting of results are provided.,,,
S221478291530021X," The aim of this pilot study was to explore the effects of an early and customized CBT intervention mainly delivered via internet for adolescents with coexisting recurrent pain and emotional distress . The intervention was based on a transdiagnostic approach to concurrently target pain and emotional distress . A single case experimental design was employed with six participants 17 21years old who were recruited via school health care professionals at the student health care team at an upper secondary school in a small town in Sweden . The intervention consisted of 5 9 modules of CBT delivered via internet in combination with personal contacts and face to face sessions . The content and length of the program was customized depending on needs . The effects of the program were evaluated based on self report inventories which the participants filled out before and after the intervention and at a six month follow up . They did also fill out a diary where they rated symptoms on a daily basis . The results were promising at least when considering changes during the intervention as well as pre and posttest ratings . However the results were more modest when calculating the reliable change index and most of the treatment effects were not sustained at the follow up assessment which raises questions about the durability of the effects . Taken together this study indicates that this type of program is promising as an early intervention for adolescents with pain and concurrent emotional distress although the outcomes need to be explored further especially in terms of long term effects . 
",I CBT has not been used as an early intervention for adolescents with coexisting pain and emotional distress The aim of this pilot study was to explore the effects of early I CBT for adolescents with pain and emotional distress The intervention consisted of 5 9 modules of customized I CBT combined with personal support The effects were quite modest and larger studies are needed to further explore the outcomes,,,
S221313371500092X," We present the project Asteroids @ home that uses distributed computing to solve the time consuming inverse problem of shape reconstruction of asteroids . The project uses the Berkeley Open Infrastructure for Network Computing framework to distribute collect and validate small computational units that are solved independently at individual computers of volunteers connected to the project . Shapes rotational periods and orientations of the spin axes of asteroids are reconstructed from their disk integrated photometry by the lightcurve inversion method . 
",Asteroids@home is a distributed computing project. It runs in the framework of Berkeley Open Infrastructure for Network Computing. Shape models of asteroids are reconstructed from disk integrated photometry.,,,
S221478291400013X," The aims of the current study were to 1 establish the efficacy of two Internet based prevention programmes to reduce anxiety and depressive symptoms in adolescents and 2 investigate the distribution of psychological symptoms in a large sample of Australian adolescents prior to the implementation of the intervention . A cluster randomised controlled trial was conducted with 976 Year 9 10 students from twelve Australian secondary schools in 2009 . Four schools were randomly allocated to the Anxiety Internet based prevention programme five schools to the Depression Internet based prevention programme and three to their usual health classes . The Thiswayup Schools for Anxiety and Depression prevention courses were presented over the Internet and consist of 6 7 evidence based curriculum consistent lessons to improve the ability to manage anxiety and depressive symptoms . Participants were assessed at baseline and post intervention . Data analysis was constrained by both study attrition and data corruption . Thus post intervention data were only available for 265 976 students . Compared to the control group students in the depression intervention group showed a significant improvement in anxiety and depressive symptoms at the end of the course whilst students in the anxiety intervention demonstrated a reduction in symptoms of anxiety . No significant differences were found in psychological distress . The Thiswayup Schools Depression and Anxiety interventions appear to reduce anxiety and depressive symptoms in adolescents using a curriculum based blended online and offline cognitive behavioural therapy programme that was implemented by classroom teachers . Given the study limitations particularly the loss of post intervention data these findings can only be considered preliminary and need to be replicated in future research . 
",This study presents baseline data on screening scales in a large sample of Australian adolescents. This study presents baseline data on suicidal ideation in Australian adolescents. This school based intervention reduced symptoms of anxiety and depression. Given the study limitations these findings can only be considered tentative.,,,
S228843001500024X," In this study a new methodology in predicting a system output has been investigated by applying a data mining technique and a hybrid type II fuzzy system in CNC turning operations . The purpose was to generate a supplemental control function under the dynamic machining environment where unforeseeable changes may occur frequently . Two different types of membership functions were developed for the fuzzy logic systems and also by combining the two types a hybrid system was generated . Genetic algorithm was used for fuzzy adaptation in the control system . Fuzzy rules are automatically modified in the process of genetic algorithm training . The computational results showed that the hybrid system with a genetic adaptation generated a far better accuracy . The hybrid fuzzy system with genetic algorithm training demonstrated more effective prediction capability and a strong potential for the implementation into existing control functions . 
",A new methodology in predicting a CNC machining output has been investigated. A data mining technique and a hybrid type II fuzzy system are applied. Two different types of membership functions were created to generate a hybrid system. Fuzzy rules are automatically modified in the process of genetic algorithm training. The results showed that the hybrid system generated a far better accuracy.,,,
S221478291400030X," There is evidence from randomized control trials that internet based cognitive behavioral therapy is efficacious in the treatment of anxiety and depression and recent research demonstrates the effectiveness of iCBT in routine clinical care . The aims of this study were to implement and evaluate a new pathway by which patients could access online treatment by completing an automated assessment rather than seeing a specialist health professional . We compared iCBT treatment outcomes in patients who received an automated pre treatment questionnaire assessment with patients who were assessed by a specialist psychiatrist prior to treatment . Participants were treated as part of routine clinical care and were therefore not randomized . The results showed that symptoms of anxiety and depression decreased significantly with iCBT and that the mode of assessment did not affect outcome . That is a pre treatment assessment by a psychiatrist conferred no additional treatment benefits over an automated assessment . These findings suggest that iCBT is effective in routine care and may be implemented with an automated assessment . By providing wider access to evidence based interventions and reducing waiting times the use of iCBT within a stepped care model is a cost effective way to reduce the burden of disease caused by these common mental disorders . 
",We examined the effectiveness of internet CBT in routine clinical care. Patients underwent an automated assessment or an assessment by a psychiatrist. Both groups showed significant improvement in symptoms of anxiety and depression. There were no differences between groups at post treatment. Automated assessments can lead to good treatment outcomes in etherapy.,,,
S174680941500172X," Geriatric depression is a pathological process that causes a great number of symptoms resulting in limited mental and physical functionality . The computation of oscillatory and synchronization patterns in certain brain areas may facilitate the early and robust identification of depressive symptoms . In this study electroencephalographic data were recorded from 34 participants suffering from both cognitive impairment and geriatric depression and 32 control subjects . Both groups were matched according to their cognitive status . The study aims at evaluating neurophysiological features of elderly participants suffering from depression and neurodegeneration . The current work focuses on the identification of depression symptoms that coexist with cognitive decline the correlation of the examined neurophysiological features with geriatric depression combined with cognitive impairment and the investigation of the role of data mining techniques in the analysis of EEG data . The EEG features were estimated through synchronization analysis . Depressive patterns were detected through data mining techniques . Random Forest Random Tree Multilayer Perceptron and Support Vector Machines were employed for data classification . The efficiency of the classifiers varied from 92.42 to 95.45 . Random Forest demonstrated the highest accuracy . Both synchronization and oscillatory features contributed to the decision trees formation with the former prevailing . Moreover synchronization features significantly contributed to the decision trees formation . In line with previous neuroscientific findings synchronization among right frontal midline anteriofrontal regions showed great correlation with depressive symptoms . Evaluation of the classifiers indicated the Random Forest as being the most robust algorithm . Synchronization of certain brain regions is more indicative of identifying depression symptoms than oscillatory since synchronization features contributed the most to the formation of the classification trees . 
",Depressive signs in concurrent cognitive decline are detected through mining EEG resting state activity. Random Forest Random Tree MLP Network and Support Vector Machines SVM are employed for data classification. Random Forest demonstrated the highest accuracy. Synchronization features significantly contributed to the decision tree formation.,,,
S221478291500038X," Objectives The first aim of this study was to describe parental attitudes towards and intentions to access computer based therapies for youth mental health problems . The second aim was to assess parental factors predicting attitudes and intentions to access computer based therapies for youth . Method Three hundred and seventy three Australian parents completed an online survey measuring demographics mental health service experience personality technology factors mental health knowledge and attitudes perceived benefits problems and helpfulness of computer based therapies and intentions to access services . Results Approximately 50 of parents reported accessing support for their child s mental health yet only 6 had used a computer based therapy . The majority of parents strongly endorsed all benefits of computer based therapies and appeared relatively less concerned by potential service problems . Computer based therapies were perceived as somewhat to extremely helpful by 87 of parents and 94 indicated that they would utilise a computer based therapy if their child required support and one was offered to them . Parental knowledge of computer based therapies significantly predicted perceived helpfulness p .001 and intentions to access p .001 computer based therapies above that of parent demographic characteristics clinical factors and engagement with technology . Conclusions Australian parents hold positive attitudes to the use of computer based therapies . 
",Parents act as gatekeepers to mental health services for children and adolescents. Parents reported their attitudes towards computer based therapies for young people. 94 of parents would access a computer based therapy if their child needed support. Computer based therapy knowledge predicted perceived helpfulness and use intentions. Parents hold positive attitudes to the use of computer based therapies.,,,
S228843001530021X," In recent years the automotive industry has known a remarkable development in order to satisfy the customer requirements . In this paper we will study one of the components of the automotive which is the twist beam . The study is focused on the multicriteria design of the automotive twist beam undergoing linear elastic deformation . Indeed for the design of this automotive part there are some criteria to be considered as the rigidity and the resistance to fatigue . Those two criteria are known to be conflicting therefore our aim is to identify the Pareto front of this problem . To do this we used a Normal Boundary Intersection algorithm coupling with a radial basis function metamodel in order to reduce the high calculation time needed for solving the multicriteria design problem . Otherwise we used the free form deformation technique for the generation of the 3D shapes of the automotive part studied during the optimization process . 
",We model the automotive twist beam. We developed an algorithm to solve multicriteria optimization problem. We solve our industrial problem with our developed algorithm. New profiles of the twist beam are proposed.,,,
S221478291500010X," Many trials on Internet delivered psychological treatments have had problems with nonadherence but not much is known about the subjective reasons for non adhering . The aim of this study was to explore participants experiences of non adherence to Internet delivered psychological treatment . Grounded theory was used to analyze data from seven in depth interviews with persons who had non adhered to a study on Internet delivered cognitive behavioral therapy for generalized anxiety disorder . The process of non adherence is described as an interaction between patient factors and treatment factors . A working model theory was generated to illustrate the experience of nonadherence . The model describes a process where treatment features such as workload text content complexity and treatment process don t match personal prerequisites regarding daily routines perceived language skills and treatment expectations respectively resulting in the decision to nonadhere . Negative effects were also stated as a reason for non adherence . Several common strategies used for increasing adherence to Internet delivered therapy in general are by these non completers regarded as factors directly related to their reason for non adherence . 
",Qualitative study on experiences of non adherence to Internet delivered treatment Explores treatment features relevant to participants experience of premature treatment termination Explores negative effects,,,
S187775031300077X," Mean field models of the mammalian cortex treat this part of the brain as a two dimensional excitable medium . The electrical potentials generated by the excitatory and inhibitory neuron populations are described by nonlinear coupled partial differential equations that are known to generate complicated spatio temporal behaviour . We focus on the model by Liley et al . 67 113 . Several reductions of this model have been studied in detail but a direct analysis of its spatio temporal dynamics has to the best of our knowledge never been attempted before . Here we describe the implementation of implicit time stepping of the model and the tangent linear model and solving for equilibria and time periodic solutions using the open source library PETSc . By using domain decomposition for parallelization and iterative solving of linear problems the code is capable of parsing some dynamics of a macroscopic slice of cortical tissue with a sub millimetre resolution . 
",A parallel open source implementation of Liley s mean field cortex model in PETSc. We implement fully implicit time integration of nonlinear and variational equations. We perform equilibrium continuation with computation of inhomogeneous eigenmodes. We compute periodic solutions with Newton Krylov iteration.,,,
S221313371400050X," The increasing number of astronomical surveys in mid and far infrared as well as in submillimetre and radio wavelengths brings more difficulties to the already challenging task of detecting sources in an automatic way . These specific images are characterized by a more complex background than in shorter wavelengths with a higher level of noise more noticeable flux variations and both unresolved and extended sources with a higher dynamic range . In order to improve the source detection efficiency in long wavelength images in this paper we present a new approach based on the combined use of a multiscale decomposition and a recently developed method called Distilled Sensing . Its application minimizes the impact of the contaminants from the background unveiling and highlighting the sources at the same time . The experimental results achieved using infrared and radio images illustrate the good performance of the approach identifying greater percentages of true sources than using both the widely used SExtractor algorithm and the Distilled Sensing method alone . 
",A new approach to detect sources in long wavelength images is presented. It is based on a multiscale decomposition and a method called Distilled Sensing. The method is tested against real infrared and radio images. The performance of the method is compared to reference state of the art algorithms.,,,
S235266451500019X," Several solutions have been proposed till date to achieve effective replication policies and admission control methods that can ensure a high Quality of Service to Video on Demand systems . In this paper we have chosen to study admission control and a replication strategy in a video on demand system called Feedback Control Architecture for Distributed Multimedia Systems . The proposed approaches are founded on the supervision and auto adaptation of the system load continued knowledge of the network and of the video servers workload and the development of a dynamic replication strategy of segments for popular video that remade to replica placement problem by discussing the problem of the replicas number to create . The workload of video servers and of network are calculated by a QoS controller . The proposed approach takes into account of replica detail and offers a replica removal policy to avoid waste and for a good management of storage space . The goal is to maintain an updated broad view of the system s behaviour from the collected measurements to databases of available videos and databases of available video servers . To demonstrate the efficiency and feasibility of our approaches and validate the results obtained we conducted a series of experiments based on a simulator that we implemented . 
",Development of a dynamic replication strategy of segments for popular video. Proposing an approach that can provide admissions control for all VoD systems. It addresses the following challenges resource allocation and several algorithms for scheduling policies. Application of a measurement algorithm. Application of a decision algorithm. To demonstrate the efficiency and feasibility of our approaches and validate the results obtained we conducted a series of experiments.,,,
S0045782513001473," We propose a new hybrid algorithm for incompressible micro and nanoflows that applies to non isothermal steady state flows and does not require the calculation of the Irving Kirkwood stress tensor or heat flux vector . The method is validated by simulating the flow in a channel under the effect of a gravity like force with bounding walls at two different temperatures and velocities . The model shows very accurate results compared to benchmark full MD simulations . In the temperature results in particular the contribution of viscous dissipation is correctly evaluated . 
",An atomistic continuum hybrid model for incompressible fluids is proposed. The model handles both momentum and heat transfer. Comparison with full MD benchmarks is very good. For large geometries the computational savings are large. Three methods for drastically reducing numerical noise are proposed.,,,
S0045782513001448," Incompressible smoothed particle hydrodynamics generally requires particle distribution smoothing to give stable and accurate simulations with noise free pressures . The diffusion based smoothing algorithm of Lind et al . 1499 1523 has proved effective for a range of impulsive flows and propagating waves . Here we apply this to body water slam and wave body impact problems and discover that temporal pressure noise can occur for these applications . This is due to the free surface treatment as a discontinuous boundary . Treating this as a continuous very thin boundary within the pressure solver is shown to effectively cure this problem . The particle smoothing algorithm is further generalised so that a non dimensional diffusion coefficient is applied which suits a given time step and particle spacing . We model the particular problems of cylinder and wedge slam into still water . We also model wave body impact by setting up undisturbed wave propagation within a periodic domain several wavelengths long and inserting the body . In this case the loads become cyclic after one wave period and are in good agreement with experiment . This approach is more efficient than the conventional wave flume approach with a wavemaker which requires many wavelengths and a beach absorber . Results are accurate and virtually noise free spatially and temporally . Convergence is demonstrated . Although these test cases are two dimensional with simple geometries the approach is quite general and may be readily extended to three dimensions . 
",We conduct truly incompressible SPH simulations for free surface flows. We further develop a stabilising particle shifting algorithm. An improved free surface boundary treatment is presented. Slamming cases and a novel efficient wave structure interaction technique are presented. Accuracy is good pressures are virtually noise free.,,,
S0045782514000607," The Lagrange Multiplier and penalty methods are commonly used to enforce incompressibility and compressibility in models of cardiac mechanics . In this paper we show how both formulations may be equivalently thought of as a weakly penalized system derived from the statically condensed Perturbed Lagrangian formulation which may be directly discretized maintaining the simplicity of penalty formulations with the convergence characteristics of LM techniques . A modified Shamanskii Newton Raphson scheme is introduced to enhance the nonlinear convergence of the weakly penalized system and exploiting its equivalence modifications are developed for the penalty form . Focusing on accuracy we proceed to study the convergence behavior of these approaches using different interpolation schemes for both a simple test problem and more complex models of cardiac mechanics . Our results illustrate the well known influence of locking phenomena on the penalty approach and its effect on accuracy for whole cycle mechanics . Additionally we verify that direct discretization of the weakly penalized form produces similar convergence behavior to mixed formulations while avoiding the use of an additional variable . Combining a simple structure which allows the solution of computationally challenging problems with good convergence characteristics the weakly penalized form provides an accurate and efficient alternative to incompressibility and compressibility in cardiac mechanics . 
",A finite element displacement formulation stemming from the Perturbed Lagrangian. The efficiency is enhanced with a Shamanskii Newton Raphson scheme. Comparison with Perturbed Lagrangian Lagrange Multiplier and penalty methods. Presence of locking in the penalty method for whole cycle cardiac mechanics. The presented form combines good convergence behavior with a simplified structure.,,,
S0045782514001492," Discrete element methods can be based on either penalties or impulses to resolve collisions . A generic impulse based method the energy tracking method is described to resolve collisions between multiple non convex bodies in three dimensions . As opposed to the standard sequential impulse method and simultaneous impulse method which also apply impulses to avoid penetration the energy tracking method changes the relative velocity between two colliding bodies iteratively yet simultaneously . Its main novelty is that impulses are applied gradually at multi point contacts and energy changes at the contact points are tracked to ensure conservation . Three main steps are involved in the propagation of the impulses during the single and multi contact resolution compression restitution related energy loss and separation . Numerical tests show that the energy tracking method captures the energy conservation property of perfectly elastic single and multi point collisions . ETM exhibits improved angular velocity estimation as compared to SMM and SQM as demonstrated by two numerical examples that model multi point contact between box shaped objects . Angles of repose estimated for multi object pack repositioning of spheres cubes and crosses are in good agreement with the reported experimental values . 
",DEM can be based on penalties or impulses to resolve collisions. The impulse based energy tracking method ETM resolves multiple collisions consistently. ETM models multiple collisions iteratively yet simultaneously while tracking the system energy. ETM does not rely on penalties and does not require computation of penetration of bodies. The method is validated in the context of energy conservation and meso scale angles of repose.,,,
S0045782513000479," This article presents a novel approach to collision detection based on distance fields . A novel interpolation ensures stability of the distances in the vicinity of complex geometries . An assumed gradient formulation is introduced leading to a continuous distance function . The gap function is re expressed allowing penalty and Lagrange multiplier formulations . The article introduces a node to element integration for first order elements but also discusses signed distances partial updates intermediate surfaces mortar methods and higher order elements . The algorithm is fast simple and robust for complex geometries and self contact . The computed tractions conserve linear and angular momentum even in infeasible contact . Numerical examples illustrate the new algorithm in three dimensions . 
",Presents asynchronous variational integrators in the context of finite elements with continuous strain fields. Illustrates an enhanced interpretation of the current space time front. Provides a strategy for estimating the critical time step size using CAG elements nodal integration or SFEM.,,,
S0045790616000021," A graph grammar is a formal tool for providing rigorous but intuitive ways to define visual languages . Based on an existing graph grammar this paper proposes new context sensitive graph grammar formalism called the Extension of Edge based Graph Grammar or E EGG . The E EGG introduces new mechanisms into grammatical specifications productions operations and so on in order to conveniently treat the bidirectional transformation between the Business Process Modeling Notation and the Business Process Execution Language . Besides formal definitions of the E EGG are provided steps and algorithms to achieve the bidirectional transformation and to check the correctness of BPMN models structure are presented . Finally a case study on transformation from BPMN models to BPEL codes is provided to show how the parsing algorithm of the E EGG works . 
",We propose a new context sensitive graph grammar formalism called E EGG in short. E EGG introduces new mechanisms to tackle bidirectional transformation between BPMN and BPEL. Steps to achieve transformation and check correctness of BPMN structure are presented. Corresponding algorithms are further designed and analyzed. A case study on transformation from BPMN to BPEL is discussed.,,,
S0045790614000457," With reference to a network consisting of sensor nodes connected by wireless links we approach the problem of the distribution of the cryptographic keys . We present a solution based on communication channels connecting sequences of adjacent nodes . All the nodes in a channel share the same key . This result is obtained by propagating the key connecting the first two nodes to all the other nodes in the channel . The key propagation mechanism is also used for key replacement as is required for instance in group communication to support forms of forward and backward secrecy when a node leaves a group or a new node is added to an existing group . 
",In a sensor network communication channels connecting sequences of adjacent nodes are created by key propagation. All the nodes in a communication channel share the same key. The key connecting the start node and the first subsequent node is propagated to all the other nodes in the channel. Communication channels are bidirectional they can be used to transmit a message from to any node in the channel.,,,
S0045790615003225," Cellular Automata are of interest in several research areas and there are many available serial implementations of CA . However there are relatively few studies analyzing in detail High Performance Computing implementations of CA which allow research on large systems . Here we present a parallel implementation of a CA with distributed memory based on MPI . As a first step to insure fast performance we study several possible serial implementations of the CA . The simulations are performed in three infrastructures comparing two different microarchitectures . The parallel code is tested with both Strong and Weak scaling and we obtain parallel efficiencies of 75 85 for 64 cores comparable to efficiencies for other mature parallel codes in similar architectures . We report communication time and multiple hardware counters which reveal that performance losses are related to cache references with misses branches and memory access . 
",We present a free open source code for Cellular Automata CA using MPI. Weak and strong scaling tests are carried out in 3 different architectures. Performance of our code compares well with performance of other mature HPC codes. Hardware counters are used to help identifying performance issues.,,,
S0045790615003936,"In underwater acoustic sensor networks long baseline localization for autonomous underwater vehicles AUVs requires distance estimation that always encounters severe problems a Time synchronization is hard to achieve in underwater environment which baffles ranging methods based on the synchronized time. b Long propagation delay of acoustic signals and the impact of AUVs mobility make it rash to use the round trip ranging RTR technology. c Sound speed uncertainty enlarges the inaccuracy of distance estimation. This work addresses those problems above and proposes an AUVs self localization algorithm with accurate sound travel time solution SL STTS which is time synchronization free and ranging optimization based. Simulation results show that under the measurement noise of time and angles the root mean square error of SL STTS is decreased by about 8 79 compared with the counterparts. In addition the average distance estimation error of SL STTS is declined by 42 compared with RTR. 
",RTR causes large ranging error. AOA and AUV s orientation are processed to produce accurate OWTT. Mathematical expectation of underwater sound speed reasonably catches its uncertainty. LMA serves as an effective ranging optimizer to localize the AUV. The strategy is sensitive to time measurement noise and less affected by angle measurement noise.,,,
S0045782516302626," Non Uniform Rational B splines and T splines can have some drawbacks when modelling damage and fracture . The use of Powell Sabin B splines which are based on triangles can by pass these drawbacks . Herein smeared as well as discrete approaches to fracture in quasi brittle materials using Powell Sabin B splines are considered . For the smeared formulation an implicit fourth order gradient damage model is adopted . Since quadratic Powell Sabin B splines employ continuous basis functions throughout the domain they are well suited for solving the fourth order partial differential equation that emerges in this higher order damage model . Moreover they can be generated from an arbitrary triangulation without user intervention . Since Powell Sabin B splines are generated from a classical triangulation they are not necessarily boundary fitting and in that case they are not isogeometric in the strict sense . For discrete fracture approaches the degree of continuity of T splines is reduced to at the crack tip . Hence stresses need to be evaluated and weighted at the integration points in the vicinity of the crack tip in order to decide when the critical stress is reached . In practice stress fields are highly irregular around crack tips . Furthermore aligning a T spline mesh with the new crack segment can be difficult . Powell Sabin B splines also remedy these drawbacks as they are continuous at the crack tip and stresses can be directly computed which vastly increases the accuracy and simplifies the implementation . Moreover re meshing is more straightforward using Powell Sabin B splines . A current limitation is that in three dimensions there is no procedure for constructing Powell Sabin B splines on arbitrary tetrahedral meshes . 
",A Powell Sabin B splines formulation for higher order gradient damage models. A discrete crack model exploiting Powell Sabin B splines based on the cohesive zone concept. A remeshing strategy for Powell Sabin B splines with discrete cracks.,,,
S0045790616000100,"This paper proposes a face recognition system based on a steerable pyramid transform SPT and local binary pattern LBP for e Health secured login. In an e Health framework patients are sometimes unable to identify themselves by traditional login modalities such as username and password. Automatic face recognition can replace the conventional login modalities if the recognition system is robust. In the proposed system SPT can decompose a face image into several subbands of different scales and orientations and LBP can encode the subbands in binary texture pattern. Therefore SPT LBP scheme represents a face image in a robust way that includes multiple information sources from different scales and orientations. The proposed system is evaluated on the facial recognition technology FERET database. According to the results the proposed system achieves 99.28 recognition in fb set 80.17 in dup I set and 79.54 in dup II set. 
",Face recognition system using steerable pyramid transform and local binary pattern is proposed. Zero norm minimization and local learning based algorithms are used for feature selection. 99.28 accuracy was obtained in FERET database with fb set.,,,
S0045790615001433," An assessment approach to assess the likelihood of rock burst in coal mines by integrating the Multi Agent System with data fusion techniques is proposed in this paper . We discuss an optimal algorithm for multi sensor data fusion to improve the accuracy and reliability of the source data . Some model based situation quantization methods are described and a rock burst situation quantitative assessment model incorporating improved Dempster Shafer theory is presented . The Auto Regressive Moving Average and Holt Winters models are used to address indefinability and inaccuracy of the prediction . A case study demonstrates that the proposed situation assessment model is capable of producing relatively accurate forecasts and thereby it can provide coal mine decision makers with an overview of the development of rock bursts . 
",The target of this research is assessment situation of rock burst in coal mine. Generally speaking an assessment approach for the rock burst situation by integrating the Multi Agent System with data fusion techniques is proposed. To improve the accuracy and reliability of source data an optimal algorithm for multi sensor data fusion is discussed. Some intelligent model based situation quantization methods is described and rock burst situation quantitative assessment model is discussed by improved Dempster Shafer theory. The Auto Regressive and Moving Average Model and Holt Winters model are used to deal with indefinability and inaccuracy in the process of predicting rock burst. Case study results shows the proposed rock burst situation assessment model can give a relatively accurate forecast and can help coal mine decision makers to grasp an overview of the development trend of the rock burst.,,,
S0045790615002670," Sparse Imputation is a relatively new method that reconstructs missing spectral components of noisy speech signal with the help of the sparse based representation approaches . In this method the redundancy of signal in the frequency domain helps to rebuild noisy spectral components from the remained reliable ones . On the other hand different parts of speech signal despite time intervals between them can be inherently similar to each other . In this paper a major modification over the SI method is proposed that in addition to data redundancy property of speech signal in small regions takes the advantages of its self similarity nature over long intervals . By identifying mostly similar frames using a method based on the marginal classification the Joint Sparsity method is applied and a method dubbed as the Joint Sparse Imputation is presented . The experiments conducted on AURORA 2 data set show that the proposed method significantly improves the recognition results in different noisy conditions compared to the original SI method . 
",The self similarity nature of speech is used to improve the Sparse Imputation method. The similar frames of speech utterance are identified using marginal classification. The Joint Sparsity method is used to reconstruct noisy component of similar frames together.,,,
S0045790615004474,"This study developed an air supply station for air powered scooters. The station comprised mechanical and electrical systems. The key components of the mechanical system were a high power air compressor low pressure cylinder pneumatic boosting cylinder high pressure accumulator and target tank. The electrical system comprised pressure sensors air flow sensors and control circuits which were equipped adequately for the air charge. An air powered scooter was used to evaluate the design specifications and charging performance of the station and the scooter was tested on a chassis dynamometer to assess performance during a modified standard driving cycle. The experimental results confirmed that the air supply station can produce high pressure air for air powered vehicles. The station design can guide the development of similar technology by companies in the transportation and green energy industries. Future research will conduct a theoretical analysis by modeling and simulating the performance of the air station and air scooter. 
",An air supply station with mechanical and electrical systems was built. Two operation modes were designed for charging air bottles. An air powered scooter was used to evaluate the design specifications and charging performance. Results confirm that the station produced high pressure air for vehicles. Theoretical analysis and performance simulation will be conducted in the future.,,,
S0045790615002372," This work investigates the digit serial polynomial basis multipliers performing multiplication in multiple binary extension fields . Designing such versatile multipliers encounters a number of difficulties . First of all the element sizes of the supported fields are different from each other and thus the elements are represented with different number of bits for each field . To deal with different sized elements designs with left or right justified operands are investigated . Secondly each field multiplication involves modular reduction with a different irreducible polynomial and thus the complexity can increase rapidly with the number of supported fields . To prevent this two methods are studied Using sparse irreducible polynomials and unifying the modular reduction computation of the fields by choosing the irreducible polynomials suitably . Our work shows that multiple fields can be supported at the cost of an increase in area and an increase in time . 
",We propose versatile multiplier architectures supporting multiple binary extension fields. We analyze the increase in the cost due to supporting multiple fields. We study a multiplier design supporting five binary fields recommended by NIST for elliptic curves.,,,
S0045790614003036," Biometric based personal authentication is receiving a widespread interest in the area of research due to its high applicability in a wide range of security applications . Among these hand based biometric systems are considered to be more successful in terms of accuracy and computational complexity . In hand based biometrics finger knuckle surface is considered as one of the emerging potential biometric traits for personal authentication . This is due to its stable and unique inherent patterns present in the outer surface of the finger back knuckle region . Further this finger knuckle has a high potentiality towards discriminating individuals with high accuracy . In this paper we present a review of various system models that are implemented for personal authentication using finger knuckle biometrics . Furthermore the challenges that could arise during the implementation of the large scale real time biometric system with finger knuckle print are explored . 
",Describes the role of finger knuckle surface in biometric authentication. Enumerates various available open datasets for finger knuckle biometric trait. Presents different view point in the classification of various finger knuckle feature recognition methods. Elaborates on finger knuckle surface preprocessing feature extraction classification and fusion methodologies. Addresses significant issues that are mandatory for deploying finger knuckle biometric system.,,,
S0045790615004140," This paper proposes a robust particle filter to deal with incomplete sensor data to predict the user s routes and represents users movements using a dynamic Bayesian network model that patterns the user s spatiotemporal routine . The proposed particle filter includes robust particle generation to supplement any incorrect and incomplete sensor information efficient switching weight functions to reduce computation complexity while considering uncertainty and resampling to enhance the accuracy of the particles by solving the degeneracy problem . The robust particle filter enhances the accuracy and efficiency with which a user s routes and destinations are determined . 
",We propose DBN based route models and prediction of users routes using robust particle filtering. We use a DBN to infer the next locations or destinations based on the observed spatio temporal data. The robust particle filter handles uncertainty and constraints to enhance accuracy and efficiency.,,,
S0045790615003638," In view of the fact that objects with different natures usually respond differently to the same external stimulus this paper proposes a no reference image quality assessment based on gradient histogram response . GHR is the gradient histogram variation of an image object under a local transform . In the metric through preprocessing a test image is transformed to a noise image and a blur image which are taken as two image objects . Each image object is exerted with a local transform as an object input and its GHR as an object output is extracted in multiscale space . The two GHRs compose a global feature vector and are mapped to an image quality score . Experiments show that GHR outperforms state of the art no reference metrics statistically in the condition that test images are degraded by different types of distortions . Especially the metric is feasible for the quality assessment of the images degraded by mixed distortions though the types of these images are not included in the training database . 
",A NR image quality assessment based on gradient histogram response GHR is proposed. A test image is preprocessed to produce a noise image object and a blur image object. GHR is the gradient histogram variation of an image object under a local transform. GHR performs well for the images degraded by different distortions or mixed distortions.,,,
S0045782516301323," An efficient and reliable stress computation algorithm is presented which is based on implicit integration of the local evolution equations of multiplicative finite strain plasticity viscoplasticity . The algorithm is illustrated by an example involving a combined nonlinear isotropic kinematic hardening numerous backstress tensors are employed for a better description of the material behavior . The considered material model exhibits the so called weak invariance under arbitrary isochoric changes of the reference configuration and the presented algorithm retains this useful property . Even more the weak invariance serves as a guide in constructing this algorithm . The constraint of inelastic incompressibility is exactly preserved as well . The proposed method is first order accurate . Concerning the accuracy of the stress computation the new algorithm is comparable to the Euler Backward method with a subsequent correction of incompressibility and the classical exponential method . Regarding the computational efficiency the new algorithm is superior to the EBMSC and EM . Some accuracy tests are presented using parameters of the aluminum alloy 5754 O and the 42CrMo4 steel . FEM solutions of two boundary value problems using MSC.MARC are presented to show the correctness of the numerical implementation . 
",A model of finite strain viscoplasticity based on Lion s approach is considered. The discretized system of constitutive relations is reduced to a scalar equation. The approach exploits the notion of the weak invariance of the material model. The symmetry incompressibility and weak invariance are exactly retained. The accuracy and stability of the algorithm are tested numerically.,,,
S0045790615002207," Wireless sensor networks may be used for the surveillance of large systems or in the management of disasters such as forest fires or oil spills . In some of these applications the topology of the network is going to be dynamic a connected system can evolve into a clustered system that presents a break of the global connectivity . Using a collector which will regularly visit the disjoint clusters enables to restore a discreet connectivity . A session layer protocol is proposed to reconstitute a consistent global information system . It enables the collector to reconstruct the communication context with the previously visited clusters knowing they could have moved merged or have split . A node sensor model has been integrated into the Riverbed Opnet Modeler network simulation environment . Simulations show the benefits of the protocol and particularly how it provides a better trajectory planning of the collector . 
",Collection of measurements on a dynamic and non connected wireless sensor network. Proposition of a session protocol to reconstitute a consistent information system. Definition of a clustering metric and of a contextual inheritance relation. Simulations of the protocol behavior on Riverbed Opnet Modeler.,,,
S0045790615003031," Finding new techniques to accelerate electromagnetic simulations has become a necessity nowadays due to its frequent usage in industry . As they are mainly based on domain discretization EM simulations require solving enormous systems of linear equations simultaneously . Available software based solutions do not scale well with the increasing number of equations to be solved . As a result hardware accelerators have been utilized to speed up the process . We introduce using hardware emulation as an efficient solution for EM simulation core solvers . Two different scalable architectures are implemented to accelerate the solver part of an EM simulator based on the Gaussian Elimination and the Jacobi iterative methods . Results show that the performance gap between presented solutions and software based ones increases as the number of equations increases . For example solving 2 002 000 equations using our Clustered Jacobi design in single floating point precision achieved a speed up of 100.88x and 35.24x over pure software implementations represented by MATLAB and the ALGLIB C package respectively . 
",We present an emulation based accelerator for EM simulations in metamaterials. We propose two hardware designs to solve systems of linear equations from the FEM. Both presented designs are run on a physical Veloce 1 emulator from Mentor Graphics. Timing results exceed the performance of several software based solvers.,,,
S0045790615003663," This paper proposes a novel unambiguous correlation function for composite binary offset carrier signal tracking based on partial correlations . In the proposed scheme first we partition sub carriers of the CBOC signal into partial sub carriers and subsequently we obtain partial correlations by correlating the partial sub carriers with the received CBOC signal . Finally a novel unambiguous correlation function with no side peak is constructed by combining the partial correlations in a specially designed way . Unlike the conventional schemes the proposed scheme does not require any auxiliary signal and from numerical results it is found to offer a better tracking performance than those of the conventional schemes in terms of the tracking error standard deviation and multipath error envelope . 
",An unambiguous tracking scheme is proposed for CBOC 6 1 1 11 signal. The proposed scheme combines the partial correlations in a specially designed way. The autocorrelation side peaks are completely removed by the proposed scheme. Better TESD and MEE performances are achieved over those of the conventional schemes.,,,
S0045790616300416," Accurate surface estimation is a critical step for autonomous robot navigation on a rough terrain . In this paper we present a new method for estimating the surface of an unknown arbitrarily shaped terrain from the range data . The terrain modeling problem is generally formulated as the estimation of a function whose zero set corresponds to the surface to be reconstructed . A Laser range scanner has been built for acquisition of range data . The range data from the scanner samples the terrain unevenly and is more sparse for distant regions from the sensor . The paper describes the surface estimation problem as a max margin based formulation of a non stationary kernel function and minimizes the objective function using sub gradient method . Unlike other methods additional geometric ray based information is used to eliminate the unnecessary bumps on the surface and increase the precision . The experimental results validate the robustness of the proposed approach . 
",The problem of detecting the surface of an unknown arbitrarily shaped scene from a set of points is proposed. Range data is obtained from in house developed Laser range scanner. The surface estimation problem is described as a max margin based formulation of a kernel function and solving the objective function using sub gradient method. Additional geometric ray based information is used to eliminate the unnecessary bumps on the surface and increase the precision.,,,
S0045790614003127," A memory efficient field programmable gate array method is described that facilitates the processing of the continuous wavelet transform arithmetic operations . The CWT computations were performed in Fourier space and implemented on FPGA following several optimization schemes . First the adapted wavelet function was stored in a lookup table instead of computing the equation each time . Second the utilization of FPGA memory was highly optimized by only storing the nonzero values of the wavelet function . This reduces 89 of the memory storage and allows fitting the entire design into the FPGA . Third the design decreases the number of multiplications and shortens the time to produce the CWT coefficients . The proposed design was tested using EEG data and demonstrated to be suitable for extracting features from the event related potentials . Fourth wavelet function scales were eliminated which saves further resources . The achieved computation speed allows for real time CWT application . 
",We design highly optimized architecture for continuous wavelet transform in FPGA. Lookup tables are highly recommended in case of implementing complex algorithms. Zero free wavelet function and scale reduction save extensive amount of block RAM. Spartan 3AN FPGA device is suitable for real time EEG analysis with the CWT.,,,
S0045782516300354," In this work we present a statistical treatment of stress life data drawn from a collection of records of fatigue experiments that were performed on 75S T6 aluminum alloys . Our main objective is to predict the fatigue life of materials by providing a systematic approach to model calibration model selection and model ranking with reference to S N data . To this purpose we consider fatigue limit models and random fatigue limit models that are specially designed to allow the treatment of the run outs . We first fit the models to the data by maximum likelihood methods and estimate the quantiles of the life distribution of the alloy specimen . To assess the robustness of the estimation of the quantile functions we obtain bootstrap confidence bands by stratified resampling with respect to the cycle ratio . We then compare and rank the models by classical measures of fit based on information criteria . We also consider a Bayesian approach that provides under the prior distribution of the model parameters selected by the user their simulation based posterior distributions . We implement and apply Bayesian model comparison methods such as Bayes factor ranking and predictive information criteria based on cross validation techniques under various a priori scenarios . 
",Several models are calibrated to the 75S T6 data set by means of the maximum likelihood method. Classical measures of fit based on information criteria are used to compare and rank models. Bayesian approach is considered to analyze some models under two different a priori scenarios. Bayes factor and predictive information criteria are used to compare Bayesian models.,,,
S0045790615002979,"Foreign fibers in cotton seriously affect the quality of cotton products. Online detection systems of foreign fibers based on machine vision are the efficient tools to minimize the harmful effects of foreign fibers. The optimum feature set with small size and high accuracy can efficiently improve the performance of online detection systems. To find the optimal feature sets a two stage feature selection algorithm combining IG Information Gain approach and BPSO Binary Particle Swarm Optimization is proposed for foreign fiber data. In the first stage IG approach is used to filter noisy features and the BPSO uses the classifier accuracy as a fitness function to select the highly discriminating features in the second stage. The proposed algorithm is tested on foreign fiber dataset. The experimental results show that the proposed algorithm can efficiently find the feature subsets with smaller size and higher accuracy than other algorithms. 
",A two stage selection algorithm combining IG and BPSO for foreign fiber data. IG can filter noisy features and the BPSO can select the highly discriminating features. The experiments show that our method can find the subsets with small size and high accuracy. The optimum subsets are significant for online detection based on machine vision of foreign fibers in cotton.,,,
S0045790615000051," Blood travels throughout the body and thus its flow is modulated by changes in body condition . As a consequence the wrist pulse signal contains important information about the status of the human body . In this work we have employed signal processing techniques to extract important information from these signals . Radial artery pulse pressure signals are acquired at wrist position noninvasively for several subjects for two cases of interest viz . before and after exercise and before and after lunch . Further analysis is performed by fitting a bi modal Gaussian model to the data and extracting spatial features from the fit . The spatial features show statistically significant changes between the groups for both the cases which indicates that they are effective in distinguishing the changes taking place due to exercise or food intake . Recursive cluster elimination based support vector machine classifier is used to classify between the groups . A high classification accuracy of 99.71 is achieved for the exercise case and 99.94 is achieved for the lunch case . This paper demonstrates the utility of certain spatial features in studying wrist pulse signals obtained under various experimental conditions . The ability of the spatial features in distinguishing changing body conditions can be potentially used for various healthcare applications . 
",Wrist pulse signals are analyzed using spatial features obtained from a bi modal Gaussian model. Statistically significant group differences are found for two cases before and after lunch before and after exercise. A recursive cluster elimination based support vector machine classifier is used for classification. High classification accuracy is obtained for both exercise case 99.71 and lunch case 99.94 . There is tangible scope for using these results in various healthcare applications.,,,
S0045790614000391," In cognitive radio networks it is well known that the cooperative spectrum sensing can overcome damaging effects of fading and shadowing . However it also increases the amount of energy consumption which is a critical factor in low powered wireless communications . In this paper based on bi threshold energy detection we maximize the network throughput such that the energy consumption is below a predefined value and also sufficient protection of primary users against interference is guaranteed . Convex optimization analysis is presented to jointly obtain the optimal values of sensing time and detection thresholds . Simulation results show that the proposed method is very flexible such that a good tradeoff between achievable throughput and energy efficiency can be established while it often outperforms the conventional sensing method significantly . 
",A bi threshold cooperative spectrum sensing method for energy efficiency and throughput enhancement is proposed. We present a convex optimization analysis. The method shows a good tradeoff between energy consumption and throughput. Both energy efficiency and throughput can be improved for a wide range of SNRs.,,,
S0045790615004127," The ever growing needs of Big Data applications are demanding challenging capabilities which can not be handled easily by traditional systems and thus more and more organizations are adopting High Performance Computing to improve scalability and efficiency . Moreover Big Data frameworks like Hadoop need to be adapted to leverage the available resources in HPC environments . This situation has caused the emergence of several HPC oriented MapReduce frameworks which benefit from different technologies traditionally oriented to supercomputing such as high performance interconnects or the message passing interface . This work aims to establish a taxonomy of these frameworks together with a thorough evaluation which has been carried out in terms of performance and energy efficiency metrics . Furthermore the adaptability to emerging disks technologies such as solid state drives has been assessed . The results have shown that new frameworks like DataMPI can outperform Hadoop although using IP over InfiniBand also provides significant benefits without code modifications . 
",Analysis and evaluation of several HPC oriented MapReduce frameworks on a cluster. Proposal of a taxonomy to classify these frameworks according to their characteristics. Experimental configuration using several workloads cluster sizes networks and disk technologies. Evaluation in terms of performance and energy efficiency. Results useful to select a suitable MapReduce framework and to identify desirable characteristics for future ones.,,,
S0045790614001840," Virtual fixtures can be used in haptic enabled hydraulic telemanipulators to facilitate certain tasks . Using this concept however the operator may tend to move the master fast due to relying on the virtual fixture . As a result the slave manipulator could start to lag due to latency in the hydraulic actuation control system . This paper describes how to mitigate the position errors between master and slave robots by overlaying an augmentation force on the master that is collinear but opposite of the master instantaneous velocity . The magnitude of this force is proportional to the position error at the slave end effector . Experiments conducted on a teleoperated hydraulic manipulator to perform several live line maintenance tasks show that the augmented scheme exhibits less position error at the slave side better task quality but longer task completion time as compared to the virtual fixture alone . 
",We introduce the concept of augmented virtual fixtures to reduce tracking error. We experimentally validate effectiveness of the proposed scheme. The application targeted in this paper is live transmission line maintenance. The proposed scheme shows a marked improvement over the virtual fixture mode alone. Specifically for presented experiments our scheme reduces position error by 71 .,,,
S0045790615003171," This paper proposes an accelerating correlation based image alignment using CPUs for time critical applications in automatic optical inspection . In order to improve computation efficiency the image pyramid search scheme is combined with the parallel computation . The image pyramid search scheme is employed first to quickly find certain objects in both monochrome and color images with rotation translation and scaling . Sub pixel accuracy is then used to attain the more accurate results at the sub pixel level . In our experimental results rotation accuracy is smaller than 0.218 and the speed is increased between 277 and 20 841 times . According to translation rotation and scaling tests the errors of rotation translation and scaling are 0.2 2.07pixel and 0.55 respectively . These results show that the proposed method is suitable for dealing with correlation based image alignment for time critical applications in automatic optical inspection . 
",We propose an accelerating CPU based correlation based image alignment. The image pyramid search scheme is combined with the parallel computation. Sub pixel accuracy is used to attain the more accurate image alignment. It can align the accurate pose of the template image within the inspected image.,,,
S0045790615002281," This paper proposes an unambiguous correlation function applicable to generic sine phased binary offset carrier signal tracking . In the proposed scheme first we view the BOC sub carrier pulse as a sum of multiple rectangular pulses . Then we obtain partial correlation functions by correlating the multiple rectangular pulses with the received signal and subsequently construct two symmetric correlation functions by combining the partial correlation functions in a specialized way . Finally we generate an unambiguous correlation function by combining the two symmetric correlation functions . The proposed correlation function has a sharper main peak and consequently provides a better tracking performance than those of the conventional correlation functions in terms of the tracking error standard deviation . 
",Ambiguity in tracking is the major problem in binary offset carrier signal tracking. The ambiguity in tracking is caused by the BOC correlation side peaks. A correlation function with no side peaks is proposed for generic sine phased BOC. The correlation function removes the ambiguity and improves the tracking performance.,,,
S0045790614001888," The main constraint of wireless sensor networks is the limited and generally irreplaceable power source of the sensor nodes . Therefore designing energy saving routing algorithm is one of the most focused research issues . In this paper we propose an energy aware routing algorithm for cluster based WSNs . The algorithm is based on a clever strategy of cluster head selection residual energy of the CHs and the intra cluster distance for cluster formation . To facilitate data routing a directed virtual backbone of CHs is constructed which is rooted at the sink . The proposed algorithm is also shown to balance energy consumption of the CHs during data routing process . We prove that the algorithm achieves constant message and linear time complexity . We test the proposed algorithm extensively . The experimental results show that the algorithm outperforms other existing algorithms in terms of network lifetime energy consumption and other parameters . 
",A new energy aware routing algorithm has been proposed for cluster based wireless sensor networks. It achieves O 1 message complexity per sensor node and O n time complexity for a WSN having n sensor nodes. It efficiently forms the directed virtual backbone of cluster heads to facilitate data routing to the sink. It is successful in balancing the relaying load among the sensor nodes with respect to their residual energy.,,,
S0045790615001263," Multiple Input Multiple Output system plays a major role in the fourth generation wireless systems to provide high data rates . In this paper a blind channel estimation approach has been proposed for finding the channel length based on the signals received from the MIMO transceiver . The resultant MIMO channel length information is utilized for estimation of the channel impulse response of the system . The estimation is used for adaptation of the equalizer weights based on the proposed Constant Modulus Algorithm which has reduced the fading and multipath propagation resulting from Inter Symbol Interference . The performance of the proposed system has been analyzed in terms of Mean Square Error and Symbol Error Rate for various Signal to Noise Ratio . 
",The channel length is blindly estimated and the channel impulse response is determined. The cost function specifies the signal noise level. Equalizer updates its tap weights based on the cost function of the proposed CMA. With 4 taps the proposed CMA achieves a faster convergence rate. The proposed CMA exhibit better MSE and reduced error rate than other CMA techniques.,,,
S0045790616300155," This study analyzes the vibration signals of fault induction motors for establishing an intelligent motor fault diagnosis system by using an extension neural network . Extension theory and a neural network are combined to construct the motor fault diagnosis system which identifies the most likely fault types in motors . First the vibration signal spectra of the 10 most common fault types are measured and organized into individual motor fault models . Subsequently according to the motor fault data representative characteristic frequency spectra are identified and the correlation between the motor fault types and their corresponding characteristic frequency spectra are established to develop the motor fault diagnosis system . Finally the test results confirm that the proposed motor fault diagnosis system is fast requires less training data and demonstrates first rate identification capability . 
",This study analyzes the vibration signals of the 10 most common motor fault types. Extension theory and NN are combined to build the motor fault diagnosis system. The proposed motor fault diagnosis system is fast requires less training data. The proposed fault diagnosis system exhibits first rate identification capability.,,,
S0045790615001123," A Mobile Ad hoc Network is an infrastructure less collection of nodes that are powered by portable batteries . Consumption of energy is the major constraint in a wireless network . This paper presents a new algorithm called Energy Aware Span Routing Protocol that uses energy saving approaches such as Span and the Adaptive Fidelity Energy Conservation Algorithm . Energy consumption is further optimized by using a hardware circuit called the Remote Activated Switch to wake up sleeping nodes . These energy saving approaches are well established in reactive protocols . However there are certain issues to be addressed when using EASRP in a hybrid protocol especially a proactive protocol . Simulation results for the EASRP protocol show an increase in energy efficiency of 12.2 and 17.45 compared with EAZRP and ZRP respectively . The EASRP protocol also proves to be effective in by producing a better packet delivery ratio for low and high density networks as measured by the NS 2 simulation tool . 
",A new Energy Aware Span Routing Protocol EASRP for wireless ad hoc networks is proposed. Proposed protocol can minimize utilization of energy source by combining energy saving approaches Span and AFECA. It uses the Remote Activated Switch and wakes up the sleeping nodes during inactive time for reduce latency problem. The performance parameter of proposed protocol is tested under Network Simulator 2.,,,
S0045790615000646," This study proposes a novel optical fiber sensor network using three dimensional wavelength time spatial optical code division multiple access scheme . The proposed 3 D modified quadratic congruence M matrix coding scheme overcome the restriction of single pulse per row inherent in traditional three dimensional codes and maintain a high signal to noise ratio even when source power is low . The 3 D system is implemented using optical switches fiber Bragg gratings optical splitter and optical combiner . The noises of phase induced intensity noise and multiple access interference in the decoding mechanism can be suppressed by using a spectral spreading scheme and balance detection in the receivers . By constructing 3 D codes using bipolar pseudorandom codes rather than unipolar codes provides a significant increase in the maximum permissible number of active sensor nodes . 
",The OFS network using 3 D wavelength time spatial optical CDMA scheme. OFS have advantages of high bandwidth and resist electromagnetic interference. The 3 D codes by extending modified quadratic congruence and M matrix codes. Overcoming the restriction of single pulse per row in traditional technology. The noises of PIIN and MAI can be suppressed by balance detection scheme.,,,
S0045790615003584,"We consider turbo coded multi carrier double space time transmit diversity DSTTD system that employs orthogonal frequency division multiplexing OFDM for the transmission of acoustic signals in underwater communication where acoustic interference and ambient noise are the major channel deficiencies. DSTTD employs two space time block codes at the transmitter. At each receiver we implement space time Block Nulling detection technique to increase throughput. We consider the iterative decoding algorithm at the receiver to alleviate the effects of ambient noise and acoustic interference. Further we implement multi carrier modulation technique to mitigate the effects of multipath propagation. We investigate the effects of eleven tap delay pertaining to shallow water channel model for the DSTTD OFDM system. Our simulation results reveal that our considered system with Block Nulling technique provides better bit error rates for lower signal to noise ratio when compared to a minimum mean square error detector based system. Further it achieves higher throughput with fewer computations at the receivers. 
",We consider DSTTD OFDM system for underwater communication. Non linear multi user detection based on Block Nulling increases throughput. Transmitter uses turbo code as an encoder and the receiver decodes iteratively. Multi carrier modulation mitigates the effects of multipath propagation. This system provides better bit error rates performance with lower signal to noise ratio.,,,
S0045790615004279,"Embedding secret information into a cover media and extracting the information and the image without any distortion is known as reversible watermarking RW . This paper analyzes the performance of hybrid local prediction error based RW using difference expansion DE . The cover medical image is split into non overlapping blocks. The border pixels in each block are predicted using median edge detection MED prediction. The other pixels in the block are predicted using least square prediction and the prediction error is expanded. The secret data are embedded into the cover medical image corresponding to the prediction error using the DE method. The predictor is also embedded into the cover medical image to recover the data at detection without any additional information. The simulation results show that this method achieves better watermarked image quality and high embedding capacity when compared to other classical prediction methods Median MED Rhombus and Gradient Adjusted Prediction. 
",A hybrid local prediction error based difference expansion reversible watermarking algorithm for hiding data into medical images is presented. The performance of hybrid local algorithm with median median edge gradient adjusted and local prediction algorithms is compared. The hybrid local prediction algorithm has the highest frequency of zero prediction error. The PSNR and embedding capacity are improved in the hybrid local algorithm.,,,
S0045790615003560,"To improve the precision of low cost vehicle mounted global position system GPS this paper presents the multi source information fusion algorithm of vehicle navigation which is based on the interacting multiple model IMM . Considering vehicle kinematics and dynamic characteristics as well as its braking capacity in extreme accelerating situations this study establishes the clothoid model which shows longitudinal and lateral constant speed motion and the Adaptive Current Statistics ACS model which shows the acceleration of a vehicle. Through the parametric estimation of interactivity filtering and updating of probability the vehicle trajectory is predicted within a period of time and high precision dead reckoning is therefore achieved. Comparative analysis shows that the above algorithm can improve the precision of vehicle mounted GPS inertial navigation system. 
",Clothoid model which shows longitudinal and lateral constant speed motion and the Adaptive Current Statistics model which shows the accelerating of the vehicle. Enhancing the positioning accuracy by the ACS model and clothoid model. IMM is better to improve the position accuracy within the limited cost.,,,
S0045790615003602,"This paper presents multi quantized local binary patterns for facial gender classification. For encoding the gray level difference GLD between a reference pixel and its neighbors local binary pattern employs a binary quantization which retains the sign of GLD but discards the magnitude information. To improve the discrimination capability the proposed method utilizes both the sign and magnitude components by performing multi level vector quantization of GLD. Each quantized level is then separately encoded to generate multiple local binary patterns. The proposed method is evaluated on four publicly available datasets FERET PAL CASIA and FEI through extensive experiments. Comparison of performance with various existing methods clearly demonstrated that the proposed method has advantages such as higher discrimination power improved noise robustness and better generalization capability. 
",Multi level quantization scheme is proposed to enhance discrimination power of LBP. The proposed method utilizes both the sign and magnitude components from GLD. The results of the experiments validate the robustness of the proposed method. Thorough evaluation of four popular features LBP CLBP ELBP and LTP is presented.,,,
S0045790615000208," The main goal of this paper is to propose and implement an experimental fully automatic face recognition system which will be used to annotate photographs during insertion into a database . Its main strength is to successfully process photos of a great number of different individuals taken in a totally uncontrolled environment . The system is available for research purposes for free . It uses our previously proposed SIFT based Kepenekci approach for the face recognition because it outperforms a number of efficient face recognition approaches on three large standard corpora . The next goal is proposing a new corpus creation algorithm that extracts the faces from the database and creates a facial corpus . We show that this algorithm is beneficial in a preprocessing step of our system in order to create good quality face models . We further compare the performance of our SIFT based Kepenekci approach with the original Kepenekci method on the created corpus . This comparison proves that our approach significantly outperforms the original one . The last goal is to propose two novel supervised confidence measure methods based on a posterior class probability and a multi layer perceptron to identify incorrectly recognized faces . These faces are then removed from the recognition results . We experimentally validated that the proposed confidence measures are very efficient and thus suitable for our task . 
",We proposed and implemented a new face corpus creation algorithm. We created a new facial corpus from the data of the Czech News Agency. We evaluated a novel face recognition method the SIFT based Kepenekci approach. We proposed and evaluated two novel confidence measure techniques. We proposed implemented and evaluated the fully automatic face recognition system.,,,
S0045782516300974," In this work we intend to address the limitation of our earlier particle method namely the Moving Particle Pressure Mesh method in handling arbitrary shaped flow boundaries . The application of the Cartesian pressure mesh system adopted in our original MPPM method which serves as the main key in recovering the divergence free velocity condition for incompressible flow in the framework of particle method is rather limited to rectangular flow domain . Here the hybrid unstructured pressure mesh is adopted to remove the geometrical constraint of our earlier MPPM method . Coupled with the moving particle strategy in the Moving Particle Semi implicit method the new method is named as the Unstructured Moving Particle Pressure Mesh method in the current work . A consistent Laplacian model namely the Consistent Particle Method recently reported in the open literature is incorporated as well in the framework of UMPPM for discretizing the viscous term on the scattered particle cloud while its implicit form is solved in the current work for overall robustness . Finally we shall verify our UMPPM method with a series of benchmark solutions available from the literatures including those obtained from the commercial code . It is appealing to find that the numerical solutions of UMPPM compare well with the benchmark solutions . In some cases the accuracy of our UMPPM is better than that of the existing particle method such as Smoothed Particle Hydrodynamics . 
",Extension of MPPM particle mesh method to handle complex flow domain. Extension of MPPM to handle non isothermal buoyant flow. The viscous term in MPPM is treated by an implicit and consistent Laplacian model. No ghost particles are used to model no slip wall boundary. Accuracy enhancement in MPPM can be achieved via particle adaptation.,,,
S0045782516300950," This paper addresses the use of isogeometric analysis to solve solid mechanics problems involving nearly incompressible materials . The present work is focused on extension of two field mixed variational formulations in both small and large strains to isogeometric analysis . Inf sup stable displacement pressure combinations for mixed formulations are developed based on the subdivision property of NURBS . Stability and convergence properties of the proposed displacement pressure combinations are illustrated by computing numerical inf sup constants and error norms . The performance of the proposed formulations is assessed by studying several benchmark examples involving nearly incompressible and incompressible elastic and elasto plastic materials in both small and large strain regime . 
",Inf sup stable displacement pressure combinations are developed based on subdivision properties of NURBS. The inf sup stability is proven numerically by evaluating inf sup constants. Optimal convergence rates are obtained for linear problems. Stable and accurate solutions can be obtained with fewer load steps.,,,
S0045790615003201," Interactive image segmentation aims to extract user specified regions from the background . In this paper an efficient two stage region merging based method is proposed for interactive image segmentation . An image is first over segmented into many super pixels using a bottom up method . The color histogram is exploited to represent each super pixel and the Bhattacharyya coefficient is computed to measure the similarity of two adjacent super pixels . Then some strokes denoting the desired object and background are manually labeled by the user on the over segmented image . With the labeled seed super pixels a merging strategy is designed to realize adaptive region merging . The whole merging process is divided into two stages which are repeatedly executed until no new merging occurs . In the first stage some unlabelled super pixels are merged into the labeled foreground or background super pixels if the labeled ones are their nearest neighbors . In the second stage any two unlabelled super pixels are merged together if one super pixel is the nearest neighbor of the other . Extensive experiments are conducted to evaluate the performance of the proposed method . The results show that the proposed method can extract the object reliably and quickly from the background . 
",Two stage region merging interactive image segmentation. User stroke labeled seed super pixels. Adaptive region merging.,,,
S0045790616300398," Active contours or snakes have a wide range of applications in object segmentation which use an energy minimizing spline to extract objects borders. Classical snakes have several drawbacks such as the initial contour sensitivity and convergence ability to local minima. Many approaches based on active contours are put forward to addressing these problems. However these approaches have limitation that they all depend too much on the amplitude of edge gradient and abandon directional information. This can lead to poor convergence toward the object boundary in the presence of strong background edges and cluttered noises. To deal with these issues we first propose a novel external force called adaptive edge preserving generalized gradient vector flow based on component based normalization CN AEGGVF which can adaptively adjust the process of diffusion according to the local characteristics of an image and preserve weak edges by adding the gradient information of an image. The experimental results show that the new model provides much better results than other approaches in terms of noise robustness weak edge preserving and convergence. Secondly an improved multi step decision model based on CN AEGGVF is presented which added new effective weighting function to attenuate the magnitudes of unwanted edges and adopted narrow band method to reduce time complexity. The novel method is analyzed visually and qualitatively on nature image dataset. Experimental results and comparisons against other methods show that the proposed method has better segmentation accuracy than other comparative approaches. 
",A multi step decision model based on adaptive edge preserving generalized gradient vector flow using component based normalization for snake model is proposed. The proposed algorithm presents a novel external force which provides better results than other approaches in terms of noise robustness weak edge preserving and convergence. An improved multi step decision model based on this novel external force is adopted which adds new effective weighting function to attenuate the magnitudes of unwanted edges and adopts narrow band method to reduce time complexity. Experimental results and comparisons against other methods show that the proposed method has better segmentation accuracy than other comparative approaches.,,,
S0045782516301049," In this paper we present a multi fidelity extension of non intrusive polynomial chaos based on regression for uncertainty quantification purposes . The proposed method uses the principle of a global correction function from a previous similar method that uses spectral projection to estimate the coefficients . Due to its usage of regression to estimate the coefficients the present method offers high flexibility in the sampling and generation of the polynomial basis . The method takes advantage of a nested sampling plan to create the samples for the low fidelity and correction expansions where the high fidelity samples are a subset of the LF ones . To build the polynomial basis a total order or hyperbolic truncation strategy is used with a highly flexible combination of the LF and correction polynomial expansions . The method is demonstrated on some artificial test problems and aerodynamic problems of the Euler flow around an airfoil and common three dimensional research models . In order to derive the strategies for successful MF approximation the effect of the correlation and the errors between the LF and HF functions is also studied . The results show that high correlation and moderately low errors are important to improve the MF approximation s accuracy . On a common research model problem the MF approach with partially converged simulations as the LF samples can successfully reduce the computational cost to about 40 for similar accuracy compared to an approach using a single HF expansion . 
",We present a multi fidelity MF extension of non intrusive polynomial chaos based on regression for uncertainty quantification purposes. The proposed method uses the principle of a global correction function nested sampling plan and uses regression to estimate the coefficients. The results show that high correlation and moderately low errors between the low and high fidelity functions are important to improve the MF approximation s accuracy. On aerodynamic problem a computational cost saving of about 60 can be obtained by using the present method.,,,
S0045790616300106,"In this paper an improved particle swarm optimization guidance IPSOG is proposed. The particle swarm optimization guidance PSOG is presented to solve nonlinear and dynamic missile guidance problems. However the miss distance MD tends to be large. The objective function of the relative distance in the PSOG leads the missile to the current position of a target. Therefore the PSOG is similar to pursuit guidance. In the IPSOG a new objective function for the PSOG is introduced to improve the guidance performance. The line of sight LOS rate is taken as the objective function. The fitness function is then evaluated according to the defined objective function. Numerical simulation results show that the guidance performance of the IPSOG is better than the PSOG. 
",The proposed is an improved PSO guidance law which is called IPSOG for a missile. The principle of IPSOG is based on PSO algorithm and parallel navigation. Both Pbest and Gbest that are evaluated by LOS rate should close to zero in IPSOG. Compared the IPSOG with the PSOG the IPSOG has better guidance performance.,,,
S0045790615000063," Wavelet packet acoustic features are found to be very promising in unvoiced phoneme classification task but they are less effective to capture periodic information from voiced speech . This motivated us to develop a wavelet packet based feature extraction technique that signifies both the periodic and aperiodic information . This method is based on parallel distributed processing technique inspired by the human speech perception process . This front end feature processing technique employs Equivalent Rectangular Bandwidth filter like wavelet speech feature extraction method called Wavelet ERB Sub band based Periodicity and Aperiodicity Decomposition . Winer filter is used at front end to minimize the noise for further processing . The speech signal is filtered by 24 band ERB like wavelet filter banks and then the output of each sub band is processed through comb filter . Each comb filter is designed individually for each sub band to decompose the signal into periodic and aperiodic features . Thus it carries the periodic information without losing certain important information like formant transition incorporated in aperiodic features . Hindi phoneme classification experiments with a standard HMM recognizer under both clean training and multi training condition is conducted . This technique shows significant improvement in voiced phoneme class without affecting the performance of unvoiced phoneme class . 
",24 subband WP decomposition according to the auditory ERB scale. Proposed wavelet subband specific periodic and aperiodic decomposition. Wiener filter is used at frontend for noise minimization. Hindi phoneme classification task has been carried out. Proposed technique outperforms others classify voiced phonemes.,,,
S0045790614002596," A wireless ad hoc network consists of a set of wireless devices . The wireless devices are capable of communicating with each other without the assistance of base stations . Space Division Multiple Access is a new technology designed to optimize the performance of current and future mobile communication systems . In this paper an SDMA based MAC protocol for wireless ad hoc networks with smart antennas is proposed . The proposed protocol exploits the SDMA system to allow reception of more than one packet from spatially separated transmitters . Using SDMA technology provides collision free access to the communication medium based on the location of a node . The proposed protocol solves the hidden terminal problem the exposed terminal problem and the deafness problem . Simulation results demonstrate the effectiveness of the proposed S MAC in improving throughput and increasing spatial channel reuse . 
",An SDMA based MAC protocol S MAC for wireless ad hoc networks is proposed. Exploiting the creation of spatial channels to enhance network capacity. Providing collision free access to the communication medium based on the location of a node. Improving the performance of throughput and spatial channel reuse.,,,
S0045790615002700," In general in order for individuals to take part in a lottery they must purchase physical lottery tickets from a store . However due to the popularity and portability of smart phones this paper proposes a lottery entry purchase protocol for joint multi participants in a mobile environment . This method integrates cryptology including elliptic curve cryptography and public key infrastructure enabling users to safely and fairly join a lottery via a mobile device . The lottery organization involves an untraceable tamperproof decryptor to generate the winning numbers and the generation of those winning numbers is fair and publicly verifiable . All participants share an equal probability of winning the prize . Subsequently a comparison table shows that the proposed protocol can withstand attacks and efficiently satisfy the known requirements in a mobile environment . In addition this study also ensures public verification and mutual authentication . 
",This study proposes a fair and secure protocol allowing users to safely and fairly purchase lottery tickets by smart phone. The proposed lottery protocol can satisfy the fairness accuracy and public verification requests in a mobile environment. The proposed protocol can prevent the various malicious attacks discussed above with low computation costs. The proposed protocol can achieve the multi participants in a mobile environment.,,,
S0045790615000105," Detecting covert information in images by means of steganalysis techniques has become increasingly necessary due to the amount of data being transmitted mainly through the Internet . However these techniques are computationally expensive and not much attention has been paid to reduce their cost by means of available parallel computational platforms . This article presents two computational models for the Subtractive Pixel Adjacency Model which has shown the best detection rates among several assessed steganalysis techniques . A hardware architecture tailored for reconfigurable fabrics is presented achieving high performance and fulfilling hard real time constraints . On the other hand a parallel computational model for the CUDA architecture is also proposed . This model presents high performance during the first stage but it faces a bottleneck during the second stage of the SPAM process . Both computational models are analyzed in detail in terms of their algorithmic structure and performance results . To the best of Authors knowledge these are the first design proposals to accelerate the SPAM model calculation . 
",Two computational parallel models are developed using FPGA and GPU platforms. To the best of our knowledge these are the first acceleration schemas for SPAM. An architectural and performance analysis of both computational models is presented. The FPGA architecture accelerates SPAM making an optimal use of hardware resources. The GPU model accelerates SPAM s 1st stage facing a bottleneck on its 2nd stage.,,,
S0045790615003183," Improving image resolution by refining hardware is usually expensive and or time consuming . A critical challenge is to optimally balance the trade off among image resolution Signal to Noise Ratio and acquisition time . Super resolution an off line approach for improving image resolution is free from these trade offs . Numerous methodologies such as interpolation frequency domain regularization and learning based approaches have been developed for SR of natural images . In this paper we provide a survey of the existing SR techniques . Various approaches for obtaining a high resolution image from a single and or multiple low resolution images are discussed . We also compare the performance of various SR methods in terms of Peak SNR and Structural Similarity index between the super resolved image and the ground truth image . For each method the computational time is also reported . 
",Comprehensive survey of super resolution algorithms. Performance comparison among different super resolution algorithms. Super resolution techniques applied to natural images.,,,
S0045790616300192,"To solve the problems of long time delay and low reliability in existing systems a Two level Load balance Monitoring Strategy TLLBMS is proposed in this paper. In monitoring terminal level based on customizable System on Chip A2F500 monitoring terminals implement parallel Principal Component Analysis and Independent Component Analysis PCA ICA algorithm to fulfil quickly preprocessing and feature extraction of electrocardiogram ECG signals and give a pre classification and alarming mechanism. Depending on the results of pre classification monitoring terminals transmit only eigenvectors to monitoring center at a Dynamically Variable Time Interval DVTI . This will reduce the quantity of data transmission dramatically which means it will lead to high robustness of communication and short time delay. In monitoring center level monitoring server processes eigenvectors directly and realizes quick classification diagnosis and rescuing. To ensure the correctness of communication and protect data from being falsified Message Digest Algorithm MD5 is realized to verify the integrity of data. 
",A Two level Load balance Monitoring Strategy TLLBMS is proposed. Parallel PCA ICA Algorithm is realized to fulfill quick preprocessing and feature extraction. Only eigenvectors are transmitted at a Dynamically Variable Time Interval DVTI . MD5 hash algorithm is realized to ensure the correctness of communication.,,,
S0045790615001032," In this paper we investigate the achievable bit error rate performance of a transmitter preprocessing aided cooperative downlink multi carrier code division multiple access system by employing three cooperation strategies . In a multi user cooperative DL communication multi user interference at the relays which prevents the achievement of relay diversity is suppressed with the aid of the TP operated at the base station . In addition inter relay interference at the destination mobile station is mitigated by the TP employed at the relays . Furthermore the channel impulse responses required for formulating the TP at the BS is based on the quantized CIRs feedback in contrast to the perfect CIRs assumption at the relays . Our study shows that the resultant BER performance of the quantized CIRs feedback based TP at the BS coupled with the PCIR based TP at the relays remains close to that attained with the PCIRs knowledge at the BS and relays . 
",The achievable BER when a DMS is supported by a cluster of relays PCIRs and VQ CIRs . The attainable BER when a DMS is assisted by one relay PCIRs and VQ CIRs . The achievable BER when one relay assists all the K DMSs PCIRs and VQ CIRs .,,,
S0045790615001111," Anycast is an important way of communication for Mobile Ad hoc Networks in terms of resources robustness and efficiency for replicated service applications . Most of the anycast routing protocols select unstable and congested intermediate nodes thereby causing frequent path failures and packet losses . We propose a mobility and quality of service aware anycast routing scheme in MANETs that employs three models node movement stability channel congestion and link route expiry time . These models coupled with Dynamic Source Routing protocol are used in the route discovery process to select nearest k servers . A server among k servers is selected based on less congestion route expiry time number of hops and better stability . The simulation results indicate that proposed MQAR demonstrates reduction in control overheads path delays and improved packet delivery ratio compared to existing methods such as flooding DSR and load balanced service discovery . 
",Anycast routing is important in terms of resources robustness and efficiency. The proposed model is based on node stability congestion and link route expiry time. Performs better in terms of control overheads PDR and end to end delay.,,,
S0045790615004334," Graphical abstract The system bifurcation diagrams for x 1 are shown for harmonic excitation force 1.14 10 3 0.1 1.0 and 2.0 respectively . There are two intervals of nonlinear motion at 0 0.006 and 1.248 2.0 respectively when harmonic excitation force increases to 2.0 . Image graphical abstract 
",Highlight Analyze the nonlinear cubic stiffness coefficient ratio effect for ultrasonic cutting system. Ultrasonic cutting system is designed for stable and synchronized vibration when the excitation force is controlled at 2.0 with the interval of nonlinear cubic stiffness coefficient ratio 0.006 1.248. For small values of the non periodic motion is more obvious meaning that the larger hardening cubic stiffness coefficient causes more severely unsteady behavior. For the variation of dynamic behavior ultrasonic cutting system includes periodic quasi periodic and chaotic motions under specific operating parameters.,,,
S0045790615001056," This paper addresses the problem of identifying meaningful patterns and trends in data via clustering . The clustering framework that we propose is based on the generalized Dirichlet distribution which is widely accepted as a flexible modeling approach and a hierarchical Dirichlet process mixture prior . A main advantage of the adopted hierarchical Dirichlet process is that it provides a principled elegant nonparametric Bayesian approach to model selection by supposing that the number of mixture components can go to infinity . In addition to capturing the structure of the data the combination of hierarchical Dirichlet process and generalized Dirichlet distribution is shown to offer a natural efficient solution to the feature selection problem when dealing with high dimensional data . We develop two variational learning approaches for learning the parameters of the proposed model . The batch algorithm examines the entire data set at once while the incremental one learns the model one step at a time . The utility of the proposed approach is demonstrated on real applications namely face detection facial expression recognition human gesture recognition and off line writer identification . The obtained results show clearly the merits of our statistical framework . 
",A statistical framework based on hierarchical Dirichlet processes and generalized Dirichlet distribution is developed. The framework simultaneously performs model parameters estimations as well as model complexity determination. The learning of the model is done via variational Bayes inference. The efficiency of the proposed algorithm is validated via challenging applications.,,,
S0045790614000706," Thresholding is a popular image segmentation method that converts a grayscale image into a binary image . In this paper we propose a cloud model based framework for range constrained thresholding with uncertainty and improve four traditional methods . The method involves four major steps including representing the image using cloud model estimating the automatic threshold for gray level ranges of object and background implementing image transformation to focus on mid region of the image and determining the binary threshold within the constrained gray level range . Cloud model can effectively represent various visual properties of the image such as intensity based class uncertainty intra class homogeneity and between class contrast . The approach is validated both quantitatively and qualitatively . Compared with the traditional state of art algorithms on a variety of synthetic and real images with or without noisy as well as laser cladding images the experimental results suggest that the presented method is efficient and effective . 
",A cloud model based framework for range constrained thresholding with uncertainty. Improving four traditional methods under the new framework. Representing the image using cloud model. Implementing image transformation to focus on mid region of the image. Cloud model based framework is efficient and effective.,,,
S0045790615002876," Current Critical Infrastructures need intelligent automatic active reaction mechanisms to protect their critical processes against cyber attacks or system anomalies and avoid the disruptive consequences of cascading failures between interdependent and interconnected systems . In this paper we study the Intrusion Detection Prevention and Response Systems that can offer this type of protection mechanisms their constituting elements and their applicability to critical contexts . We design a methodological framework determining the essential elements present in the IDPRS while evaluating each of their sub components in terms of adequacy for critical contexts . We review the different types of active and passive countermeasures available categorizing them and assessing whether or not they are suitable for Critical Infrastructure Protection . Through our study we look at different reaction systems and learn from them how to better create IDPRS solutions for CIP . 
",We study IDPRS solutions for CIP considering their components and constraints. We design a methodological framework for IDPRS solutions within critical scenarios. ICS need automated intelligent solutions for early detection and protection. Current IDPRS solutions for CIP lack automatic active reaction mechanisms. We give recommendations for adaptation or development of IDPRS solutions for CIP.,,,
S0045790615000658," Backbone nodes are effective for routing in wireless networks because they reduce the energy consumption in sensor nodes . Packet delivery only occurs through the backbone nodes which depletes the energy in the backbone drastically . Several backbone construction algorithms including energy aware virtual backbone tree virtual backbone tree algorithm for minimal energy consumption and multihop cluster based stable backbone tree fail to form a complete backbone when converting important nodes such as a cut vertex tree node to a non backbone node . Thus we propose a fault tolerant virtual backbone tree algorithm that addresses all of these conflicts and we give theoretical derivations of the bounds for the probability that a sensor node can connect with the backbone . Furthermore randomized FTVBT improves FTVBT by redistributing non tree nodes randomly among all the eligible tree nodes based on their fitness values thereby decreasing the rapid depletion of energy in a particular node and increasing the network lifetime . We performed simulations in NS2 and analyzed the experimental results . 
",Need of energy efficient backbone routing for WSNs. FTVBT virtual backbone tree construction algorithm proposed to minimize energy consumption. Identifies hotspots distributes dependents across tree nodes and the distribution is also bounded. Randomized FTVBT elects tree nodes based on a weight function and increases network lifetime.,,,
S0045782516300834," This work proposes a stochastic shape optimization method for continuous structures using the level set method . Such a method aims to minimize the expected compliance and its variance as measures of the structural robustness . The behavior of continuous structures is modeled by linear elasticity equations with uncertain loading and material . This uncertainty can be modeled using random variables with different probability distributions as well as random fields . The proper problem formulation is ensured by the proof of the existence colorrev of solution under certain geometrical constraints on the set of admissible shapes . The proposed method addresses the stochastic linear elasticity problem in its weak form obtaining the explicit expressions for the continuous shape derivatives . Some numerical examples are presented to show the effectiveness of the proposed approach . 
",Robust shape optimization considering uncertainty in loads and material is proposed. Stochastic shape derivatives of continuous structures are obtained. The existence of a solution for the robust shape optimization problem is proved. Different probability distributions and random fields are considered in experiments.,,,
S0045790615000634," The biometrics the password and the storage device are the elements of the three factor authentication . In 2013 Yeh et al . proposed a three factor user authentication scheme based on elliptic curve cryptography . However we find that it has weaknesses including useless user identity ambiguous process no session key and no mutual authentication . Also it can not resist the user forgery attack and the server spoofing attack . Moreover Khan et al . propose a fingerprint based remote authentication scheme with mobile devices . Unfortunately it can not withstand the user impersonation attack and the De synchronization attack . Furthermore the user s identity can not be anonymous either . To overcome the disadvantages we propose a new three factor remote authentication scheme and give a formal proof with strong forward security . It could provide the user s privacy and is secure . Compared to some recent three factor authentication schemes our scheme is secure and practical . 
",We point out that Yeh et al. s scheme is not secure because it has several disadvantages in security. We point out that Khan et al. s scheme is not secure with some weaknesses. We present a new three factor scheme based on ECC. We prove our scheme secure with a formal proof and analysis. By comparing with some latest schemes our scheme is more practical for application due to the security and efficiency.,,,
S0045790615003018," A honeypot system can be deployed to decoy and record malicious intrusions over the Internet . However events logged by a honeypot can rapidly accumulate an enormous amount of data which an administrator will be unable to handle . The proposed system combines episode mining and pruning and allows an administrator to identify suspected intrusions and thus focus his energy on addressing them instead of reading enormous amounts of raw data . An attack episode is composed of a series of events and represents an Internet intrusion as a series of relevant events occurring to a victim host in a specific sequence . Due to the variety of internet attacks this paper focuses on discovering attack episodes for the Server Message Block protocol which provides Microsoft Windows Network services . Experiments show that the proposed approach can locate suspicious episodes that are very likely novel attacks from an immense amount of logged data . 
",All on line attempts to access a honeypot can be regarded as attacks. A honeypot s logs can be very large which an administrator will be unable to handle. We apply serial episode mining and two round pruning to identify suspected attack. The experiments conducted in this paper focus on port 445 for the SMB protocol.,,,
S0045782516301372," This paper presents a novel topology optimization method for designing structures with multiphase embedded components under minimum distance constraints in the level set framework . By using the level set representation for both the component layout and the host structure topology the shapes of the components can be easily preserved and optimal structural topologies with smooth boundary material interface can be obtained . With the purpose of preventing the components moving too close to each other a minimum distance constraint based on virtual boundary offset is proposed . Different from existing distance detection methods relying on explicit topology representation the proposed constraint is imposed as a unified integral form for which the design sensitivity can be readily obtained . Moreover this constraint is effective for detecting the distance between any complex shaped components . Several numerical examples are presented to demonstrate the validity and effectiveness of the proposed method . 
",We present topology optimization with distance control of embedded components. Both structural topology and multi component layout are represented by level set. Specified geometries of component can be well preserved thanks to level set model. A unified level set based distance constraint for arbitrary components is proposed. Explicit integral form of the distance constraint facilitates sensitivity analysis.,,,
S0045790615001470," With the rapid advancement in very large scale integration technology it is the utmost necessity to achieve a reliable design with low power consumption . The Quantum dot Cellular Automata can be such an architecture at nano scale and thus emerges as a viable alternative for the current CMOS VLSI . This work targets design of logic module in QCA . It reports a modular design methodology to build the fault tolerant 1 multiplexer with optimized wire crossings delay and power consumption . A 2 1 QCA multiplexer is proposed as the basic logic module that in turn is utilized to synthesize 4 1 and 8 1 multiplexers . It shows significant achievement in terms of clock speed wire crossing fault tolerance and power consumption over the existing designs . The effectiveness of proposed multiplexer is further established through synthesis of configurable logic block for field programmable gate arrays . 
",A modular design methodology around multiplexer is designed. This work make a trade off between modular design and its reliability associated with fault tolerance and energy consumptions. Cascading lower order multiplexer to synthesize higher order multiplexer mitigating wire crossing and delay. Application of proposed design in CLB is also done.,,,
S0045790615001019," A major optimization problem in the synthesis of sequential circuits is State Assignment or State Encoding in Finite State Machines . The state assignment of an FSM determines the complexity of its combinational circuit and thus area delay testability and power dissipation . Since optimal state assignment is an NP hard problem and existing deterministic algorithms produce solutions far from best known solutions we resort to the use of non deterministic iterative optimization heuristics . This paper proposes the use of cuckoo search optimization algorithm for solving the state assignment problem of FSMs with the aim of minimizing area of the resulting sequential circuit . Results obtained from the CSO algorithm are compared with those obtained from binary particle swarm optimization algorithm genetic algorithm and the well known deterministic methods of NOVA and JEDI . The results indicate that CSO outperforms deterministic methods as well as other non deterministic heuristic optimization methods . 
",A major optimization problem in synthesis of sequential circuits is State Assignment SA . Cuckoo search optimization CSO algorithm is employed for solving the SA problem. CSO targets area minimization of synthesized sequential circuits. CSO results outperform deterministic and non deterministic heuristic optimization methods.,,,
S0045790615001342," Genetic programming is an evolutionary method that allows computers to solve problems automatically . However the computational power required for the evaluation of billions of programs imposes a serious limitation on the problem size . This work focuses on accelerating GP to support the synthesis of large problems . This is done by completely exploiting the highly parallel environment of graphics processing units . Here we propose a new quantum inspired linear GP approach that implements all the GP steps in the GPU and provides the following significant performance improvements in the GP steps elimination of the overhead of copying the fitness results from the GPU to the CPU and incorporation of a new selection mechanism to recognize the programs with the best evaluations . The proposed approach outperforms the previous approach for large scale synthetic and real world problems . Further it provides a remarkable speedup over the CPU execution . 
",A new quantum inspired linear genetic programming system that runs on the GPU. Allows the synthesis of solutions for large scale real world problems. Eliminates the overhead of copying the fitness results from the GPU to the CPU. Proposes a new selection mechanism to recognize the programs with best evaluations. Improves performance of the GP execution through exploiting the GPU environment.,,,
S0045790614003115," Supervisory control and data acquisition systems currently use the polling technique for monitoring electric utility networks . Unfortunately conventional SCADA systems do not suit the needs of smart grids in terms of the required data rate . Polling based wireless networks can extend the capabilities of SCADA systems as they provide low cost transceivers and bounded packet delay . However the harsh environment of power stations negatively impacts the performance of wireless links . This paper introduces a field measurement based study that focuses on the effect of power system noise and transients on packet delivery reliability of Zigbee and WiFi polling based wireless networks . Extensive experiments show that the electromagnetic interference emitted from high voltage substations during normal operation conditions do not significantly affect wireless communication in the gigahertz range . Moreover we analytically and experimentally demonstrate that abnormal operation conditions of power systems may negatively impact the reliability of packet delivery in polling based wireless networks . Furthermore we show that this negative impact can be mitigated by following some proposed technical considerations regarding the wireless standard the operating frequency the location and the number of wireless transceivers used . 
",We measure and model the path loss for wireless links in low voltage substations. No effect of EMI on Zigbee links is normally observed in high voltage substations. The probability of unsuccessful polling transactions P ut is analytically studied. Sending multiple copies of data by WiFi 5GHz transmitters achieves near zero P ut,,,
S0045790615003110," We propose a novel technique for automatic classification of modulation formats bit rates of digitally modulated signals as well as non data aided NDA estimation of signal to noise ratio SNR in wireless networks. The proposed technique exploits modulation format bit rate and SNR sensitive features of asynchronous delay tap plots ADTPs for the joint estimation of these parameters. Simulation results validate successful classification of three commonly used modulation formats at two different bit rates with an overall accuracy of 99.12 . Similarly in service estimation of SNR in the range of 0 30 dB is demonstrated with mean estimation error of 0.88 dB. The proposed technique requires low speed asynchronous sampling of signal envelope and hence it can enable simple and cost effective joint modulation format bit rate classification and NDA SNR estimation in future wireless networks. 
",A technique for joint modulation format and bit rate classification is proposed. The proposed technique can simultaneously enable non data aided SNR estimation. The technique uses asynchronous delay tap plots with artificial neural networks. The signal classification accuracy is 99.12 and mean SNR estimation error is 0.88 dB. Due to its simplicity it is attractive for future cognitive wireless networks.,,,
S0045790616300088,"Network on Chips NoCs have become the mainstream for Chip Multi Processors CMPs design. Multicast a one to many communication pattern is widely used in barrier clock synchronization multithreading programs and cache coherence protocols for CMPs. Even though several multicast routing algorithms have been proposed for CMPs few can adaptively deal with heavy traffic loads. With the increase of multicast traffic load deterministic routing schemes suffer from long latency and low throughput whereas adaptive routing algorithms can improve the routing performance by providing multiple redundant paths. In this paper we proposed a novel multicast routing algorithm based on partition to reduce the latency of multicast packets by finding multiple routing paths and adaptively choosing available output ports based on the size of buffer space in downstream routers. We evaluate our scheme through simulations and results show that under various configurations both latency and energy consumption have been significantly reduced in comparison with recent multicast routing schemes. 
",An efficient algorithm for routing multicast traffic using recursive partition is proposed. A novel and easy method for minimizing the link usage of a multicast tree is introduced. This algorithm uses minimal adaptive routing to balance the multicast traffic loads.,,,
S0045790616300532," Inspired by the observation that a healthcare system usually involves various intelligent technologies from different disciplines especially metaheuristics and data mining this paper provides a brief survey of metaheuristics for healthcare system and a roadmap for researchers working on metaheuristics and healthcare to develop a more efficient and effective healthcare system . This paper begins with a discussion of changes for healthcare followed by a brief review of the features of up to date technologies for healthcare . . Then a learnable big data analytics framework for healthcare system is presented which provides a high performance solution to the forthcoming challenges of big data . Finally changes potentials open issues and future trends of metaheuristics for healthcare are addressed . 
",A brief survey of metaheuristics for healthcare system is given. A learnable big data analytics framework for healthcare system is presented. Open issues and future trends of metaheuristics for healthcare system are addressed.,,,
S0045790616300052,"In recent years exchanging data has become far easier with the rapidly growing popularity and increased mobility of mobile devices. However data exchange between mobile devices is performed as a peer to peer communication. Each mobile device that serves as a candidate has a whole range of hardware statuses which may influence data exchange performance. Thus the selection of a mobile device with sufficient computing resources to facilitate round robin data exchange is an interesting process worth exploring. This study proposes an optimal mobile device selection approach for round robin data exchange via the monitoring of the hardware status of each candidate device s system profile. A case study demonstrates the proposed approach step by step. The experimental results show that the proposed approach can be used to improve round robin data exchange performance. The contribution of this study is to provide an approach which selects an optimal candidate mobile device for round robin data exchange in a local wireless communication network. 
",We provide a mobile device selection approach for round robin data exchange. When a data exchange round is finished an adaptive function is carried out to adjust the criteria evaluation in order to obtain a more adaptive ranking order of candidate mobile devices for the next round. As a built in system mechanism it requires no user intervention.,,,
S0045790615002220," This paper proposes a cross layer mobility support scheme for the IPv6 over low power wireless personal area network wireless sensor network . This scheme combines the handover in the network layer with the handover in the link layer so that the L3 handover and the L2 one can be performed simultaneously . During the L3 handover process a sensor node neither needs a care of address nor participates in the handover process . During the L2 handover process a node uses the channel information to directly achieve the L2 handover without scanning all channels . Finally this paper analyzes and evaluates the performance of this protocol and the data results show that this protocol improves the mobility handover performance . 
",The L3 handover and the L2 handover can be performed simultaneously. A sensor node can directly achieve the L2 handover without scanning all channels. A sensor node neither needs a care of address nor participates in the L3 handover.,,,
S0045790615003237,"This paper presents a method to predict the milling cutting force and cutting coefficient for aluminum 6060 T6 which is a general commercial alloy with 170 190 MPa of tensile strength and is the most commonly used for anodizing and providing extra protection if needed. We introduce two cutting force prediction methods Altintas and recursive least square RLS and compare their results with experimental values. The influence of the feed per tooth and the tool diameter on the cutting force and cutting coefficient was investigated. After accurately determining the cutting coefficients the cutting parameters including the friction angle and shear stress were estimated using the oblique cutting theory. The forces simulated by the RLS method are in good agreement with the experimentally determined forces. An increase in the feed per tooth is shown to increase the cutting force and reduce the cutting coefficient for shearing forces in the tangential direction. The shear stress in the model is close to the actual shear strength of the material. 
",Predict the milling cutting force and cutting coefficient for aluminum 6060 T6 successfully. An increase in the feed per tooth increases the cutting force and reduces the cutting coefficient for shearing forces in the tangential direction. The shear angle increased with the feed per tooth and shear stress was close to the actual shear strength of the material. Variation of the feed per tooth and tool diameter did not change the magnitude of the friction angle.,,,
S0045782516301001," We develop a fractional extension of a mass conserving Allen Cahn phase field model that describes the mixture of two incompressible fluids . The fractional order controls the sharpness of the interface which is typically diffusive in integer order phase field models . The model is derived based on an energy variational formulation . An additional constraint is employed to make the Allen Cahn formulation mass conserving and comparable to the Cahn Hilliard formulation but at reduced cost . The spatial discretization is based on a Petrov Galerkin spectral method whereas the temporal discretization is based on a stabilized ADI scheme both for the phase field equation and for the Navier Stokes equation . We demonstrate the spectral accuracy of the method with fabricated smooth solutions and also the ability to control the interface thickness between two fluids with different viscosity and density in simulations of two phase flow in a pipe and of a rising bubble . We also demonstrate that using a formulation with variable fractional order we can deal simultaneously with both erroneous boundary effects and sharpening of the interface at no extra resolution . 
",A new fractional mass conserving Allen Cahn model. A second order time spectral space method for the coupled system of fractional equations. A variable fractional order model to control multi rate diffusion. First numerical solution of the fractional Navier Stokes equations.,,,
S0045790615003493,"Code Division Multiple Access which allows multiple users to access and transmit data over wireless channel simultaneously is implemented using different code assignment schemes including Pair wise Code Assignment. In this paper the existing pair wise code assignment procedure is enhanced and a new such scheme is presented with at most codes where is the maximum degree of a network. In addition a co channel interference called secondary interference that exists in the existing schemes has been addressed and an interference free pair wise code assignment procedure is proposed with at most 2 c codes where c is a constant. On simulation over different synthesized actual and random networks it is found that our scheme improves successful transmission rate and blocking probability by 17 and 12.5 respectively. Although the code requirement of the proposed interference free scheme is increased by 40 it would improve the network performance. 
",Two pair wise code assignment schemes are proposed for CDMA based MAC protocol. A co channel secondary interference is detected in the existing code assignment schemes. The proposed schemes deals with the minimum code requirement spatial reuse of codes and avoidance of secondary interference. Improves the network performance.,,,
S0045790615003195," Enhancing the throughput of cognitive radio network in the presence of malicious cognitive radio user is addressed in this paper . We have considered the impact of MCRU on the sensing performance and achievable throughput of secondary network during two phases namely sensing phase and secondary user transmission phase respectively . This impact is mitigated by employing cognitive radio users equipped with multiple receiving antennas . We have performed diversity combining and have formulated the optimization problems with solutions during SP and SUTP . Moreover the method of estimating the channel between MCRU and CR receiver is integrated into the solution . Simulation results show that this method achieves 89 detection and 3.5 bits s Hz of achievable throughput while maximal ratio combining technique offers only 22 and 0.4 bits s Hz respectively when MCRU is active with ten times stronger power than the primary user . 
",The detection performance of the cognitive radio CR network is improved when malicious cognitive radio user coexists in the network. Channel between malicious CR user and CR receiver is estimated. Throughput of the network is enhanced amidst the presence of malicious CR user. We have shown that detection performance and throughput are not compromised by the presence of malicious user.,,,
S0045790615004772," Extracting river information from remote sensing images is of great importance in the investigation and monitoring of water resources and navigation of ships. In order to extract the river target from remote sensing images more accurately a method based on image decomposition and distance regularized CV Chan Vese model is proposed. Firstly the remote sensing image is decomposed based on tensor diffusion. The original image is decomposed into a cartoon image and a texture image and the river is contained in the cartoon part. Secondly the cartoon image is segmented based on the distance regularized CV model. Experimental results show that the method proposed is more accurate in extracting the river target comparing with 6 other methods including image segmentation based on CV model region scalable fitting energy level set model bias field correction level set model and some methods based on image decomposition and active contour model. 
",A new river target extraction method is proposed based on image decomposition and distance regularized CV Chan Vese model. The original image is decomposed based on tensor diffusion and the river target is contained in the cartoon part. The river target is extracted from the cartoon image using distance regularized CV model.,,,
S0045790615004760," The joint multipath routing and Network Coding method has been shown to improve the reliability and energy efficiency of multi hop relay wireless sensor networks . However NC is a kind of all or nothing code the destination node can not decode any information unless it successfully receives as many NC coded packets as the raw ones . Observed that WSNs often measure physical signals which show a high degree of correlation in this paper we propose a more deliberated scheme combined with Compressed Sensing . Depending on the proposed measurement and recovery method we prove that the original data can be recovered progressively while preserving the advantage of NC based scheme . Detailed mathematical analysis of the performance of our proposed scheme along with other existing schemes is given . The experimental results also show the efficiency and robustness of the proposed scheme . 
",A scheme of reliable data transmissions for the WSNs is presented. The encoding decoding method is proposed and its effectiveness is proved. Performances of the proposed and other existing schemes are also analyzed.,,,
S0045790615003596,"We present a watermarking algorithm to synchronously transmit Gong Che notation musical scores and their musical instrument digital interface MIDI information. Our algorithm transforms the MIDI information into a binary sequence which is used to form watermarking data. This watermark is embedded into the Gong Che notation musical score image using an inward clockwise rotation. The algorithm then uses optical music recognition to judge whether the musical information in the image is loss. Experiments have validated the effectiveness of our watermarking and expansion algorithms. The expansion process increases transmission accuracy and the added processing time is minimal. Furthermore our methodology can be applied to other musical notations such as conventional musical notation or numbered musical notation. 
",Achieve synchronous transmission of GCN musical scores and their MIDI information. Examine difference numbers of connected components in original and watermarked scores. Run expansion algorithm on original score to improve note extraction accuracy rate. Minimal added time complexity needed to expand the score and avoid information loss. Our algorithms are suitable for Conventional Musical Notation.,,,
S0045790615003626,"In this paper a novel architecture of Vedic multiplier with Urdhava tiryakbhyam methodology for 16 bit multiplier and multiplicand is proposed with the use of compressor adders. Equations for each bit of 32 bit resultant are calculated distinctly and compressor adders are used to implement these equations. They are chosen as they decrease vertical critical delay in comparison to the conventional architectures of compressors implemented using half and full adders only and so make the multiplier fast. The designs are coded in VHDL Very High speed Integrated Circuits Hardware Description Language and synthesized with Xilinx ISE 13.1 using Spartan 3e series of FPGA Field Programmable Gate Array . The combinational delay calculated for proposed 16 16 bit multiplier is 32 ns. Further speed comparisons of compressor adders with traditional ones and proposed multiplier with popular methods for multiplication are shown. Results clearly indicate the better speed performance of our proposed Vedic multiplier. 
",A 16 bit Vedic multiplier with Urdhava Tiryakbhyam sutra is proposed. Higher compressor adders with the help of lower Compressor adders are proposed. Proposed compressor adders are used in architecture of Vedic multiplier. Proposed multiplier shows good speed results over Traditional multipliers.,,,
S0045790614002705," Previous distributed file systems aim at storing very large data sets . Their architectures are often designed to support large scale data intensive applications which can not cope with massive daily users who want to store their data on the Internet . In this paper CSTORE is proposed to support mass data storage for a large number of users . The user independent metadata management can ensure data security through assigning an independent namespace to every user . Operating logs are applied to synchronize simultaneous sessions of the same user and resolve conflicts . We also implement a block level deduplication strategy based on our three level mapping hash method for the large quantity of repeated data . The migration and rank extension on the hash rules are defined to achieve load balancing and capacity expansion . Performance measurements under a variety of workloads show that CSTORE offers the better scalability and performance than other public cloud storage systems . 
",A desktop oriented distributed public cloud storage system is proposed. Three level mapping hash method is used to distribute and locate data. Using migration and rank extension to implement load balancing and fault recovery. Sequence numbers are used to guarantee consistency. Implement data deduplication and use Bloom filter to recycle rubbish.,,,
S0045790616000148," Botnet is one of the most serious threats to cyber security as it provides a distributed platform for several illegal activities . Regardless of the availability of numerous methods proposed to detect botnets still it is a challenging issue as botmasters are continuously improving bots to make them stealthier and evade detection . Most of the existing detection techniques can not detect modern botnets in an early stage or they are specific to command and control protocol and structures . In this paper we propose a novel approach to detect botnets irrespective of their structures based on network traffic flow behavior analysis and machine learning techniques . The experimental evaluation of the proposed method with real world benchmark datasets shows the efficiency of the method . Also the system is able to identify the new botnets with high detection accuracy and low false positive rate . 
",A new traffic flow behavior analysis method has been proposed to detect botnets irrespective of their control structures. Benchmark datasets collected from various sources such as ISOT Botnet dataset from University of Victoria Conficker dataset from CAIDA dataset from CVUT University dataset from Dalhousie University and dataset from Centro University. It can successfully detect the various types of botnets with a high detection rate and a low false positive rate.,,,
S0045790615001329," BPEL or Business Process Execution Language is so far the most important standard language for effective composition of Web services . However like most available process orchestration engines BPEL does not provide automated support for reacting according to many changes that are likely to arise in any Web services composition like downtime services modifications in the business logic or even new policies to govern the composition . Also low level specification of these new changes which would be integrated at runtime in the BPEL process will be far from being used conveniently . Moreover the complexity of interaction in composite Web services and the diversity of rules and policies can lead to critical behavioral conflicts . We propose in this paper AOMD a novel aspect oriented and model driven approach that defines new grammar to address both adaptability and behavioral conflicts problems and offers extension for WS BPEL meta model for high level specification of aspects . Further we formally verify our proposition and we present real life case study examples and experimental results that demonstrate the feasibility and effectiveness of our work . 
",We provide aspect oriented constructs for context adaptable Web services composition. We afford aspect oriented constructs that can assure conflict free composition. We present new model that allows high level specification of AOP BPEL aspects. We formally verify consistency accuracy conflicts and deadlocks in the composition. We provide a plugin integrated in Eclipse BPEL environment.,,,
S0045790614003188," Networks on Chip have emerged as a promising solution for the communication crisis in today s high performance Multi Processor System on Chip architectures . Routing methods have a prominent role in taking advantage of the potential benefits offered by NoCs . As a result designing high performance and efficient routing algorithms is highly desirable . In this paper the Hamiltonian based Odd Even turn model is proposed for both unicast and multicast routing in wormhole switched 2D mesh networks . HOE is able to maximize the degree of adaptiveness by minimizing the number of prohibited turns such that the algorithm remains deadlock free without adding virtual channels . By increasing the number of alternative minimal paths the hotspots are less likely to be created and the traffic is efficiently distributed throughout the network . The simulation results in terms of latency and power consumption indicate the better performance of the proposed method in comparison with the existing routing methods . 
",A highly adaptive routing method is proposed for wormhole switched 2D mesh networks. The degree of adaptiveness is maximized by minimizing the number of prohibited turns. The deadlock freedom is guaranteed without adding virtual channels. The proposed method is minimal and can be used for unicast multicast routing. The number of hotspots is diminished and the traffic distribution is efficient.,,,
S0045790615004498," The scalability of Network on Chip designs has become a rising concern as we enter the manycore era . Multicast support represents a particular yet relevant case within this context mainly due to the poor performance of NoCs in the presence of this type of traffic . Multicast techniques are typically evaluated using synthetic traffic or within a full system which is either simplistic or costly given the lack of realistic traffic models that distinguish between unicast and multicast flows . To bridge this gap this paper presents a trace based multicast traffic characterization which explores the scaling trends of aspects such as the multicast intensity or the spatiotemporal injection distribution for different coherence schemes . This analysis is the basis upon which the concept of multicast source prediction is proposed and upon which a multicast traffic model is built . Both aspects pave the way for the development and accurate evaluation of advanced NoCs in the context of manycore computing . 
",Multicast traffic is characterized and modeled with an emphasis on scalability. Intensity concentration and burstiness increase with the system size. Growing correlation suggests the use of prediction to optimize NoC designs. Simple multicast source predictors achieve modest but promising accuracies.,,,
S0045790615002803,"Efficient data delivery accompanied with low end to end delay is important in vehicular ad hoc networks and optimal route selection is vital to improve these performance metrics. Different routing algorithms are proposed in VANETs. This paper reviews and analyzes these algorithms briefly and based on the analysis a new opportunistic based routing algorithm OSTD is proposed for urban scenarios. The proposed algorithm severely considers the type of vehicular distribution in the calculation of utility function. This utility function is used to evaluate the routes in the network. The reason of such a severe consideration is expressed and evaluated in the paper. Vehicle s driving path predictability is also used in the algorithm to forward the packet to a more suitable next hop as vehicular mobility is the reflection of human social activity. Simulation results show that OSTD achieves a higher delivery ratio and lower end to end delay and packet loss compared to the other well known protocols. 
",An opportunistic routing algorithm is proposed for the urban scenarios in VANETs. The algorithm is composed of two phases intersection and next hop selection phases. The type of vehicular distribution is used to evaluate the routes in the network. Vehicle s driving path is also used to select the next hop. Packet delivery packet loss packet e2e delay are used to evaluate the protocols. Our algorithm shows better performance compared to the other well known protocols.,,,
S0045790615001512," Based on previous results on periodic non uniform sampling and using the well known Non Uniform Fourier Transform through Bartlett s method for Power Spectral Density estimation we propose a new smart sampling scheme named the Dynamic Single Branch Non uniform Sampler . The idea of our scheme is to reduce the average sampling frequency the number of samples collected and consequently the power consumption of the Analog to Digital Converter . In addition to that our proposed method detects the location of the bands in order to adapt the sampling rate . In this paper through we show simulation results that compared to classical uniform sampler or existing multi coset based samplers our proposed sampler in certain conditions provides superior performance in terms of sampling rate or energy consumption . It is not constrained by the inflexibility of hardware circuitry and is easily reconfigurable . We also show the effect of the false detection of active bands on the average sampling rate of our new adaptive non uniform sub Nyquist sampler scheme . 
",We have proposed a new smart sampler scheme to reduce the power consumption of the Analog to Digital Converter by reducing its average sampling frequency. Its more efficient in terms of sampling rate than classical sampler uniform or multi coset when the false detection of bands localisation is low.,,,
S0045790615002153," Street lighting is a ubiquitous utility but sustaining its operation presents a heavy financial and environmental burden . Many schemes have been proposed which selectively dim lights to improve energy efficiency but little consideration has been given to the usefulness of the resultant street lighting system . This paper proposes a real time adaptive lighting scheme which detects the presence of vehicles and pedestrians and dynamically adjusts their brightness to the optimal level . This improves the energy efficiency of street lighting and its usefulness a streetlight utility model is presented to evaluate this . The proposed scheme is simulated using an environment modelling a road network its users and a networked communication system and considers a real streetlight topology from a residential area . The proposed scheme achieves similar or improved utility to existing schemes while consuming as little as 1 2 of the energy required by conventional and state of the art techniques . 
",TALiSMaN detects road users and sets streetlight brightness appropriately. A utility model is detailed to quantify the usefulness of street lighting. Street lighting schemes are evaluated with StreetlightSim. TALiSMaN offers comparable usefulness as conventional lighting schemes. TALiSMaN consumes 2 55 of the energy of conventional state of the art schemes.,,,
S0045782516300913," This contribution presents B zier extraction of truncated hierarchical splines and the application of the approach to adaptive isogeometric analysis . The developed procedures allow for the implementation of hierarchical splines and NURBS without the need for an explicit truncation of the basis . Moreover standard procedures of adaptive finite element analysis for error estimation and marking of elements are directly applicable due to the strict use of an element viewpoint . Starting from a multi level nested mesh that results from uniform refinement standard B zier extraction is applied to active elements that contribute to the hierarchical approximation . This results in a multi level system of equations without communication between individual hierarchy levels . A hierarchical subdivision operator is developed to recover this communication by transforming the multi level system of equations into a hierarchical system of equations . It is demonstrated that this approach implicitly defines the truncated hierarchical basis in terms of a simple matrix multiplication . In this way the implementation effort is reduced to a minimum as shape function routines and B zier extraction procedures remain unchanged compared to standard isogeometric analysis . The convergence and the computational efficiency of the approach are examined in three different demonstration problems of heat conduction linear elasticity and the phase field modelling of brittle fracture . 
",B zier extraction of truncated hierarchical splines and the application of the approach to adaptive isogeometric analysis are presented. An explicit computation of the truncation is avoided. A strict element viewpoint is adopted in the refinement algorithms which facilitates the application of standard procedures of adaptive finite element analysis. Numerical examples demonstrate optimal convergence rates for problems that involve singularities and strong gradients.,,,
S0045790615003079," The paper introduces a new automated seizure detection model that integrates Weighted Permutation Entropy and a Support Vector Machine classifier model to enhance the sensitivity and precision of the detection process . The proposed system utilizes the fact that entropy based measures for the EEG segments during epileptic seizure are lower than in normal EEG . The new suggested model better tracks abrupt changes in the signal and assigns less complexity to segments that exhibit regularity or are subjected to noise effects . The Weighted Permutation Entropy algorithm relies on the ordinal pattern of the time series along with the amplitudes of its sample points . The proposed technique is implemented and tested on hundreds real EEG signals and the performance is compared based on sensitivity specificity and accuracy . Various experiments have been applied in different scenarios including healthy with eyes open healthy with eyes closed epileptic patients during no seizure state from two different location of the brain . Other scenarios have been applied accompanied by background simulated noise resulting from physiological and environmental artifacts . Results showed outstanding performance and revealed promising results in terms of discrimination of seizure and seizure free segments . It also manifests high robustness against noise sources . 
",A new automated seizure detection model is proposed. Applying Weighted Permutation Entropy to classify raw and decomposed EEG signals. Simulating physiological and environmental artifacts to test the model robustness against noise. Comparison between Support Vector Machine and Artificial Neural Network classifier. Comparison with other automatics seizure detection models found in literature.,,,
S0045782516000037," In this paper a discontinuous Galerkin method for a nonlinear shear flexible shell theory is proposed that is suitable for both thick and thin shell analysis . The proposed method extends recent work on Reissner Mindlin plates to avoid locking without the use of projection operators such as mixed methods or reduced integration techniques . Instead the flexibility inherent to discontinuous Galerkin methods in the choice of approximation spaces is exploited to satisfy the thin plate compatibility conditions a priori . A benefit of this approach is that only generalized displacements appear as unknowns . We take advantage of this to craft the method in terms of a discrete energy minimization principle thereby restoring the Rayleigh Ritz approach . In addition to providing a straightforward and elegant derivation of the discrete equilibrium equations the variational character of the method could afford numerous advantages in terms of mesh adaptation and available solution techniques . The proposed method is exercised on a set of benchmarks and example problems to assess its performance numerically and to test for shear and membrane locking . 
",Discontinuous Galerkin method for a nonlinear shear flexible shell theory. Locking free method in the thin shell limit without the use of projection operators. Specially useful in the development of shell fracture methods that incorporate shear fracture mechanisms.,,,
S0045790615000294," Increasingly companies are adopting service oriented architectures to respond to rapid changes in the market . Even though there are excellent tools and frameworks for service oriented architecture adoption and service development the latest adaptation to context has not been properly dealt with yet . Current approaches are mostly focused on solving context aware issues for web applications only focusing mainly on client side adaptation and there is a clear lack of context taxonomies which facilitate context aware applications . In our previous work we proposed Model Driven Adaptable Services a methodology and tool for the development of context aware services . In this paper we propose two key improvements on MoDAS firstly leveraging the proposal s abstraction level facilitating the use of a larger collection of contexts through the definition of an extensible context taxonomy by means of a metamodel secondly providing additional opportunities for code generation . 
",A context metamodel to classify different types of contexts has been provided. MoDAS was improved through an XML document that facilitates context code generation. MoDAS provides model driven trustworthy and well structured code. The aspect oriented implementation facilitates system evolution and maintenance.,,,
S0045790615002669," Computer systems are required to process data more rapidly than ever due to recent software and internet technology developments . The server computers work continuously and provide services to many clients simultaneously which results in greater heat production and high temperature that must be managed in order to avoid malfunction and failure of critical hardware . In this study three cooling systems were used comparatively to examine the temperature and performance of the CPU and motherboard . The temperature characteristics and performance of the CPU were tested with a heat sink water cooling system and thermoelectric cooler . According to the test results the thermoelectric cooling system has better cooling performance than the other two systems under continuous operating conditions . Additionally the performance rating of the CPU was the best with a thermoelectric cooler under varying workloads . 
",The cooling efficiency of the specific cooling systems on critical server hardware is investigated. The performance of the cooling systems on server CPU is outlined. The applied cooling systems in the context of temperature and performance are compared. The superior performance rating and cooling efficiency is obtained by the thermoelectric cooler.,,,
S0045790616300258,"Feature selection is a well studied problem in the areas of pattern recognition and artificial intelligence. Apart from reducing computational cost and time a good feature subset is also imperative in improving the classification accuracy of automated classifiers. In this work a wrapper based feature selection approach is proposed using the evolutionary harmony search algorithm whereas the classifiers used are the wavelet neural networks. The metaheuristic algorithm is able to find near optimal solutions within a reasonable amount of iterations. The modifications are accomplished in two ways initialization of harmony memory and improvisation of solutions. The proposed algorithm is tested and verified using UCI benchmark data sets as well as two real life binary classification problems namely epileptic seizure detection and prediction. The simulation results show that the standard harmony search algorithm and other similar metaheuristic algorithms give comparable performance. In addition the enhanced harmony search algorithm outperforms the standard harmony search algorithm. 
",A feature selection approach is proposed using the harmony search algorithm. A new harmony memory initialization is adopted and dynamic parameters are used. The proposed method and other metaheuristic algorithms give comparable performance. The enhanced harmony search algorithm outperforms the standard algorithm.,,,
S0045790614003073," In this paper an Android based home automation system that allows multiple users to control the appliances by an Android application or through a web site is presented . The system has three hardware components a local device to transfer signals to home appliances a web server to store customer records and support services to the other components and a mobile smart device running Android application . Distributed cloud platforms and Google services are used to support messaging between the components . The prototype implementation of the proposed system is evaluated based on the criteria considered after the requirement analysis for an adequate home automation system . The paper presents the outcomes of a survey carried out regarding the properties of home automation systems and also the evaluation results of the experimental tests conducted with volunteers on running prototype . 
",A cloud based and Android supported scalable home automation system is proposed. The mechanism relies on the distributed Google Cloud Platform. Google Cloud Messaging supports the communication infrastructure in the system. The prototype is evaluated based on a list of criteria for an adequate system.,,,
S0045790614002729," A fault detection method based on dynamic kernel slow feature analysis is presented in the paper . SFA is a new feature extraction technology which can find a group of slowly varying feature outputs from the high dimensional inputs . In order to analyze the nonlinear dynamic characteristics of the process data DKSFA is presented which applies the augmented matrix to consider the dynamic characteristic and uses kernel slow feature analysis to extract the nonlinear slow features hidden in the observed data . For the purpose of fault detection the D monitoring statistic index is built based on DKSFA model and its confidence limit is computed by kernel density estimation . Simulations on a nonlinear system and Tennessee Eastman benchmark process show that the proposed method has a better fault detection performance compared with the conventional KPCA based method . 
",A nonlinear dynamic process monitoring method is presented. The proposed method can extract the inherent slow features from the high dimensional observed data. A statistic index is built based on slow features to carry out process monitoring. The method is more sensitive to process faults than the conventional KPCA based method.,,,
S0045790615001408," The paper deals with the level control of a Modified Quadruple Tank Process . In that the interaction is introduced between the bottom two tanks which causes an additional non linear dynamical component of the conventional Quadruple Tank Process . Since the process has an inbuilt and imposed uncertainties a robust Sliding Mode Controller is initially designed to drive the system to the desired operating point via the sliding surface . Further the conventional sliding surface is altered with fractional order dynamics . It makes a Fractional Order SMC and is proposed for both finite time convergence and counteract to the uncertainties present in the system . The undesirable chattering effect is reduced by introducing a novel exponential Multi Level Switching technique in order to protect control valve . Simulation results show the efficacy of the proposed Multi Level Switching FrSMC in terms of fast convergence with high robustness when compared to conventional SMC . 
",Devised a new and complex nonlinear process model named as MQTP. Initial implementation of nonlinear and robust control over the MQTP using SMC. Proposed a FrSMC for Finite Time Convergence with adequate mathematical proofs. Implemented a novel approach for reducing the chattering effect by means of MLS.,,,
S0045790615000725," In this paper we propose a blind color image watermarking scheme based on quaternion discrete Fourier transform and on an improved uniform log polar mapping . The proposed watermarking scheme embeds dual watermarks one is a meaningful binary image watermark and the other is a bipolar watermark . The former is embedded in the real part of mid frequency QDFT coefficients using quantization index modulation . The latter is used to resynchronize the watermark after the watermarked image has been attacked making the scheme resistant to geometric attacks . In particular the IULPM allows for greater accuracy when estimating the rotation angle of a geometric attack . At the same time the watermark embedding employs the image holistically rather than in a block pattern . Experimental results demonstrate that the proposed scheme achieves better performance of robustness against both common signal operations and geometric attacks compared to other existing schemes . 
",A robust blind color image watermarking scheme is proposed. An improved uniform log polar mapping method is used to resist geometric attacks. A dual watermark is embedded into quaternion discrete Fourier transform domain. The holistic embedding manner rather than block based achieves better robustness.,,,
S0045790615001792," Automating actions based on collected context from Internet of Things controlled systems is one of the most important requirements of IOT systems this paper seeks to satisfy this requirement by offering a context aware service framework on top of IOT controlled systems . The fault management process in electric power distribution networks is taken here as a case study and used for implementing our proposed framework . We discuss the different aspects that need to be covered in such a framework and its components . The obtained results from simulating and testing the framework show a significant improvement in the task management process compared to the traditional approach . 
",Detailed explanation of an automatic dispatcher its components and functions. Framework structure is based on context awareness integration. with IOT middleware Framework tested in electrical distribution systems as a case study. High progress in the system performance in terms of cost time and effort. Acceleration in faults repairing and increased user satisfaction.,,,
S0045790616000033,"Recognition of age separated face images is a challenging and open research problem. In this paper we propose a facial asymmetry based matching score space MSS approach for recognition of age separated face images. Motivated by its discriminatory information we evaluate facial asymmetry across small and large temporal variations and use asymmetric facial features to recognize age separated face images. We extract three different facial features including holistic feature descriptors using Principal Component Analysis PCA local feature descriptors using Local Binary Patterns LBP and Densely Sampled Asymmetric Features DSAF to represent face images. Then we develop MSS to discriminate genuine and imposter classes using support vector machine SVM as a classifier. Experimental results on three widely used face aging databases the FERET MORPH and FG NET show that proposed approach has superior performance compared to some existing state of the art approaches. 
",Facial asymmetry based approach is proposed to classify age separated face images. Facial asymmetry is measured and evaluated across temporal variations. A 3 D matching scores space is built using holistic local and asymmetric features. SVM is used as classifier to separate genuine and imposter classes in score space. Results show better performance of proposed approach compared to existing methods.,,,
S0045790615003559," In this paper we propose a client based solution to detect evil twin attacks in wireless local area networks . An evil twin is a kind of rogue Wi Fi access point which has the same SSID name as a legitimate one and is set up by an attacker . After a victim associates his device with an evil twin an attacker can eavesdrop sensitive data forwarded through the evil twin . Most existing detection solutions are administrator based which are used by wireless network administrators to verify whether a given AP is in an authorized list or not . Such administrator based solutions are limited hardly maintained and difficult to protect users 24 7 . Hence we propose a client based detection mechanism called evil twin detector to detect this type of attacks . An evil twin detector changes its wireless network interface card to monitor mode to capture wireless TCP IP packets . Through analyzing captured packets our detector allows client users to easily and precisely detect an evil twin thus avoids threats created by evil twins . Our method does not need to know any authorized AP list and does not rely on data training or machine learning technique . Finally we implement a detecting system on Windows 7 . 
",A client side detection mechanism for evil twins. The mechanism relies on packet forwarding behavior of an evil twin attack. Proposes the first idea of detecting evil twin by operating the wireless network interface controller in monitor mode.,,,
S0045790615002712," Recent technological trends such as cloud computing wireless communication and wireless sensor networks provide a strong infrastructure and offer a true enabler for health information technology services over the Internet . This system is based on the cloud computing environment integrating mobile communication technology context aware technology and wireless sensor networks to build a mobile web for a personalized health information service which includes two health information recommendation service functions a collaborative recommender and a physiological indicator based recommender . We further propose a hybrid predictive model which combines the Grey Theory and Markov chain to predict the moving object s path . This will decrease the cost which arises from tracking errors and prolong the network s lifetime . From the experiment results of usability it has been discovered that subjects have positive responses towards usability measurement dimensions of the system satisfaction expectation confirmation perceived trust perceived usefulness and perceived value . 
",The development of a Mobile Health Information Recommendation system which integrates Cloud Computing and Context aware technology and applies Collaborative Filtering technique for information recommendation. Through the collaborative recommender users can effectively obtain consistent health information and reduce energy and time consumption.,,,
S0045790615004401,"Removal of noise and restoration of images has been one of the most interesting researches in the field of image processing in the past few years. Existing filter based methods can remove image noise however they cannot preserve image quality and information such as lines and edges. The proposed noise reduction image restoration technique initially performs noise removal by employing a hybrid denoising filter with an adaptive genetic algorithm. The performance of the proposed technique is evaluated by comparing the result for the proposed technique with existing denoising filters and genetic algorithm and particle swarm optimisation methods. The comparison results show that the proposed method achieves higher quality denoising and a high restoration ratio for noisy images than the existing methods. 
",We modelled a Hybrid filter with AGA to remove the noise from the images. The noisy free images are restored by shining the pixel values using AGA. The performance is analysed by taking number of images and comparing the result with existing technique.,,,
S0045790615001081," Service oriented architecture is a self contained service with a collection of services . Services communicate with each other using a web service . Web services use a collection of open protocols and standards for communication between different web services and their applications . In today s environment users are not satisfied with a single web service . To fulfill the users needs two or more atomic services must be combined to provide a single complex service . Hence several atomic web services must be orchestrated based on user preference by using a multi agent system . In the traditional approach the orchestration of web services is a manual process . The proposed system reduces human intervention and supports the orchestration of atomic web services into a complex service by using a multi agent system . Therefore this paper addresses the automatic orchestration by using a multi agent i.e . two agents . The first agent is used to select an appropriate web service from the available atomic service . The second agent is used to orchestrate the selected web services to form a complex service based on a user s requirement and quality of service . 
",The proposed approach uses a multi agent system to automate the process. It increases the overall system performance over traditional approach. It supports several QoS parameter such as reliability throughput response time and reusability. The graph depicts the response time between proposed and traditional approach.,,,
S0045790615001421," Steganography has become a hot topic in information hiding and the reversibility technology allows the recovery of the original image without distortion when the embedded secret information is extracted . In this paper a high payload image steganographic scheme based on an extended interpolating method is proposed . In the premise of image quality assurance the proposed scheme increases the capacity by maximizing the difference between neighboring pixels . Meanwhile it has low complexity and retains good image quality . Extensive experiments on images have been conducted and the experimental results demonstrate that the proposed approach performs better than several state of the art methods . 
",By analysis of statistical properties of adjacent pixels as well as image interpolation techniques a novel interpolation method is put forward. Under the conditions of ensuring the image quality the proposed image interpolation method maximizes the difference values between neighboring pixels and improves the capacity. The proposed scheme still has the advantage of lower computing complexity and better results between interpolating pixels. The experimental results show that the performance of the proposed method is superior to several state of the art methods.,,,
S0045790615001482," We propose a new optical reconfigurable Network on Chip named ReFaT ONoC . ReFaT is a dynamically reconfigurable architecture which customizes the topology and routing paths based on the application characteristics . ReFaT as an all optical NoC routes optical packets based on their wavelengths . For this purpose we propose a novel architecture for the optical switch which eliminates the need for optical resource reservation and thus avoids the corresponding latency and area overheads . As a key idea for dynamic reconfiguration each application is mapped to a specific set of wavelengths and utilizes its dedicated routing algorithm . We compare the proposed reconfigurable optical NoC with traditional electrical NoCs as well as non reconfigurable optical NoCs in terms of data transmission delay power consumption and energy dissipation . The simulation results show that on average ReFaT reduces power and energy consumption by 54.75 and 40 respectively compared to the non reconfigurable ONoC . 
",ReFaT enables run time reconfiguration of network topology and routing algorithm. ReFaT adopts wavelength routing and avoids electrical control packets. ReFaT utilizes a novel wavelength assignment approach to map various topologies. ReFaT architecture facilitates simultaneous multicast and unicast data transmission. ReFaT improves power and energy efficiency compared to non reconfigurable topologies.,,,
S0045790615002992,"The traditional approach for energy detection ED consists in the comparison of the energy received against a fixed detection threshold estimated according to an expected noise level. However the noise power is not a static parameter since it varies as a function of the random sources of noise and interference present in the network. This random nature of the noise power originates the so called noise uncertainty problem which adversely affects the performance of the ED. In this paper we propose and evaluate by means of computer simulation an adaptive energy detector that incorporates a noise power estimation strategy for adjusting the detection threshold according to the noise power present at each sensing epoch. As it can be proved by simulation results our strategy helps in reducing the sensing errors and to improve the ED s sensitivity by alleviating the negative effects of the noise uncertainty. 
",Noise uncertainty in energy detection is counteracted by adapting detection threshold. Multiple receiver architecture is exploited for maximum likelihood estimation. Threshold adaptation allows maintaining a constant and low false alarm rate. The probability of detection is improved in the low SNR regime without increasing sensing time.,,,
S0045790615001457," Affine Transform is widely used in high speed image processing systems . This transform plays an important role in various high speed image processing applications . AT an important process during the intensity based image registration is applied iteratively during the registration . This is also used for the analysis of the interior of an organ and to get a better view of the organs from various angles in 3D coordinate system . Hence for real time medical image registration and visualization of the acquired volumetric images acceleration of AT is very much sought for . In this paper a parallel and pipelined architecture of the proposed AT algorithm has been presented . This will accelerate the transform process and reduce the processing time of medical image registration . The architecture is mapped in Field Programmable Gate Array for prototyping and verification . The results show that the computational complexity of the proposed parallel algorithm is almost 4 times better than that of the conventional algorithm . 
",In the proposed algorithm simultaneously 4 pixel voxel locations can be transformed to compute AT of the entire image. The proposed architecture requires only relevant frames instead of all the frames simultaneously. The faster implementation of the affine transform is useful during image registration of the 3D bio medical images.,,,
S0045790615002736," This paper studies sectorization increase in horizontal and vertical plane . We have simulated downlink macro long term evolution network using three dimensional antenna and propagation models . New network layouts have been proposed based on 4 sector site deployment . The challenge was to offer lower network cost and complexity . Simulation results have showed good tradeoff between capacity and mobility performance in comparison to known sectorization schemes . Besides carrier aggregation and active antenna system have been used to exploit the sector vertical plane . Low frequency band improves coverage and indoor signal penetration in urban environment . Our proposed sectorization schemes give multi objective network quality of service . 
",Radio site design with four horizontal and four vertical sectors is proposed. Vertical sectorization is applied using active antennas. Low band carrier aggregation and sectorization boosted radio performance. Throughput and mobility performance tradeoff is maintained.,,,
S0045790614001761," The number of wireless sensor network deployments for real life applications has rapidly increased in recent years . However power consumption is a critical problem affecting the lifetime of wireless sensor networks . A number of techniques have been proposed to solve this power problem . Among the proposed techniques data compression scheme is one that can be used to reduce the volume of data to be transmitted . This paper therefore proposes a fast and efficient lossless adaptive compression scheme for WSNs . FELACS was proposed to enable a fast and low memory compression algorithm for WSNs . FELACS generates its coding tables on the fly and compresses data very fast . FELACS is lightweight robust to packet losses and has very low complexity . FELACS achieved compression rates of 4.11 bits per sample . In addition it achieved power savings up to 70.61 using the real world test datasets . 
",A fast and low memory data compression scheme is proposed for WSNs. The scheme performs compression losslessly using 8 variable length code options. The scheme which uses a very simple data model is fast and computationally simple. The scheme is efficient and requires no coding dictionary. The scheme achieve better compression performance than previously proposed schemes.,,,
S0045790615002840," A number of mobile payment studies have been proposed in recently years . Most of the schemes are largely focused on transaction security not on users privacy . In this paper we propose an Unlinkable Anonymous Payment Scheme to provide a secure and anonymous mobile commerce environment . In the proposed protocol a user applies an anonymous virtual credit card from a trusted service manager . The sensitive information of the applied credit card is stored in the secure elements of user s mobile device . Our proposed protocol ensures various imperative security properties such as anonymity unlinkability and non repudiation etc . 
",We propose an anonymous mobile payment protocol to protect users privacy. Using anonymizing schemes to improve anonymity and unlinkability in a mobile transaction. Users can use mobile phones with NFC to perform commercial transactions.,,,
S0045790615004243," As a typical media access protocol S MAC has drawn much attention in the past decades . However most of the attention focuses on energy efficiency but fails to guarantee QoS performance . Firstly we develop one state machines model and elaborate on the states transition processes of S MAC . Secondly combining our state machines with the specific parameters of S MAC we propose an analytical probabilistic model to investigate the latency and stability performances of S MAC employing three different mechanisms without sleeping mechanism with sleeping mechanism and with adaptive listening mechanism . Also we analyze the latency and stability performances of S MAC with different dutyCycle values . This research shows two interesting results 1 Sleeping mechanism inside S MAC does have negative impacts on its latency and stability fortunately adaptive listening mechanism can optimize them to some extent . 2 Latency and stability performances of S MAC can be further improved through manipulating dutyCycle values . The simulations in NS2 exhibit consistent results with those obtained from our analytical probabilistic model . 
",We propose one state machines model for S MAC protocol. We use a probabilistic model to analyze the latency and stability performances. Sleeping mechanism does have negative impacts on the latency and stability performances. Adaptive listening mechanism does optimize the latency and stability performances. Appropriate dutyCycle can be used to further improve the latency and stability.,,,
S0045790616300349," Graphical abstract Image graphical abstract stator rotor and load resistive respectively stator rotor and magnetizing inductance respectively the area swept by the blades of the wind turbine in m terminal excitation capacitance the load current along the d axis and q axis respectively direct axis stator rotor current respectively quadrature axis stator rotor current respectively the inertia of the rotor in constants representing initial induced voltages along the d axis and q axis respectively due to the remaining magnetic flux in the core . d dt the number of poles the wind turbine power the sweep diameter of the wind turbine . the wind turbine torque in the capacitor voltage along the d axis and q axis respectively the wind speed m s the angular velocity of the wind turbine rad s. the electrical angular velocity of the rotor air density in Kg m quadrature axis stator rotor flux linkage respectively direct axis stator rotor flux linkage respectively tip speed ratio 
",Nonlinearity and asymmetry characteristics of the load are described mathematically. The dynamic study of the isolated wind turbine based SEIG under nonlinear resistive load is included. Detailed Mathematical model of wind turbine SEIG and nonlinear load are presented.,,,
S0045790615000191," Diabetic retinopathy is a condition that occurs in individuals with several years of diabetes mellitus and causes a characteristic group of lesions in the retina and progressively damages it . Detecting retinal fundus diseases in advance helps ophthalmologists to apply proper treatments that may cure the disease or decrease its severity and thus protect patients from vision loss . Diabetic retinopathy is usually diagnosed by ophthalmologists using dilated images that are captured by pouring a chemical solution into the patient s eye which causes inconvenience and irritation to the patient . In this paper we propose a method to detect lesion exudates automatically with the aid of a non dilated retinal fundus image to help ophthalmologists diagnose the disease . The exudates from the low contrast images are detected and localised using a neighbourhood based segmentation technique . A support vector machine and probabilistic neural network classifiers are proposed to assess the severity of the disease and the results are compared with the same segmentation technique . The average classification accuracy for the SVM and PNN classifiers are determined to be 97.89 and 94.76 respectively . 
",A method for investigating the severity of diabetic retinopathy is proposed. Earlier diagnosis of the disease prevents vision loss in diabetic patients. The exudates are the major symptoms and are detected via a segmentation algorithm. The degree of severity is assessed using the SVM and PNN classifier algorithms. The SVM classifier is found to provide better results than the PNN classifier.,,,
S0045790614002195," MapReduce is considered the key behind the success of cloud computing because it not only makes a cluster highly scalable but also allows applications to use resources in a cluster . However MapReduce achieves this simplicity at the expense of flexibility for data partitioning localization and processing procedures by handling all issues on behalf of application developers . Unfortunately MapReduce currently has no solution capable of giving application developers flexibility in customizing data partitioning localization and processing procedures . To address the aforementioned flexibility constraints of MapReduce we propose an architecture called Flexible Architecture for Cluster Evolution which is both language independent and platform independent . FACE allows a MapReduce cluster to be designed to match various application requirements by customizing data partitioning localization and processing procedures . We compare the performance of FACE with that of a general MapReduce system and then demonstrate performance improvements with our implemented procedures . 
",FACE supports system primitives that allow application developers to develop various applications in clouds. FACE allows application developers to customize data partitioning localization and processing procedures. FACE designs its system primitives in a language independent and platform independent way. FACE makes extensible the Master of a MapReduce system by application developers.,,,
S0045790615004322," This paper proposes an alternative for building a data hiding algorithm into digital images . The method is based on chaos theory and the least significant bit technique for embedding a secret message in a image . Specifically the Bernoulli s chaotic maps are used to perform the following processes encrypt the bits of the message before embedding them into the cover image a random selection of the image s compositions must be performed and the insertion of the secret message in a random way to rows and columns of the image . Several experimental results are shown under different evaluation criteria such as entropy autocorrelation homogeneity contrast energy peak signal to noise ratio mean squared error and maximum absolute squared deviation . Experimental results show a good improvement in the peak signal to noise ratio and Image Fidelity value of the proposed algorithm in comparison to the results obtained from similar algorithms . 
",A steganographic method for hiding information is proposed using four Bernoulli s chaotic maps. It is applied a two step scheme Pixel selection and masking of sensitive information. A module function is applied to the chaotic maps considering an iterated process. There is no evidence of periodicity in the sequences produced by the modified Bernoulli s maps. It is assumed that each map uses two 64 bits variables and the key space has been estimated in 2508.,,,
S0045790615003468,"Set Partitioning In Hierarchical Tree SPIHT is considered one of the most important algorithms for reducing the size of the vision data collected by the sensor node within wireless multimedia sensor network WMSN . The traditional SPIHT algorithm suffers from image coders complexity due to large memory requirement. This is an essential problem for the implementation on limited resource environments such as WMSN. The main objective of this paper is to introduce a listless pipelined strip based SPIHT for WMSN to reduce system complexity and minimize processing time and memory usage. The proposed algorithm is implemented using discrete wavelet transform DWT lifting based instead of DWT convolution based filter. The experimental results show the superiority of the proposed algorithm in terms of peak signal to noise ratio PSNR which reaches 1 dB for all bit rates. In addition the memory requirement is reduced to 71 with 27 of energy saving. 
",Proposed a modified listless strip based zero tree structure LZT based on SPIHT. Image quality in terms of signal to noise ratio is improved besides memory reduction obtained. Decreasing processing time reduces system complexity and power consumption.,,,
S0045790615000178," Wireless multimedia sensor networks are capable of retrieving audio image and video data in addition to scalar sensor data . The lifetime of these networks is mainly dependent on the communication and computational energy consumption of the node . In this paper compressed sensing based image transmission is proposed to reduce the energy consumption considerably with acceptable image quality . A unique encoding algorithm is formulated for the CS measurements attained with the Bernoulli measurement matrix . The proposed CS method produces better results at a lower sparsity range . Experimental analysis is performed using the Atmega 128 processor of Mica2 to compute the execution time and energy consumption in the hardware platform . The proposed CS method has a considerable reduction in energy consumption and better image quality than the conventional CS method . The simulation results show the efficiency of the proposed method . 
",Energy efficient CS methodology for image transmission in WMSNs is proposed. Unique encoding algorithm for CS measurements with the Bernoulli matrix is formulated. Experimental analysis in the Atmega 128 of Mica2 to compute the execution time. Optimal range of communication distance for the proposed methodology is evaluated.,,,
S0045790615002074," In this paper we propose a systematic rateless erasure code namely systematic Random code based on Random code for short messages transmission . Given a message of k symbols the sender will first send the message to the receiver as Part I coded symbols . The rest of coded symbols starting from th onwards are termed as Part II coded symbols and they are generated by adding the message symbols randomly . The receiver reconstructs the original message instantly if all the Part I coded symbols are received intact . Otherwise the receiver reconstructs the original message from any 10 coded symbols of Part I and II with high probability of complete decoding i.e . 99.9 success probability . Though SYSR code inherits the high decoding complexity of Random code i.e . both analysis and simulation results show that SYSR code achieves better PCD and fewer decoding steps than Random code . 
",Systematic Random code reconstruct the original message with zero extra coded symbols if the first k coded symbols are received intact. Reconstruct the original message with 99.9 success probability using extra ten overhead symbols. In channels of low erasure probability systematic Random code outperforms Random code in term of lesser extra coded symbols and decoding steps to achieve complete decoding.,,,
S0045790615003481," In order to reduce aquaculture risks and optimize the operation of water quality management in prawn engineering culture ponds this paper proposes a novel water temperature forecasting model based on empirical mode decomposition and back propagation neural network . First the original water temperature datasets are decomposed into a collection of intrinsic mode functions and a residue by EMD yields relatively stationary sub series that can be readily modeled by BPNN . Second both IMF components and residue is applied to establish the corresponding BPNN models . Then each sub series is predicted using the corresponding BPNN . Finally the prediction values of the original water temperature datasets are calculated by the sum of the forecasting values of every sub series . The proposed hybrid model was applied to predict water temperature in prawn culture ponds . Compared with traditional models the simulation results of the hybrid EMD BPNN model demonstrate that de noising and capturing non stationary characteristics of water temperature signals after EMD comprise a very powerful and reliable method for predicting water temperature in intensive aquaculture accurately and quickly . 
",The novel model which combines EMD and BPNN algorithm is presented to predict water temperature in intensive aquaculture.. Using EMD technology adaptively decomposed the original water temperature data into a finite set of IMFs and a residue. EMD BPNN has higher prediction accuracy and better generalization performance than standard BPNN and standard SVR. EMD BPNN can be used as a suitable and effective modeling tool for predicting water temperature in intensive aquaculture.,,,
S0045790614002614," Online induction machine faults diagnosis is a concern to guarantee the overall production process efficiency . Nowadays the industry demands the integration of smart wireless sensors networks to improve the fault detection in order to reduce cost maintenance and power consumption . Induction motors can develop one or more faults at the same time that can produce sever damages . The origin of most recurrent faults in rotary machines is in the components stator rotor bearing and others . This work presents a novel methodology for the online faults diagnosis in induction motors . This technique uses the smart WSN to obtain the machine condition based on the motor stator current analysis . The implementation of the proposed smart sensor methodology allows the system to perform online fault detection in a fully automated way . Simulation results presented show the efficiency of the proposed method to detect simple and multiple faults in induction machine . It provides detailed analysis to address challenges in designing and deploying WSNs in industrial environments and its reliability . 
",A new method has been proposed for online faults diagnosis in induction motors based on smart WSN combined with motor current signature analysis using FFT. The proposed method is novel as it is important to install low cost sensors and detection mechanisms along with induction machines to achieve short detection time and an automated way of reporting the fault. The system can distinguish a faulty motor from a healthy motor with a probability of 99 with less than 5 of false alarm. Simulation results presented show the efficiency of the proposed method to detect faults in induction machine.,,,
S0045790615001330," In this paper we propose GeoRank a geographic routing approach for the IPv6 enabled large scale low power and lossy networks . We discuss the main drawbacks of the RPL for P2P communication in large scale 6LoWPAN networks . Then we address such drawbacks by proposing a routing protocol named GeoRank which integrates RPL protocol with the position based routing protocol GOAFR . The results obtained with simulations show that GeoRank finds shorter routes than RPL in high link density conditions and than GOAFR in low link density conditions . Thus GeoRank shows to be adaptive to variable link densities found in large scale networks . Further GeoRank avoids the use of DAO control messages required in RPL while being more scalable in terms of memory usage than storing mode RPL . 
",We combine RPL and GOAFR protocols for IPv6 enabled large scale wireless networks. Main drawbacks of the RPL protocol for P2P communication are discussed. A routing protocol GeoRank is proposed and its scalability is compared to RPL. Simulations were performed on networks extracted from real street maps. GeoRank is adaptive to variable link densities found in large scale networks.,,,
S0045790615003444,"This paper presents a novel methodology for improving efficiency and power consumption of networks on chip NoCs . The proposed approach applies queue length considerations of a modified version of RED algorithm. Moreover a stochastic learning automata based algorithm has been used to optimize the threshold values required in RED algorithm. Furthermore a new architecture has been provided for dynamic flow control of virtual channels. The proposed method contributes to reduction in queue blockages and power consumption in addition to determining an appropriate size for virtual channels. The proposed algorithm was evaluated under various synthetic traffic patterns for different injection rates and trace driven SPLASH 2 benchmark suite. The experimental results demonstrate that the algorithm reduces latency and power consumption by 23 and 52 respectively compared to the conventional NoC. Further compared to Express Virtual Channels EVC scheme it showed 13 and 36 improvement in latency and power consumption respectively. 
",We suggest a new flow control mechanism for improving power and latency. We present a novel methodology to improve the buffer space utilization. Our method reduces queue blockages to determine a proper size for virtual channels. We apply queue length considerations of a modified version of RED algorithm. We utilize learning automata to adapt the threshold values of RED algorithm.,,,
S0045790615004747,"Wireless sensor networks comprise nodes with limited power supply. The dense deployment of nodes on a terrain for monitoring the environment causes contention in wireless channels that leads to interference and high energy consumption. The topology of the network changes frequently because of failure of nodes addition of new nodes and channel fading. Topology control is an important technique for reducing energy consumption interference and maintaining connectivity in the network. This paper provides a survey on state scheduling based topology control techniques for unattended wireless sensor networks. Topology control algorithms for both flat and hierarchical networks are discussed. The algorithms are further categorized according to the key parameters used for state scheduling. The advantages disadvantages and results of the algorithms are tabulated. In addition we highlight future research directions in designing state scheduling based topology control algorithms. 
",We survey the state scheduling based topology control protocols suitable for unattended wireless sensor networks. We categorize the protocols based on the parameters used for state scheduling. Advantages drawbacks and results of existing methods are tabulated. We delineate future research issues in design of state scheduling based topology control protocols.,,,
S0045790615002128," Mobility currently evolves far beyond owning a car or using public transit services . Passenger transport can be managed by mobility providers by combining and extending various mobility services either directly or by using available mobility service platforms . This paper evaluates the capabilities and technical features of existing mobility service platforms with a special focus on electric mobility . Based upon this evaluation criteria are presented which future platforms should address . As part of this work a marketplace approach is developed which addresses the identified criteria . Potential marketplace architectures are presented which are deemed to establish marketplace interconnectivity . The developed marketplace approach and the proposed architectures contribute to the vision of an interconnected service ecosystem for mobility services . 
",Current electric mobility service marketplaces are proprietary and closed systems. Data exchange among marketplaces is not possible because no common protocol exists. Roaming provides easy service access and extends the end customers operation area. The access at foreign services can be achieved via interconnectivity and roaming. The selected architecture is critical for the success of interconnectivity.,,,
S0045782516300524," The avascular multiphase model for tumor growth developed by the authors in previous works is enhanced to include angiogenesis . The original model comprises the extracellular matrix as porous solid phase and three fluid phases living and necrotic tumor cells host cells and the interstitial fluid . In this paper we add transport of tumor angiogenic factor and of endothelial cells . The density of the endothelial cells represents the newly created vessels in a smeared manner . Co opted blood vessels can be added as line element with flow or can be taken into account as boundary condition . The model is hence of the continuum discrete type . Two examples show the potential of the newly enhanced model . The first deals with growth of a 2D tumor spheroid in a square tissue domain . From a blood vessel posed on one side of the domain angiogenesis takes place through the migration of endothelial cells from the vessel to the tumor . The second one is the simulation of cutaneous melanoma growth with the diffusion of TAF from the living tumor cells and the consequent development of a new vessel network represented by the endothelial cells density . The introduction of angiogenesis will allow for simulating the delivery of chemotherapeutic and nanoparticle mediated agents to the vascular tumor and for evaluation of the therapeutic effect . Equation Equations Thermodynamically Constrained Averaging Theory Coefficient in the pressure saturations relationship Nonlinear coefficient of the discretized capacity matrix Rate of strain tensor Effective diffusion coefficient for the species dissolved in the phase Endothelial cells Discretized source term associated to the primary variable Nonlinear coefficient of the discretized conduction matrix Compressibility of the phase and Intrinsic permeability tensor of the ECM Relative permeability of the phase Vector of shape functions related to the primary variable Pressure in the phase Pressure difference between fluid phases and Saturation degree of the phase Effective stress tensor of the solid phase Total stress tensor of the solid phase Yield limit of the solid phase which defines the boundary of elastic domain Tumor angiogenic factors Displacement vector of the solid phase Velocity vector of the phase Solution vector Biot s coefficient Growth coefficient Necrosis coefficient Nutrient consumption coefficient related to growth Nutrient consumption coefficient not related to growth Porosity Volume fraction of the phase Viscosity parameter of the solid phase Dynamic viscosity of the phase Density of the phase Interfacial tension between fluid phases and Adhesion of the phase Mass fraction of necrotic cells in the tumor cells phase Nutrient mass fraction in the interstitial fluid Critical nutrient mass fraction for growth Reference nutrient mass fraction in the environment TAF mass fraction in the interstitial fluid Critical TAF mass fraction Endothelial cells mass fraction in the interstitial fluid Critical endothelial cells mass fraction for the death of the endothelial cells Coefficient for uptake of TAF by endothelial cells Degradation rate coefficient for TAF demise Coefficient for TAF and endothelial cells production Coefficient for new oxygen brought by the. 
",We introduce in our multiphase model for avascular tumor growth two new species. The new species are the tumor angiogenic factor TAF and endothelial cells EC . Their diffusion and interaction simulate angiogenesis. Two numerical simulations of angiogenesis are proposed. The new capillary network will allow for simulating the chemotherapeutic agents delivery.,,,
S0045790614002626," Smart Grid makes use of Information and Communications Technology infrastructures for the management of the generation transmission and consumption of electrical energy to increase the efficiency of remote control and automation systems . One of the most widely accepted standards for power system communication is IEC 61850 which defines services and protocols with different requirements that need to be fulfilled with traffic engineering techniques . In this paper we discuss the implementation of a novel management framework to meet these requirements through control and monitoring tools that provide a global view of the network . With this purpose we provide an overview of relevant Software Defined Networking related approaches and we describe an architecture based on OpenFlow that establishes different types of flows according to their needs and the network status . We present the implementation of the architecture and evaluate its capabilities using the Mininet network emulator . 
",We describe the IEC 61850 communication model and provide an SDN based framework. This framework mainly uses the OpenFlow sFlow and OVSDB protocols. It controls an IEC 61850 network including resource analysis and management. It integrates traffic engineering techniques such as QoS or traffic filtering.,,,
S0045790615004735,"This paper proposes two architectures including an Opportunistic Large Array Concentric Routing Algorithm with Geographic Relay Nodes OLACRA GRN architecture and an Opportunistic Large Array Concentric Routing Algorithm with Relay Nodes OLACRA RN architecture. First the OLACRA GRN architecture with the geographic information of relay nodes reduces node energy consumption and finds the optimum number of relay nodes to forward the data an analysis of the characteristics of the energy model is also presented. Besides the OLACRA RN architecture without the geographic information of relay nodes is proposed which can find the number layer of concentric circles in the sensing field. The optimal number layer of concentric circles is calculated according to the distance between the sink and field boundary. Simulation results show that our proposed OLACRA GRN and OLACRA RN architectures can effectively reduce node energy consumption more than Opportunistic Large Array Concentric Routing Algorithm OLACRA architecture. 
",The optimal number layer of concentric circles is calculated by the distance. We calculate the optimum number of relay nodes to forward the data. Analysis of the characteristics of the energy model. Our proposed architectures can effectively reduce energy consumption of nodes.,,,
S0045790615003547," This study examines a multicore processor based on a system on chip and configured by a Tensilica Xtensa LX2 . The multicore processor is a heterogeneous configurable dual core processor . In this study one core was used as the host to control the processor chip and the other was used as a slave to extend digital signal processing applications . Each core not only owned its local memory but also shared common data memory . In addition the proposed multicore processors had a virtual memory . This additional memory supported the processor by enabling it to easily manage complex programs it also allowed the two cores to access data from the unified data memory of different tasks . For bus management a bus arbitration mechanism was added to handle the cores and to distribute the priority of asynchronous access requests . The benefits of the proposed structure include avoiding hardwired memory and reducing interface handshaking . To verify the proposed processor it was simulated on the model level using a Petri net graph and on the system level using ARM SoC designer tools . In the performance simulation we found that the lowest latency to cost ratios were achieved using a 32 bit bus interface and a 4 entry data queue . 
",Multicore processor based on SoC configured by Tensilica Xtensa LX2 was examined. One core used as the host to control the processor chip. Other used as slave that is an extension of digital signal processing applications. Lowest latency to cost ratios by a 32 bit bus interface and 4 entry data queue.,,,
S0045790615002724," The emerging IEEE 802.22 based Wireless Regional Area Network is the first wireless standard based on the Cognitive Radio technology . WRAN is designed to offer wireless access services in a large coverage area by allowing Secondary Users to opportunistically exploit the under utilized licensed portion of spectrum that is primarily allocated for TV services . To enable efficient WRAN communications the operation of a WRAN system should address two types of coexistence problem incumbent co existence and self coexistence . In this paper we investigate the self coexistence problem in a multi cell WRAN system that adopts an exclusive spectrum sharing policy . Specifically we present an adaptive channel allocation scheme based on a cooperative max min weighted fair allocation strategy . The proposed scheme is based on a centralized sensing mechanism that identifies the available spectrum opportunities for the WRAN cells . Our scheme dynamically allocates the available spectrum to the WRAN cells based on their expected traffic loads such that the total number of simultaneously admitted SU transmissions in the WRAN system is maximized . The expected traffic load is accurately estimated using a sample mean estimator based on previously monitored traffic in each cell . Simulation results indicate that our scheme is quite robust to traffic estimation error . Compared to reference spectrum allocation schemes simulation results indicate that the proposed scheme effectively exploits the available spectrum opportunities by increasing the total number of served SU transmissions which consequently results in a significant enhancement in the overall WRAN system performance . 
",We investigate the self coexistence problem in a multi cell WRAN system. We propose an adaptive traffic aware exclusive channel allocation scheme. The proposed scheme is based on a cooperative max min weighted fair mechanism. Compared to other schemes results reveal that our scheme provides significant performance gain.,,,
S0045790616300337," The Internet of Things has plenty of applications including Smart Grid . IoT enables smooth and efficient utilization of SG . It is assumed as the prevalent illustration of IoT at the moment . IP based communication technologies are used for setting SG communication network but they are challenged by huge volume of delay sensitive data and control information between consumers and utility providers . It is also challenged by numerous security attacks due to resource constraints in smart meters . Sundry schemes proposed for addressing these problems are inappropriate due to high communication computation overhead and latency . In this paper we propose a hybrid Diffie Hellman based lightweight authentication scheme using AES and RSA for session key generation . To ensure message integrity the advantages of hash based message authentication code are exploited . The scheme provides mutual authentication thwarting replay and man in the middle attacks and achieves message integrity while reducing overall communication and computation overheads . 
",Designed an authentication scheme for IoT based smart grid communication. Analyzed the scheme using automated tool ProVerif. The proposed scheme is more lightweight and secure than existing schemes.,,,
S0045790616300295," This paper presents a reconfigurable fault tolerant routing for Networks on Chip organized into hierarchical units . In case of link faults or failure of switches the proposed approach enables the online adaptation of routing locally within each unit while deadlock freedom is globally ensured in the network . Experimental results of our approach for a 16 16 network show a speedup by a factor of almost four for routing reconfiguration compared to the state of the art approach . Evaluation with transient faults shows that a dedicated reconfiguration unit enables successful reconfiguration of routing tables even in case of high error probabilities . 
",Adding logical hierarchy to networks on chip enables table based routing without excessive chip area overhead. For a 256 node network the routing table occupies only less than 20 of the switches area. Thanks to the hierarchical network organization double data throughput is achieved compared to a flat network of same size. Table based routing can be used to implement fault tolerant routing by reconfiguring table entries. The article shows how table entries can be computed efficiently and how the reconfiguration process can be organized to function reliably even in presence of transmission errors. With proper choice of logical hierarchy the reconfiguration process takes less than one third of the time required by Ariadne the state of the art approach for non hierarchical networks. The additional hardware overhead for fault tolerant routing table reconfiguration amounts to only 6 of the chip area of a network switch.,,,
S0045790615000221," Government agencies and many non governmental organizations often need to publish sensitive data that contain information about individuals . The sensitive data or private data is an important source of information for the agencies like government and non governmental organization for research and allocation of public funds medical research and trend analysis . The important problem here is publishing data without revealing the sensitive information of individuals . This sensitive or private information of any individual is essential to several data repositories like medical data census data voter registration data social network data and customer data . In this paper a personalized anonymization approach is proposed which preserves the privacy while the sensitive data is published . The main contributions of this paper are three folds the definition of the data collection and publication process the privacy framework model and personalized anonymization approach . The experimental analysis is presented at the end it shows this approach performs better over the distinct l diversity measure probabilistic l diversity measure and k anonymity with t closeness measure . 
",It prevents homogeneity skewness similarity and background knowledge attacks. The privacy is ensured while publishing sensitive data. Only fewer partitioning need to be done for a stronger privacy requirement. It gives better efficiency over the previous approaches.,,,
S0045790614002560," Botnets continue to be used by attackers to perform various malicious activities on the Internet . Over the past years many botnet detection techniques have been proposed however most of them can not detect botnets in an early stage of their lifecycle or they often depend on a specific command and control protocol . In this paper we propose BotGrab a general botnet detection system that considers both malicious activities and the history of coordinated group activities in the network to identify bot infected hosts . BotGrab tracks suspected hosts participating in some coordinated group activities and calculates a negative reputation score for each of them based on the history of their participation in these activities . A suspected host will be identified as being bot infected if it has a high negative reputation score or performs some malicious activities while having a low negative reputation score . We demonstrate the effectiveness of BotGrab to detect various botnets including HTTP IRC and P2P based botnets using a testbed network consisting of some bot infected hosts . 
",A novel negative reputation system is proposed to detect bot infected hosts. It considers both malicious activities and history of coordinated group activities. A proposed online incremental clustering technique facilitates the online learning. The negative reputation threshold can adjust the sensitivity of the system. It can successfully detect various botnets with a high DR and a low FAR.,,,
S0045790614002687," This paper focuses on the design of a novel low power twelve transistor static random access memory cell . In the proposed structure two voltage sources are used one connected with the bit line and the other one connected with the bitbar line in order to reduce the swing voltage at the output nodes of the bit and the bitbar lines respectively . Reduction in swing voltage reduces the dynamic power dissipation when the SRAM cell is in working mode . Low threshold voltage transmission gate and two high threshold voltage sleep transistors are used for applying the charge recycling technique . The charge recycling technique reduces leakage current when the transistors change its state from sleep to active and active to sleep modes . Reduction in leakage current causes the reduction in static power dissipation . Stability of the proposed SRAM has also improved due to the reduction in swing voltage . Simulation results of power dissipation access time current leakage stability and power delay product of the proposed SRAM cell have been determined and compared with those of some other existing models of SRAM cell . Simulation has been done in 45nm CMOS environment . Microwind 3.1 is used for schematic design and layout design purpose . 
",A novel low power 12T MTCMOS based SRAM cell is proposed. Charge recycling technique used for reducing the current leakage during transition mode. Use voltage sources to reduce the dynamic power dissipation. Improving the stability of SRAM cell.,,,
S0045790615000713," Traffic accidents are a fact of life . While accidents are sometimes unavoidable studies show that the long response time required for emergency responders to arrive is a primary reason behind increased fatalities in serious accidents . One way to reduce this response time is to reduce the amount of time it takes to report an accident . Smartphones are ubiquitous and with network connectivity are perfect devices to immediately inform relevant authorities about the occurrence of an accident . This paper presents the development of a system that uses smartphones to automatically detect and report car accidents in a timely manner . Data is continuously collected from the smartphone s accelerometer and analyzed using Dynamic Time Warping and the Hidden Markov Models to determine the severity of the accident reduce false positives and to notify first responders of the accident location and owner s medical information . In addition accidents can be viewed on the smartphone over the Internet offering instant and reliable access to the information concerning the accident . By implementing this application and adding a notification system the response time required to notify emergency responders of traffic accidents can potentially reduce the response time which may help in reducing fatalities . 
",Use smartphones with their built in smart sensors to automatically detect report car accidents. Reporting is done as soon as an accident happens. Data is continuously collected from the smartphone s accelerometer. Data is intelligently analyzed using Hidden Markov Models HMM and Dynamic Time Warping DTW . Data analysis determines the severity of the accident and reduces false positives. Once the accident is confirmed an SMS is sent to the police medical services family members. The SMS includes the accident s location severity and owner s medical information. The application will notify the emergency responders within seconds of the accident. The application will help reduce the response time and perhaps help in reducing fatalities.,,,
S0045790615000348," In this study a novel single image based dehazing framework is proposed to remove haze artifacts from images through local atmospheric light estimation . We use a novel strategy based on a physical model where the extreme intensity of each RGB pixel is used to define an initial atmospheric veil . Across bilateral filter is applied to each veil to achieve both local smoothness and edge preservation . A transmission map and a reflection component of each RGB channel are constructed from the physical atmospheric scattering model . The proposed approach avoids adverse effects caused by the error in estimating the global atmospheric light . Experimental results on outdoor hazy images demonstrate that the proposed method produces image output with satisfactory visual quality and color fidelity . Our comparative study demonstrates a higher performance of our method over several state of the art methods . 
",A novel strategy is proposed based on the minimum and maximum of each RGB pixel. The concept of local atmosphere light veil is defined in this study. The reflection component of the scene is calculated accurately with imaging physical model. The proposed method yields even better results than the state of the art techniques.,,,
S0045790614003097," Collaboration between mobile nodes is significant in Mobile Ad Hoc Networks . The great challenges of MANETs are their vulnerabilities to various security attacks . Because of the lack of centralized administration secure routing is challenging in MANETs . Effective secure routing is quite essential to protect nodes from anonymous behaviours . Game theory is currently employed as a tool to analyse formulate and solve selfishness issues in MANETs . This work uses a Dynamic Bayesian Signalling Game to analyse strategy profiles for regular and malicious nodes . We calculate the Payoff to nodes for motivating the particular nodes involved in misbehaviour . Regular nodes monitor continuously to evaluate their neighbours by using the belief evaluation and belief updating system of the Bayes rule . Simulation results show that the proposed scheme could significantly minimize the misbehaving activities of malicious nodes and thereby enhance secure routing . 
",Collaboration between mobile nodes is significant in Mobile Ad Hoc Networks. Dynamic Bayesian Signalling Game model is proposed to enhance secure routing and motivate effective cooperation among nodes. A novel scheme of payoff formulation is developed to motivate the misbehaving nodes. We used a belief updating system to update a node s belief in terms of action chosen message sent and strategy chosen. The proposed scheme could significantly minimize the misbehaving activities of malicious nodes and thereby enhance secure routing.,,,
S0045790615000269," This paper presents an analysis of a mobile manipulator movement executing a pick up task . The robot has to reach a target point with its end effector . The configuration of the manipulator and the pose of the mobile robot define the inputs of the problem . The random profile approach is applied to deal with the aforementioned issue . The trajectory which minimizes the execution time of the task is generated . Furthermore the manipulability index is introduced in the optimization process in order to allow a comfortable configuration of the manipulator to reach the target point . The obtained results in free and cluttered environments are presented and discussed . The feasibility of the computed trajectories is also validated against experimentation in the case of the RobuTER mobile robot carrying the Ultra Light Manipulator . 
",Reaching Target point by the end effector of a mobile manipulator. The optimized trajectory minimizing the execution time of the task is generated. Introduction of manipulability index for configuring the manipulator far from singularities. The feasibility of the computed trajectories is validated against experimentation on a real robot.,,,
S0045790615001780," Indefinite and uncontrollable growth of cells leads to tumors in the brain . The early diagnosis and proper treatment of brain tumors are essential to prevent permanent damage to the brain or even patient death . Accurate data regarding the position of the tumor and its size are essential for effective treatment . Hence an entirely computerized automatic system to provide accurate tumor data is compulsory for physicians . Such developments are necessary to diagnose brain tumors during brain surgery . Brain magnetic resonance images are proposed for the detection and segmentation of the tumor region via a completely automatic and highly accurate method . The approach discussed in this paper employs an adaptive neuro fuzzy inference system based on the automatic seed point selection range . The pixels intensity of the proposed algorithm is not dependent on the tumor type . The tumor s segmentation results are evaluated based on various criteria including similarity index overlap fraction extra fraction and positive predictive value which corresponded to values of 0.817 0.817 0.182 and 0.817 respectively in this study . These results indicate that the approach proposed in this study performs better compared to many conventional processes . The significance of this work is the differentiation of brain abnormalities from the healthy brain tissue . 
",Diagnosis and proper treatment of brain tumors are essential to prevent permanent damage to the brain or even patient death. The brain magnetic resonance MR images are used to detect the severities of tumor detections. The performance analysis is measured on various criteria. This work is the differentiation of brain abnormalities from the healthy brain tissue.,,,
S0045790614002845," This study presents a new weak signal detection method based on the van der Pol Duffing oscillator . The principle of the proposed method is described . A weak signal is detected through the transition from the chaotic to the periodic state . Numerical simulation shows that the van der Pol Duffing oscillator is sensitive to a weak signal under strong noise conditions . Several aspects of the proposed method including the noise influence influence of different frequency signals and influence of the phase shift are studied in detail . Results indicate that the application of the van der Pol Duffing oscillator to weak signal detection is feasible . 
",Proposed van der Pol Duffing oscillator based weak signal detection method. The signal to noise ratio of the weak signal can be detected 45dB. Compared to Duffing oscillator more robust against the different frequency signal.,,,
S0045790615002050," Image contrast enhancement and brightness preservation are fundamental requirements for many vision based applications . However these are two conflicting objectives when the image is processed by histogram equalization approaches . Current available methods may not provide results simultaneously satisfying both requirements . In this work a pipelined approach including color channel stretching histogram averaging and re mapping is developed . By using stretching color information from a scene is restored . Averaging against a uniform distribution enables the output image to recover the information lost . Furthermore histogram re mapping reduces artifacts that often arise from the equalization procedure . The technique also employs a search process to find optimal algorithmic parameters such that the mean brightness difference between the input and output images is minimized . The effectiveness of the proposed method was tested with a set of images captured in adverse environments and compared against available methods . High performing qualitative and quantitative results were obtained . 
",A histogram equalization method is proposed to preserve original image brightness. A histogram averaging technique is developed to recover image information lost. A histogram remapping technique is used to reduce artifacts introduced to images. An optimization minimizes the brightness change between input and output images. Results show that image brightness is preserved while image contrast is enhanced.,,,
S0045790615000701," This paper presents our ongoing efforts toward the development of a multi agent distributed framework for autonomous control of mobile manipulators . The proposed scheme assigns a reactive agent to control each degree of freedom of the manipulator a hybrid agent to control the mobile base and a supervisory agent to coordinate and synchronize the work of the control agents . Each control agent implements a Simulation Verification technique to optimize locally and independently from the other agents a predefined objective function . The final goal consists of bringing the end effector as close as possible to imposed operational targets . Different simulation scenarios are described and carried out for the case of RobuTER ULM robot with and without considering failures of some articulations of the manipulator or the mobile base . Results show that the main advantage of the proposed approach is that the system pledges a fault tolerant response to some breakdowns without needing any specific additional treatment . it represents the situation of the imposed Target it represents the initial current situation of the end effector of the robot in the absolute frame . The first three values are the position of the end effector the last three represent its orientation angles it corresponds to the new situation of the end effector after the Move Up Move Down movement of the joint i of the manipulator it represents the new situation of the end effector of the robot after the forward backward turn left turn right movement of the mobile base it represents the initial current final situation of the mobile base it represents the new situation of the mobile base after the forward backward turn left turn right movement carried out by the Mobile base agent it corresponds to the initial current final configuration of the manipulator joints it corresponds to the new configuration of the manipulator joints after the Move Up Move Down movement carried out by the Joint agent it represents the initial current final value of the objective function 
",We propose a generic multi agent control architecture for mobile manipulators. Robot complex mathematical models are not required so computing time is reduced. The strategy is fault tolerant to breakdowns without needing specific treatments. The proposed approach works in 3D environments with high dof mobile manipulators. Accuracy of the proposed approach is good comparatively to classical approaches.,,,
S0045790615000075," In this paper a new approach of edge based quantization for the compression of gray scale images using an Adaptive Block Truncation Coding technique is proposed to improve the compression ratio with high picture quality . Quantization is done based on the edge information contained in each block of pixels of the image . Conventional BTC method retains the visual quality of the reconstructed image but it shows some artifacts near the edges . In conventional BTC and variants same quantization is done for all pixel values with different block sizes so that CR is static for images with a fixed block size . But in the case of proposed method since the quantization is done based on the edge information CR become dynamic and consequently achieves better visual quality with better CR . The experimental analysis based on subjective and quantitative analysis proved that the proposed method outperforms other BTC variants . 
",An Adaptive block truncation coding technique ABTC EQ is proposed. Edge image is taken from the input image and divides it into blocks of pixels. Quantization is done based on the edge information of each of these blocks. Bi clustering is done for the non edge block and tri clustering for the edge block. Experimental analysis shows that ABTC EQ outperforms BTC and its variants.,,,
S0045790615004309," Sample variations are one of the main problems associated with speaker recognition . Most approaches use multiple templates in the gallery database . But this requires enormous memory space . In order to minimize classification errors and intra class variations adaptive online and offline template update methods using vector quantization and Gaussian mixture model are proposed . Online and offline feature update as well as model update techniques are considered here . Feature update utilizes the vector quantization approach while Gaussian mixture model approach is considered for model updating . The proposed methods automatically update the feature in accordance with the biometric sample variations over time and they continually adapt the templates based on semi supervised learning strategies . Experiments with 50 subjects reveal that the proposed template update strategies improve the recognition accuracy and reduce the classification errors for voice recognition systems even under sample variations . 
",Created a new database for speaker template updating. MFCC super template for speaker recognition is proposed. Proposed an online and offline MFCC feature and GMM based model update. Secondary template for speaker template model update is also suggested.,,,
S0045790615001494," Manycore CMP systems are expected to grow to tens or even hundreds of cores . In this paper we show that the effective co design of both the network on chip and the coherence protocol improves performance and power meanwhile total area resources remain bounded . We propose a snoopy aware network on chip topology made of two mesh of tree topologies . Reducing the complexity of the coherence protocol and hence its resources and moving this complexity to the network leads to a global decrease in power consumption meanwhile area is barely affected . Benefits of our proposal are due to the high throughput and low delay of the network but also due to the simplicity of the coherence protocol . The proposed network and protocol minimizes communication amongst cores when compared to traditional solutions based either on 2D mesh topologies or in directory based protocols . 
",A high throughput low latency network for snoop based cache coherence protocol is proposed. Execution time and power consumption are reduced meanwhile area over head is kept low. Benefits rely on the synergy between the snoop protocol and the high throughput network.,,,
S0045790615000750," Recently audio steganography has become an important covert communications technology . This technology hides secret data in a cover audio without perceptual modification of the cover audio . Most of the existing audio steganography techniques are unsuitable for real time communication . Although field programmable logic array technologies offer parallel processing in hardware that can improve the speed of steganographic systems the research activities in this area are very limited . This paper presents a parallel hardware architecture for dual mode audio steganography based FPGA technology . The proposed DMAS reconfigures the same hardware blocks in both hiding and recovery modes to reduce the hardware requirements . It has been successfully implemented on a Xilinx XC6SLX16 FPGA board to occupy only 97 slices . Furthermore it processes data simultaneously at an operating frequency of up to 58.82MHz and accomplishes full message retrieval at an embedding rate of 25 with an audio quality above 45dB in terms of signal to noise ratio . 
",We propose concurrent hardware architecture for real time audio steganography. We implement and test the proposed hardware on Xilinx XC6SLX16 FPGA board. The implemented hardware requires only 97 slices and consumes less than 148mW. The implemented hardware processes data simultaneously with frequency up to 58.82. Full data retrieval at embedding rate of 25 of the cover audio size.,,,
S0045790614002328," This work presents an analysis on efficiency of solar energy harvesting circuits focused on low power low voltage sensor platforms . Two different approaches were tested in order to operate a solar panel closely to its maximum power point . The first circuit precisely matches the solar cells with the batteries . The second one is based on a Maximum Power Point Tracker chip . The paper addresses the circuits design and evaluation . Two tests were performed outdoors under two different irradiance conditions . Although the MPPT chip may be efficient for a variety of low power devices experiments have shown that it did not extract more energy from the environment than the directly coupled circuit . A mathematical energy consumption analysis shows that in both cases the directly coupled circuit is more efficient . Therefore this work shows that there is still a lack of industry solutions for low power low voltage solar harvesting circuits . 
",Two solar energy harvesting circuits have been tested for low power applications. The directly coupling technique has been compared with an MPPT integrated circuit. A mathematical energy consumption analysis shows both circuits efficiency. For all tests performed the directly coupled circuit has been more efficient. Solar cell s input current simulations have confirmed the experimental results.,,,
S0045790615003043," This paper introduces a new scheme for encrypting images with a few details based on wavelet fusion . In this scheme the image with a few details to be encrypted is fused with another image that is rich in details utilizing the Discrete Wavelet Transform prior to encryption . The fusion is a pre processing step to change the homogeneity of flat areas in the images having a few details . RC6 or chaotic Baker map encryption are then performed on the fused image . Encryption with chaotic Baker map is just a permutation algorithm that can not perform well on flat areas of the images because the permutation yields approximately the same intensities . So circular shifts on pixels are performed on the fused image prior to chaotic encryption to remove flat areas or reduce the degree of homogeneity . Chaotic encryption is then performed in the wavelet domain to increase the degree of diffusion . Several metrics are used in this paper for performance evaluation of the suggested ciphering schemes like visual inspection histogram test encryption quality analysis and diffusion analysis . The robustness of the suggested image ciphering schemes is tested in the presence of noise before decryption . Simulation results demonstrated that the suggested image ciphering schemes provide a secure and effective way for encrypting images with few details . 
",This paper introduces a new scheme for encrypting images with few details based on wavelet fusion.. The fusion is a pre processing step to change the homogeneity of flat areas in the images having a few details. RC6 or chaotic Baker map encryption are then performed on the fused image. Encryption with chaotic Baker map is just a permutation algorithm that cannot perform well on flat areas of the images because the permutation yields approximately the same intensities. So circular shifts on pixels are performed on the fused image prior to chaotic encryption to remove flat areas or reduce the degree of homogeneity. The robustness of the suggested image ciphering schemes is tested in the presence of noise before decryption. Simulation results demonstrated that the suggested image ciphering schemes provide a secure and effective way for encrypting images with few details.,,,
S0045790614002110," In this study the Quality of Service needed to support service continuity in heterogeneous networks is achieved by a Distributed Multi Agent Scheme based on cooperation concepts and an awareness algorithm . A set of problem solving agents autonomously process local tasks and cooperatively interoperate via an in cloud blackboard system to provide QoS and mobility information . A Q Learning awareness algorithm calculates the exceptive rewards of a handoff to all access networks . These rewards are then used by problem solving agents to determine what actions must be performed . Agents located in the integrated IMS 4G Cloud networks handle service continuity by using a handoff mechanism . Through operations and cooperation among active agents these phases select a policy for predictive and anticipated IP Multimedia Subsystem handoff management . Compared with conventional IMS handoff management the proposed DMAS scheme achieves shorter handoff delay and better QoS for real time service applications . 
",Quality of Service supports service continuity in heterogeneous networks is achieved by agent based concepts. The shorter handoff delay and better QoS for real time service applications is proposed. The QoS mechanism and the intelligent agent are required for cooperative QoS awareness networking. The QoS management mechanism based on agents develops a cost effective manner in IMS 4G Cloud network.,,,
S0045790616300180,"Wireless body area networks WBANs consist of tiny sensors that enable monitoring the health status of a person. quality of service QoS is a major challenge for WBANs due to the importance of vital sign information. Therefore many QoS based medium access control MAC protocols and technologies have been developed to overcome this problem. Standardization of various technologies and protocols must be addressed. ISO IEEE 11073 personal health data standards aim to provide interoperability between healthcare devices and technologies. This paper presents a new QoS aware cross layer MAC protocol based on the ISO IEEE 11073 standards that employs a slot allocation scheme multi channel architecture priority mechanism admission control and cross layer solution. The proposed MAC protocol has been modeled and simulated by OPNET Modeler. In addition the proposed MAC protocol is compared with standard technologies and recent protocols in the literature and it achieves better results for end to end delay packet loss ratio and throughput parameters. 
",The proposed MAC protocol guarantees QoS requirements of all subscribers as described in the ISO IEEE 11073 standards. A slot allocation scheme priority classification admission control mechanism and channel allocation with bit error rate reduction mechanism are described to ensure QoS. The proposed MAC protocol is compared with standard technologies and recent protocols in the literature and it achieves better results for end to end delay packet loss ratio and throughput parameters.,,,
S0045790615000695," Ubiquitous environments are often considered as highly dynamic environments and contextual information can change at runtime . The user interface should provide the right information for a given user considering runtime context . Such an objective can be achieved only when we deduce the user s requirements in terms of information and present it to the user according to his current context of use . The overall objective of our research is to generate a user interface adapted to the current context of use for critical fields . This paper explores some key issues related to the architecture of context aware applications . A formal approach for the analysis of pervasive Human Computer System is presented . XML Petri nets are used to model the pervasive HCS . The proposed approach is illustrated with a case study which presents a hypoglycemic diabetic patient in a smart hospital . 
",We propose a formal approach for specification of pervasive user interface. We define a Petri nets modeling language for pervasive Human Computer System. We demonstrate that the accuracy of the generated interface depends on the context and task s validity. We demonstrate the validity of our approach by a case study in a medical domain. We propose a system for monitoring diabetic patient in smart hospital.,,,
S0045790615001251," This study develops a hierarchical scheme with three processing layers for human behavior recognition . The proposed scheme is an audio based approach that employs a microphone array of the Kinect sensor for sensing and acquiring acoustic data to classify human behavior . The three processing layers namely the feature layer acoustic event classification layer and specific behavior recognition layer are interrelated and the sensing data fusion Gaussian mixture model with a classification tree and state machine diagram to regulate human behavior are employed in these three layers respectively . With enhanced performance of the feature and acoustic event classification layers the proposed scheme exhibits increased human behavior classification accuracy . Human behavior recognition experiments were conducted in a research office and three specific office behavior modes namely Laboratory meeting Classmate chatting and Laboratory study interaction were effectively classified using the proposed method . 
",Proposed three layered hierarchical human behavior recognition uses only audio data. The use of Kinect sensor microphone array for data capture and fusion is explored. The proposed scheme increased recognition robustness compared with conventional GMM.,,,
S0045790616300568,"The paper describes the development of an algorithm for detecting and classifying MRI brain slices into normal and abnormal. The proposed technique relies on the prior knowledge that the two hemispheres of a healthy brain have approximately a bilateral symmetry. We use the modified grey level co occurrence matrix method to analyze and measure asymmetry between the two brain hemispheres. 21 co occurrence statistics are used to discriminate the images. The experimental results demonstrate the efficacy of our proposed algorithm in detecting brain abnormalities with high accuracy and low computational time. The dataset used in the experiment comprises 165 patients with 88 having different brain abnormalities whilst the remaining do not exhibit any detectable pathology. The algorithm was tested using a ten fold cross validation technique with 10 repetitions to avoid the result depending on the sample order. The maximum accuracy achieved for the brain tumors detection was 97.8 using a Multi Layer Perceptron Neural Network. 
",Automated algorithm for detecting normality or abnormality in MRI brain scans. A new Modified Grey level Co occurrence Matrix MGLCM method is presented to extract second order statistical texture features for discriminating brain abnormality. MGLCM generates efficient texture feature that is used for measuring the symmetry of MRI brain scan than the tradition GLCM. An accuracy of 97.8 was achieved using MGLCM with 9 orientations and 1 distance.,,,
S0045790614002584," Openflow a novel Software Defined Network technology is developing rapidly and has already been utilized in many fields . It facilitates decoupling between the control and forwarding plane enabling users to code the network functions easily and replace the traditional high cost network functions devices . The FlowTable of Openflow its base of operating the network packets consists of many flow entries and is stored in the Ternary Content Addressable Memories of the Openflow switch . When the FlowTable occupies the entire storage space of the Openflow switch and more flow entries are added the delete operation on the TCAMs increases and the latency and loss of packets deteriorates this is the primary issue . In order to solve this problem this paper proposes a distributed storage framework which stores the FlowTable in multiple Openflow switches equipped with small TCAMs . To conclude this paper simulates the algorithms used in the framework and builds a testbed . The experimental results prove the framework s feasibility and successful performance . 
",SDN enabled network functions are limited by the switch TCAM capacity. We propose a distributed storage framework of FlowTable. Our framework can achieve high availability and failure resilience. There is a tradeoff between cost and high availability failure resilience. We achieve the storage capacity without inviting large communication overhead.,,,
S0045790615000099," Data compression is a challenging process with important practical applications . Specialized techniques for lossy and lossless data compression have been the subject of numerous investigations during last several decades . Previously we studied the use of the pseudo distance technique in lossless compression of color mapped images and its parallel implementation . In this paper we present a new technique to improve compression gain of PDT . We also present a parallelized implementation of the new technique which results in substantial gains in compression time while providing the desired compression efficiency . We demonstrate that on non dithered images PDT2 outperforms PDT by 22.4 and PNG by 29.3 . On dithered images PDT2 achieves compression gains of 7.1 over PDT and 23.8 over PNG . We also show that the parallel implementation of PDT2 while compromising compression less than 0.3 achieves near linear speedup and utilization of Intel Hyper Threading technology on supported systems improves speedup on average 18 . 
",A pseudo distance technique PDT2 for lossless compression of images is given. Parallelization of the technique is presented. The performance of the PDT2 in Hyper Threading is evaluated.,,,
S0045790615003389," A nonlinear pull in behavior analysis of a cantilever nano actuator was carried out and an Euler Bernoulli beam model was used in the examination of the fringing field and the surface and Casimir force effects in this study. In general the analysis of an electrostatic device is difficult and usually complicated by nonlinear electrostatic forces and the Casimir force at the nanoscale. The nonlinear governing equation of a cantilever nano beam can be solved using a hybrid computational scheme comprising differential transformation and finite difference to overcome the nonlinear electrostatic coupling phenomenon. The feasibility of the method presented here as applied to the nonlinear electrostatic behavior of a cantilever nano actuator was analyzed. The numerical results for the pull in voltage were found to be in good agreement with previously published results. The analysis showed that the surface effects had significant influence on the dynamic characteristics of the cantilever nano actuator. 
",Nano actuator is considered to incorporate the influence of the surface the fringing field and the Casimir force effects using an Euler Bernoulli beam model. Nonlinear governing equation is solved using the hybrid computational scheme comprising the differential transformation and the finite difference methods. The results indicate that for a constant detachment length the beam thickness and surface effect are linked and it can be seen that the influence of surface effects decreases with increasing beam thickness.,,,
S0045790615000506," Critical infrastructures require protection systems that are both flexible and efficient . Flexibility is essential to capture the multi organizational and state based nature of these systems efficiency is necessary to cope with limitations of hardware resources . To meet these requirements we consider a classical protection environment featuring subjects that attempt to access the protected objects . We approach the problem of specifying the access privileges held by each subject . Our protection model associates a password system with each object the password system features a password for each access privilege defined for this object . A subject can access the object if it holds a key matching one of the passwords in the password system and the access privilege corresponding to this password permits to accomplish the access . Password systems are implemented as hierarchical bidimensional one way chains . Trade offs are possible between the memory requirements for storage of a password system and the processing time necessary to validate a key . 
",A significant protection problem is to specify the access privileges held by each active subject on the protected objects. We associate a password system with each object featuring a password for each access privilege. Access privileges are partitioned into privilege levels for distribution and revocation. Password systems are implemented as hierarchical bidimensional one way chains.,,,
S0045790616000136,"This paper proposes a face recognition system based on a steerable pyramid transform SPT and local binary pattern LBP for e Health secured login. In an e Health framework patients are sometimes unable to identify themselves by traditional login modalities such as username and password. Automatic face recognition can replace the conventional login modalities if the recognition system is robust. In the proposed system SPT can decompose a face image into several subbands of different scales and orientations and LBP can encode the subbands in binary texture pattern. Therefore SPT LBP scheme represents a face image in a robust way that includes multiple information sources from different scales and orientations. The proposed system is evaluated on the facial recognition technology FERET database. According to the results the proposed system achieves 99.28 recognition in fb set 80.17 in dup I set and 79.54 in dup II set. 
",A metric learning model directly calibrating measured numerics of feature data. A 2D semi metric model by relaxing one degree of freedom. Simple and intuitively natural models solvable by convex quadratic programming.,,,
S0045782514004812," Issues related to space time adaptivity for a class of nonlinear and time dependent problems are discussed . The dG methods are adopted for the time integration and the a posteriori error control is based on the appropriate dual problem in space time . One key ingredient is to decouple the error generation in space and time with a hierarchical decomposition of the discrete space of dual solutions . The main idea put forward in the paper is to increase the computational efficiency of the adaptive scheme by avoiding recursive adaptations of the whole time mesh rather the space mesh and the time step defining each finite space time slab are defined in a truly sequential fashion . The proposed adaptive strategy is applied to the coupled consolidation problem in geomechanics involving large deformations . Its performance is investigated with the aid of a numerical example in 2D . 1 . Compute based on that was computed for the previous time slab . Then solve for the enhanced dual solution from the decoupled dual problem whereby it is noted that has been replaced by the background dual solution as the load for the current space time slab . Compute the error contributions SOL FEM etc . Check the stopping criterion in If TOL then exit and take a new time step . Refine the space mesh the time interval or both If FEM FEM FEM then refine in space and time uniformly . Else if FEM FEM then refine in space Else if FEM FEM then refine in time 
",A novel approach for space time adaptive finite element analysis is presented. Global error control is obtained by the technique of solving a dual problem. Space and time errors are estimated using a hierarchical decomposition of the dual. Recursive adaptations of the whole space time mesh is avoided. The coupled consolidation problem in geomechanics is considered as an application.,,,
S0045790615001366," The use of computational accelerators such as NVIDIA GPUs and Intel Xeon Phi processors is now widespread in the high performance computing community with many applications delivering impressive performance gains . However programming these systems for high performance performance portability and software maintainability has been a challenge . In this paper we discuss experiences porting applications to the Titan system . Titan which began planning in 2009 and was deployed for general use in 2013 was the first multi petaflop system based on accelerator hardware . To ready applications for accelerated computing a preparedness effort was undertaken prior to delivery of Titan . In this paper we report experiences and lessons learned from this process and describe how users are currently making use of computational accelerators on Titan . 
",Lessons learned are given for moving applications to the GPU based Titan system. A carefully managed readiness effort is essential to preparing for new hardware. Applications typically require code restructuring to port to accelerators. Exposing more parallelism and minimizing data traffic are common porting themes. Performance gains of 2X 7X have been realized for application codes on Titan.,,,
S0045790614002407," An important issue to be addressed when data are to be published is data privacy . In this paper the problem of data privacy based on a prominent privacy model Anonymous is addressed . Our scenario is that when a new dataset is to be released there may be at the same time datasets that were released elsewhere . A problem arises because some attackers might obtain multiple versions of the same dataset and compare them with the newly released dataset . Although the privacy of all of the datasets has been well preserved individually such a comparison can lead to a privacy breach which is a so called incremental privacy breach . To address this problem effectively we first study the characteristics of the effects of multiple dataset releases with a theoretical approach . It has been found that a privacy breach that is subjected to an increment occurs when there is overlap between any parts of the new dataset with any parts of an existing dataset . Based on our proposed studies a polynomial time algorithm is proposed . This algorithm needs to consider only one previous version of the dataset and it can also skip computing the overlapping partitions . Thus the computational complexity of the proposed algorithm is reduced from to only pn where p is the number of partitions n is the number of tuples and m is the number of released datasets . At the same time the privacy of all of the released datasets as well as the optimal solution can be always guaranteed . In addition experiment results that illustrate the efficiency of our algorithm on real world datasets are presented . 
",An efficient algorithm is developed to prevent incremental privacy breach. Only the most recent previously released data is required for privacy preservation. The solution can always be guaranteed the optimal result.,,,
S0045782514002874," In level set methods for structural topology and shape optimization the level set function gradients at the design interface need to be controlled in order to ensure stability of the optimization process . One popular way to do this is to enforce the level set function to be a signed distance function by periodically using initialization schemes which is commonly known as re initialization . However such re initialization schemes are time consuming as additional partial differential equations need to be solved in every iteration step . Furthermore the use of re initialization brings some undesirable problems for example it may move the zero level set away from the expected position . This paper presents a level set method with distance suppression scheme for structural topology and shape optimization . An energy functional is introduced into the level set equation to maintain the level set function to close to a signed distance function near the structural boundaries meanwhile forcing the level set function to be a constant at locations far away from the structural boundaries . As a result the present method not only can avoid the need for re initialization but also can simplify the setting of the initial level set function . The validity of the proposed method is tested on the mean compliance minimization problem and the compliant mechanisms synthesis problem . Different aspects of the proposed method are demonstrated on a number of benchmarks from the literature of structural optimization . 
",A level set method with distance suppression scheme is developed. An energy functional is developed and built into the level set equation. The need for re initialization can be eliminated. The initialization of the level set function can be simplified.,,,
S0045790615001548," In this paper we present two novel interference management stratagems for coexisting one primary user and multiple secondary users by exploiting the unused spatial directions at PU . The cognitive stations sense their environment to determine the users they are interfering with and adapt to it by designing the corresponding precoders using interference alignment in order to avoid causing performance degradation to nearby PU and SUs . The first proposed approach judiciously designs the set of precoders based on an improved version of minimum weighted leakage interference algorithm . However there are still leftover interference signals in SUs desired signal space due to the limited iterative times in the first algorithm . To tackle this problem another scheme combining the first one and power allocation method at the secondary stations is developed . Numerical results validate the effectiveness of the proposed algorithms . 
",The cognitive interference alignment feasibility condition is investigated. A new convex optimization problem of the CIA problem is then obtained. An efficient algorithm is then proposed to solve the convex problem based on matrix distance. An improved algorithm considering power allocation is derived. The convergences of our algorithms are analyzed.,,,
S0045790615003511," Spectrum allocation scheme in cognitive radio networks becomes complex when multiple CR users concomitantly need to be allocated new and suitable bands once the primary user returns . Most existing schemes focus on the gain of individual users ignoring the effect of an allocation on other users and rely on the periodic sensing and transmission cycle which reduces spectrum utilization . This paper introduces a scheme that exploits collaboration among users to detect PU s return which relieves active CR users from the sensing task and thereby improves spectrum utilization . It defines a Capacity of Service metric based on the optimal sensing parameters which measures the suitability of a band for each contending user and takes into consideration the impact of allocating a particular band on other band seeking users . The proposed scheme significantly improves capacity of service reduces interference loss and collision and hence enhances dynamic spectrum access capabilities . 
",A scheme is proposed to allocate bands among multiple cognitive radio users. Suitability Throughput Gain defines the suitability of a band for contending users. The scheme takes into account the impact of allocating a band to a user on others. It offers higher user satisfaction less collision and better spectrum utilization.,,,
S0045790615004346," In this paper we present a set of server selection schemes that includes all crucial components of resources available at each relevant node of the network to reach at the server selection decision and at the same time attempts to maximise the system wide resource utilisation . First in the series a server centric scheme is proposed that formulates the problem as distributed constraint optimisation problem and solves it using Fast max sum algorithm . Though it does not scale well in large network scenario yet it provides an estimate of the optimal values of performance metrics achievable under a specific scenario . Next a client centric scheme is proposed that outperforms the existing ones but does not guaranty to attain optimal Nash Equilibrium . This motivates us to design another client centric scheme that can avoid the price of anarchy by attaining superior NE of mix strategies . 
",An efficient server centric server selection scheme is proposed for small scenario. A client centric scheme is proposed that outperforms the existing ones. The client centric scheme does not guaranty optimal Nash Equilibrium NE . A Mixed strategy MS based solution is proposed that can attain more efficient NE. The MS based scheme performs better than existing and previously proposed schemes.,,,
S0045790615001536," Cooperative spectrum sensing is a process of achieving spatial diversity gain to make global decision for cognitive radio networks . However accuracy of global decision effects owing to the presence of malicious users nodes during cooperative sensing . In this work an extended generalized extreme studentized deviate method is proposed to eliminate malicious nodes such as random nodes and selfish nodes in the network . The random nodes are carried off based on sample covariance of each node decisions on different frames . Then the algorithm checks the normality of updated soft data using Shapiro Wilk test and estimates the expected number of malicious users in cooperative sensing . These are the two essential input parameters required for classical GESD test to eliminate significant selfish nodes accurately . Simulation results reveal that the proposed algorithm can eliminate both random and frequent spectrum sensing data falsification attacks in cooperative sensing and outperforms the existing algorithms . 
",Cooperative sensing with most frequent data falsification SSDF attacks. Proposed an extended generalized extreme studentized deviate EGESD method. It can eliminate both random and selfish attacks in cooperative sensing. It estimates two important input parameters required for GESD test. It is reliable and has low misdetection probability compared to existing algorithms.,,,
S0045790614002213," A fundamental challenge in the design of Wireless Sensor Network is the proper utilization of resources that are scarce . The critical challenge is to maximize the bandwidth utilization in data gathering and forwarding from sensor nodes to the sink . The main design objective is to utilize the available bandwidth efficiently . The proposed Bandwidth Efficient Cluster based Data Aggregation algorithm presents the solution for the effective data gathering with in network aggregation . It considers the network with heterogeneous nodes in terms of energy and mobile sink to aggregate the data packets . The optimal approach is achieved by intra and inter cluster aggregation on the randomly distributed nodes with the variable data generation rate . The proposed algorithm uses the correlation of data within the packet for applying the aggregation function on the data generated by nodes . BECDA shows significant improvement in PDR and throughput as compared to the state of the art solutions . 
",Cluster head is elected according to the highest energy among the CM number of neighbor nodes with one hop connectivity. Network uses random distribution of heterogeneous nodes with mobile sink. Packets are generated at variable rate by each node and aggregated at CH and then at sink. It uses the perfectly compressible aggregation function on data generated by nodes. The performance is measured in terms of PDR and throughput to show the effective utilization of bandwidth.,,,
S0045790615003055," Signals associated with eye blinks are orders of magnitude larger than electric potentials generated on the scalp because of cortical activity . These and other such non cortical biological artifacts spread across the scalp and contaminate the Electroencephalogram . We present here a novel approach for efficient detection and effective suppression of these artifacts using single channel EEG data by combining Ensemble Empirical Mode Decomposition along with Principal Component Analysis . We present a methodology for ocular artifact suppression by performing EEMD on the contaminated EEG data segment to get the intrinsic mode functions and subsequent elimination of artifacts by automatic selection of particular principal components which capture ocular artifact features after using PCA on IMFs . 
",A novel methodology for ocular artifact suppression in EEG data using EEMD with PCA. The proposed method eliminates the ocular artifacts from the measured EEG without using reference electrooculogram channel. The proposed method exhibits effective suppression of ocular artifact with low distortion compared to wavelet approach.,,,
S0045790616000112," High concentration photovoltaic modules employing high efficiency III V solar cells promise greater system level efficiency than conventional photovoltaic systems . Nevertheless the output power of an HCPV system is very sensitive to rapidly fluctuating tracking errors and weather patterns . The fractional open circuit voltage based maximum power point tracking technique benefits from simplified processing circuits with speed response . To investigate the feasibility of using the FOCV technique for MPP estimation on HCPV modules a theoretical model and simulation are presented in this study . A MATLAB based MJSC circuit model of an HCPV module with buck type converter and load is proposed and validated . In addition the magnitude of the optical loss caused by Fresnel lens shape deformation and air mass ratio is modeled and quantized . The FOCV technique is then employed and compared with the conventional perturb and observe method on the HCPV module under varying irradiance and temperature conditions to study its effectiveness . The results suggest that the FOCV technique could help an HCPV module to attain greater power efficiency . 
",A performance evaluation model of a high concentration photovoltaic module is proposed. The output power performance is simulated and assessed by MATLAB Simulink software. The magnitude of the optical loss caused by Fresnel lens shape deformation and AM is modeled and quantized. Two maximum power tracking techniques are simulated and compared. The simulation results suggest that the fractional open circuit voltage technique has better performance.,,,
S0045790615001287," Power consumption is emerging as one of the main concerns in the High Performance Computing field . As a growing number of bioinformatics applications require HPC techniques and parallel architectures to meet performance requirements power consumption arises as an additional limitation when accelerating them . In this paper we present a comparative study of optimized implementations of the Non negative Matrix Factorization that is widely used in many fields of bioinformatics taking into account both performance and power consumption . We target a wide range of state of the art parallel architectures including general purpose low power processors and specific purpose accelerators like GPUs DSPs or the Intel Xeon Phi . From our study we gain insights in both performance and energy consumption for each one of them under a number of experimental conditions and conclude that the most appropriate architecture is usually a trade off between performance and energy consumption for a given experimental setup and dataset . 
",Performance and energy consumption study of NMF is performed on different systems. General purpose CPUs yield better execution times at the cost of high energy rates. Low power architectures offer better trace off in energy and performance.,,,
S0045790616300064,"Next generation wireless technology supports notably high data rates and smart hand held devices together with data processing which creates a great challenge for moving these contents from server to client. The classical issue of mobile data synchronization for high speed data networks can be addressed through Software Defined Networking SDN Control. The proposed IDBSync Improved Database Synchronization mechanism expedites data synchronization between the server side and mobile database in a SDN setup. The tables of the synchronization server are maintained in the control plane including replicas of the data tables at the server and client devices collected from the data planes for application of the synchronization policy. The IDBSync uses BaSyM Batch Level Synchronization Methodology to group similar cases for synchronization to considerably reduce network usage and energy consumption in a heterogeneous environment. Comparative performance analysis of the proposed mechanism is performed with reference to commercial approaches. 
",SDN based batch level synchronization. Reduced response time. Live test results of obstacle and non obstacle mode of 2 G and 3 G networks. IDBSync consumes fewer discharging units than the existing solutions. Vendor independency.,,,
S0045790614002146," This paper presents a novel scheme to implement blind image watermarking based on the feature parameters extracted from a composite domain including the discrete wavelet transform singular value decomposition and discrete cosine transform . Multiple bits can be embedded into a single image block by adjusting designated parameters via a progressive quantization index modulation technique . The quantization with respect to the feature parameters obtained in the DWT SVD DCT domain leads to efficient watermark extraction without referring to the original image . Experimental results show that the embedded watermarks exhibit exceptional robustness against image compression using JPEG and JPEG2000 coding standards . 
",A blind image watermarking scheme exploiting the DWT SVD DCT features is presented. The proposed PQIM reaches a trade off between robustness and imperceptibility. Multiple watermarks can be embedded into a host image. The watermarks exhibit exceptional robustness against JPEG and JPEG2000 compression.,,,
S0045790616000070," In wireless LANs handover is usually performed based on either signal power or congestion level . However considering only the congestion level could be insufficient for handover since it may cause traffic loss . Therefore besides the load of access points it is necessary to consider the physical conditions of different WLANs for performing a seamless handover . This article introduces a novel scheme for seamless handover of IPTV streams in WLAN carrying IPTV traffic called Physical Constraint and Load Aware handover . The PCLA can compute the load of APs for congestion detection purposes . In PCLA a mobile node chooses the best network considering signal strength bit error rate in the relevant environment and the congestion of APs for making a seamless handover . The simulation results show the appropriateness of PCLA in improving handover performance . 
",A seamless handover mechanism for IPTV services in WLAN is proposed. The mechanism relies on received signal strength congestion level and bit error rate. New method for estimation of congestion level of each access point is proposed.,,,
S0045790615002797," Typical MAC protocols for IEEE 802.11 based ad hoc networks employ a direct transmission strategy whenever the transmitter can directly reach the receiver . While such a design enjoys simplicity it limits the number of admitted transmissions in a given neighborhood . Due to the performance anomaly of 802.11 based wireless networks transmission with low data rates occupy the shared medium for longer periods of time . Occupying the transmission medium for a longer period of time results in less available transmission time for other nodes which consequently reduces the number of transmitted packets during the same period of time leading to a reduction in network throughput . To improve the overall network throughput we present a cooperative multi channel MAC protocol for single hop wireless mobile ad hoc networks that attempts at computing the path with the minimum required transmission time for a given source destination pair . The proposed protocol attempts at improving network performance by means of cooperative communications . According to our approach if the one hop path between communicating nodes supports low data rate the source selects a multi hop path to the destination such that the total amount of required transmission time is minimized . Through simulations we show that our proposed protocol achieves significant throughput and fairness improvement compared to the standard IEEE 802.11 based protocol . 
",We propose a cooperative channel assignment and packet forwarding protocol. Our protocol reduces the transmission times for the low data rate direct link users. Our protocol selects multi hop path with min transmit time even direct path exists. Results show that our protocol notably improves network throughput over IEEE 802.11.,,,
S0045790615001378," Malaria one of the deadliest diseases is responsible for nearly 627 000 deaths every year . It is diagnosed manually by pathologists using a microscope . It is time consuming and subjected to inconsistency due to human intervention so computerized image analysis for diagnosis has gained importance . In this article an edge based segmentation of erythrocytes infected with malaria parasites using microscopic images has been developed to facilitate the diagnostic process . The color space transformation and Gamma equalization reduce the effects of colors and correct luminance differences of images . Fuzzy C means clustering is applied to extract infected erythrocytes which is further processed for the final segmentation . The experimental results showed that the proposed method can gain 98 93.3 98.65 and 90.33 of sensitivity specificity prediction value positive and prediction value negative respectively . In conclusion the proposed method provides a consistent and robust method of edge based segmentation of parasite infected erythrocytes using microscopic images for diagnosis . 
",Edge based segmentation of erythrocytes infected with malaria parasites in microscopic images. Gamma equalization for contrast enhancement can improves the segmentation performance. Proposed method outperforms others traditional edge segmentation methods. To facilitate segmentation performance a confusion matrix is proposed. The proposed method is useful for pathologists in the diagnosis of malaria.,,,
S0045790616300301," X ray angiography images are widely used to identify irregularities in the vascular system . Because of their high spatial resolution and the large amount of images generated daily coding of X ray angiography images is becoming essential . This paper proposes a diagnostically lossless coding method based on automatic segmentation of the focal area using ray casting and shapes . The diagnostically relevant Region of Interest is first identified by exploiting the inherent symmetrical features of the image . The background is then suppressed and the resulting images are encoded using lossless and progressive lossy to lossless methods including JPEG LS JPEG2000 H.264 and HEVC . Experiments on a large set of X ray angiography images suggest that our method correctly identifies the Region of Interest . When compared to the case of coding with no background suppression the method achieves average bit stream reductions of nearly 34 and improvements on the reconstruction quality of up to 20 dB SNR for progressive decoding . 
",An automatic segmentation of the diagnostically relevant focal area of X ray angiography images was developed. A background suppression coding strategy was proposed based on the accurate segmentation results. Our segmentation method identifies the Regions of Interest with an average Dice Similarity Coefficient of 0.98 with respect to manual segmentation. The coding performance improvement reaches 34 compared to the case of coding with no background suppression.,,,
S0045790616300027," In this paper we consider the performance of a cellular uplink multi user feedback multiple input multiple output system assisted by joint transmitter receiver design and polarization multiplexing . PM is realized with the aid of dual and triply polarized antennas . At the transmitter we take advantage of only the individual user s channel impulse responses obtained through feedback channels that endure noise fading and delay to construct the preprocessing matrix while at the receiver the post processing matrix construction relies on the perfect CIRs of all the users . In multi user UL MIMO transmissions multiple access interference and inter antenna interference can severely degrade the system s performance . Our study shows that the joint Tx Rx is capable of completely eliminating the IAI as well as the MAI when it employs perfect CIRs based preprocessing and post processing . On the other hand noise fading and delay tainted quantized CIRs based preprocessing results in noticeable performance degradation due to imperfect removal of IAI . Nevertheless our results demonstrate that when the preprocessing is based on the quantized CIRs obtained through ideal feedback channels the resultant achievable symbol error rate and sum capacity remain close to that obtained with the perfect CIRs based design . 
",SER and capacity performance of a uplink feedback MIMO system is studied under. Perfect CIRs based joint Tx Rx design with polarization multiplexing ideal feedback. Quantized CIRs aided joint Tx Rx with polarization multiplexing noisy feedback. Quantized CIRs based joint Tx Rx with polarization multiplexing fading feedback. Quantized CIRs assisted joint Tx Rx with polarization multiplexing delayed feedback.,,,
S0045790616300039," The Binary Offset Carrier modulated signals have been introduced in global navigation satellite systems . They possess good frequency compatibility with existing Binary Phase Shift Keying signal . However this type of modulation creates multiple side peaks in auto correlation function . Therefore it is hard to make full use of its ranging capability . To alleviate this problem this paper analyzes the Double Estimator Technique thoroughly from a general formulation of two dimension correlation and proposes a new subcarrier aided code tracking approach . In addition based on the fact that the pseudo random noise code can reflect the absolute delay an adaptive code monitoring scheme is investigated to tap the potential tracking accuracy . Utilizing BOCsin signal Monte Carlo simulations demonstrate that the proposed method improves the tracking performance with respect to thermal noise significantly . 
",A general analytical expression of the two dimension correlation of DET is formulated. The crosstalk between SLL and DLL will degrade the ranging capability of BOC signals. The subcarrier aided code tracking SACT technique is proposed. The adaptive adjustment by code monitoring is proposed to tap the potential ranging capability.,,,
S0045790615002839," Broadcasting is the simplest form of communication in which nodes disseminate the same information simultaneously to all of their neighbors . Broadcasting has been widely used in many types of networks including wireless networks wireless sensor networks and mobile ad hoc networks . Likewise these networks broadcasting is also used in cognitive radio networks to accomplish various tasks such as spectrum sensing spectrum sharing spectrum management and spectrum mobility . This article investigates and provides a comprehensive overview of various broadcasting strategies that have been proposed so far for cognitive radio networks . Moreover it provides a detailed study of broadcast storm problem in CRNs . Finally it discusses issues challenges and future research directions for broadcasting strategies in CRNs . 
",We give an overview of various broadcasting strategies proposed so far for cognitive radio networks CRNs . We identify required key characteristics of broadcasting strategies in CRNs. We propose a comprehensive and detailed classification of broadcasting strategies in CRNs. We provide a detailed study of broadcast storm problem in CRNs. We discuss the possible scenarios for the generation of broadcast storm problem and its related challenges in CRNs.,,,
S0045790616300015,"Switching Devices such as IGBT used in Pulse Width Modulation PWM Inverter feeding an induction motor often suffer from different types of incipient faults like improper contact points poor connections and problematic solder joints. These are due to ageing or prolonged operation in unfriendly environments. These faults need to be detected at their initial stages to prevent subsequent spreading of faults. In the present work different variations of the above mentioned faulty cases in a PWM Inverter have been studied by recording three phase inverter output current profiles and converting them to Concordia patterns. It has been observed that the Concordia patterns are quite different in shapes for different types of faults. A suitable image based shape descriptor has been applied to extract relevant information from these Concordia patterns. Finally Nearest Neighbor Algorithm is employed on this information to identify the nature and location of faults. Performance of the algorithm is found to be quite satisfactory when its results are compared with two more related algorithms. 
",Incipient faults in Inverter of Induction motor drive are detected and classified. Concordia pattern is obtained from 3 phase output current of faulty inverter. Concordia pattern is converted to binary image. Shape context based Nearest Neighbor algorithm is used to classify the Concordia image.,,,
S0045790615000245," In this article we investigate the performance of a coded multiple input multiple output multi carrier system in underwater communication where acoustic interference and ambient noise are the two major channel impairments . The channel model considered in this work is based on the shallow water model that has eleven paths . In our work the acoustic signal is spread with the aid of a high rate spreading sequence to alleviate the effects of acoustic interference . At the receiver non linear detectors based on Zero Forcing and Minimum Mean Square Error are employed for signal detection . The simulation results reveal that the system under consideration with the MMSE detector provides better performance in terms of the achievable bit error rate than the system with the ZF detector while achieving higher data rates . 
",We investigate the performance of VBLAST assisted MIMO OFDM system in underwater communications. Performance is analyzed with non linear MUD when communicating over multi path environment. BER performance is compared for coded and uncoded ZF OSIC and MMSE OSIC. Coded multi carrier MIMO system out performs uncoded system in terms of BER performance. Coded multi carrier MIMO system with MMSE OSIC provides better BER performance than system with coded ZF OSIC.,,,
S0045790616300179," In this paper we propose a Parzen window entropy based spectrum sensing algorithm for enhancing the signal to noise ratio wall of cognitive radio primary user detection . We compute the information entropy using a non parametric Kernel Density Estimation method . Single node sensing is extended to cooperative sensing using the weighted gain combining fusion method . The weights of WGC technique are computed using a Differential Evolution algorithm and compared with the log likelihood ratio method . In addition the detection performance of the proposed Parzen window entropy is compared with Shannon entropy and energy detection techniques . We consider a DVB T signal with Additive White Gaussian Noise subjected to Rayleigh fading under noise uncertainty as a primary user signal for simulation . The simulation result reveals that in the case of a single node and cooperative sensing the proposed method achieves SNR wall of 19 dB and 24 dB respectively at the probability of false alarm 0.1 . 
",A Parzen window entropy detection technique for spectrum sensing. Performance is compared with energy and Shanon entropy detection method. In multi node weights are evaluated using the Differential evolution algorithm. Single node sensing achieved SNR wall of 19 db at Pd 0.9 and Pf 0.1. Multi node sensing achieved SNR wall of 24 dB at Pd 0.9 and Pf 0.1.,,,
S0045790615003705," The present study integrates information system engineering and management methodologies to solve a real life case problem . We develop an agricultural product recommendation service on a mobile platform and then to understand users acceptance to confirm that it can be used to solve the problem . For consumers making the decision to purchase is complex and can be full of contradictions and conflicts . This raises the need to design a product recommendation service that uses multiple criteria to assist consumers . This study employs a modified Elimination Et Choice Translating Reality method determine a ranking order which will assist consumers in deciding which agricultural product to buy . Two major findings are proposed . First we identified five criteria that assist consumers making buying decisions regarding agricultural products . Second we find that when the system is established on a mobile platform perceived ease of use does not play a critical role in user acceptance . 
",Proposed an MCDM based approach which contributes to mobile commerce marketing. Identified five criteria to assist purchasing decisions regarding agricultural products. Perceived ease of use does not play a critical role regarding mobile app.,,,
S0045790615002086," Power efficiency is a crucial issue for embedded systems and effective power profiling and prediction tools are in high demand . This paper presents a cloud based power profiling tool for recording system calls and their associated parameters to predict hardware power consumption when running target applications . Based on hardware power consumption and system profiling from the operating system kernel the proposed network model can effectively summarize running behavior of the target applications and the relationship among system calls . This model is also used to develop an energy efficient cluster scheduling for user inactive processes to reduce the power consumption and extend the service time of embedded systems . These profiling data can be integrated into a cloud model to be maintained by software designers or OS developers to accommodate power estimation and scheduling data for a variety of platforms . 
",We predict power consumption of embedded systems to extend their service time. A cloud model is proposed. A power aware scheduling organizes system calls into DAGs.,,,
S0045790614002377," Glossy is a reliable and low latency flooding mechanism designed primarily for distributed communication in wireless sensor networks . Glossy achieves its superior performance over tree based wireless sensor networks by exploiting identical concurrent transmissions . WSNs are subject to wireless attacks aimed to disrupt the legitimate network operations . Real world deployments require security and the current Glossy implementation has no built in security mechanisms . In this paper we explore the effectiveness of several attacks that attempt to break constructive interference in Glossy . Our results show that Glossy is quite robust to approaches where attackers do not respect the timing constraints necessary to create constructive interference . Changing the packet content however has a severe effect on the packet reception rate that is even more detrimental than other physical layer denial of service attacks such as jamming . We also discuss potential countermeasures to address these security threats and vulnerabilities . 
",Presentation of novel attacks aiming to break Glossy s constructive interference. Results showing Glossy s robustness except for attacks on relay counter. Discussion of potential security solutions to protect Glossy based networks.,,,
S0045790616300143," Computational complexity and power consumption are prominent issues in wireless telemonitoring applications involving physiological signals . Because of its energy efficient data reduction procedure compressed sensing emerged as a promising framework to address these challenges . In this work a multi channel CS framework is explored for multi channel electrocardiogram signals . The work focuses on the successful joint recovery of the MECG signals using a low number of measurements by exploiting the correlated information across the channels . A CS recovery algorithm based on weighted mixed norm minimization is proposed that exploits the joint sparsity of MECG signals in the wavelet domain and recovers signals from all the channels simultaneously . The proposed WMNM algorithm follows a weighting strategy to emphasize the diagnostically important MECG features . Experimental results on various MECG databases show that the proposed method can achieve superior reconstruction quality with high compression efficiency as compared to its non weighted counterpart and other existing CS based ECG compression techniques . 
",A weighted mixed norm minimization based joint sparse recovery algorithm is proposed for multichannel ECG signals. Algorithm aims to improve the joint signal recovery in compressed sensing based WBAN applications. It is based on a weighing rule that emphasizes the clinically important ECG features. Required measurements could be reduced significantly without compromise with the reconstruction accuracy. The approach leads to higher compression efficiency with reduced on node computations and power consumptions in resource constrained WBANs.,,,
S0045790615004267,"An economical and portable on line motor condition monitoring system based on an advanced microprocessor with a graphic user interface for abnormality detection is described. Both electrical and vibrational signal analysis for fault detection of motors were applied. A Cortex M4 microcontroller and its built in high speed analog to digital converter were used to process the on line voltage current and vibration signals of the motors during operation. The acquired signals were digitally filtered through an infinite impulse filter and then fast Fourier transform for spectral analysis was applied for abnormal pattern recognition. The analytic results were displayed in real time on an embedded touch screen monitor to instantly provide users with the motor s operational conditions. Finally a prototype was devised and verified through onsite experimentation. Comparing the results to commercial tools showed similar spectral characteristics the difference for electrical and vibration signals was less than 10 . Thus the system proposed shows promising ability and feasibility for online use. 
",A set of economical handheld online real time data acquisition devices for motor abnormal detection based on vibrational and electrical detection methods was developed. The errors of analysis results from electrical and vibration signal were less than 10 comparing our proposed device with commercial industrial computer. This is a low cost small size and simple use motor condition monitoring system.,,,
S0045790615000270," The main challenge in image denoising is how to preserve the information such as edges and textures to get satisfactory visual quality when improving the signal to noise ratio . In this paper we propose a hybrid filter bank for denoising based on wavelet filter bank and quincunx diamond filter bank . The noisy image is decomposed into different subbands of frequency and orientation using DMeyer wavelet . The quincunx diamond filter bank is designed from finite impulse response filters using Kaiser window which is applied on the detail subband of wavelet filter bank . The directional subband coefficients are modeled with Gaussian scale mixture model . The Bayes least squares estimator is used to obtain the denoised detail coefficients from the noisy image decomposition . Experimental results show that the new method performs spatial averaging without smoothing edges and thereby enhances the visual quality and peak signal to noise ratio . 
",We propose a new denoising algorithm using wavelet and quincunx diamond filter bank. The subband coefficients are modeled with Gaussian scale mixture model. Bayes least means square is used to obtain the denoised coefficients. The new method does spatial averaging without smoothing the edges.,,,
S0045782516301347," This paper explores the application of maximum entropy methods to time harmonic acoustic problems . Max ent basis functions are mesh free approximants that are constructed observing an equivalence between basis functions and discrete probability distributions and applying Jaynes s maximum entropy principle . They are continuous and therefore they are particularly suited for the resolution of Helmholtz problems where classical finite element methods show a poor accuracy in the high frequency region . In addition it was recently shown that max ent approximants can be blended with isogeometric basis functions on the boundary of the domain . This preserves the correct representation of the boundary like in Isogeometric Analysis with the advantage that the discretization of the interior of the domain is straightforward . In this paper the max ent mathematical formulation is reviewed and then some numerical applications are studied including a 2D car cavity geometry defined by B spline curves . In all cases if the same nodal discretization is used finite elements results are significantly improved . 
",Time harmonic acoustic problems are simulated with maximum entropy meshless methods. Maximum entropy basis functions handle dispersion errors better than finite elements. Short wavelength propagation problems can be studied with coarser discretizations. A blending with Isogeometric functions is possible on the boundary of the domain. Several examples are studied including a 2D car cavity model defined by B Splines.,,,
S0045790615002633," The multifaceted nature of cyber physical systems needs holistic study methods to detect essential aspects and interrelations among physical and cyber components . Like the systems themselves security threats feature both cyber and physical elements . Although to apply divide et impera approaches helps handling system complexity to consider just one aspect at a time does not provide adequate risk awareness and hence does not allow to design the most appropriate countermeasures . To support this claim in this paper we provide a joint application of two model driven techniques for physical and cyber security evaluation . We apply two UML profiles namely SecAM and CIP VAM in combination . In such a way we demonstrate the synergy between both profiles and the need for their tighter integration in the context of a reference case study from the railway domain . 
",Cyber physical systems need holistic methods to discover relationships between physical and cyber components. Security threats attempt to both physical and cyber elements. Model driven techniques accounting for joint physical and cyber security modelling and evaluation are explored. Synergies between physical and cyber security UML profiles remarked in a case study of the railway domain.,,,
S0045790615002359," To enhance confidentiality and reduce the number of arrayed waveguide grating routers a dynamic scrambling scheme is proposed in which cyclic and free spectral range properties of AWG routers are employed to use a maximal length sequence code as the signature address code . In this paper a changing codeword mechanism is presented in which a flexible dynamic scrambling algorithm is embedded in the encoding and decoding codebook to control the state of both the wavelength and spatial optical switch matrices . Hence the code family and code changing flexibility of the proposed scheme increases substantially because of the multiple effects of the wavelength and spatial switch matrices . Compared with the existing dynamic reconfigurable AWG based schemes without multiple changing codewords in the FSR groups and spatial domains the confidentiality of the proposed scheme is approximately 104 times higher than that of schemes without scrambling codeword mechanisms . 
",Scrambling cryptography with 2 D wavelength spatial scheme in physical layer. Increasing cardinality with cyclic and FSR property of shared AWG router. Evaluating the power assumption to prove feasible wavelength spatial network. Larger cardinality and dynamic scrambling improve significantly FTTx security. Dynamic scrambling is superior to traditional scheme using confidential analysis.,,,
S0045790616000069," In this paper the performance of a turbo coded triply Polarized multiple input multiple output MIMO aided Code Division Multiple Access CDMA is investigated for Stanford University Interim SUI and Long term Evolution LTE channel model specifications. The minimum mean square error algorithm based on ordered successive interference cancellation OSIC Multi user detection MUD technique is implemented at each mobile station for diminishing the effects of multi stream interference MSI . We observe from the simulation results that a better bit error rate BER performance with less signal to noise ratio SNR is exhibited by our considered coded system in comparison with an un coded system. The simulation results reveal that the MIMO CDMA system with triply Polarized antenna structure requires a higher SNR than a uni Polarized antenna system for achievement of the same BER. However it provides the advantage of replacement of three uni polarized antennas by a single triply Polarized antenna thereby achieves a higher data rate with reduced size of MS. 
",We investigate the BER performance of turbo coded MIMO CDMA system using polarization diversity for DL transmission. Performance is analyzed using MMSE OSIC algorithm when communicating over SUI and LTE channel models specifications. BER Performance of turbo coded MIMO system is compared with uncoded system and Found that coded system provides superior performance. Our considered coded triply Polarized MIMO CDMA system achieves higher data rate with reduced size of Mobile unit.,,,
S0045790615001068," Modalities other than GPS need to be employed to localize mobile sensor node enabled subjects in indoor conditions . The location of some mobile nodes needs to be computed precisely while this may not be essential for the remaining nodes . We have proposed the concept of graded precision localization which allows mobile nodes to localize themselves to heterogeneous precision levels in a common framework . With graded precision localization we define a modular node for the subjects specify deployment strategies customizable for each site and propose a framework for evaluating site specific localization performance . We comprehensively evaluate graded precision localization with experiments and simulation for indoor conditions and highlight its advantages over similar systems . 
",Graded precision localization GPL offers a range of localization precision levels. GPL for user modules is achieved with an infrastructure of hotspots and radio nodes. A user module prototype is built with a laptop a Cricket node and a mobile phone. Performance of GPL for indoor spaces is discussed with simulation and experiments. An analytical Markov model for evaluating GPL for constrained spaces is proposed.,,,
S0045790615003213," The wireless sensor networks deployed in hostile environments suffer from a high rate of node failures . Such failures may convert a fully connected sensor network into multiple disjoint sub networks leading to the network partition problem . The placement of relay nodes is the only way to restore the lost connectivity because these devices compared to the sensor nodes have a higher energy backup with a longer communication range . In this paper a new solution is proposed to heal the network partition problem in the wireless sensor network . The solution is based on a zero gradient point inside the convex hull polygon . The proposed solution is compared with various naive approaches along with existing state of the art solutions that is the Spider Web 1C heuristic and Steiner minimum tree based optimal relay node placement algorithm . The simulation experiment results confirm the effectiveness of our proposed approach . 
",Two naive approaches and two state of the art heuristics are implemented along with our proposed solution to recover the lost connectivity of the partitioned WSN. The proposed solution considers a global zero gradient point inside the convex hull polygon. The proposed solution works perfectly for any number of disjoint segments. The proposed solution shows better performance compared to the other well known state of the art approaches.,,,
S0045790614002717," Service composition is an evolving approach that increases the number of applications of cloud computing by reusing existing services . However the available methods focus on generating composite services from a single cloud which limits the benefits that are derived from other clouds . This paper proposes a novel COMbinatorial optimization algorithm for cloud service COMposition that can efficiently utilize multiple clouds . The proposed algorithm ensures that the cloud with the maximum number of services will always be selected before other clouds which increases the possibility of fulfilling service requests with minimal overhead . The experimental results demonstrate that the COM2 successfully competes with previous multiple cloud service composition algorithms by examining a small number of services which directly relates to execution time without compromising the number of combined clouds . 
",The COM2 algorithm efficiently considers multiple clouds while composing services. The proposed algorithm successfully competes with previous algorithms. Low examined service number is achieved without impacting the combined cloud number.,,,
S0045790615003924," This work studies a digital hardware implementation of a radial basis function neural network RBF NN Firstly the architecture of the RBF NN which consists of an input layer a hidden layer of nonlinear processing neurons with Gaussian function an output layer and a learning mechanism is presented. The supervising learning mechanism based on the stochastic gradient descent SGD method is applied to update the parameters of RBF NN. Secondly a very high speed IC hardware description language VHDL is adopted to describe the behavior of the RBF NN. The finite state machine FSM is applied for reducing the hardware resource usage. Thirdly based on the electronic design automation EDA simulator link a co simulation work by Simulink and ModelSim is applied to verify the VHDL code of RBF NN. Finally some simulation cases are tested to validate the effectiveness of the proposed digital hardware implementation of the RBF NN. 
",A high accuracy method to compute the Gaussian function in the radial basis function neural network RBF NN is proposed. The computational accuracy of the propose method in the Gaussian function and the overall RBF NN can reach up10 6 10 7. Fast computational ability for a complicated RNF NN is presented by using the digital hardware implementation. Succeed in making the co simulation environment between Simulink and ModelSim to verify the correctness and the effectiveness of the VHDL Very high speed IC Hardware Description Language code for a 3 5 1 RBF NN. The application of the VHDL code for a 3 5 1 RBF NN to the dynamic identification in the linear system and the PMSM drive system are successfully demonstrated.,,,
S0045790615002827," This paper introduces a new fast integer based algorithm to convert the RGB color representation to HSV and vice versa . The proposed algorithm is as accurate as the classical real valued one . The use of only integer operations increases performance and portability . Performance measurement results show a speed gain of about two times when compared with the classical C language implementation on PC platforms . Lookup tables are not involved thus the memory usage is minimal . The resulting HSV color can be packed into 48 bits . The proposed method can safely replace the commonly used floating point implementation . 
",Conversion algorithm between 24 bit RGB and integer HSV is proposed. The algorithm is as accurate as the classical real valued one. Performance measurement results show a speed gain of about two times. The resulting HSV color can be packed into 48 bits. Floating point variables and Lookup tables are not used.,,,
S0045790614001918," Cognitive radio is an emerging technology in wireless communications for dynamically accessing under utilized spectrum resources . In order to maximize the network utilization vacant channels are assigned to cognitive users without interference to primary users . This is performed in the spectrum allocation module of the cognitive radio cycle . Spectrum allocation is a NP hard problem thus the algorithmic time complexity increases with the cognitive radio network parameters . This paper addresses this by solving the SA problem using Differential Evolution algorithm and compared its quality of solution and time complexity with Particle Swarm Optimization and Firefly algorithms . In addition to this an Intellectual Property of DE based SA algorithm is developed and it is interfaced with PowerPC440 processor of Xilinx Virtex 5 FPGA via Auxiliary Processor Unit to accelerate the execution speed of spectrum allocation task . The acceleration of this coprocessor is compared with the equivalent floating and fixed point arithmetic implementation of the algorithm in the PowerPC440 processor . The simulation results show that the DE algorithm improves quality of solution and time complexity by 29.9 and 242.32 19.04 and 46.3 compared to PSO and Firefly algorithms . Furthermore the implementation results show that the coprocessor accelerates the SA task by 76.79 105 and 5.19 6.91 compared to floating and fixed point implementation of the algorithm in PowerPC processor . It is also observed that the power consumption of the coprocessor is 26.5mW . 
",To maximize the network utilization spectrum allocation technique fairly allocates the channels to secondary users. SA problem is solved by Differential Evolution algorithm and compared the performance with PSO and FA. DE improved the quality of solution and time complexity by 29.9 242.32 and 19.04 46.3 compared to PSO and FA. We propose FPGA based coprocessor for DE SA IP and interfaced to PowerPC. The coprocessor accelerates SA task by 76.79 105x and 5.19 6.91x compared to float and fixed DE SA software.,,,
S0045790615003080," Graphical abstract Image graphical abstract interfacial area gas concentration drag coefficient bubble induced turbulence parameter bubble induced turbulence parameter first experimental model constant for dissipation rate second experimental model constant for dissipation rate gas bubble diameter diffusion coefficient E tv s number gravity vector Henry s constant turbulent kinetic energy mass transfer coefficient mass transfer rate from gas to liquid gas molecular weight bubbles per volume pressure reference pressure ideal gas constant bubble induced turbulence temperature gas phase velocity liquid phase velocity slip velocity volume fraction of gas volume fraction of the liquid phase gradient operator gas density liquid density liquid dynamic viscosity turbulent or eddy viscosity surface tension coefficient Prandtl number for kinetic energy Prandtl number for dissipation rate turbulent dissipation rate 
",The Euler Euler method was used to model the flow mixing inside a carbonating tank. Carbonation process increased at high gas and water inlet velocities. L shaped nozzle design increased the carbonation process. Mass transfer coefficient increased with the increase of the inlet gas velocities. Carbonation process was improved with the use of small gas bubbles.,,,
S0045790615004450," The analysis of the surface Electrocardiogram is the most extended non invasive technique in cardiological diagnosis . The ectopic beats are heart beats remarkably different to the normal beat morphology that provoke serious disturbances in electrocardiographic analysis . These beats are very common in atrial fibrillation causing important residua when ventricular activity has to be removed for atrial activity analysis . These beats may occur in both normal subjects and patients with heart disease and their presence represents an important source of error which must be handled before any other analysis . In this work a method is proposed to cancel out ectopics by classification between normal and abnormal beats . The systems is based on Radial Basis Function Neural Network . This new approach is compared to state of the art techniques for the ectopic classification and cancellation in the MIT database . The results clearly demonstrated the improved ECG beats classification accuracy compared with other alternatives and a very accurate reduction of ectopic beats together with low distortion of the QRST complex . 
",A method based on radial basis function system for cancelling out ectopic beat by classification between normal and abnormal beats is proposed. The proposed solution addresses for ECG recordings. Several experiments have demonstrated the enhancement of the proposed method in comparison to other techniques. The results suggest that clinical information can be maintained. This method provides the best approach for obtaining both ectopic beat reduction and low distortion of the signal recordings.,,,
S0045790616300246," Nowadays intelligent and pervasive environments are characterized by a great number of devices and sensors that develop continuously and capture enormous amounts of data . Designing a context aware system able to provide the most tailored services to users according to their behaviors preferences and needs is still a research challenge . In such environments although the context is very complex dynamic and full of data captured and produced users aspire to automatically receive contextualized services . The Cultural Heritage domain represents a domain where exchanged and produced data can be opportunely exploited by a set of applications and services in order to transform a static space into a smart environment . In this perspective this paper presents a context aware system named Context Evolution System able to represent and manage the evolution of the context through its instances such an evolution is driven by occurring events and opportunely modeled by a graph structure . To assess the proposed solution a Cultural Heritage case study of a real temporary art exhibition named the Beauty or the Truth and located in Naples is presented and discussed . 
",Model the context evolution by means of a graph approach. Design a Context Evolution System CES to manage the context evolution and the consequently tailoring of data and services. Improve user experience by means of this kind of system. Our apporach is suitable within the Cultural Heritage domain.,,,
S0045790615003146," Vehicular Cyber Physical Systems are the most popular systems of the modern era due to their abilities to disseminate the safety related information to the moving vehicles on time . For efficient data dissemination vehicles form a cluster with other vehicles in VCPS environment . But due to high velocity and constant topological changes cluster maintenance is one of the most difficult tasks to be performed in this environment . To address this issue in this paper we propose a novel Learning Automata based hybrid clustering scheme for vehicles in VCPS environment . We have improved our existing solution Energy Efficient Predictive Clustering approach by incorporating the future mobility prediction computed by LA stationed on the vehicles . For this purpose a Predictive Clustering Algorithm using Learning Automata is proposed . Extensive simulations are performed to evaluate the performance of the proposed scheme with respect to various metrics . Results obtained confirm the effectiveness of the proposed scheme in comparison to the existing EEPC scheme . 
",LA based predictive clustering is proposed by predicting the future mobility of the nodes in VANETs. Connecting dominating set is constructed to maintain connectivity of the vehicles. Predictive Clustering Algorithm using LA is proposed. The performance of the proposed scheme is evaluated in different network scenarios with respect to various metrics.,,,
S0045790615001573," In order to retrieve an image from a large image database the descriptor should be invariant to geometric transformations . It must also have enough discriminating power and immunity to noise for retrieval from a large image database . The Exponent moments descriptor has many desirable properties such as expression efficiency robustness to noise geometric invariance fast computation and multi level representation . In this paper we analyze the rotation scaling and translation invariant property of EMs and propose a content based image retrieval approach using invariant EMs . Experimental results show that the EMs can be used as an effective descriptor of global image content and the proposed retrieval approach yields higher retrieval accuracy than some current state of the art retrieval methods . 
",A new image descriptor namely Exponent moments EMs is introduced. The geometric invariant property of EMs is derived and analyzed. A EMs magnitudes based color image retrieval scheme is presented.,,,
S0045790615001299," Geometric distortions are more difficult to tackle than other types of attacks . It is a challenging work to design a robust image watermarking scheme against geometric distortions . In this paper we propose a robust digital image watermarking scheme based on local polar harmonic transform . The proposed scheme has the following advantages the stable and uniform image feature points are extracted by the improved speeded up robust feature detector in which the probability density gradient is utilized the affine invariant local feature regions are constructed adaptively according to the variation of local probability density and a new and effective 2D transform named polar harmonic transform is introduced to embed watermark in the digital image . Experiments are carried out on a digital image set of 100 images collected from Internet and the preliminary results show that the proposed image watermarking is not only invisible and robust against common image processing operations such as filtering noise adding and JPEG compression but also robust against the geometric distortions . 
",The stable and uniform image feature points are extracted by the improved SURF detector. The affine invariant local feature regions are constructed adaptively. A new and effective 2D transform is introduced to embed watermark in the digital image.,,,
S0045790615002311," In this paper a dynamic modeling simulation control and energy management of photovoltaic water pumping network system is presented . A fuzzy logic controller has been proposed for a real time control of the system . The controller generates the reference speeds needed for the pulse width modulation generator to control each DC DC boost converter taking into account the water levels in three tanks and the instantaneous value of the solar radiation . The main objectives of the fuzzy logic controller are the design of an adequate maximum power point tracker to extract the maximum power regulate the water in the three tanks and finally ensure the correct operation for all the conversion strings in order to optimize the quantity of pumped water . The system performance under different scenarios has been checked carrying out Matlab Simulink simulations using a practical load demand profile and real weather data and comparing them to another control algorithm . photovoltaic panel ideal factor normal operating cell temperature rated voltage short circuit current temperature coefficient of ISC constant of proportionality between G and Iph photocurrent PVP output voltage optimal values V electronic charge Boltzmann s constant cell temperature PVP shunt resistance optimal values of I reference of solar radiation temperature coefficient stator voltage stator current stator inductance rotor resistance mutual inductance mechanical speed of the machine electromagnetic torque reference speed of motor i level of tank I output flow of tank I DC link capacity band gap energy maximum power rated current serial cells open circuit voltage parallel cells solar radiation PVP output current ideality factors cell reverse saturation current reverse saturation current PV array series resistance cell reference temperature DC link voltage DC link capacity saturation current for Tr rotor voltage rotor current rotor inductance stator resistance coefficient of friction total inertia of the machine load torque duty cycle of DC inverter i input flow of pump i DC link voltage 
",A modeling and simulation of photovoltaic water pumping network are affected. An energy management and a control strategy of the system are presented. A real time control of photovoltaic water pumping network system is proposed. The optimization of the pumped water and the tanks regulation were ensured. An adequate maximum power point tracker should be controlled.,,,
S0045782516300925," Recently new families of mixed finite elements have been proposed to address the analysis of linear elastic bodies on regular grids adopting a limited number of degrees of freedom per element . A two dimensional mixed discretization is implemented to formulate an alternative topology optimization problem where stresses play the role of main variables and both compressible and incompressible materials can be dealt with . The structural compliance is computed through the evaluation of the complementary energy whereas the enforcement of stress constraints is straightforward . Numerical simulations investigate the features of the proposed approach comparisons with a conventional displacement based scheme are provided for compressible materials stress constrained solutions for structures made of incompressible media are introduced . 
",A topology optimization problem is formulated in terms of stresses only. An efficient mixed finite element is used to provide robustness and accuracy on regular grids. Stress constrained optimization is dealt with also in case of incompressible media. Stress constrained optimal layouts take advantage of the accuracy of the adopted fem. Peculiar optimal layouts are found in case of incompressible material and plane strain.,,,
S0045790615002268," In real time collaborative environments address space transformation strategy can be used to achieve consistency maintenance of shared documents . However as for the execution of compound operations they are firstly decomposed into primitive operations the relationships between the referencing objects and referenced objects are lost during the decomposition process . Besides the Undo operations in this environment are targeted at compound operations but not decomposed basic ones . However the traditional algorithms take primitive operation as the manipulation unit thus leading to semantic inconsistencies of compound Undo operations . This paper appends two history buffers to maintain the relationships between the original operations and the decomposed ones and introduces Retrace Undo VT Redo Retrace strategy to realize the consistency maintenance of compound operations . Also this paper introduces the version decomposition strategy describes the main algorithms of the compound Undo operations and analyses the validity of the strategy . Case analysis is given to show the effectiveness of the strategy . 
",We append two history buffers to maintain the relationships between original operations and decomposed ones. Combined with AST method the Retrace Undo VT Redo Retrace strategy is adopted to realize the consistency maintenance of compound operations. Based on multi version strategy the Undo Redo algorithms are proposed with case analyses to verify the effectiveness.,,,
S0045790615001147," An improved image magnification algorithm for gray and color images is presented in this paper to meet the challenge of preserving high frequency components of an image including both image edges and texture structures . In the proposed algorithm a new edge detection method that uses the well known Otsu automatic optimum thresholding is proposed to distinguish strong edge pixels . The parameters of the original directional cubic convolution interpolation algorithm which were selected based on training were eliminated . As a result our algorithm achieves more accurate edge detection better interpolation results and less computational complexity . Simulation results demonstrate that the improved algorithm can reconstruct the magnified image preserve edges and textures simultaneously and reduce common interpolation artifacts . Furthermore it generates higher visual quality of the magnified images and achieves higher peak signal to noise ratio structural similarity and feature similarity compared with other state of the art methods . 
",An improved version of DCCI algorithm is proposed to magnify gray and color images. A new method to detect strong edge pixels is proposed using Otsu thresholding. Parameters of the DCCI algorithm which selected based on training are eliminated. Visual results show that the proposed algorithm outperforms the compared methods. Better quantitative results in terms of time PSNR SSIM and FSIM are obtained.,,,
S0045790615001469," Inter Cell Interference from neighboring cells is the major challenge that degrades the performance of Orthogonal Frequency Division Multiple Access cellular mobile systems particularly for cell edge users . An efficient technique to mitigate ICI is the interference coordination . The most commonly ICI Coordination technique is the Fractional Frequency Reuse which effectively mitigates ICI by applying different reuse factors to Users Equipments situated in different regions in each cell . This paper presents a novel Self Organized FFR Resource Allocation scheme that automatically selects the optimal RA to inner and outer regions of the cell based on coordination between neighboring evolved NodeBs through a message passing approach over Long Term Evolution X2 interfaces . The performance of the proposed scheme is evaluated using MATLAB and compared with different combinations of RA as well as with frequency reuse 1 and reuse 3 schemes . Simulation results show that the proposed scheme improves the cell edge performance and achieves high degree of fairness among UEs compared to reference RA schemes . 
",We proposed a self organized dynamic resource allocation SORA scheme using FFR. SORA FFR scheme tracks the variation of UEs distribution through the LTE network. SORA FFR scheme dynamically allocates optimal resources to inner and outer regions. SORA FFR scheme improved throughput of CEUs and fairness performance.,,,
S0045790615004292,"Decimal arithmetic circuits based on IEEE 754 2008 standard commonly use 10 bit densely packed decimal DPD encoding of three binary coded decimal BCD digits. Binary coded chiliad BCC encoding as storage arithmetic efficient as DPD BCD equivalently packs three BCD digits. No unpacking packing to from BCD entailing extra delay power per each arithmetic operation required in case of DPD are necessary for BCC. Therefore while abiding to DPD standard we are motivated to design decimal arithmetic operators that accept BCC operands and produce BCC results. As such DPD data from memory or input devices are converted to BCC manipulated in BCC and stored in the BCC register file during multi operation decimal computations and converted back to DPD only on reporting results to memory or output devices. In this paper following a previous simple mixed BCC binary adder we design and synthesize more efficient ones and compare them with previous relevant BCD and BCC adders to show advantages in area and power. 
",Decimal arithmetic adders that accept BCC radix 1000 encoded operands and produce BCC results are proposed. Six different conditional speculation options are studied. The best proposed design show advantages in area 17 power 13 and PDP 14 measures over the best previous relevant work.,,,
S0045790615000592," As a part of character recognition character segmentation plays an important role in automatic license plate recognition system . In recent years lots of methods on CS have been proposed and they work well on their own datasets . However it is still challenging to segment characters from images with frame declination and quality degradation because of noises and overlapped connected or fragmented characters . In this paper we propose a two stage segmentation method for Chinese license plate . At the first stage a novel template matching method is presented using a harrow shaped filter bank and minimum response . It finds the locations of the segmenting points between characters roughly . Then the accurate segmentations between connected or overlapped characters are adjusted by a variant of A path finding algorithm at the second stage . Experiments on a challenging dataset including 2334 images demonstrate the effectiveness and efficiency of the proposed method . 
",Characters on license plate with interferences are cut using a two stage method. Plate frame is removed and vertical tilt is corrected before segmentation. A harrow shaped filter with values of 0 and 1 is designed for matching. Boundaries of the remarkable space are located accurately in initial segmentation. A algorithm is used for the precise segmentation of connected characters.,,,
S0045782516300111," This work is concerned with the development of an efficient and robust isogeometric Reissner Mindlin shell formulation for the mechanical simulation of thin walled structures . Such structures are usually defined by non uniform rational B splines surfaces in industrial design software . The usage of isogeometric shell elements can avoid costly conversions from NURBS surfaces to other surface or volume geometry descriptions . The shell formulation presented in this contribution uses a continuous orthogonal rotation described by Rodrigues tensor in every integration point to compute the current director vector . The rotational state is updated in a multiplicative manner . Large deformations and finite rotations can be described accurately . The proposed formulation is robust in terms of stable convergence behavior in the nonlinear equilibrium iteration for large load steps and geometries with large and arbitrary curvature and in terms of insensitivity to shell intersections with kinks under small angles . Three different integration schemes and their influence on accuracy and computational costs are assessed . The efficiency and robustness of the proposed isogeometric shell formulation is shown with the help of several examples . Accuracy and efficiency is compared to an isogeometric shell formulation with the more common discrete rotational concept and to Lagrange based finite element shell formulations . The competitiveness of the proposed isogeometric shell formulation in terms of computational costs to attain a pre defined error level is shown . 
",A rotation based isogeometric Reissner Mindlin shell formulation which is able to handle finite rotations and large deformations is presented. A continuous rotation of the director vector with multiplicative update ensures robust computations of arbitrarily curved structures. Three integration schemes are presented and compared in terms of computational costs and influence on accuracy. The efficiency of the proposed shell formulation is shown to be competitive to standard Lagrange based finite element shell formulations.,,,
S0045790616300271," In this paper we propose a cross layer framework for joint routing and rate adaptation in multi rate Multi Channel Multi Radio infrastructure Wireless Mesh Networks . These networks use MCMR capabilities of mesh routers to achieve high performance . However the MCMR nodes introduce interference in the multi hop mesh networks and can degrade QoS . Thus the design of routing metrics to improve the QoS has become an important research issue . Furthermore as the routing metric and rate adaptation decisions are closely related the joint approach is needed to improve the performance of the network . Towards this we analytically derive our routing metric using IEEE 802.11 Distributed Coordination Function basic access mechanism . Using this model we propose Passive Interference and Delay Aware routing metric which estimates the delay and interference by exploiting cross layer information . We extend the work by performing joint routing and rate adaptation . The simulation results using NS2 reveal that proposed framework improves throughput and delay compared to existing approaches with reduced routing overhead . 
",New routing metric is analytically derived using IEEE 802.11 DCF mechanism. Routing metric and rate adaptation decisions are strongly related. Joint routing and rate adaptation framework is proposed by exploiting cross layer information. Link quality is estimated using passive monitoring to minimize routing overhead. NS2 results reveals that joint framework performs better than existing approaches.,,,
S0045790615000166," Orthogonal Frequency Division Multiplexing system has lead to significant advancement in wireless communication systems . In OFDM system multi carriers are present . During modulation the sub carriers are added together with same phase which increases the value of Peak to Average Power Ratio . High PAPR leads to more interference and reduced resolution of analog to digital converter and digital to analog converter . The Partial Transmit Sequence is a popular technique used for PAPR reduction in OFDM systems . The modified PTS technique proposed in this paper overcomes the drawbacks of Original PTS by making use of Group Phase Weighting Factor and Recursive Phase Weighting Factor along with All Pass Filtering . Simulations show that the proposed scheme performs very well in terms of PAPR and achieves almost the same Bit Error Rate performance under Rayleigh fading channel . 
",The major drawback of OFDM system is its high PAPR. The drawback is overcome by use of GPW and RPW with All Pass Filtering technique. The GPW and RPW generate more optimum phase factors when implemented together. All Pass Filtering technique maintains magnitude response and optimum phase shift.,,,
S0045790615001317," One of the primary objectives of Wireless Sensor Network is to provide full coverage of a sensing field as long as possible . The deployment strategy of sensor nodes in the sensor field is the most critical factor related to the network coverage . However the traditional deployment methods can cause coverage holes in the sensing field . Therefore this paper proposes a new deployment method based on Multi objective Immune Algorithm and binary sensing model to alleviate these coverage holes . MIA is adopted here to maximize the coverage area of WSN by rearranging the mobile sensors based on limiting their mobility within their communication range to preserve the connectivity among them . The performance of the proposed algorithm is compared with the previous algorithms using Matlab simulation for different network environments with and without obstacles . Simulation results show that the proposed algorithm improves the coverage area and the mobility cost of WSN . 
",A new deployment approach for WSNs based on the multi objective immune algorithm is proposed. The approach rearranges sensors to remit the coverage holes and improves the network coverage. The approach is energy efficient and ensures the connectivity by limiting the mobility cost of sensors. Simulation experiments were completed in MATLAB correctly. The approach has many advantages comparing with other algorithms.,,,
S0045790615000336," Wireless multimedia sensor networks have been used for sensitive applications such as video surveillance and monitoring applications . In a WMSN storage and transmission become complicated phenomena that can be simplified by the use of compressed sensing which asserts that sparse signals can be reconstructed from very few measurements . In this paper memory efficient measurement matrices are proposed for a discrete wavelet transform discrete cosine transform hybrid approach based video compressed sensing . The performance of the framework is evaluated in terms of PSNR storage complexity transmission energy and delay . The results show that the proposed matrices yield similar or better PSNR and consume less memory for generating the matrix when compared with a Gaussian matrix . The DWT DCT based VCS yields better quality and compression when compared with DCT and DWT approaches . The transmission energy is 50 less and the average delay is 52 less when compared to raw frame transmission . 
",Implemented DWT DCT hybrid based video compressive sensing framework. Proposed and implemented two memory efficient measurement matrices for WMSN. Proposed matrices yields similar or better PSNR compared to Gaussian matrix. Storage and energy complexity is less for proposed matrices compared to Gaussian matrix.,,,
S0045790615001391," Accurate image retrieval is required to index and retrieve large number of images from huge databases . In this paper an efficient approach is presented to encode the color and textural features of images from the local neighborhood of each pixel . The color features are extracted by quantizing the RGB color space into a single channel with reduced number of shades . The texture information is encoded with structuring patterns generated from the locally structured elements chosen as a basis . Color and textural features are fused together to construct the inherently rotation and scale invariant hybrid image descriptor . This fusion is carried out by extracting textural cues over each shade independently . RSHD has been tested on the Corel dataset and experimental results suggest that RSHD outperforms state of the art descriptors . The performance of the RSHD is promising under rotation and scaling . It can also be effectively used under more complex image transformations . 
",A rotation and scale invariant hybrid descriptor is proposed for content based image retrieval. Color and textural data are used to construct the descriptor. Color is encoded by quantizing RGB color space into 64 shades. Texture is extracted using 5 rotation invariant structuring element.,,,
S0045790615004310," In this paper a distributed base station cooperation based handover management method is proposed for WiMAX Point to Multi Point networks to provide quality of service to handover nodes . Moreover a delay reduction method is proposed to reduce the packet delivery delays during handover . A Call Admission Control algorithm is proposed to handle handover calls of various service classes fairly according to their priorities . A bandwidth borrowing scheme is proposed to reduce the handover call dropping probabilities of various service classes while not starving the ongoing calls of lower priority service classes . A Markov model is developed to analyze the proposed CAC method and to obtain the approximate handover call dropping probabilities of various service classes . Simulation experiments are conducted to establish the performance advantages of the proposed handover management and CAC methods . 
",A distributed base station cooperation method for WiMAX PMP networks is proposed. A delay reduction method to support delay requirements during handover is proposed. A fair and starvation preventing call admission control method is proposed. Analytical dropping probabilities of various service classes are obtained.,,,
S0045790615002761," Wireless sensor networks have a wide range of applications in our lifetime . Indeed WSNs perform a various missions and tasks in odor localization firefighting medical service surveillance and security . In order to accomplish these tasks the sensors have to perform partitioning protocols to form an organization into clusters and cliques . The hierarchical clustering is the key solution for WSNs to deal with the scalability problems in a network composed of millions of nodes . In this paper a new hierarchical partitioning scheme is presented named MCCC . It is cliques and clusters based hierarchical scheme that takes into account the size of cliques and clusters it also minimizes the number of hops between the cluster head and its nodes . The proposed scheme is motivated by the need to have minimum and maximum size for cliques and clusters . This hierarchical scheme is proposed to respond to the energy and memory constraints for WSNs . 
",This paper presents a hierarchical scheme for sensor networks clustering. The solution proposed combines clustering into cliques and clusters. It takes into account the sizes of cliques and clusters for scalability and energy saving purposes. It improves the end to end delay compared to existing works.,,,
S0045790615003420," With lot of hype surrounding policy based computing XACML has become the widely used de facto standard for managing access to open and distributed service based environments like Web services . However like any other policy language XACML has complex syntax which makes the policies specification process both time consuming and error prone especially with large size policies that govern complex systems . Moreover with the diversity of rules and conditions hidden conflicts redundancies and access flaws are more likely to arise which expose Web services to security breaches at runtime . This paper proposes a UML profile that allows systematic model driven specification of XACML policies to resolve the complexity of policies designation . Based on mathematical sets that explore the rules meanings the paper provides also a design level analysis to detect anomalies in the specified policies prior to their enforcement in the system . A real life case study demonstrates the feasibility and efficiency of the proposition . 
",We provide UML profile for model driven specification of XACML policies. We propose a set based design level XACML policy analysis approach. We devise algorithms for design level detection of conflicts redundancies and flaws. We provide dynamic policies evaluation to control access to critical resources.,,,
S0045790615004176," With the development of the Internet of Things more and more devices that have diverse computing and communication capabilities are interconnected . For multimedia multicast in the IoT a fixed flow rate can not meet the quality of service requirements of heterogeneous devices and each device may not get the information from all other devices . In order to satisfy the heterogeneous requirements we develop a distributed algorithm to solve the inter layer flow optimization problem based on network coding for multimedia multicast in the IoT by using primal decomposition and primal dual analysis . We also apply Lyapunov theory to prove the convergence and global stability of the proposed iterative algorithm . Numerical results demonstrate that the proposed algorithm has better flexibility stability and implementation advantages than the previous intra layer ones . 
",A method to save bandwidth for multimedia multicast in IoT. A distributed algorithm of joint rate control and multicast layer flow allocation. The inter layer network coding is more stable than the intra layer ones.,,,
S0045790616000057,"The scale free topology is robust when confronted with random faults but it is fragile when confronted with selective remove attacks. In this paper we propose a new scale free topology model which has both fault tolerance against random faults and intrusion tolerance against selective remove attacks at the same time. Then the mathematical expression of the topological degree distribution is derived. Through analyzing the effect of topological degree distribution on these properties of topological fault tolerance and topological intrusion tolerance the optimal scale free topology which can keep the fault tolerance and maximize intrusion tolerance is obtained. We performed extensive experiments on the proposed model and compared it with other existing models. The simulation results show that the new scale free topology model can keep the character that the scale free topology has a stronger robustness to random faults. And it also can reduce their fragility for selective remove attacks and further prolong its lifetime. 
",Two criterions of measuring the topological properties are discussed. Effect of degree distribution on topological properties is analyzed mathematically. The scale free topology model has adjustable scaling exponent is proposed. The proposed model is robustness to random faults and selective remove attacks.,,,
S0045790615004115,"This paper aims to develop a location based services supported Dr.What Info system i.e. a master multi agent system on what the information is using Google maps and an image recognition technology as a tourism information provider and as a route planner for tourists. Users can have great fun during vacation travels through an easy to use interface integrating smartphone GPS function a QR Bar code reader and easy access to a cloud database to find all of the required web services. In particular given an archeological site in New Taipei City Taiwan for testing purposes the presented system is demonstrated not only as a provider of information on popular tourist attractions but also as a high performance GPS navigation device to guide users toward their desired destinations. The complete system developments displays and corresponding experiments and comparisons show that the research results demonstrate performance superiority over a number of previous studies. 
",A location based services and Google maps based Dr.What Info multi agent system is proposed. An archeologic site in New Taipei City Taiwan for testing purposes produces results regarding the completeness and feasibility of the proposed architecture. The deep and complete system developments displays and corresponding experiments and comparisons show that the research results demonstrate performance superiority over a number of previous studies.,,,
S0045790616300131," Digital watermarking based biometric images protecting has been an active research focus . In this paper we propose a face image protection scheme based on semi fragile self recoverable watermarking . Authentication watermark is generated from the singular value decomposition coefficients for each image block and information watermark is generated from the principal component analysis coefficients . Both of them are embedded into wavelet medium frequency coefficients by using the proposed group based wavelet quantization method . On the authentication side after identifying the tampered regions the proposed method can recover the tampered face images by using the hidden information watermark . Experimental results demonstrate that the proposed watermark scheme has high localization accuracy and robustness comparing with the existing techniques and the recovered PCA coefficients can be used to reconstruct the Eigen face image or be directly used for recognition system . 
",PCA coefficients are used to generate information watermarks and the authentication watermarks are generated using SVD coefficients. The embedding position is generated by a chaotic map and it improves the security of the proposed method. Watermarks are embedded into the mid frequency bands by using our proposed SDPQ method.,,,
S0045790615000993," Representation and reasoning about context information is a main research area in Ambient Intelligence . Context modeling in such applications is facing openness and heterogeneity . To tackle such problems we argue that usage of semantic web technologies is a promising direction . We introduce CONSERT an approach for context meta modeling offering a consistent and uniform means for working with domain knowledge as well as constraints and meta properties thereof . We provide a formalization of the model and detail its innovative implementation using techniques from the semantic web community such as ontology modeling and SPARQL . A stepwise example of modeling a commonly encountered AmI scenario showcases the expressiveness of our approach . Finally the architecture of the representation and reasoning engine for CONSERT is presented and evaluated in terms of performance . 
",We formally define an ontology based context meta model. Approach offers uniform modeling of context content annotation and dependencies. Reasoning cycle performs automatic computation of temporal continuity of situations. OSGi compatible reasoning engine exploits the context meta model in customizable way.,,,
S0045790615004449," This paper analyzes the performance of spectrum sensing based energy detection in cognitive radio networks over generalized fading channels . The fading channel is modeled by the extended generalized K distribution . Exact and accurate analytical expressions for the average detection probability with different detection schemes such as single channel diversity reception and cooperative spectrum sensing are derived and evaluated . Obtained expressions can be reduced to other well known fading channels such as Weibull Nakagami m and Rayleigh . It is shown that the analytical framework can be used for both integer and non integer values of the fading shadowing parameters . The impact of key fading shadowing parameters on the performance of energy detectors is discussed with receiver operating characteristics curves . The accuracy of the derived analytical expressions is corroborated via Monte Carlo simulation results . 
",Performance of ED in CRNs over EGK fading channels is analyzed. Unified analytical expressions for the average detection probability are derived. Performance of diversity reception and cooperative sensing is analyzed.,,,
S0045790615002785," Data centers use dynamic virtual machine consolidation to reduce power consumption . Existing consolidation mechanisms are not efficient in cloud data centers which have heterogeneous hardware infrastructure huge scale and highly variable non stationary workloads . We use game theory to develop a novel distributed mechanism for both heterogeneous and homogeneous data centers of cloud computing . Our mathematical analysis shows that our mechanism converges after a finite number of migrations . In addition we show that our worst case power consumption is only 23 more than the theoretical minimum . In order to validate our claim we preform simulation in CloudSim with real workload traces from Google data centers . 
",We propose a game for reducing power consumption in data centers. We show that the game always converges to a pure Nash equilibrium. We establish upper bounds on the convergence time of the game. We prove that PoA 1.23. We show that PoS 1.,,,
S0045790614002572," This paper focuses on the spectrum sensing mechanisms which could improve network throughput through the sensing strategy optimization and cooperative spectrum sensing methods . In order to guarantee an integrated and effective research we take the whole channel scenarios into consideration i.e . Single Secondary user with Single and Multiple Channels Multiple Secondary users with Single and Multiple Channels . Moreover according to the specific feature of each scenario different sensing methods are adopted i.e . optimal sensing period to maximize network throughput for SSSC a novel sensing method to minimize searching time for SSMC partial cooperative spectrum sensing mechanism for MSMC and setting a spectrum pool in the fusion center to record the channel states for MSMC . Simulation results demonstrate that our methods can improve spectrum efficiency network throughput and channel utilization especially when the spectrum is utilized inadequately . 
",Study spectrum sensing mechanisms by the optimization of network parameters and strategies. The proposed mechanisms have fewer sensing overhead. SUs can access into the spectrum holes efficiently and transmit with a low power.,,,
S0045790615002384," Stealthy attackers move patiently through computer networks taking days weeks or months to accomplish their objectives in order to avoid detection . As networks scale up in size and speed monitoring for such attack attempts is increasingly a challenge . This paper presents an efficient monitoring technique for stealthy attacks . It investigates the feasibility of proposed method under number of different test cases and examines how design of the network affects the detection . A methodological way for tracing anonymous stealthy activities to their approximate sources is also presented . The Bayesian fusion along with traffic sampling is employed as a data reduction method . The proposed method has the ability to monitor stealthy activities using 10 20 size sampling rates without degrading the quality of detection . 
",A scalable monitoring scheme for stealthy attacks on computer networks is presented. Bayesian fusion along with traffic sampling is used as a data reduction method. Stealthy activities can be detected using 10 20 size sampling rates. A tracing algorithm for anonymous stealthy activities to their sources is presented. The effect of network parameters on detection is investigated.,,,
S0045782516300901," Discontinuous deformation analysis is a numerical method for analyzing dynamic behaviors of an assemblage of distinct blocks with the block displacements as the basic variables . The contact conditions are approximately satisfied by the open close iteration which needs to fix or remove repeatedly the virtual springs between blocks in contact . The results from DDA are strongly dependent upon stiffness of these virtual springs . Excessively hard or soft springs all incur numerical problems . This is believed to be the biggest obstacle to more extensive application of DDA . To avoid the introduction of virtual springs huge efforts have been made with little progress related to low efficiency in solution . In this study the contact forces instead of the block displacements are taken as the basic variables . Stemming from the equations of momentum conservation of each block the block displacements can be expressed in terms of the contact forces acting on the block . From the contact conditions a finite dimensional quasi variational inequality is derived with the contact forces as the independent variables . On the basis of the projection contraction algorithm for the standard finite dimensional variational inequalities an iteration algorithm called the compatibility iteration is designed for the quasi variational inequality . The main processes can be highly parallelized with no need to assemble the global stiffness matrix . A number of numerical tests including those very challenging suggest that the proposed procedure has reached practical level in accuracy robustness and efficiency and the goal to abandon completely virtual springs has been reached . 6 dimensional generalized force vector in Eq . matrix formed by all the local frames at block Eq . matrix of the th contact local frame at block Eq . cohesion at the th contact Eq . incremental displacement vector of block Eq . time interval of a time step Eq . Young s modulus in all the examples stiffness matrix of block Eq . tolerance of contact force vector in the compatibility iteration direction normal strain of block Eq . direction normal strain of block Eq . flexibility matrix Eq . 6 dimensional flexibility vector Eq . vector valued gap function in Eq . acceleration of gravity in all the examples a part of normal gap of the contact pair formed by an edge of and a vertex of in Eq . a part of tangential slide of the contact pair formed by an edge of and a vertex of in Eq . normal gap of the th contact Eq . tangential slide of the th contact Eq . shear strain component of block Eq . equivalent stiffness matrix of block Eq . allowable maximum displacement within a time step in all the example density of block Eq . mass matrix of block Eq . friction coefficient at the th contact Eq . the number of contact pairs within a time step in Section 4 unit normal vector of local frame of a contact pair and and dimensional vectors. 
",The dual form of discontinuous deformation analysis abbreviated as DDA d with the contact forces as the basic variables is established. DDA d does not need the virtual springs the stiffness of which has strong influence on the computational results. An iterative algorithm for the finite dimensional quasi variational inequality in DDA d called the compatibility iteration is designed. DDA d is more accurate and robust than the classic discontinuous deformation analysis and comparable in efficiency with DDA.,,,
S0045790615001275," Advances in wireless ad hoc network techniques have spurred the development of new approaches to increase network efficiency . One of the more popular approaches is swarm intelligence . Swarm intelligence imitates the collective behavior of biological species to solve network routing problems . Meanwhile weakly connected dominating sets can serve as auxiliary structures for clustering nodes in the network . This paper uses the clustering concept of WCDS to propose an improved ant based on demand clustering routing protocol for wireless ad hoc networks . Network states information is obtained from the Forward Ant and is only broadcast by the head of every cluster thus decreasing the overhead required to transmit ant packets . To increase network efficiency the pseudo random proportional selection strategy is used to evaluate the best path from the source node to the destination node by the Backward Ant . 
",We propose a novel WCDS assisted ACO based routing protocol. The on demand feature of AODV is used to improve the ACO scheme. WCDS works as an auxiliary structure to broadcast the Forward Ant message. A LCC strategy is used to maintain the WCDS architecture. The pseudo random proportional selection scheme selects the efficient route.,,,
S0045790615004413,"In cognitive wireless network throughput scheduling optimization under interference temperature constraints has attracted more attentions in recent years. A lot of works have been investigated on it with different scenarios. However these solutions have either high computational complexity or relatively poor performance. Throughput scheduling is a constraint optimization problem with NP Non deterministic Polynomial hard features. In this paper we proposed an immune clone based suboptimal algorithm to solve the problem. Suitable immune clone operators are designed such as encoding clone mutation and selection. The simulation results show that our proposed algorithm obtains near optimal performance and operates with much lower computational complexity. It is suitable for slowly varying spectral environments. 
",A clone selection algorithm was proposed for the throughput problem in cognitive wireless network. The proposed algorithm presents a very reasonable tradeoff between computational complexity and performance. The proposed solution is more suitable for slowly varying spectral environments such as IEEE 802.22 networks in realistic network settings.,,,
S0045790615001159," This article presents a new method for detecting objects on waters surfaces using colour elimination based on image erosion with a morphological variable . The proposed object detection method includes definitions of the target s colour space and colour deviation using Euclidean distance . It also introduces a procedure for image erosion using a morphological variable . In order to evaluate the proposed object detection method the experiments were performed on two different image databases the MSRA salient object database and a proprietary image database containing pictures of water activities typically encountered near hydro power plants . The experimental results show that the proposed object detection method enables efficient and robust detection of objects on of waters surfaces compared to other methods primarily based on the optimisation of image contrast and edge detection . 
",A new method for detecting salient objects on waters surfaces is proposed. The proposed object detection method enables an efficient and robust detection. Four different methods were compared with the proposed CEIEMV method. The proposed method equals or outperforms the methods being compared.,,,
S0045782516300639," The numerical manifold method surmounting the mesh dependence has successfully solved very complicated problems involving small deformation and large movement but had few applications to large deformation and large rotation problems because the false volume expansion and other issues exist . In this study it is shown that the false volume expansion in NMM can be excellently resolved by using the S R decomposition theorem which can precisely reflect complex physical behaviors occurring in the process of large rotation and large deformation . The numerical methods based on the S R decomposition theorem have been limited to the static analysis of large deformations . To remove this limitation a new formulation taking into account dynamical features is proposed based on the weak form of momentum conservation law . Under the framework of NMM the generalized method is employed to discretize the temporal variables . The updates of variables are described using the updated co moving coordinate system . Thus a new method named S R D based NMM is established . The new formulation can be implemented in any other partition of unity based methods as well so as to improve the performances of such methods in the analysis of dynamic large deformations . Global reference system Co moving coordinate system Position vector of a point before deformation Position vector of a point after deformation Displacement vector of a point Velocity vector of a point Acceleration vector of a point Specified vector of a point Specified traction vector of a point Force per unit volume Basis vector corresponding to initial co moving coordinate system Basis vector corresponding to the co moving coordinate system after deformation Transformation function between and Kronecker delta Displacement derivative Christoffel symbol of the second kind Strain tensor Linear strain tensor Nonlinear strain tensor Local mean rotation tensor Unit vector of the rotation axis Mean rotation angle Area coordinates of any point Partition of unity function Star coordinates Area of a triangular mesh covering one manifold element Parameters in terms of star coordinates Constants with regard to Stress Material tensor Material density Time Time increment or time step size Virtual work of inertia force Virtual work of constraint force of specified displacement Virtual work of external force Penalty parameter Displacement vector of a manifold element Velocity vector of a manifold element Acceleration vector of a manifold element Interpolation matrix Strain vector of a manifold element Linear strain vector of a manifold element Nonlinear strain vector of a manifold element Linear strain displacement matrix Nonlinear strain displacement matrix Element stiffness matrix Mass matrix of a manifold element Equivalent force vector of a manifold element Algorithmic parameters of generalized Spectral radius of generalized 
",A dynamics formulation based on the S R decomposition is deduced. An update scheme for the co moving coordinate used in the S R decomposition is built. A new procedure named S R D based NMM is established for addressing small or large deformation together with impact contact. Some issues such as the false volume expansion in the conventional small deformation based NMM are effectively overcome.,,,
S0045790613002504," With the development of the 6LoWPAN standard sensors can be natively integrated into the IP world becoming tiny information providers that are directly addressable by any Internet connected party . To protect the information gathered by sensors from any potential attacker on the Internet it is essential to have trustworthy real time information about the legitimacy of every attempt to interact with a sensor . Our approach to address this issue is Ladon a new security protocol specifically tailored to the characteristics of low capacity devices . In this paper we study the performance of Ladon showing that it successfully meets the requirements of the targeted environments . To that end we evaluate the delay and energy consumption of the execution of Ladon . The obtained results show that the cost of Ladon is bounded even in situations of high packet loss rates and comparable to that of other protocols that implement fewer security features . 
",There is a lack of end to end security mechanisms tailored to IP enabled sensors. Ladon is proposed as end to end authentication and authorization protocol. We evaluate the time and energy cost of executing Ladon on the protected sensors. Ladon is proved to be a suitable security protocol for resource deprived devices.,,,
S0045790615000683," The increasing costs of healthcare along with the increasing availability of new Personal Health Devices are the ingredients of the connected health vision . Also a growing number of consumer electronic and mobile devices are becoming capable of taking the role of a health gateway thus operating as a data collector for PHDs . In this context we present a system that enables PHDs to share information in home networks and with the Internet based on a new Internet of Things protocol namely the Constrained Application Protocol . CoAP is used along with the IEEE 11073 family of standards which is the main exchange data model for PHD communication . We discuss how the proposed system can be integrated to other connected health systems such as a Universal Plug and Play healthcare system . We detail how the CoAP communication model was adapted to the IEEE 11073 model . We also present a real PHD prototype and its evaluation results . These results demonstrated the feasibility of the proposed solution showing how its network overhead is around 50 lighter when compared to other protocols . Finally we tested the proposed solution based on different scenarios including a proof of concept integration with a service in the cloud using different wireless physical interfaces . 
",We present a system that enables Personal Health Devices to share information in home networks and with the Internet. The Constrained Application Protocol CoAP is used as underlying protocol. The CoAP communication model was adapted to the ISO IEEE 11073 model in the proposed system. Results showed how the network overhead is around 50 lighter when compared to other protocols. Interoperability experiments demonstrated how the proposed solution can work with legacy systems.,,,
S0045790615001135," This research proposes a framework for a real time implementation of a Brain Computer Interface . This interface is designed with a future objective of providing a testing platform as an interactive and intelligent Image Search and Retrieval tool that allows users disabled or otherwise to browse and search for images using non invasive electroencephalography signals . As a proof of concept a prototype system was designed and implemented to test real time data collection and navigation through the interface by detection and classification of event related potentials . A comparison of different feature extraction methods and classifiers for the detection of ERPs is performed on a standard data set to determine the best fit for the real time implementation of the BCI . The preliminary results of the real time implementation of the prototype BCI system are quantified by the success rate of the user subject in navigating through the interface and spelling a search keyword using the mental typewriter Hex O Speller . 
",A framework for a real time implementation of a Brain Computer Interface. Implementation comparison of different feature extraction methods and classifiers. Accuracy processing time comparison for detection of event related potentials ERP. An implementation of a prototype system using the proposed BCI framework. Real time EEG data collection and classification of ERPs using Hex O Speller.,,,
S0045790615000324," Automated recognition of brain tumors in magnetic resonance images is a difficult procedure owing to the variability and complexity of the location size shape and texture of these lesions . Because of intensity similarities between brain lesions and normal tissues some approaches make use of multi spectral anatomical MRI scans . However the time and cost restrictions for collecting multi spectral MRI scans and some other difficulties necessitate developing an approach that can detect tumor tissues using a single spectral anatomical MRI images . In this paper we present a fully automatic system which is able to detect slices that include tumor and to delineate the tumor area . The experimental results on single contrast mechanism demonstrate the efficacy of our proposed technique in successfully segmenting brain tumor tissues with high accuracy and low computational complexity . Moreover we include a study evaluating the efficacy of statistical features over Gabor wavelet features using several classifiers . This contribution fills the gap in the literature as is the first to compare these sets of features for tumor segmentation applications . 
",A fully automatic system for detection of slices that contain tumor in MR images is presented. A fully automatic system for tumor segmentation using single spectral MR images is presented. A study for evaluating the efficacy of statistical features over Gabor wavelet features is included.,,,
S0045790616300040," Modeling traffic flow and gathering accurate traffic congestion information are two challenging problems in smart transportation systems . Most of the traffic flow models and velocity estimation methodologies that have been proposed so far gather the data from GPS equipped smart phones and extract the flow model based on GPS sampling . However these approaches tend to fail in real life scenarios due to the insufficient vehicle data and unpredictable dynamics of the flow . Furthermore utilization of GPS sensor leads to a battery drainage and hence reduces the overall system performance . In this paper we propose a new battery friendly data acquisition model to obtain the raw data . We then evaluate our model under various traffic conditions to determine its feasibility in vehicle speed estimation . The proposed model results in 88 location accuracy whereas it reduces the battery consumption by half . 
",Evaluate the effectiveness of using cellular positioning techniques along with GPS sensor based data collection via performing experiments on energy consumption and location accuracy for various data acquisition modes where only GPS positioning only cellular positioning and both of the techniques are executed. Our proposed system is designed to be used for average velocity calculation which is an essential parameter in traffic monitoring systems. Therefore we perform experiments to compute the average velocity by extracting location data with the hybrid data acquisition model. We assume average speed measurements obtained from GPS positioning as the ground truth data to assess the performance of our model. Assess the feasibility of cellular positioning in cases where the GPS sensors are not available for location estimation. In this paper we propose a hybrid acquisition model which applies cellular positioning techniques to obtain the raw location data where GPS sensor is not available or battery of a particular smart phone is too low for location lookup.,,,
S0045790615002335," In this paper we address the problem of broadcast routing and scheduling of video streaming for delay sensitive applications in backbone wireless mesh networks . Given a source node and a set of destinations we aim to build a broadcast tree and compute an optimal schedule such that the throughput for the source to broadcast streaming data to all the destinations is maximized . We divide the whole period for video broadcast into identical time frames and prove that maximizing the throughput can be converted into minimizing the length of a time frame . We propose a three step method as a solution . Firstly we build the broadcast tree by defining a new routing metric to select relay nodes . Then we use local search method to adjust the tree structure . Last we propose a greedy method to schedule concurrent transmissions . Simulations have demonstrated that our method can improve the performance significantly compared with existing methods . 
",We formulate the problem as a mixed integer quadratically constrained program. We use a three step method to build a broadcast tree and schedule transmissions. We show the effectiveness of our method by simulation.,,,
S0045790615001500," Progress in the area of environmental sustainability for the mobile computing industry could be achieved by making advancement on two fronts reducing the energy consumed by individual devices throughout their life cycle and reducing the rate at which these devices are discarded . In this work to address both fronts we propose the use of a thin client approach whereby a mobile device relies mainly on the resources at a remote server to carry out application tasks . To assess the benefits of the proposed approach this work develops an analytical model as well as performs an empirical evaluation of performance and energy consumption on Android based smartphones . In terms of energy a reduction of approximately 11 in the average life cycle energy is seen by increasing the device s usage life by even three months through a thin client approach . In terms of performance a thin client device is shown to improve execution by 57 compared to a self reliant device . 
",An analytical model for lifecycle energy consumption of mobile devices is proposed and evaluated. A derivation of the theoretical conditions for energy savings under thin client paradigm. A comparative empirical evaluation of performance and energy consumption of old and new Android based smartphone operating under the thin client paradigm was performed.,,,
S0045782514002291," We consider the extension of the Nitsche method to the case of fluid structure interaction problems on unfitted meshes . We give a stability analysis for the space semi discretized problem and show how this estimate may be used to derive optimal error estimates for smooth solutions irrespectively of the mesh interface intersection . We also discuss different strategies for the time discretization using either fully implicit or explicit coupling schemes . Some numerical examples illustrate the theoretical discussion . 
",Unfitted finite element method for a fluid structure interaction. Proof of stability and accuracy. Different coupling scheme s for time advancement fully coupled or loosely coupled.,,,
S0045790615004152," Various signaling techniques have been considered for I O interconnects at 25 Gb s or higher . However they either suffer from high baud rate as in the case of NRZ or an excess signal to noise ratio penalty as in the case of PAM 4 . To overcome these problems a four phase shifted sinusoid symbol signaling method is proposed that can be readily applied to high speed I O transceivers . By using PSS 4 the SNR penalty can be reduced from PAM 4 our theoretical analysis has revealed that the SNR performance of the PSS 4 is 6.5 dB better than that of PAM 4 with comparable bandwidth . The transistor level simulation results have shown that PSS 4 has an average of twice larger vertical eye opening than PAM 4 . In addition the supply voltage sensitivity of PSS 4 is 55 and 20 less than that of PAM 4 and NRZ respectively and the power consumption of PSS 4 is 13.5 lower than that of PAM 4 . 
",Instead of using amplitude modulation a four phase shifted sinusoid symbol signaling PSS 4 is proposed for high speed I O interconnects from the phase aspect. PSS 4 reduces the signal to noise ratio SNR penalty of 6.5 dB from PAM 4. A novel pre emphasis technique is proposed for PSS 4 to improve the spectral performance.,,,
S0045790616000094,"Due to intra flow and inter flow interference problems the throughput performance decreases dramatically in a multi hop wireless network. These two kinds of bandwidth unfair sharing problems could cause serious collisions and congestion hence affecting the performance of multi hop wireless networks. That is to say data packets that need to traverse more hops to arrive at the destination will get lower throughput and result in the inter flow fairness problem. Furthermore the quality of video transmission is especially poor in traditional multi hop wireless network environments. Therefore in this paper we propose a virtual queue management scheme that does not require the modification of any communication protocol. According to the number of flows it adjusts the queue management scheme to achieve each flow s fair sharing of channel resource. It also improves the quality of video transmission. Through NS2 simulations the results show that our proposed scheme can mitigate the inter flow fairness problem and effectively improves the quality of video transmission. 
",Packets that traverse more hops to the destination will get lower throughput and result in the inter flow fairness issue. We propose a virtual queue management scheme that does not require the modification of any communication protocol. According to the number of flows our scheme adjusts the parameters to achieve each flow s fair sharing of channel resource. Our proposed scheme can mitigate the inter flow fairness problem and effectively improves the quality of video transmission.,,,
S0045790614002109," Compressed sensing recovers a sparse signal from a small set of linear nonadaptive measurements . A sparse signal can be represented by compressed measurements with a reduced number of projections on a set of random vectors . In this paper a multiscale compressed sensing based processing is investigated for an electrocardiogram signal which yields coded measurements . In case of an electrocardiogram signal the coded measurements are expected to retain the clinical information . To achieve this compressed sensing based processing is applied at each wavelet scale and measurements are coded using Huffman coder . The measurements at each scale use random sensing matrix with independent identically distributed entries formed by sampling a Gaussian distribution . The proposed method is evaluated using pathological ECG signals from the CSE database synthetic and normal ECGs . It helps preserve the pathological information and clinical components in compressed signal . The compressed signal quality is evaluated using standard distortion measures and mean opinion score . The MOS values for the signals range from to 8.3 with a wavelet energy based diagnostic distortion value of 9.46 which falls under the excellent category . 
",Multiscale compressive sensing based processing is applied for Electrocardiograms. The wavelet coefficients at different subbands are sparse in nature. Compressed measurements are taken at wavelet scales and measurements are encoded for further compression. Method is evaluated using pathological ECG signals from CSE database synthetic and normal ECGs. Distortion introduced is evaluated by quantitative and qualitative analysis like PRD WEDD RMSE and MOS.,,,
S0045790615003122," Designing a lightweight secure communication protocol for Wireless Sensor Networks remains a challenging issue since sensor networks are resource limited and are left unattended . Sensor nodes in WSNs are subjected to varying forms of attacks . An adversary may destroy or damage the communications done in multi hop WSNs by means of packet dropping and modification . Hence it is essential to have an efficient cryptographic scheme to protect the communications done in WSNs . This study introduces a Reliable Anonymous Secure Packet forwarding scheme that can prevent not only traffic analysis attack but also the attacks done through compromised forwarding nodes . The mechanisms followed here are effective with low computation and communication overhead . The performance of the proposed scheme is evaluated over NS2 with a series of simulation . The simulated results show that the proposed scheme performs better than other comparable schemes . 
",Establishing an optimal communication path for reliable data delivery. Privacy for sensor readings through security service anonymity. Incremental hashing in establishing secure channel. Symmetric key management with key refreshness.,,,
S0045790615002244," The existing clustering algorithms are either static or dynamic depending on the frequency of clustering . In static clustering clusters are formed once which reduces the clustering overhead but leads to early energy drain of a few nodes in the network . The network lifetime can be improved by dynamic clustering in which clusters are reformed after every round which increases the clustering overhead . To optimize the parameters including clustering overhead network lifetime energy hole FND and LND in WSN a hybrid unequal clustering with layering protocol is proposed . The HUCL is a hybrid of static and dynamic clustering approaches . In HUCL the network is divided into layers and clusters of various sizes . The cluster heads are selected based on available energy the distance to the sink and the number of neighbors . Once the cluster is formed the same structure is maintained for a few rounds . The data are forwarded to the sink through a multi hop layer based communication with an in network data compression algorithm . In comparison with the existing protocols the HUCL balances energy and achieves a good distribution of clusters extends the lifetime of the network and avoid the energy hole problem . 
",Low overhead date collection framework for WSN to mitigate the energy hole problem. The clusters formed by this protocol are unequal size clusters. Hybrid of static and dynamic clustering for maximizing lifetime in WSN. In network data compression by cluster heads is proposed.,,,
S0045790615000142," This paper investigates an optimal adaptive rate and power transmission algorithms for Orthogonal Frequency Division Multiplexing based Cognitive Radio systems . The aim was to study the problem of maximizing the overall rate achieved by the Secondary User while keeping the interference powers introduced by the SU on the spectrum band of Primary User s below the specified thresholds and considering the total transmit power budget constraints . In addition the novel suboptimal power allocation algorithm was proposed and consequently the maximum modulation level according to allocated power based on maximizing the overall achievable rate was obtained . The performance of the proposed suboptimal algorithm is compared with the optimal and existing algorithms including uniform loading and water filling algorithms . Numerical results revealed that the proposed suboptimal algorithm had a better performance than the uniform loading and water filling algorithms . 
",Investigate adaptive rate and power allocation in OFDM based Cognitive Radio system. Investigate an optimal adaptive rate and power transmission algorithms. Proposed novel suboptimal algorithm for power allocation. Compare proposed suboptimal algorithm with optimal and conventional algorithms.,,,
S0045790615002980," Variational decomposition has been widely used in image denoising however it can t distinguish texture from noise well . Replacing the fixed parameter in the decomposition with a monotone increasing sequence and iteratively taking the residual of the previous step as the input to decompose we propose a multiscale variational decomposition model in this paper . Unlike the fixed scale decomposition the new model can decompose the input image into a sum of a series of features with different scales . So texture can be distinguished from noise . In addition we prove the nontrivial property and the convergence of this multiscale decomposition and introduce a hybrid iteration algorithm that combines the first order primal dual algorithm with the gradient decent method to numerically solve the multiscale decomposition model . Numerical results validate the effectiveness of the proposed model . Furthermore we apply this multiscale decomposition for image hierarchical restoration . Compared with the classical hierarchical decomposition hierarchical wavelet decomposition and fixed scale decomposition our model has better performance for both synthetic and real images in terms of PSNR and MSSIM . 
",We propose a multiscale variational decomposition model. We apply the proposed model for hierarchical image restoration and reconstruction. We prove the nontrivial property and the convergence of our model. Experimental results verify the correctness of the theoretical analysis.,,,
S0045790615002116," In this paper we present a novel pose and expression invariant approach for 3D face registration based on intrinsic coordinate system characterized by nose tip horizontal nose plane and vertical symmetry plane of the face . It is observed that distance of nose tip from 3D scanner is reduced after pose correction which is presented as a quantifying heuristic for proposed registration scheme . In addition motivated by the fact that a single classifier can not be generally efficient against all face regions a two tier ensemble classifier based 3D face recognition approach is presented which employs Principal Component Analysis for feature extraction and Mahalanobis Cosine matching score for classification of facial regions with weighted Borda Count based combination and a re ranking stage . The performance of proposed approach is corroborated by extensive experiments performed on two databases GavabDB and FRGC v2.0 confirming effectiveness of fusion strategies to improve performance . 
",A 3D face registration and recognition approach is proposed. Pose correction is evaluated through various correction parameters. Face range images are divided into different regions and features are extracted. Ensemble classifier is used to fuse results using features from different regions. Results show mark improvement in registration accuracy and recognition rates.,,,
S0045782516300895," This work evaluates the performance of a NURBS based isogeometric finite element formulation for solving stationary acoustic problems in two dimensions . An initial assessment is made by studying eigenvalue problems for a square and a circular domain . The spectral approximation properties of NURBS functions of varying order are compared to those of conventional polynomials and are found to be superior yielding more accurate representations of eigenvalues as well as eigenmodes . The higher smoothness of NURBS shape functions yields better approximations over an extended frequency range when compared to conventional polynomials . Two numerical case studies including a geometrically complex domain are used to benchmark the method versus the traditional finite element method . A convergence analysis confirms the higher efficiency of the isogeometric method on a per degree of freedom basis . Simulations over a wider frequency range also illustrate that the method suffers less from the dispersion effects that deteriorate the acoustic response towards higher frequencies . The tensor product structure of NURBS however also imposes practical considerations when modelling a complex geometry consisting of multiple patches . 
",The performance of NURBS based IGA for time harmonic acoustics in 2D is studied. NURBS functions exhibit better spectral approximation properties than polynomials. IGA converges faster than conventional FEM on a per DOF basis. IGA suffers less from the pollution effect than conventional FEM. IGA also performs well for complex geometries and impedance boundary conditions.,,,
S0045790615003134," Nonlocal means based denoising is an efficient and simple method for image sequence denoising which has been applied successfully in many image sequence denoising applications . We extend the method for image sequence denoising using Zernike moments called NLM ZMs which provides better denoising performance as compared to NLM based approach . In addition the proposed method is faster because the number of computations needed for block matching and weight computation are significantly reduced . The NLM approach uses the photometric distance between two image patches for determining the similarity distance . In the proposed approach low order ZMs are used for the block matching process resulting in better denoising performance at a much lower computation cost . Detailed experimental results are provided to demonstrate better performance and higher speed of the proposed approach as compared to the NLM approach . The results are also compared with the state of the art NLM based image sequence denoising methods and the denoising results are observed to be competitive with higher speed . 
",Image sequence denoising using NLM and Zernike moments proposed. Significant improvement in image sequence denoising achieved. Denoising results are comparable to state of the art methods with much faster speed. Proposed method is faster by a factor lying between 11 and 12. Better rates of improvement achieved at higher levels of noise.,,,
S0045790615004395,"This study developed a Kinect microphone array based method for the voice based control of humanoid robot exhibitions through speech and speaker recognition. A support vector machine SVM a Gaussian mixture model GMM and dynamic time warping DTW were used for speaker verification speaker identification and speech recognition respectively they were effectively combined for realizing advanced voice based control of humanoid robot exhibitions. Speech recognition capability was enhanced by using the Kinect microphone array and by combining the DTW based recognition decisions associated with all the microphones through a fuzzy control scheme. A humanoid robot with the proposed voice based control can be controlled through voice commands by authenticated users. The robot first verifies the authenticity of the personal operator following which it identifies the operator and validates the command. Subsequently it executes the command if both the user and command are valid. Experimental results demonstrated the effectiveness and accuracy of the proposed method. 
",Kinect microphone array based voice control for operating a robot is proposed. Speech and speaker recognition are effectively combined for fine robot control. Kinect fuzzy DTW with an accurately designed fuzzy controller is proposed.,,,
S0045782516301517," In this work a novel comparative method for highly brittle materials such as aragonite crystals is proposed which provides an efficient and accurate in sight understanding for multi scale fracture modeling . In particular physically motivated molecular dynamics simulations are performed to model quasi static brittle crack propagation on the nano scale and followingly compared to macroscopic modeling of fracture using the phase field modeling technique . A link between the two modeling schemes is later proposed by deriving PFM parameters from the MD atomistic simulations . Thus in this combined approach MD simulations provide a more realistic meaning and physical estimation of the PFM parameters . The proposed computational approach that encompasses mechanics on discrete and continuum levels can assist multi scale modeling and easing for instance the simulation of biological materials and the design of new materials . 
",A link between molecular and continuum models for brittle fracture is proposed. Parameters obtained from the molecular scale are used in the continuum approach. Under this approach the phase field parameters acquire an entirely physical meaning. This approach can assist multi scale modeling of materials.,,,
S0045790614002390," This paper presents a new approach for verifying user identity in pervasive environments using a non intrusive behaviour tracking technique that offers minimum interruption to the user s activities . The technique termed Non intrusive Identity Assertion System uses knowledge of how the user uses the environment s services to infer their identity . The technique monitors user behaviour through identifying certain types of activity without the need for detailed tracking of user behaviour thus minimising intrusion on the user s normal activities . The technique was evaluated using a simulated environment to assess its reliability . Simulation results show that the technique can be applied in various situations such as strict and relaxed security settings by applying different security policies . They also show that the technique is particularly effective where the environment has a mixture of high and low security resources in which case reliability could exceed 99 . 
",A user centric unobtrusive approach for verifying the user s identity is proposed. The approach uses knowledge about the user s behaviour to infer their identity. The approach uses a simple numerical algorithm to assert the user s identity.,,,
S0045790615001809," In a wireless sensor network where sensors are arranged into a flat topology sensors near the sink consume much more energy than sensors at the boundary of the network . Sensors near the sink relay many packets than far away sensors to the sink . After these sensors expire energy holes are created near the sink . Therefore other sensors can not reach to the sink and the network becomes disconnected . In this paper we propose some strategies that can balance energy consumption of the deployed sensors and reduce energy holes from the network by balancing the communication load as equally as possible . We performed extensive experiments on the proposed algorithm using various network scenarios and compared it with other existing algorithms . The experimental results verify the effectiveness and feasibility of the proposed work in terms of network lifetime energy consumption and other important network parameters . 
",Load management strategy for large scale wireless sensor networks. Energy efficient on demand clustering strategy. Load balancing data routing mechanism. Load balancing data routing mechanism reduces energy holes from the network. Experimental results for verifying the effectiveness and feasibility of the proposed work.,,,
S0045790614002353," Widely deployed real time embedded systems can improve the performance of industrial applications but these systems also face the critical challenge of providing high quality security in an unpredictable network environment . We measure the time and energy consumptions of commonly used cryptographic algorithms on a real embedded platform and introduce a method to quantify the security risk of real time applications . We propose a Dynamic Security Risk Management mechanism to manage the aperiodic real time tasks for networked industrial applications . Inspired by the feedback design philosophy DSRM is designed as a two level control mechanism . The upper level component makes efforts to admit or reject the arrival tasks and assigns the reasonable security level for each admitted task . With three proportional feedback controllers at the lower level the security level of each ready task can be adjusted adaptively according to the dynamic environments . Simulation results show the superiority of the proposed mechanism . 
",Introducing security critical applications based on embedded control server systems for industrial networks. Establishing the security aware task security overhead and risk models for aperiodic real time applications. Combining the soft real time and security requirements into a unified framework. Deploying proportional controllers to achieve satisfied fine grained control.,,,
S0045790615002189," This paper presents a new particle swarm optimization algorithm called the PSO IAC algorithm to resolve the goal of reaching with the obstacle avoidance problem for a 6 DOF manipulator of the home service robot . The proposed PSO IAC algorithm integrates the improved adaptive inertia weight and the constriction factor with the standard PSO . Both the free space and obstacle avoidance states are established for evaluations in computer simulations and real time experiments . The performance comparisons of the PSO IAC algorithm with respect to the existing inertia weighted PSO constriction factor based PSO constriction factor and inertia weighted PSO and adaptive inertia weighted PSO algorithms are examined . Simulation results indicate that the PSO IAC algorithm provides the fastest convergence capability . Finally the proposed control scheme can make the manipulator of the home service robot arrive at the goal position with and without obstacles in all real time experiments . 
",A new adaptive PSO method is proposed and verified by simulations and a real robot. Our proposed method has been successful applied to three dimensional obstacle avoidance with manipulator for the home service robot. Both the free space and obstacle avoidance states are established for evaluations in computer simulations and real time experiments. Our PSO IAC algorithm has achieved outstanding performance compared to other methods in these experiments.,,,
S0045790615000300," Face detection is one of the most important parts of biometrics and face analysis science . In this paper a novel multi stage face detection method is proposed which can remarkably detect faces in different images with different illumination conditions variety of poses and disparate sizes . The idea is to utilize a preprocessing step to filter many non face windows by means of a skin segmentation procedure in order to boost the speed of the detection and also utilize the color information as much as possible . Subsequently candidate windows are fed to a Local Hierarchical Pattern generator unit where a new texture pattern is produced . Based on this pattern a kernel probability map is calculated for each window and by summing probabilities of all kernels and comparing it with a predefined threshold decision is made about content of the window . Not only does this algorithm effectively eliminate many non face regions but it is also capable of detecting faces with relatively acceptable rate in different conditions . 
",A multi stage face detection method is proposed using color and texture information. Using skin detection candidate windows are extracted. Candidate windows are verified based on combination of KPM and LHP. Both qualitative and quantitative results confirm the merit of the algorithm.,,,
S0045790615003675," Hierarchical clustering technique is an effectual topology control methodology in Wireless Sensor Networks . This technique is used to increase the life time of the network by reducing the number of transmissions towards the base station . We propose and validate a new routing protocol termed as Sleep awake Energy Efficient Distributed clustering algorithm . We divide the network sensing field into three energy regions because in SEED cluster heads are communicating directly with the base station . The cluster heads of the high energy region are communicating with the base station through a longer distance and paying extra energy cost as compared to the cluster heads of the low energy region . Same application base sensor nodes form sub clusters to decrease the number of transmissions towards the base station . In every round one node from these sub clusters nodes awake and transmit the data and the rest of them sleep to save available resources . We select six criteria to check the performance of our algorithm . The simulation results show that SEED achieves longer network life time and high throughput as compared to the existing clustering protocols . 
",A new energy efficient routing algorithm has been proposed for cluster based wireless sensor networks. SEED achieves O 1 message exchange complexity per sensor node and time complexity for cluster formation is O N for WSN having N sensor nodes. We introduce sub clustering in SEED to decrease the redundant data transmissions towards the base station. The proposed model utilizes sleep awake awareness to save the available power.,,,
S0045790615000233," In this paper we discuss the application of two dimensional linear cellular automata rules with the help of fuzzy heuristic membership function to the problems of edge detection in image processing applications . We proposed an efficient and simple thresholding technique of edge detection based on fuzzy cellular automata transition rules optimized by Particle Swarm Optimization method . Finally we present some results of the proposed linear rules for edge detection to the selected 22 images from the Berkeley Segmentation Dataset and compare with some classical Sobel and Canny results . Also Baddeley Delta Metric is used for the performance index to compare the results . 
",This study is the application of 2D linear cellular automata CA rules with the help of fuzzy membership function to the problems of edge detection. An efficient and simple thresholding technique of edge detection based on CA transition rules optimized by Particle Swarm Optimization method PSO is proposed. Results of the proposed to the selected 22 images from the Berkeley Segmentation Dataset BSDS are presented. Comparison with some classical Sobel and Canny results is included. Baddeley Delta Metric BDM is used for the performance index to compare the results.,,,
S0045790615000580," This paper presents a new medical image enhancement method that adjusts the fractional order according to the dynamic gradient feature of the entire image . The presented method can extract the edges of an image accurately and enhance them while preserving smooth areas and weak textures these improvements can be particularly helpful to doctors diagnoses . The primary contribution of this paper is the Adaptive Fractional Differential Algorithm which uses the improved Otsu algorithm to segment the edges textures and smooth areas of images . This algorithm allows the optimal fractional order of each pixel to be obtained using an adaptive fractional differential function constructed based on the area feature of image . As a result the image can be enhanced adaptively . Experimental results show that for medical images AFDA shows better image enhancement than other methods by making edges clearer and textures richer . 
",We use the improved Otsu algorithm to segment medical images. The function of order v is constructed based on the features of image areas. The fractional differential order of each pixel is obtained by the function. Each pixel of medical image is processed by adaptive fractional differential mask. The edge of medical image is enhanced while the weak texture is preserved.,,,
S0045782516000050," We extend the hierarchical multiscale design framework of Nakshatrala et al . to nonlinear elastodynamics wherein we use topology optimization to design material micro structures to achieve desired energy propagation in nonlinear elastic material systems subjected to impact loading . As in Part I a well posed topology optimization problem is obtained via relaxation to design the macroscale which requires homogenization theory to relate the macroscopic homogenized response to its micro structure and via restriction to design the microscale to obtain a well defined micro structural length scale . It is assumed that the primary wavelengths of interest are much longer than the micro structural length scale and hence the effective properties are computed using the static homogenization theory . An adjoint sensitivity analysis is performed to compute the derivatives of the objective function with respect to the micro structural design parameters and a gradient based optimization algorithm is used to update the design . The numerical implementation of the computationally challenging terminal value adjoint problems is discussed and a structural design example for tailored energy propagation is provided . 
",Adjoint sensitivity analysis for nonlinear elastodynamics. Composite design for desired energy propagation. Multiscale topology optimization.,,,
S0045790616300167,"Energy efficiency EE maximization with limited interference to the primary user PU is one of the primary concerns in cognitive radio networks CRNs . To achieve this objective we first propose an algorithm to select less spatially correlated secondary users SUs to lessen the shadowing effect in wireless environment. Further the aid of parametric transformation with the Lagrangian duality theorem in our proposed algorithm called Novel Iterative Dinkelbach method NIDM is used to optimise both sensing time and transmission power allocation of the SUs for maximising EE under the constraints of maximum transmission power interference to the PU overall outage of secondary transmission and minimum data rate requirement. Extensive simulation results demonstrate the effectiveness of our proposed algorithm. It is also observed that our proposed scheme outperforms the other existing schemes in enhancing the EE with the same system parameters. 
",Less spatial correlated secondary users SUs are selected based on Hungarian method. The joint optimization of sensing time and transmission power of SUs is achieved through Novel Iterative Dinkelbach method NIDM algorithm. The combination of parametric transformation with the Lagrangian duality provides better performance with lesser computational complexity. The proposed method is validated through the simulation results.,,,
S0045782516300287," This work proposes a hybrid modelling technique for efficient analysis of poroelastic materials which are widely used for noise reduction in acoustic problems . By combining the finite element method and the wave based method in a direct manner the proposed hybrid technique maximises the advantages and compensates the drawbacks of both numerical methods . The considered poroelastic domain described by Biot s theory is divided into two groups of domains according to their geometrical characteristics and boundary conditions . The group with complex geometries and or boundary conditions leading to singularities is discretised into a large number of small finite elements . The other group consisting of large geometrically moderate poroelastic domains is partitioned into wave based subdomains where the field variables are expanded with analytical poroelastic wave functions . Both groups modelled by the finite element method and the wave based method respectively are combined in a hybrid framework in this work to ensure their interacting dynamic behaviours . The properties of the hybrid model are investigated and are compared to existing modelling methods for some numerical examples . The proposed direct hybrid modelling technique provides stable predictions and exhibits fast convergence performances for the analysis of poroelastic materials especially when singularities arise in the poroelastic domain . 
",A hybrid modelling technique for efficient analysis of Biot s poroelastic materials. Efficient analysis performance for addressing singularities in poroelastic domain. Domain with singularities and or complex geometry discretised into finite elements. Geometrically moderate domain partitioned into convex wave based subdomains. Direct interface coupling of finite element and wave based poroelastic domains.,,,
S0045790614000688," When wireless sensor networks are deployed in areas inaccessible by human beings security becomes extremely important as they are prone to different types of malicious attacks . We propose a scheme to build a security mechanism in a query processing paradigm within WSNs with clustered architecture . This work aims to preserve the basic security features such as confidentiality and integrity as well as to protect from replay attack in presence of mote class attacker . Considering the limitations of such an attacker the probability of attacking cluster head and member nodes is higher than attacking the base station . Paying attention to this fact in all communication between cluster head and member nodes the key is neither transmitted nor pre deployed . Performance of the scheme is evaluated and compared through qualitative and quantitative analyses results show the present scheme s dominance over the competing schemes . 
",Low overhead secured query processing mechanism in clustered WSN environment. Preserves security features confidentiality integrity and defence against replay attack. Query is sent from base station to cluster heads in encrypted form. The cluster heads register their member nodes by exchanging messages. The cluster heads send aggregated responses to the base station.,,,
S0045790616300350," Due to the limited network bandwidth a noise robust low bit rate compression scheme of Mel frequency cepstral coefficients MFCCs is desired for distributed speech recognition DSR services. In this paper we present an efficient MFCCs compression method based on weighted least squares W LS polynomial approximation through the exploitation of the high correlation across consecutive MFCC frames. Polynomial coefficients are quantized by designing a tree structured vector quantization TSVQ based scheme. Recognition experiments are conducted on the noisy Aurora 2 database under both clean and multi condition training modes. The results show that the proposed W LS encoder slightly exceeds the ETSI advanced front end ETSI AFE baseline system for bit rates ranging from 1400 bps to 1925 bps under clean training mode. However a negligible degradation is observed in case of multi condition training mode around 0.6 and 0.2 at 1400 bps and 1925 bps respectively . Furthermore the obtained performance generally outperforms the ETSI AFE source encoder at 4400 bps under clean training and provides similar performance at 1925 bps under multi condition training. 
",A low bit rate source coding scheme for distributed speech recognition DSR systems is proposed. The algorithm is based on weighted least squares W LS polynomial approximation. The efficiency of the algorithm is tested with the noisy Aurora 2 database for bit rates ranging from 1400 bps to 1925 bps. The obtained results generally outperform the ETSI AFE encoder for clean training and provide similar performance at 1925 bps for multi condition training.,,,
S0045790614003152," Advanced medical diagnosing and research requires precise information which can be obtained from measured electrophysiological data e.g . electroencephalogram and electrocardiograph . However they are often contaminated with noise and a variety of bioelectric signals called artefacts e.g . electromyography . These noise and artefacts which need to be reduced make it difficult to distinguish normal from abnormal physiological activity . Electromagnetic contamination of recorded signals represents a major source of noise in electrophysiology and impairs the use of recordings for research . In this paper we present an effective method for cancelling 50Hz interference using a radial basis function Wiener hybrid filter . One of the main points of this paper is the hybridization of the RBF filter and a Wiener filter to make full use of both merits . The effectiveness and validity of those filters are verified by applying them to ECG and EEG recordings . The results show that the proposed method is able to reduce powerline interference from the noisy ECG and EEG signals more accurately and consistently in comparison to some of the state of the art methods and this method can be efficiently used with very low signal to noise ratios while preserving original signal waveform . 
",A method based on radial basis function and Wiener filter system is proposed for filtering powerline in biomedical recordings. The proposed solution addresses both ECG and EEG recordings. Several simulations have demonstrated the enhancement of the proposed method in comparison to other techniques. The results suggest that clinical information can be maintained. This method provides the best approach for obtaining both more signal reduction and low distortion of the signal results.,,,
S0045790615004139,"A new recurrent wavelet fuzzy neural network RWFNN with adaptive learning rates is proposed to control the rotor position on the axial direction of a thrust magnetic bearing TMB mechanism in this study. First the dynamic analysis of the TMB with differential driving mode DDM is derived. Because the dynamic characteristics and system parameters of the TMB mechanism are high nonlinear and time varying the RWFNN which integrates wavelet transforms with fuzzy rules is proposed to achieve precise positioning control of the TMB. For the designed RWFNN controller the online learning algorithm is derived using back propagation method. Moreover since the improper selection of learning rates for the RWFNN will deteriorate the control performance an improved particle swarm optimization IPSO is adopted to adapt the learning rates of the RWFNN on line. Numerical simulations show the validity of TMB system using the proposed RWFNN controller with IPSO under the occurrence of uncertainties. 
",A new recurrent wavelet fuzzy neural network RWFNN controller is proposed. RWFNN is adopted to control the rotor position of a thrust magnetic bearing TMB . The online learning algorithm of RWFNN is derived using back propagation method. The adaptive learning rates are performed via improved particle swarm optimization. Numerical simulations show the validity of TMB using the RWFNN controller.,,,
S0045790615004371," The clarity and accuracy of echocardiography images are greatly reduced by speckle noise. Noise suppression however is difficult to achieve without also obscuring both rapidly moving structures and object edges. This research seeks to address these challenges by introducing a novel filtering framework based on temporal information and sparse representation. The proposed method involves smoothing intensity variation time curves IVTCs assessed in each pixel. Using an over complete dictionary that contains prototype signal atoms IVTCs can be reconstructed as linear combinations of a few of these atoms. After a comprehensive comparison of sparse recovery algorithms three were selected for our method Bayesian Compressive Sensing BCS the Bregman iterative algorithm and Orthogonal Matching Pursuit OMP . The performance of the proposed method was then evaluated and compared with other speckle reduction filters. The experimental results demonstrate that the proposed algorithm can be used to achieve better preserved edges and reduce blurring. 
",Noise reduction in echocardiography images is proposed. Filtering framework is based on temporal information and sparse representation. Proposed method consists of smoothing intensity variation time curves assessed in each pixel. A smooth version of signal can be reconstructed by using a proper sparse recovery which is followed by an adaptive thresholding method to locate the most important atoms. After a comprehensive comparison of sparse recovery algorithms three were selected for our method Bayesian Compressive Sensing BCS Bregman Iterative algorithm and Orthogonal Matching Pursuit OMP . The proposed method preserves the edges and rapidly moving structures.,,,
S0045790614003218," XACML policies which are widely adopted for defining and controlling dynamic access among Web cloud services are becoming more complex in order to handle the significant growth in communication and cooperation between individuals and composed services . However the large size and complexity of these policies raise many concerns related to their correctness in terms of flaws conflicts and redundancies presence . This paper addresses this problem through introducing a novel set and semantics based scheme that provides accurate and efficient analysis of XACML policies . First our approach resolves the complexity of policies by elaborating an intermediate set based representation to which the elements of XACML are automatically converted . Second it allows to detect flaws conflicts and redundancies between rules by offering new mechanisms to analyze the meaning of policy rules through semantics verification by inference rule structure and deductive logic . All the approach components and algorithms realizing the proposed analysis semantics have been implemented in one development framework . Experiments carried out on synthetic and real life XACML policies explore the relevance of our analysis algorithms with acceptable overhead . Please visit http www.azzammourad.org projects to download the framework . 
",We provide policy analysis scheme to detect access contradictions among web services. We propose semantic based policy analysis through deductive logic and inference rules. We present flaw conflict and redundancy detection algorithms for XACML policy analysis. We show through experiments that SBA XACML provides efficient detection mechanisms.,,,
S0045790615001445," Cloud computing provides an effective approach to deliver multimedia services to end users with the desired user quality of experience . However cloud based multimedia applications require many of servers and consume huge energy . To reduce energy consumption a multimedia service provider should balance the energy and QoE . In this paper a theoretic model is developed to explore the trade off between energy consumption and QoE for multimedia cloud . Based on objective factor a QoE quantifying model is proposed . Employing Lyapunov Optimization techniques an optimal control framework is designed and analyzed to make energy and QoE decisions in MSPs . An approximate online algorithm is proposed with the explicitly provable performance upper bound . Extensive experiments have been conducted to verify the effectiveness of EUE RP algorithm in the practical settings . The algorithm can guarantee desired QoE and reduce energy consumption even without any information about the future fluctuation of user demands . 
",We investigate the trade off of QoE and energy in multimedia cloud. A QoE quantifying model is proposed based on objective factor. We develop a theoretic model by applying Lyapunov optimization method. An approximate online algorithm EUE RP is proposed with the explicitly provable performance upper bound. The proposed method flexibly manages the trade off between QoE and energy for cloud based multimedia services.,,,
S0045790615000312," Mobile Cloud Computing augments capabilities of mobile devices by offloading applications to cloud . Resource allocation is one of the most challenging issues in MCC which is investigated in this paper considering neighboring mobile devices as service providers . The objective of the resource allocation is to select service providers minimizing the completion time of the offloading along maximizing lifetime of mobile devices satisfying deadline constraint . The paper proposes a two stage approach to solve the problem first Non dominated Sorting Genetic Algorithm II is applied to obtain the Pareto solution set second entropy weight and Technique for Order Preference by Similarity to Ideal Solution method are employed to specify the best compromise solution . Furthermore a context aware offloading middleware is developed to collect contextual information and handle offloading process . Moreover to stimulate selfish users a virtual credit based incentive mechanism is exploited in offloading decision . The experimental results demonstrate the ability of the proposed resource allocation approach to manage the trade off between time and energy comparing to traditional algorithms . 
",A resource allocation method to minimize time and energy of tasks in mobile cloud is proposed. A two stage approach using NSGA II entropy and TOPSIS methods is employed. A context aware Offloading Middleware for Mobile Cloud is developed to manage offloading. A virtual credit based incentive mechanism is used to motivate users to cooperate in offloading. The proposed method appropriately manages the trade off between time and energy.,,,
S0045790615000282," Provisioning of appropriate resources to cloud workloads depends on the Quality of Service requirements of cloud workloads . Based on application requirements of cloud users discovery and allocation of best workload resource pair is an optimization problem . Acceptable QoS can not be provided to the cloud users until provisioning of resources is offered as a crucial ability . QoS parameters based resource provisioning technique is therefore required for efficient provisioning of resources . In this paper QoS metric based resource provisioning technique has been proposed . The proposed technique caters to provisioned resource distribution and scheduling of resources . The main aim of this research work is to analyze the workloads categorize them on the basis of common patterns and then provision the cloud workloads before actual scheduling . The experimental results demonstrate that QoS metric based resource provisioning technique is efficient in reducing execution time and execution cost of cloud workloads along with other QoS parameters . 
",Cloud workloads have been analyzed and clustered through workload patterns. QoS metrics of each workload have been identified. We have analyzed the effect of number of workloads and resources on execution time and cost. Proposed technique demonstrates the minimization of cost and time simultaneously while adhering to workload deadline.,,,
S0045790614002419," Independent fine grain web services can be integrated to a value added coarse grain service through service composition technologies in Service Oriented Architecture . With the advent of cloud computing more and more web services in cloud may provide the same function but differ in performance . In addition the development of cloud computing presents a geographically distributed manner which elevates the impact of the network on the QoS of composited web services . Therefore a significant research problem in service composition is how to select the best candidate service from a set of functionally equivalent services in terms of a service level agreement . In this paper we propose a composition model that takes both QoS of services and cloud network environment into consideration . We also propose a web service composition approach based on genetic algorithm for geo distributed cloud and service providers who want to minimize the SLA violations . 
",A service composition model in geo distributed cloud environment is proposed. The composition approach considers both QoS of Web services and QoS of network. We propose a genetic algorithm to solve the composition problem. The notion of skyline is used to generate the initial population.,,,
S0045790615000130," Wireless sensor networks are formed by a large number of sensor nodes which are commonly known as motes . In the past few years several reliable congestion controlled and energy efficient transport layer protocols in wireless sensor networks have been developed and proposed in the literature . In this paper we have presented a hybrid and dynamic reliable transport protocol which provides the mechanism to dynamically assign the timing parameters to the nodes as well as enhance the protocol performance by using a hybrid Acknowledgement Negative Acknowledgement scheme . The performance of proposed protocol is tested under TinyOS Simulator varying different parameters and protocol settings and found that proposed protocol is able to program all the nodes when given proper pump fetch ratios is able to solve the booting sensor nodes problem by being able to wait till all the nodes finished booting and solves the all packets lost problem by acknowledging the receipt of its first packet delivered that is the inform message . 
",A hybrid and dynamic reliable transport protocol for wireless sensor networks is proposed. It provides hybrid Acknowledgement Negative Acknowledgement scheme. It controls the booting sensor nodes problem and the all packets lost problem. The performance of proposed protocol is tested under TinyOS Simulator.,,,
S0045790615004231,"This paper presents an important step towards the standardization of research works on Optical Character Recognition in Persian language. It describes the formations of a standard handwritten database including isolated digits isolated signs multi digit numbers numerical strings courtesy amounts and postal codes. In this regard binary images of 72 180 samples were extracted from the designed forms. These forms were filled by 180 writers selected from different ages genders and jobs. Then these forms were scanned at 300 dpi with a high speed scanner. Finally forms are segmented into samples and are stored in bitmap format. This database is named PHOND Persian Handwritten Optical Numbers Digits and it is available to the research community. Comparisons with the previous related databases illustrate the advantages of PHOND against other databases. Different experiments are done using PHOND database and the results are compared with other research works in handwritten recognition. 
",Introduction of a new database PHOND for handwriting Recognition of Digits Signs etc. in Persian. Presentation of a modified framing feature for handwriting Recognition. Recognition of digits signs and numerical strings in Persian using PHOND and other databases.,,,
S0045790616300222," Extreme Learning Machine proposes a non iterative training method for Single Layer Feedforward Neural Networks that provides an effective solution for classification and prediction problems . Its hardware implementation is an important step towards fast accurate and reconfigurable embedded systems based on neural networks allowing to extend the range of applications where neural networks can be used especially where frequent and fast training or even real time training is required . This work proposes three hardware architectures for on chip ELM training computation and implementation a sequential and two parallel . All three are implemented parameterizably on FPGA as an IP core . Results describe performance accuracy resources and power consumption . The analysis is conducted parametrically varying the number of hidden neurons number of training patterns and internal bit length providing a guideline on required resources and level of performance that an FPGA based ELM training can provide . 
",Extreme Learning Machine ELM on chip learning is implemented on FPGA. Three hardware architectures are evaluated. Parametrical analysis of accuracy resource occupation and performance is carried out.,,,
S0045782516301128," The present work is dedicated to the detection of Lagrangian Coherent Structures in viscous flows through the Finite Time Lyapunov Exponents which have been addressed by several works in the recent literature . Here a novel numerical technique is presented in the context of the Smoothed Particle Hydrodynamics models . Thanks to the Lagrangian character of SPH the trajectory of each fluid particle is explicitly tracked over the whole simulation . This allows for a direct evaluation of the FTLE field supplying a new way for the data analysis of complex flows . The evaluation of FTLE can be either implemented as a post processing or nested into the SPH scheme conveniently . In the numerical results three test cases are presented giving a proof of concept for different conditions . The last test case regards a naval engineering problem for which the present algorithm is successfully used to capture the submerged vortical tunnels caused by the splashing bow wave . 
",Algorithms for the detection of Lagrangian Coherent Structures LCSs are presented. Formulae for the Finite Time Lyapunov Exponent FTLE are derived for SPH models. The FTLE dependence on the use of enhanced SPH models is discussed. Post processing and nested in time implementations are discussed. Some test cases are considered giving a proof of concept for different conditions.,,,
S0045790615003006,"The druse an abnormal yellow white deposit on retina is a dominant characteristic of age related macular degeneration AMD which is a retinal disorder associated with age. The early detection of drusen is useful for ophthalmologists to diagnose the patients that suffer from AMD. An automated method has been proposed in this work to detect and segment drusen using retinal fundus images by gradient based segmentation to find true edges of drusen connected component labeling to remove suspicious pixels from drusen region and edge linking to connect all labeled pixels into a meaningful boundary. The proposed method outperforms other existing methods in detection of drusen with an accuracy sensitivity specificity of 96.17 89.81 99.00 on two publicly available retinal image databases. In order to grade the severity of AMD the detected drusen by the proposed method are further quantified into small intermediate and large with an accuracy of 88.46 98.55 and 88.37 respectively. 
",Designed a new drusen detection and segmentation method finding meaningful drusen boundaries . To find true edges of drusen a gradient based segmentation procedure is described . Connected component labeling is applied to remove suspicious pixels from drusen region . Edge linking is used to connect all labeled pixels into a meaningful boundary to detect drusen . The performance of proposed method is evaluated by statistical measures and quantification of drusen to grade severity of age related macular degradation . The proposed work characterizes the detected drusen in small intermediate and large soft to show its ability to grade age related macular degradation severity level helpful in early age related macular degradation diagnosis .,,,
S0045790615002694," In this paper we use and extend a parallel optoelectronic processor for image preprocessing and implement software tools for testing and evaluating the presented algorithms . After briefly introducing the processor and showing how images can be stored in it we adapt a number of local image preprocessing algorithms for smoothing edge detection and corner detection such that they can be executed on the processor in parallel . These algorithms are performed on all pixels of the input image in parallel and as a result in steps independent of its dimensions . We also develop a compiler and a simulator for evaluating and verifying the correctness of our implementations . 
",We use a parallel optoelectronic processor for image preprocessing. We extend the processor to improve its parallelism and to make it more compact. We adapt several preprocessing operators for the processor. We write a compiler and simulator for evaluating the presented algorithms.,,,
S0045790615001822," Spectrum sensing is an important aspect of cognitive radios . This paper describes a method for spectrum sensing based on the autocorrelation of the received samples . The proposed method was evaluated by means of experiments wherein the probabilities of detection and false alarm at different signal to noise ratios were observed . The platform used for the experiments was a set of Universal Software Radio Peripheral devices acting as radio frequency front ends in combination with GNU Radio software . Since the signal processing was performed in the software domain Gaussian noise of different levels was emulated by changing the standard deviation of a Python random number generator . In addition the output power of a signal generator was varied to obtain different levels of SNR . A metric called the Euclidean distance was derived to analyze the autocorrelation of the samples received by the USRP device in order to decide between two possible situations only noise present or signal plus noise present . The proposed method was compared with two methods one based on the value of the autocorrelation at the first lag and another one based on the power of the signal known as energy detection spectrum sensing technique . 
",Gaussian noise samples are delta correlated ACF t t . We can do spectrum sensing with the Euclidean distance between ACF t and a reference line. The Euclidean distance method performs better than the ED and ACF 1 methods.,,,
S0045790615003158," In ubiquitous high performance computing performing concurrent services is an important task of middleware . Because a major development effort is not easily achieved isolating a virtual machine may be a helpful solution but will likely suffer from additional overhead costs . This challenge can also be resolved by reusing the device driver . However these solutions are difficult to implement without the VM technique . In this study we present an alternative approach to minimizing overhead and develop a middleware called userspace virtualized middleware . Instead of bypassing the VM the proposed approach depends on userspace transparency and contention management to shift the VM concept into middleware . We introduce two strategies to enhance the system adaptively comprehensive restructuring by simplifying the VM memory mechanism and implementing zero copy buffers to reuse devices . The result demonstrates that Uware is feasible and could be applied in a broad variety of UHPC . 
",Uware is a virtualization based approach to effectively improve the UHPC application design. Uware is an alternative approach to minimize the overhead and to develop a middleware. The approach relies on contention management and zero copy buffer mechanism to from isolation model.,,,
S0045790614001347," Cognitive radio technology can solve the problems of spectrum scarcity and low spectrum utilization . However random behavior of the primary user appears to be an enormous challenge . In this paper we propose a PU behavior aware joint channel selection and allocation scheme . In the first step the channels are ranked based on statistics of the PU usage whereas in the second phase a proportional fair oriented channel allocation scheme is employed to allocate channels among CRs . We also introduce the concept of a time varying framing process that minimizes the overall data transmission time . Simulation results show that the proposed scheme outperforms existing schemes in terms of the transmission time and the number of collisions with the PUs . In addition it helps to save a significant amount of transmission power . Moreover it provides a significantly higher system throughput as compared to the existing schemes . 
",A proportional fair oriented sharing scheme is proposed to allocate channels. A PU activity model is developed to provide most stable channels to users. A time varying framing process is used to make variable sized frames at MAC layer. The proposed scheme reduces the delay collisions and increases the throughput.,,,
S0045790614003103," In this paper a locality aware NoC communication architecture is proposed . The architecture may reduce the energy consumption and latency in MultiProcessor Systems on Chips . It consists of two network layers which one layer is dedicated to the packets transmitted to near destinations and the other layer is used for the packets transmitted to far destinations . The actual physical channel width connecting the cores is divided between the two layers . The locality is defined based on the number of hops between the nodes . The relative significances of the two types of communications determine the optimum ratio for the channel width division . To assess the efficiency of the proposed method we compare its communication latency with that of conventional one for different channel widths communication traffic profiles and mesh sizes . 
",We propose two layer network on chip to separate local and non local traffics. For each locality rate we obtain the best division ratio for the bitwidth of channels. We define locality based on the number of hops between source and destination nodes. For each traffic locality is defined to include 50 percent of all communication.,,,
S0045790615002748," The increasing demand of cloud computing motivates the researchers to make cloud environment more efficient for its users and more profitable for the providers . Though virtualization technology helps to increase the resource utilization still the operational cost of cloud gradually increases mainly due to the consumption of large amount of electrical energy . So to reduce the energy consumption virtual machines are dynamically consolidated to lesser number of physical machines by live VM migration technique . But this may cause SLA violation and the provider is penalized . So to maintain an energy performance trade off the number of VM migration should be minimized . VM migration primarily takes place in two cases for hotspot mitigation and to switch off the underutilized nodes by migrating all its VMs . If a host is found to be overloaded then instead of immediately migrating some of its VMs we can check whether the migration is really required or not . For this we have proposed a load prediction algorithm to decide whether the migration will be performed or not . After the decision has been taken the algorithm finds a suitable destination host where the VM will be shifted . For this we have proposed a novel approach to decide whether a particular host is suitable as destination depending on its probable future load . We have simulated our algorithms in CloudSim using real world workload traces and compared them with the existing benchmark algorithms . Results show that the proposed methods significantly reduce the number of VM migration and subsequent energy consumption while maintaining the SLA . 
",Time series based forecasting methods are used to predict future load of a system. If the current and the next predicted load of a server exceed the dynamic upper threshold then migration will take place. The forecasting methods are also used to predict multiple n future load of the system. Our algorithms are able to find more suitable destination host for VM placement. They are capable of saving energy by reducing number of over utilized hosts and virtual machine migration. Maximum QoS requirements are fulfilled due to less violation of SLA.,,,
S0045790613002474," Industrial image processing tasks especially in the domain of optical metrology are becoming more and more complex . While in recent years standard PC components were sufficient to fulfill the requirements special architectures have to be used to build high speed image processing systems today . For example for adaptive optical systems in large scale telescopes the latency between capturing an image and steering the mirrors is critical for the quality of the resulting images . Commonly the applied image processing algorithms consist of several tasks with different granularities and complexities . Therefore we combined the advantages of multicore CPUs GPUs and FPGAs to build a heterogeneous image processing pipeline for adaptive optical systems by presenting new architectures and algorithms . Each architecture is well suited to solve a particular task efficiently which is proven by a detailed evaluation . With the developed pipeline it is possible to achieve a high throughput and to reduce the latency of the whole steering system significantly . 
",Image processing applications can benefit from heterogeneous computing architectures. Using FPGAs GPUs and CPUs together enables a fast image processing pipeline. FPGA architectures within smart cameras can increase throughput and decrease latency. The image processing pipeline was demonstrated at the example of optical metrology.,,,
S0045782516301281," Variational Multiscale Finite Element Methods are robust for the development of general formulations for the solution of multiphysics and multiscale transport problems . To obtain a tractable and computationally efficient model VMS methods often rely on a residual based algebraic approximation of the sub grid scales using a so called intrinsic time scales matrix which depends on the problem s overall differential operator and represents the main model parameter . A novel technique for approximating the intrinsic time scales matrix for generic transport problems in a relatively inexpensive manner is presented . The method is denoted Transport Equivalent Scaling and is based on the monolithic casting of the transport problem as a system of transient advective diffusive reactive equations and a subsequent scaling of the coefficient matrices such to preserve each type of transport flux . An algebraic VMS formulation incorporating the TES method is complemented with a discontinuity capturing approach and implemented within a FEM solver for the solution of TADR problems . The solution of the global discrete system is accomplished using a generalized alpha time stepper together with a globalized inexact Newton Krylov nonlinear solver . The effectiveness of the TES formulation is verified with the simulation of benchmark incompressible compressible and magnetohydrodynamic flow problems . The results demonstrate that the TES method seamlessly handles incompressible compressible flows in a unified manner . The convergence process using the TES approach and a more standard approximation for the intrinsic time scales as well as the effect of the DC approach are also investigated . Analysis of the intrinsic time scales for a one dimensional incompressible flow model reveals the similitudes and differences between the TES formulation and other conventional methods . 
",Transport Equivalent Scaling TES for algebraic VMS models is introduced. TES leads to intrinsic time scales expressions suitable for generic transport systems. The convergence process of TES and other standard approximations is investigated. Method validated with incompressible compressible magnetohydrodynamic benchmarks. TES is suitable for the seamless unified handling of incompressible and compressible flows.,,,
S0045790615001810," Detecting the linear features in an image is a key technology for different applications . In this paper a simple and effective algorithm based on the hit or miss transform is proposed . To detect linear features with different directions multi structuring elements corresponding to different directions are constructed . To detect linear features with different widths a multi scale extension of the constructed multi structuring elements is used . Then the grey level hit or miss transform that utilizes the constructed multi scales of multi structuring elements could effectively extract all of the possible linear features without thresholding . Therefore after refining the extracted linear feature regions using three simple steps the final linear features could be effectively detected . Experimental results on different images from different applications show that the proposed algorithm performs well for detecting linear features with different widths different grey distributions and noises . 
",Multi scale extension of multi structuring elements for hit or miss transform. Utilizing the grey level hit or miss transform for linear feature detection. Performing linear feature detection without thresholding.,,,
S0045790614003164," Backside illuminated pixel structure is proposed and evaluated as the building block for the image sensor being used as epiretinal prosthesis implant . The image sensor pixel is designed with the parameters of 90nm technology node of standard CMOS process . The image sensor is consisted of a p sub n well structure as the photosensitive area with the pixel pitch of 20 m. The maximum fill factor is observed due to separation of photosensitive area with the readout transistor surface in backside illumination technology . 90 quantum efficiency at 600nm wavelength and the dark current of 74.6nA cm2 at room temperature is achieved for the optimized pixel . The application of deep backside Deep Trench Isolation with high depth n well doping profiles results in a significant reduction of crosstalk . 
",Backside illuminated pixel structure is proposed and evaluated as epiretinal prosthesis implant. The maximum fill factor is observed due to separation of photosensitive area. The application of deep backside Deep Trench Isolation DTI results in a significant reduction of crosstalk.,,,
S0045790615002256," Preclinical micro computed tomography images are of utility for 3D morphological bone evaluation which is of great interest in cancer detection and treatment development . This work introduces a compression strategy for microCTs that allocates specific substances in different Volumes of Interest . The allocation procedure is conducted by the Hounsfield scale . The VoIs are coded independently and then grouped in a single DICOM compliant file . The proposed method permits the use of different codecs identifies and transmit data corresponding to a particular substance in the compressed domain without decoding the volume and allows the computation of the 3D morphometry without needing to store or transmit the whole image . The proposed approach reduces the transmitted data in more than 90 when the 3D morphometry evaluation is performed in high density and low density bone . This work can be easily extended to other imaging modalities and applications that work with the Hounsfield scale . 
",Preclinical micro computed tomographies are used for morphological bone evaluation. A Volume of Interest VoI strategy for microCT images is proposed. The proposal allocates substances in different VoIs using the Hounsfield scale. The proposed approach may reduce the transmitted data in more than 90 . This proposal can be used to other imaging modalities work with the Hounsfield scale.,,,
S0045790615001238," Key distribution is required to secure e health applications in the context of Internet of Things . However resources constraints in IoT make these applications unable to run existing key management protocols . In this paper we propose a new lightweight key management protocol . This protocol is based on collaboration to establish a secure end to end communication channel between a highly resource constrained node and a remote entity . The secure channel allows the constrained node to transmit captured data while ensuring confidentiality and authentication . To achieve this goal we propose offloading highly consuming cryptographic primitives to third parties . As a result the constrained node obtains assistance from powerful entities . To assess our protocol we conduct a formal validation regarding security properties . In addition we evaluate both communication and computational costs to highlight energy savings . The results show that our protocol provides a considerable gain in energy while its security properties are ensured . 
",We introduce an energy aware key management protocol for e health applications. The protocol guarantees the end to end principle. Constrained nodes offload asymmetric cryptographic operations to third parties. We conduct a formal validation regarding the security properties of our protocol. Analysis results highlight energy gains on constrained nodes.,,,
S0045790614002663," Active authentication is the process of continuously verifying a user based on their on going interaction with a computer . In this study we consider a representative collection of behavioral biometrics two low level modalities of keystroke dynamics and mouse movement and a high level modality of stylometry . We develop a sensor for each modality and organize the sensors as a parallel binary decision fusion architecture . We consider several applications for this authentication system with a particular focus on secure distributed communication . We test our approach on a dataset collected from 67 users each working individually in an office environment for a period of approximately one week . We are able to characterize the performance of the system with respect to intruder detection time and robustness to adversarial attacks and to quantify the contribution of each modality to the overall performance . 
",Behavioral biometrics keystroke dynamics mouse movement stylometry. A parallel binary decision fusion architecture with 11 sensors. A dataset collected from 67 users each working in an office environment for a week. Achieve below 1 error rates FAR FRR after only 30s of activity. Characterize robustness of system to adversarial attacks.,,,
S0045790615000981," Recently several image encryption algorithms based on DNA encoding and chaotic maps have been proposed which create a novel direction in image encryption . By a careful examination on most of these image cryptosystems we find that DNA operators can only influence one DNA base which leads to poor diffusion . A recent image encryption scheme based on DNA encoding and chaos is treated as a case study . The flaws of this algorithm are illustrated . By applying a known plaintext attack we demonstrate that a hacker can determine the chaotic sequences used to confuse the image and reveal the plain image . Finally a suggestion is given to enhance the diffusion ability of image encryption scheme based on DNA encoding and chaos . The experiment results prove that the suggestion is effective . 
",The flaws of an image encryption algorithm based on DNA encoding and chaos are founded. According to the flaws a known plaintext attack is presented to reveal the plain image from the cipher image. A remedy is suggested to enhance the security of the image encryption algorithm.,,,
S0045790615000257," The world of home automation is an exciting field that has exploded with new technologies and today is known as an area where The internet of things vision becomes reality . The primary advantages that stem from this concept include how each device forms a small part of the Internet by which the advanced system is able to interact and communicate maximizes safety security comfort convenience and energy savings . This paper proposes an implementation of Sensor Web node as a part of IoT using a Raspberry Pi inexpensive fully customizable and programmable small computer with support for a large number of peripherals and network communication . Using this technology in an example of monitoring and determining the confidence of fire in building a full system based on Sensor Web elements is created and developed starting from a scratch . The given example confirms the advantage of Raspberry Pi flexibility and extensive possibility of its usage . 
",Implementation of Sensor Web node as a part of Internet of Things using a Raspberry Pi. Ubiquitous solution of Home automation system appears everywhere and anywhere. Do It Yourself approach user build own solution that meets specific needs. Interdisciplinary approach electronic programming services and soft computing. System for monitoring confidence of fire created and developed starting from a scratch.,,,
S0045790615003572," The problem of Support Vector Machines tuning parameters has been paramount in the last years mainly because of the high computational burden for SVM training step . In this paper we address this problem by introducing a recently developed evolutionary based algorithm called Social Spider Optimization as well as we introduce SSO for feature selection purposes . The model selection task has been handled in three distinct scenarios feature selection tuning parameters and feature selection tuning parameters . Such extensive set of experiments against with some state of the art evolutionary optimization techniques demonstrated SSO is a suitable approach for SVM model selection since it obtained the top results in 8 out 10 datasets employed in this work . Notice the best scenario seemed to be the combination of both feature selection and SVM tuning parameters . In addition we validated the proposed approach in the context of theft detection in power distribution systems . 
",Social Spider Optimization for model selection in Support Vector Machines. Three distinct scenarios were evaluated. Proposed approach validated in the context of of theft detection in power distribution systems.,,,
S0045790614001669," Having a direct effect on network lifetime balanced energy consumption is one of the key challenges in wireless networks . In this paper we investigate the effects of node mobility on energy balancing in wireless networks . We construct a Linear Programming framework that jointly captures data routing mobility and energy dissipation aspects . We explore the design space by performing numerical analysis using the developed LP framework . Our results show that mobility has significant effects on the energy dissipation trends of wireless nodes . Mobility can improve the energy balancing up to a certain level however extreme mobility may lead to a degradation in energy balancing of wireless networks . 
",Utilizing mobility for energy balancing is the key insight of our work. We construct a Linear Programming LP framework that jointly captures data routing mobility and energy dissipation aspects. Mobility can improve the energy balancing up to a certain level. We delineate future research opportunities in the area.,,,
S0045782516300068," A novel non intrusive reduced order model for fluid structure interaction has been developed . The model is based on proper orthogonal decomposition and radial basis function interpolation method . The method is independent of the governing equations therefore it does not require modifications to the source code . This is the first time that a NIROM was constructed for FSI phenomena using POD and RBF interpolation method . Another novelty of this work is the first implementation of the FSI NIROM under the framework of an unstructured mesh finite element multi phase model and a combined finite discrete element method based solid model . The capability of this new NIROM for FSI is numerically illustrated in three coupling simulations a one way coupling case a two way coupling case and a vortex induced vibration of an elastic beam test case . It is shown that the FSI NIROM results in a large CPU time reduction by several orders of magnitude while the dominant details of the high fidelity model are captured . 
",First non intrusive reduced order model for fluid structure interactions. First implementation of such NIROM to a combined fluid FLUIDTY and solid Y2D models. The POD RBF NIROM does not require any change knowledge of original code. First demonstration of NIROM in one and two way fluid and solid coupling cases. A large reduction in the CPU computation cost by 5 6 orders of magnitude while the accuracy of the solutions is maintained.,,,
S0045782516300214," While the interest in higher order models in physics and mechanics grows their numerical simulation still poses a challenge especially for arbitrary shaped three dimensional domains . This contribution presents the mathematical framework as well as the application to different problems in the field of material science fracture mechanics and diffusion problems . All models under consideration require at least continuity which prevents the application of standard finite element analysis and local mesh refinements . Introducing isogeometric analysis for the discretization in a finite element framework enables us to deal with these requirements . Moreover a general hierarchical refinement scheme based on a subdivision projection is presented here for one two and three dimensional simulations . This technique allows to enhance the approximation space using finer splines on each level but preserves the partition of unity as well as the continuity properties of the original discretization . Using this mathematical framework the improved convergence of a Kuramoto Sivashinsky model a mesh adapted thermal diffusion simulation and computations of a priori unknown crack propagation in different fracture modes underline the versatility of the presented hierarchical refinement scheme . 
",We present a hierarchical refinement scheme for isogeometric analysis. We investigate different higher order phase field models. A Kuramoto Sivashinsky system. A higher order phase field approach to 3D brittle fracture. A higher order temperature controlled diffusion in polymer blends.,,,
S0045790616300283," Homeland security represents one of the most relevant application contexts for smart cities attracting the interest of both authorities and the research community . In case of a crisis event occurring in the urban area authorities are responsible for effectively managing response operations . A critical challenge in emergency management is the lack of real time coordinated reaction capabilities driven by integrated decision making facilities based on the information obtained by first responders acting on the crisis site . This work aims at supporting coordinated emergency management in smart cities based on the localization of first responders during crisis events . We present a hybrid cloud architecture for managing computing and storage resources needed by command control activities in emergency scenarios complemented by a first responder localization service relying on a novel positioning approach which combines the strength of signals received from landmarks placed by first responders on the crisis site with information obtained from motion sensors . 
",Emergency management solution for smart cities based on a hybrid decentralized service oriented cloud platform. Uses mobile communication and smart sensing facilities for managing command control activities in urban areas. Supports a first responder localization service based on a hybrid positioning approach that combines Landmark based and Landmark free systems and leverages cloud facilities. Such approach relies on both the strength of signals received from landmark nodes and information gathered from motion sensors.,,,
S0045790615002815,"Parallelization of task is considered to be a huge challenge for future extreme scale computing system. Sophisticated parallel computing system necessitates solving the bus contention in a most efficient manner with high computation rate. The major challenge to deal with is the achievement of high CPU core usage through increased task parallelism by keeping moderate bus bandwidth allocation. In order to tackle the aforesaid problems a novel arbitration technique known as Parallel Adaptive Arbitration PAA has been proposed for the masters designed according to the traffic behaviour of the data flow. These masters are implemented using a synthetic benchmark program that measures sustainable memory bandwidth and the corresponding computational rate. The proposed arbitration technique is a strong case in favour of fair bandwidth optimization and high CPU utilization as it consumes the processor cores up to 77 through high degree of task parallelization and also reduces bandwidth fluctuation. Some of the works published so far in this area are reviewed classified according to their objectives and presented in an organised manner with a conclusion. 
",If we understand Moore s law and Amdahl s law then we can conclude that increasing number of CPU cores in a computing system is of no use just to attain high computation rate. We have designed a new arbitration technique which can use the CPU cores in a most optimised manner and can achieve high degree of task parallelism. The designed arbitration technique is superior to other existing arbitration technique in terms of CPU usage bandwidth optimization and latency. The designed arbitration technique has been tested using high performance benchmark program to analyse its efficiency.,,,
S0141938215000293," The effect of 1 3 5 Trisbenzene doping on electroluminescent properties of poly N N bisbenzidine was investigated . A series of organic light emitting devices integrated with single layer poly TPD blended single layer poly TPD TPBi or bilayer poly TPD TPBi were fabricated and characterized . An excimer emission band at 500nm was found in the poly TPD film poly TPD TPBi blend film and poly TPD TPBi bilayer film . It was observed that the planar geometry of poly TPD was related to the formation of excimers . The electromer emission which was absent in photoluminescence was investigated by applying an external electrical field to devices with non doped and TPBi doped poly TPD . Only the electromer emission was observed in the devices with TPBi doped poly TPD due to the impeded intrinsic and excimer emissions . The planar geometry of poly TPD molecules may be destroyed due to the longer inter ion distance with the doping of TPBi . 
",Excimer emission is observed in the solid doped and un doped poly TPD film. Monomer excimer and electromer emission are obtained in the devices with single layer poly TPD or bilayer poly TPD TPBi. Single electromer emission is acquired in TPBi doped poly TPD OLEDs. Electromer emission is increased under high TPBi contentation and electric field.,,,
S0167839613001052," We develop a method for computing all the generalized asymptotes of a real plane algebraic curve implicitly defined by an irreducible polynomial . The approach is based on the notion of perfect curve introduced from the concepts and results presented in Blasco and P rez D az . In addition we study some properties concerning perfect curves and in particular we provide a necessary and sufficient condition for a plane curve to be perfect . Finally we show that the equivalent class of generalized asymptotes for a branch of a plane curve can be described as an affine space for a certain m . 
",We introduce the notions of perfect curve and generalized asymptote g asymptote . We provide an algorithm that computes a g asymptote for each infinity branch of an algebraic plane curve. We state necessary and sufficient conditions for a curve to be perfect. We propose a method to obtain under general conditions all the g asymptotes approaching a given infinity branch.,,,
S0141938215300445," We report on the temperature dependence of dielectric properties of nematic liquid crystals impregnated with BaTiO3 ferroelectric nanoparticles . The behavior of ion transport at low frequencies is discussed by means of dielectric spectroscopy which allows the ionic concentration and the relaxation time of electrode polarization to be deduced . The experimental results imply that the ferroelectric nanoparticles can not only increase the traveling time of ions between two electrodes but also suppress the buildup of the electric double layers . Verified by the voltage holding ratio of cells containing various contents of BTO nanoparticles it is obvious that doping BTO into liquid crystals is a low cost and easy way to improve the device performance . 
",Electric and dielectric properties of BTO FNPs dispersed in E44 have been studied. BTO FNPs suppress the ion transport lowering the mobile ion concentration by 50 . BTO FNPs prolong the electrode polarization relaxation time up to 16 times. BTO FNPs as a dopant promote the voltage holding ratio up to twice higher.,,,
S0098300414001873," Landslide susceptibility mapping is making increasing use of GIS based spatial analysis in combination with multi criteria evaluation methods . We have developed a new multi criteria decision analysis method for LSM and applied it to the Izeh River basin in south western Iran . Our method is based on fuzzy membership functions derived from GIS analysis . It makes use of nine causal landslide factors identified by local landslide experts . Fuzzy set theory was first integrated with an analytical hierarchy process in order to use pairwise comparisons to compare LSM criteria for ranking purposes . FMFs were then applied in order to determine the criteria weights to be used in the development of a landslide susceptibility map . Finally a landslide inventory database was used to validate the LSM map by comparing it with known landslides within the study area . Results indicated that the integration of fuzzy set theory with AHP produced significantly improved accuracies and a high level of reliability in the resulting landslide susceptibility map . Approximately 53 of known landslides within our study area fell within zones classified as having very high susceptibility with the further 31 falling into zones classified as having high susceptibility . 
",We have developed a new multi criteria decision analysis method for landslide mapping. Our method is based on fuzzy membership functions derived from GIS analysis. We integrated extended fuzzy with AHP to overcome of associated uncertainty with GIS Multicriteria Decision Making. The results demonstrate that the accuracy of GIS Multicriteria can be improved through FAHP.,,,
S0141938216300087," The objective of this study is to put forward a new non contact resistance measurement method for repeating bending tests of transparent electrodes deposited on flexible display substrates . The study utilizes a terahertz time domain spectroscopy method to measure electrical properties of flexible polyethylene terephthalate indium tin oxide samples up to 20 000 bending times . In addition this study utilizes THz TDS method to measure electrical characteristics of flexible substrates with hard coat films . Accordingly the percentage errors of measured sheet resistances based on THz TDS method are less than or equal to 5.5 for comparison with a contact type four point probe method or our previously reported flexible characteristic inspection system method . The values show a reasonable agreement with contact mode sheet resistance measurements . Therefore the electrical properties of thin films are measured offline or online easily by using this method . 
",Non contact measurement based on a terahertz time domain spectroscopy is presented. Electrical properties of flexible display substrates under bending test are spectroscopically measured. Measurement results show a reasonable agreement with contact mode measurements.,,,
S0140366415003631," Twitter is a popular social network which allows millions of users to share their opinions on what happens all over the world . In this work we present a system for real time Twitter data analysis in order to follow popular events from the user s perspective . The method we propose extends and improves the Soft Frequent Pattern Mining algorithm by overcoming its limitations in dealing with dynamic real time detection scenarios . In particular in order to obtain timely results the stream of tweets is organized in dynamic windows whose size depends both on the volume of tweets and time . Since we aim to highlight the user s point of view the set of keywords used to query Twitter is progressively refined to include new relevant terms which reflect the emergence of new subtopics or new trends in the main topic . The real time detection system has been evaluated during the 2014 FIFA World Cup and experimental results show the effectiveness of our solution . 
",A framework for real time Twitter data analysis We propose improvements to the Soft Frequent Pattern Mining SFPM algorithm The stream of tweets is organized in dynamic windows whose size depends both on the volume of tweets and time The set of keywords used to query Twitter is progressively refined to highlight the user s point of view Comparisons with two state of the art systems,,,
S0167642315000659," A hierarchical approach for modelling the adaptability features of complex systems is introduced . It is based on a structural level S describing the adaptation dynamics of the system and a behavioural level B accounting for the description of the admissible dynamics of the system . Moreover a unified system called is defined by coupling S and B . The adaptation semantics is such that the S level imposes structural constraints on the B level which has to adapt whenever it no longer can satisfy them . In this context we introduce weak and strong adaptability i.e . the ability of a system to adapt for some evolution paths or for all possible evolutions respectively . We provide a relational characterisation for these two notions and we show that adaptability checking i.e . deciding if a system is weakly or strongly adaptable can be reduced to a CTL model checking problem . We apply the model and the theoretical results to the case study of a motion controller of autonomous transport vehicles . 
",Hierarchical model for multi level adaptive systems. Relational characterisation of strong and weak adaptability. Adaptability checking is reduced to a CTL model checking problem. Application to the case study of ATVs motion control.,,,
S0167839614000806," In this paper two kinds of bivariate S basis functions tensor product S basis functions and triangular S basis functions are constructed by means of the technique of generating functions and transformation factors . These two kinds of bivariate S basis functions have lots of important properties such as non negativity partition of unity linear independence and so on . The framework of the tensor product S basis functions provides a unified scheme for dealing with several kinds of tensor product basis functions such as the tensor product Bernstein basis functions the tensor product Poisson basis functions and some other new tensor product basis functions . The framework of the triangular S surface basis functions includes the triangular Bernstein basis functions the rational triangular Bernstein basis functions and some other new triangular basis functions . Moreover the corresponding two kinds of S surfaces are constructed by means of these two kinds of bivariate basis functions respectively . These two kinds of S surface patches have the important properties of surface modeling such as affine invariance convex hull property and so on . 
",We construct two kinds of bivariate S bases tensor product S bases and triangular S bases. We show that the frameworks of these two kinds of S bases include the classical Bezier type bases. We construct two kinds of S surface patches by means of the corresponding S bases. We obtain the important properties of these two kinds of bivariate S bases and the corresponding S surface patches.,,,
S0167839615000813," We introduce and analyze univariate linear and stationary subdivision schemes for refining noisy data by fitting local least squares polynomials . This is the first attempt to design subdivision schemes for noisy data . We present primal schemes with refinement rules based on locally fitting linear polynomials to the data and study their convergence smoothness and basic limit functions . Then we provide several numerical experiments that demonstrate the limit functions generated by these schemes from initial noisy data . The application of an advanced local linear regression method to the same data shows that the methods are comparable . In addition several extensions and variants are discussed and their performance is illustrated by examples . We conclude by applying the schemes to noisy geometric data . 
",We construct univariate subdivision schemes for noisy data. The constructed schemes are based on fitting local least squares polynomials. We study the convergence smoothness and basic limit functions of these schemes. A statistical model is analyzed and validated by several numerical examples. We present applications of the schemes for data sampled from curves and surfaces.,,,
S0097849313000459,"Recent hardware technologies have enabled acquisition of 3D point clouds from real world scenes in real time. A variety of interactive applications with the 3D world can be developed on top of this new technological scenario. However a main problem that still remains is that most processing techniques for such 3D point clouds are computationally intensive requiring optimized approaches to handle such images especially when real time performance is required. As a possible solution we propose the use of a 3D moving fovea based on a multiresolution technique that processes parts of the acquired scene using multiple levels of resolution. Such approach can be used to identify objects in point clouds with efficient timing. Experiments show that the use of the moving fovea shows a seven fold performance gain in processing time while keeping 91.6 of true recognition rate in comparison with state of the art 3D object recognition methods. 
",Object recognition foveation speedup 7x compared to the non foveated approach. True recognitions rates are kept high with false recognitions at 8.3 . Faster setups with 91.6 recognition rate and 14x improvement were also achieved. The slowest configuration still shows almost 3x faster computing times,,,
S0140366415003928," This study proposes a capable scalable and reliable edge to edge model for filtering malicious traffic through real time monitoring of the impact of user behavior on quality of service regulations . The model investigates user traffic including that injected through distributed gateways and that destined to gateways that are experiencing actual attacks . Misbehaving traffic filtration is triggered only when the network is congested at which point burst gateways generate an explicit congestion notification to misbehaving users . To investigate the behavior of misbehaving user traffic packet delay variation ratios are actively estimated and packet transfer rates are passively measured at a unit time . Users who exceed the PDV bit rates specified in their service level agreements are filtered as suspicious users . In addition suspicious users who exceed the SLA bandwidth bit rates are filtered as network intruders . Simulation results demonstrate that the proposed model efficiently filters network traffic and precisely detects malicious traffic . 
",Simulation scenarios are conducted to monitor user violations on QoS regulations. Possible violations are investigated to differentiate malicious traffic from normal traffic. ECN sensors are deployed at network edges to monitor misbehaving traffic. PDV and PTR ratios are compared with SLA guarantees to filter malicious traffic. Research findings show improvements on accuracy scalability and reliability.,,,
S0140366415003813," To satisfy the demand for higher data rate while maintaining the quality of service a dense long term evolution cells environment is required . This imposes a big challenge to the network when it comes to performing handover . Cell selection has an important influence on network performance to achieve seamless handover . Although a successful handover is accomplished it might be to a wrong cell when the selected cell is not an optimal one in terms of signal quality and bandwidth . This may cause significant interference with other cells handover failure or handover ping pong consequently degrading the cell throughput . To address this issue we propose a multiple criteria decision making method . In this method we use an integrated fuzzy technique for order preference by using similarity to ideal solution on S criterion availability of resource blocks and uplink signal to interference plus noise ratio . The conventional cell selection in LTE is based on S criterion which is inadequate since it only relies on downlink signal quality . A novel method called fuzzy multiple criteria cell selection is proposed in this paper . FMCCS considers RBs utilization and user equipment uplink condition in addition to S criterion . System analysis demonstrates that FMCCS managed to reduce handover ping pong and handover failure significantly . This improvement stems from the highly reliable cell selection technique that leads to increased throughput of the cell with a successful handover . The simulation results show that FMCCS outperforms the conventional and cell selection scheme methods . 
",Optimal cell selection scheme that allows a roaming UE to reconnect to the most suitable cell while maintaining its quality of service QoS requirements. Very low ping pong handover ratio and handover failure ratio. Higher cell throughput gain. Considering uplink and downlink conditions make a reliable radio link connection. A comprehensive investigation of proposed scheme at varying UE speeds is demonstrating its robustness.,,,
S0164121215001740,"There exists no generally accepted theory in software engineering and at the same time a scientific discipline needs theories. Some laws hypotheses and conjectures exist but yet no generally accepted theory. Several researchers and initiatives emphasize the need for theory in the discipline. The objective of this paper is to formulate a theory of software engineering. The theory is generated from empirical observations of industry practice including several case studies and many years of experience in working closely between academia and industry. The theory captures the balancing of three different intellectual capitals human social and organizational capitals respectively. The theory is formulated using a method for building theories in software engineering. It results in a theory where the relationships between the three different intellectual capitals are explored and explained. The theory is illustrated based on an industrial case study where it is shown how decisions made in industry practice are explainable with the formulated theory and the consequences of the decisions are made explicit. Based on the positive results it is concluded that the theory may have a good explanatory power although more evaluations are needed. 
",Software engineering is knowledge intensive and intellectual capital is crucial. Intellectual capital may be divided into human social and organizational capitals. A theory of software engineering is formulated based on these three capitals. The theory is based on industrial observations and illustrated in a case study. The theory may be used by both researchers and practitioners.,,,
S0141938214000808," Thin amorphous silicon films were crystallized into polycrystalline silicon by combining solid phase crystallization and subsequent excimer laser annealing . Then thin film transistors were fabricated by using the poly Si formed in the single and double excimer laser scanned area . The device performance of the TFTs fabricated with the excimer laser energy density of 230mJ cm2 is almost equal for the single and double scanned area . This observation indicates that the overlapping laser irradiation with the laser energy density below 230mJ cm2 does not change the characteristics of TFTs . Based on this result we discuss the correlation between performance of active matrix organic light emitting display panels and excimer laser energy density during ELA for SPC treated and non treated poly Si films . 
",Poly Si TFTs were fabricated by SPC ELA process. Correlation between the performance of AMOLED and ELA condition was studied. We found the optimized ELA condition for SPC treated a Si for AMOLED.,,,
S0167839614000582," In this work we propose a structured computational framework for modelling the envelope of the swept volume that is the boundary of the volume obtained by sweeping an input solid along a trajectory of rigid motions . Our framework is adapted to the well established industry standard brep format to enable its implementation in modern CAD systems . This is achieved via a local analysis which covers parametrizations and singularities as well as a global theory which tackles face boundaries self intersections and trim curves . Central to the local analysis is the funnel which serves as a natural parameter space for the basic surfaces constituting the sweep . The trimming problem is reduced to the problem of surface surface intersections of these basic surfaces . Based on the complexity of these intersections we introduce a novel classification of sweeps as decomposable and non decomposable . Further we construct an invariant function on the funnel which efficiently separates decomposable and non decomposable sweeps . Through a geometric theorem we also show intimate connections between local curvatures and the inverse trajectory used in earlier works as an approach towards trimming . In contrast to the inverse trajectory approach of testing points is a computationally robust global function . It is the key to a complete structural understanding and an efficient computation of both the singular locus and the trim curves which are central to a stable implementation . Several illustrative outputs of a pilot implementation are included . 
",A complete computational framework for solid sweep is proposed. A novel classification of sweeps into simple decomposable nondecomposable is given. A geometric invariant locates all the trim curves for nondecomposable sweeps. The funnel serves as the parametrization space for the envelope.,,,
S0140366414000954," In this paper we introduce a moving target defense mechanism that defends authenticated clients against Internet service DDoS attacks . Our mechanism employs a group of dynamic hidden proxies to relay traffic between authenticated clients and servers . By continuously replacing attacked proxies with backup proxies and reassigning the attacked clients onto the new proxies innocent clients are segregated from malicious insiders through a series of shuffles . To accelerate the process of insider segregation we designed an efficient greedy algorithm which is proven to have near optimal empirical performance . In addition the insider quarantine capability of this greedy algorithm is studied and quantified to enable defenders to estimate the resource required to defend against DDoS attacks and meet defined QoS levels under various attack scenarios . Simulations were then performed which confirmed the theoretical results and showed that our mechanism is effective in mitigating the effects of a DDoS attack . The simulations also demonstrated that the overhead introduced by the shuffling procedure is low . 
",We design a moving target mechanism to defend against Internet service DDoS attacks. We propose a shuffling model to segregate innocent clients from malicious insiders. A greedy algorithm is designed to accelerate the segregation of insiders. Greedy algorithm enables defenders to plan defense resource to meet QoS goals.,,,
S0165168413003307," Video transcoding is a legitimate operation widely used to modify video format in order to access the video content in the end user s devices which may have some limitations in the spatial and temporal resolutions bit rate and video coding standards . In many previous watermarking algorithms the embedded watermark is not able to survive video transcoding because this operation is a combination of some aggressive attacks especially when lower bit rate coding is required in the target device . As a consequence of the transcoding operation the embedded watermark may be lost . This paper proposes a robust video watermarking scheme against video transcoding performed on base band domain . In order to obtain the watermark robustness against video transcoding four criteria based on Human Visual System are employed to embed a sufficiently robust watermark while preserving its imperceptibility . The quantization index modulation algorithm is used to embed and detect the watermark in 2D Discrete Cosine Transform domain . The watermark imperceptibility is evaluated by conventional peak signal to noise ratio and structural similarity index obtaining sufficiently good visual quality . Computer simulation results show the watermark robustness against video transcoding as well as common signal processing operations and intentional attacks for video sequences . 
",Four criteria based on HVS and DCT domain are used to embed a robust watermark. We apply a quantization index modulation scheme to embed and detect the watermark. Watermark scheme is robust to transcoding video codec and low bit rate conversions. Watermark scheme is robust to signal processing and video frame based attacks. Better performance than four recently reported video watermarking schemes.,,,
S0141938214000705," Three kinds of lanthanide phosphors have been successfully synthesized based on three different ways such as molten salts co precipitation supersonic and microwave irradiations . The as prepared powder materials all exhibited red luminescence . Their crystal structures or morphologies were studied by means of X ray powder diffraction and scanning electronic microscope . Eu3 doped LaF3 CaF2 phosphor can be emissive under excitation at longer wavelengths excitations . Supersonic and microwave irradiations have shortened the reaction time of LaF3 Eu3 crystals in 40min under very low temperature . 
",LnF3 has been synthesized through three different ways. They all can exhibit red emissions. Eu3 doped LaF3 CaF2 phosphor can be emissive under longer wavelengths.,,,
S0045790616300854,"We consider a novel variational model for image reconstruction based on second order partial differential equations in this paper. This inpainting approach is inspired by the anisotropic diffusion based denoising solutions. A stable variational scheme is proposed first. Then a nonlinear differential model is derived from it by determining the corresponding Euler Lagrange equation and applying the steepest descent method. A rigorous mathematical treatment of this anisotropic diffusion scheme is provided next. This nonlinear second order diffusion model is then numerically approximated a consistent and explicit finite difference based discretization scheme being developed. Some successful image inpainting experiments and method comparison are also provided in this article. 
",A novel variational scheme for image inpainting is proposed in this paper first. A nonlinear anisotropic diffusion model is obtained from this PDE variational scheme. A mathematical treatment of the obtained second order PDE based model is provided. An explicit finite difference based numerical approximation scheme is constructed for it. Successful image reconstruction experiments and method comparison are also described.,,,
S0165168414003971," The analysis of the stability and numerical simulation of Costas loop circuits for high frequency signals is a challenging task . The problem lies in the fact that it is necessary to simultaneously observe very fast time scale of the input signals and slow time scale of phase difference between the input signals . To overcome this difficult situation it is possible following the approach presented in the classical works of Gardner and Viterbi to construct a mathematical model of Costas loop in which only slow time change of signal s phases and frequencies is considered . Such a construction in turn requires the computation of phase detector characteristic depending on the waveforms of the considered signals . While for the stability analysis of the loop near the locked state it is usually sufficient to consider the linear approximation of phase detector characteristic near zero phase error the global analysis can not be accomplished using simple linear models . The present paper is devoted to the rigorous construction of nonlinear dynamical model of classical Costas loop which allows one to apply numerical simulation and analytical methods for the effective analysis of stability in the large . Here a general approach to the analytical computation of phase detector characteristic of classical Costas loop for periodic non sinusoidal signal waveforms is suggested . The classical ideas of the loop analysis in the signal s phase space are developed and rigorously justified . Effective analytical and numerical approaches for the nonlinear analysis of the mathematical model of classical Costas loop in the signal s phase space are discussed . 
",Analytical computation of PD characteristic of Costas loop is done. The classical ideas of Gardner and Viterbi are developed and rigorously justified. Nonlinear analysis of Costas loop in the signal s phase space is demonstrated.,,,
S0141938215000311," Using a ray tracing technique we investigate the dependence of the moir effect on the crossing angles between touch screen panels and display panels the metal grid structures and metal grid shapes . Of those design parameters adjusting the crossing angle reducing the grid width and employing a random grid in the shape of irregular hexagon are found to suppress the moir phenomenon to a great extent . We also provide the simulation scheme that can capture the moir patterns observed experimentally and useful design guidelines for metal grids . 
",We examine the effect of metal meshes on the moir phenomenon of touch panels. The ray tracing simulation reproduces the moir fringes observed experimentally. The moir phenomenon is quantified using the contrast ratio and standard deviation. A random metal grid is shown to suppress the moir phenomenon substantially. However it exhibits even higher standard deviation due to point defects.,,,
S0140366415002406," Complex networks facilitate the understanding of natural and man made processes and are classified based on the concepts they model biological technological social or semantic . The relevant subgraphs in these networks called network motifs are demonstrated to show core aspects of network functionality and can be used to analyze complex networks based on their topological fingerprint . We propose a novel approach of classifying social networks based on their topological aspects using motifs . As such we define the classifiers for regular random small world and scale free topologies and then apply this classification on empirical networks . We then show how our study brings a new perspective on differentiating between online social networks like Facebook Twitter and Google Plus based on the distribution of network motifs over the fundamental topology classes . Characteristic patterns of motifs are obtained for each of the analyzed online networks and are used to better explain the functional properties behind how people interact online and to define classifiers capable of mapping any online network to a set of topological communicational properties . 
",Large scale computational generation and motif distribution analysis for the synthetic topology classes. We obtain a distinct motif pattern for each such class. Comprehensive motif analysis of online social networks Facebook Twitter Google Plus from which we obtain three quantifiable characteristic motif fingerprints. Mapping and similarity assessment of empirical networks onto topology classes and defining a general methodology for such an approach. Correlation and discussion of the individual motifs that occur in each fingerprint and an outlining of the functional properties behind the three online social platforms.,,,
S0165168414004848," Multimodulus algorithms based adaptive blind equalizers mitigate inter symbol interference in a digital communication system by minimizing dispersion in the quadrature components of the equalized sequence in a decoupled manner i.e . the in phase and quadrature components of the equalized sequence are used to minimize dispersion in the respective components of the received signal . These unsupervised equalizers are mostly incorporated in bandwidth efficient digital receivers which rely on quadrature amplitude modulation based signaling . These equalizers are equipped with nonlinear error functions in their update expressions which makes it a challenging task to evaluate analytically their steady state performance . However exploiting variance relation theorem researchers have recently been able to report approximate expressions for steady state excess mean square error of such equalizers for noiseless but interfering environment . In this work in contrast to existing results we present exact steady state tracking analysis of two multimodulus equalizers in a non stationary environment . Specifically we evaluate expressions for steady state EMSE of two equalizers namely the MMA2 2 and the MMA . The accuracy of the derived analytical results is validated using different set experiments and found in close agreement . 
",We provide performance analysis of two multimodulus blind equalizers. The analysis evaluates EMSE performance in both stationary and non stationary environments. The analysis can provide the optimal equalizer length for the given channel and step size. We validate our analytical findings for both fixed and time varying channels.,,,
S0141938215000323," When reading from an electronic screen many individuals report symptoms such as tired eyes or eye strain . To minimize these symptoms a 3 acuity reserve has been suggested i.e . the minimum print size should be at least 3 times larger than the size of the letters at the reader s acuity limit . This study evaluated whether the 3 rule is appropriate or if an alternative relationship between visual acuity and letter size would be preferable . The experiment was performed on 25 visually normal subjects who viewed a series of random words on a computer monitor . The threshold distance at which the text could be resolved was determined . Both reading speed and accuracy were measured during a 10min task performed at T and at 0.5T 0.33T and 0.25T . In a second study the procedure was repeated at 0.5T and 0.33T for 1h . Immediately following the 1h trial subjects completed a questionnaire concerning symptoms experienced during the task . The mean T for the 10min trial was 131.4cm . While the mean reading speeds for the 0.5T 0.33T and 0.25T conditions were significantly different from the 1T condition they were not significantly different from each other . For the 1h trial reading speed at 1T was significantly different from both the 0.5T and 0.33T conditions but no significant difference was observed between the 0.5T and 0.33T trials . A significant increase in the total post task symptoms was found after reading at the threshold distance compared with 0.5T and 0.25T but no significant difference in symptom score was found between the 0.5T and 0.25T conditions . Accordingly based on a change in viewing distance in young visually normal subjects a 2 rule may be appropriate i.e . for sustained comfortable reading the text size should be at least twice the individual s visual acuity . However higher values may be necessary for older subjects or individuals with visual abnormalities . 
",Computer Vision Syndrome describes eye and vision problems associated with computer use. Up to 40 of individuals report tired eyes at least half the time during computer use. For optimum comfort the text size should be at least twice the visual acuity. Higher values of acuity reserve may be necessary for older subjects.,,,
S0141938215000530," Dysprosium doped strontium calcium magnesium di silicate phosphor namely SrCaMgSi2O7 Dy3 was prepared by the solid state reaction method . The crystal structure of the prepared phosphor was an akermanite type structure which belongs to the tetragonal crystallography with space group P 42 m. From field emission scanning electron microscopy agglomerations of particles were observed due to the high temperature synthesis process . The chemical composition of the sintered SrCaMgSi2O7 Dy3 phosphor was confirmed by energy dispersive X ray spectroscopy . Under the ultra violet excitation the characteristic emission of Dy3 ions peaking at 478 580 and 674nm originating from the transitions of 4F9 2 6H15 2 4F9 2 6H13 2 and 4F9 2 6H11 2 in the 4f9 configuration of Dy3 . Commission International de I Eclairage color coordinates of SrCaMgSi2O7 Dy3 are suitable as white light emitting phosphor . Decay graph indicate that this phosphor contains fast decay and slow decay process . The mechanoluminescence intensity of SrCaMgSi2O7 Dy3 phosphor increases linearly with increasing impact velocity of the moving piston which suggests that this phosphor can be used as sensors to detect the stress of an object . Thus the present investigation indicates that the piezo electrically induced de trapping model is responsible to produce ML in SrCaMgSi2O7 Dy3 phosphor . The possible mechanism of white light emitting long lasting phosphor is also investigated . 
",The phase structure of the SrCaMgSi2O7 Dy3 phosphor is consistent with standard tetragonal crystallography. CIE color coordinates of SrCaMgSi2O7 Dy3 are suitable as white light emitting phosphor. Decay graph indicate that this phosphor also contains fast decay and slow decay process. ML intensity increases linearly with increasing impact velocity of moving piston.,,,
S0141938215300457," We present a simple and efficient way to improve the performance of the twisted nematic liquid crystal system by doping a small amount of n alcohols . The LCs modified by n hexanol demonstrates the optimum electro optical properties of lower driving voltage and shorter response time . Our measurements indicate that the liquid dopant n alcohols can be used to modify LCs for lowering zenithal anchoring energy relative to undoped LCs . Without the drawbacks of precipitation and aggregation that the nanoparticles could have the method of doping n alcohol liquids provides a more stable and reliable choice to apply in the various LC display systems . 
",We present a simple and efficient way to improve the performance of the twisted nematic liquid crystal LC system. The LCs modified by liquid dopant exhibit the functions of lower driving voltage and shorter response time. We replace the solid dopant by the liquid dopant for avoiding the drawbacks of precipitation and aggregation. The result of time evolved transmittance shows the voltage can be held after the doping of n hexanol liquid.,,,
S0167839615000965," This paper proposes a geometric algorithm for computation of geodesic on surfaces . The geodesics on surfaces are traced in a simple way which is independent of the complex description of the geodesic equations . Through derivation process the calculation error of this algorithm is obtained . A step size adjustment strategy which enables the step size adapt to the geometry of surface is introduced . The proposed method is also compared to some other well known methods in this study . Many geodesics computed using these approaches on various B spline surfaces or their equivalent tessellated surfaces have been presented . Experiments demonstrate that the proposed algorithm is efficient . Meanwhile the results show that the step size adjustment strategy works well for most of the cases . 
",An efficient geometric algorithm aims to trace geodesic on parametric surfaces. Independent of the complex description of the geodesic equations. Simpler and faster than the existing geometric method. Step size of the algorithm adapts to the geometry of parametric surface.,,,
S0141938215300044," 3D has been one of the most important technologies of the last decade having an exponential progression . Nevertheless several problems such as visual discomfort and visual fatigue have slowed its progression for homes . This factor decreases significantly the overall quality of experience from watching 3D content and reduces the level of satisfaction of a user . This study explores the accumulation of visual fatigue when watching 3D video in close to real life conditions . In order to obtain more information about visual discomfort an hour of eye tracking experiments have been conducted . Investigations have been made by analyzing information provided both by users through questionnaires and visual gaze characteristics recording . Obtained results are compared to data produced when watching 2D . The deep statistical analysis showed that time and video content have an influence on video fatigue accumulation and visual functions . With respect to the obtained results a model has been proposed based on video characteristics and the previous state of visual fatigue . 
",This paper s studies factors causing visual discomfort for 3D watching. The study based on eye tracking explored eye movements in addition to blinking. Visual fatigue accumulation depends strongly on time and content on S3D content. Features as motion activity and disparity impact visual fatigue accumulation. A model for fatigue prediction based on features and fatigue state is proposed.,,,
S0097849316300218,"In this paper we present an intuitive tool suitable for 2D artists using touch enabled pen tablets. An artist oriented tool should be easy to use real time versatile and locally refinable. Our approach uses an interactive system for 3D character posing from 2D strokes. We employ a closed form solution for the 2D strokes to 3D skeleton registration problem. We first construct an intermediate 2D stroke representation by extracting local features using meaningful heuristics. Then we match 2D stroke segments to 3D bones. Finally 3D bones are carefully realigned with the matched 2D stroke segments while enforcing important constraints such as bone rigidity and depth. Our technique is real time and has a linear time complexity. It is versatile as it works with any type of 2D stroke and 3D skeleton input. Finally thanks to its coarse to fine design it allows users to perform local refinements and thus keep full control over the final results. We demonstrate that our system is suitable for 2D artists using touch enabled pen tablets by posing 3D characters with heterogeneous topologies bipeds quadrupeds hands in real time. 
",Fast artist oriented 2D stroke driven incremental refinement of 3D poses. Novel linear time stroke chain matching and feature extraction techniques. Modular joint rotation computation preserving joint depth and bone rigidity. Intuitive integration stroke driven 3D pose design with Inverse Kinematics. Usability study with 22 subjects resulting in high SUS score.,,,
S0167839613000356," Motivated by applications in freeform architecture we study surfaces which are composed of smoothly joined bilinear patches . These surfaces turn out to be discrete versions of negatively curved affine minimal surfaces and share many properties with their classical smooth counterparts . We present computational design approaches and study special cases which should be interesting for the architectural application . 
",Modeling smooth surfaces from bilinear patches motivated by applications in architecture. Novel link between discrete differential geometry discrete affine minimal surfaces and CAGD smooth patchworks from Bezier surfaces of degree . A geometric approach to discrete affine minimal surfaces based on smooth patchworks. New results on discrete affine minimal surfaces especially those with parallel affine normals improper affine spheres .,,,
S0140366415002868," This study examines motives for virtually endorsing others on social media focusing on the Facebook like function . Motives are studied in terms of uses and gratifications Theory of Reasoned Action and personality and technology factors . Data from an online survey of 213 respondents were examined using factor and hierarchical regression analyses . Findings showed enjoyment and interpersonal relationship as most salient motives . Two types of user profiles emerged . Those with higher self esteem more diligence more emotional stability and less subjective norm clicked like to express enjoyment . Those with lower self esteem less diligence less emotional stability and higher subjective norm clicked like for pleasing others . 
",Enjoyment and interpersonal relationship as most salient motives. Those with higher self esteem more diligence more emotional stability and less subjective norm clicked like to express enjoyment. Those with lower self esteem less diligence less emotional stability and higher subjective norm clicked like for pleasing others. Enjoyment interpersonal relationship and perceived ease of like positively predicted the attitude toward like. Subjective norm and the passing time motive positively predicted clicking like. The pleasing others motivation negatively predicted like behavior.,,,
S0141938213000619," There is a large demand for more fashionable style Chinese characters in advertising art designing and publishing markets . However it becomes challenging to create a new font style for so many Chinese characters . To solve this problem a comprehensive Chinese fonts generating scheme is proposed in this paper . Firstly a decomposition database for stroke splitting and feature extraction is proposed . Secondly stroke segmentation rules are defined based on splitting merging and structural model location definition and minimum feature extraction . Thirdly a radical searching algorithm based on stroke splitting is presented . Finally it is realized that the generated characters can be zoomed rotated and moved . Experimental result shows that Chinese characters with a new style can be generated rapidly with the proposed scheme . The created characters fit the real ones well with a high fidelity of 96.4 . The usability tests are run and participants subjective report show that the performance from the generated characters is similar to the original characters in both recognizability test and style consistency test . The fonts generating method is also reliable for the other stroke constructed block characters such as Japanese and Korean characters . 
",We present a decomposition database to generate new style Chinese characters. Stroke segmentation rules are discussed. A stroke feature searching algorithm is proposed. The evaluation and experiments show a high fidelity of 96.4 . Recombination database can be applied for other stroke constructed characters.,,,
S0140366415002303," In a question driven survey the answers to one question may decide which question is presented next . In this case encrypting the answers of the participants is not enough to protect their privacy since the system is able to learn them by inspection of the next question the participants request . In this article we explore the technologies involved in surveys performed through a mobile phone . Participants receive the questions using VoIP technologies and since their answers affect which questions are presented next they must protect the selection of the relevant questions . In addition this paper considers the performance of the proposed encryption technologies in mobile phones . Finally the answers to the poll must be sent to the server . This paper proposes an eVoting framework to preserve the privacy of the users while sending the answers to the system . Such a scenery involves many different communication channels and technologies . As we will show the decisions taken in some of the modules force some technologies and decisions in the others . 
",We propose an eVoting framework to preserve the privacy of the voters. The voters will receive questions depending on their answers to previous questions. The platform is built upon a cloud computing system VoIP technologies and a new eVoting platform.,,,
S0164121214001381," Numerous component models have been proposed in the literature a testimony of a subject domain rich with technical and scientific challenges and considerable potential . Unfortunately however the reported level of adoption has been comparatively low . Where successes were had they were largely facilitated by the manifest endorsement where not the mandate by relevant stakeholders either internal to the industrial adopter or with authority over the application domain . The work presented in this paper stems from a comprehensive initiative taken by the European Space Agency and its industrial suppliers . This initiative also enjoyed significant synergy with interests shown for similar goals by the telecommunications and railways domain thanks to the interaction between two parallel project frameworks . The ESA effort aimed at favouring the adoption of a software reference architecture across its software supply chain . The center of that strategy revolves around a component model and the software development process that builds on it . This paper presents the rationale the design and implementation choices made in their conception as well as the feedback obtained from a number of industrial case studies that assessed them . 
",We propose a component based approach for embedded real time software systems. The approach meets requirements from the space railway and telecom domains. The approach enforces separation of concerns throughout the development process. The approach supports model based analysis and code generation. The approach was assessed in four case studies in two parallel research projects.,,,
S0167839615000849," We provide a simple efficient technique for computing bases for quadric surfaces from their rational quadratic parametrizations . Our major innovation is to simplify the computations by using complex parameters even though all the surfaces we treat have only real coefficients in both their implicit and parametric representations . In addition to the theory we provide several examples to illustrate our method . 
",A simple efficient technique for computing bases for quadric surfaces from their rational quadratic parametrizations. We use complex parameters even though all the surfaces we treat have only real coefficients.,,,
S0164121213002744," Many organisations are dependent upon long term sustainable software systems and associated communities . In this paper we consider long term sustainability of Open Source software communities in Open Source software projects involving a fork . There is currently a lack of studies in the literature that address how specific Open Source software communities are affected by a fork . We report from a study aiming to investigate the developer community around the LibreOffice project which is a fork from the OpenOffice.org project . In so doing our analysis also covers the OpenOffice.org project and the related Apache OpenOffice project . The results strongly suggest a long term sustainable LibreOffice community and that there are no signs of stagnation in the LibreOffice project 33 months after the fork . Our analysis provides details on developer communities for the LibreOffice and Apache OpenOffice projects and specifically concerning how they have evolved from the OpenOffice.org community with respect to project activity developer commitment and retention of committers over time . Further we present results from an analysis of first hand experiences from contributors in the LibreOffice community . Findings from our analysis show that Open Source software communities can outlive Open Source software projects and that LibreOffice is perceived by its community as supportive diversified and independent . The study contributes new insights concerning challenges related to long term sustainability of Open Source software communities . 
",First comprehensive analysis of Open Source projects involving a fork. The LibreOffice project which was forked from the OpenOffice.org project shows no sign of long term decline. LibreOffice has attracted the long term and most active committers in OpenOffice.org. Open Source communities can outlive Open Source software projects. LibreOffice is perceived by its community as supportive diversified and independent.,,,
S0045794915000449," A robust finite element procedure for modelling the localised fracture of reinforced concrete beams at elevated temperatures is developed . In this model a reinforced concrete beam is represented as an assembly of 4 node quadrilateral plain concrete 3 node main reinforcing steel bar and 2 node bond link elements . The concrete element is subdivided into layers for considering the temperature distribution over the cross section of a beam . An extended finite element method has been incorporated into the concrete elements in order to capture the localised cracks within the concrete . The model has been validated against previous fire test results on the concrete beams . regular strain displacement transformation matrix enhanced strain displacement transformation matrix material constitutive matrix of plain concrete element internal force vector enhanced element internal force vector regular element internal force vector element internal force vector corresponding to traction fracture energy of concrete enhanced element stiffness matrix regular element stiffness matrix element stiffness matrix corresponding to traction tangent stiffness of traction separation relation traction within the cracks sign function vector of continuous displacement field vector of discontinuous displacement field enhancement function 
",Develop an extended finite element model for modelling concrete beams in fire. Propose a criterion to determine the initiation of individual cracks. Consider the influence of the bond condition on the fire resistance of reinforced concrete beams. Assess the integrity of reinforced concrete beams under fire conditions.,,,
S0097849313000551,"This paper focuses on the problem of 3D shape categorization. For a given set of training 3D shapes a 3D shape recognition system must be able to predict the class label for a test 3D shape. We introduce a novel discriminative approach for recognizing 3D shape categories which is based on a 3D Spatial Pyramid 3DSP decomposition. 3D local descriptors computed on the 3D shapes have to be extracted to be then quantized in order to build a 3D visual vocabulary for characterizing the shapes. Our approach repeatedly subdivides a cube inscribed in the 3D shape and computes a weighted sum of histogram of visual word occurrences at increasingly fine sub volumes. Additionally we integrate this pyramidal representation with different types of kernels such as the Histogram Intersection Kernel and the extended Gaussian Kernel with 2 distance. Finally we perform a thorough evaluation on different publicly available datasets defining an elaborate experimental setup to be used for establishing further comparisons among different 3D shape categorization methods. 
",We introduce the 3D spatial pyramid representation for 3D shape categorization. This pyramid representation repeatedly subdivides a cube inscribed in the 3D shape. Then a weighted sum of histogram of visual word occurrences is computed. Different types of kernels are integrated into the approach for training a SVM. Results on publicly available benchmarks have been reported.,,,
S0141938214000493," A method for estimating the non linear gamma transfer function of liquid crystal displays without the need of a photometric measurement device was described by Xiao et al . . It relies on observer s judgments of visual luminance by presenting eight half tone patterns with luminances from 1 9 to 8 9 of the maximum value of each colour channel . These half tone patterns were distributed over the screen both over the vertical and horizontal viewing axes . We conducted a series of photometric and psychophysical measurements to evaluate whether the angular dependency of the light generated by three different LCD technologies would bias the results of these gamma transfer function estimations . Our results show that there are significant differences between the gamma transfer functions measured and produced by observers at different viewing angles . We suggest appropriate modifications to the Xiao et al . paradigm to counterbalance these artefacts which also have the advantage of shortening the amount of time spent in collecting the psychophysical measurements . 
",We study a popular method for estimating the gamma of LCD displays. Photometric measures of the angular dependency for different LCD technologies. Psychophysical measures to support the photometric results. We issue a series of recommendations to make the method more robust.,,,
S0141938213000395," Large screen display technology has in recent years become available to industrial control rooms as a supplement to smaller displays . Due to the greater complexity and scale measured in meters not inches it is now a challenge to design for readability and Situation Awareness . Information Rich Design is a design concept for large displays used in many real life complex processes for almost a decade . The concept simplifies the understanding of large data sets through alignment and Gestalt grouping of process data through a few generic process objects . This paper describes recent design modifications where new functionality is integrated into existing graphical objects keeping the original simplicity . This paper proposes design principles for large screen displays based on theoretical discussions of Situation Awareness and a user test using crews of certified operators . The user test shows positive results on pattern recognition of process data and a newly developed animation of unacknowledged alarms however the concept still suffers from colour and readability issues . 
",Large screen displays should be designed from the ground up. We have built an approach that we call Information Rich Design. Use of pattern recognition and animation of new alarms tested well. Grey colour layering was difficult through video projectors. We propose design principles for large screen displays for complex processes.,,,
S0141938215300536," The MoO3 doped N N bis diphenyl 1 1 biphenyl 4 4 diamine and 4 4 N N dicarbazole biphenyl as p doped hole transport layers have been used in inverted organic light emitting diodes . Compared to the NPB 20nm NPB MoO3 structure the NPB 10nm CBP MoO3 10nm NPB MoO3 structure showed increased device performance mostly because the hole transport barrier from CBP MoO3 to NPB was smaller than that from NPB MoO3 to NPB it also presented improved device performance than the NPB 20nm CBP MoO3 structure ascribed to the higher conductivity of NPB MoO3 than that of CBP MoO3 . We provide a manageable way to unlock the merits of p doped hole transport layers for markedly increasing the performance of IOLEDs . 
",Inverted OLEDs with two combined p doped layers show enhanced hole current. The combined NPB MoO3 and CBP MoO3 outperform single NPB MoO3 and CBP MoO3. Using two p doped layers gives better balance between hole conduction and barrier.,,,
S0140366415004570," The normalized Laplacian spectrum is a powerful tool for comparing graphs with different sizes . Recently we showed that two NLS features namely the weighted spectral distribution and the multiplicity of the eigenvalue 1 are particularly relevant to the Internet topology at the inter domain level . In this paper we examine the physical meaning of the two metrics for the Internet . We show that the WSD reflects the transformation from single homed nodes to multi homed nodes for better fault tolerance and that the ME1 quantifies the initial star based structure associated with node classification both of which are critical to the robustness of the Internet structure . We then investigate the relation between the metrics and graph perturbations . We show that these two NLS metrics can be a good choice for study on the Internet optimization . Our work reveals novel insights into the Internet structure and provides useful knowledge for statistical analysis on complex networks . 
",Hightlights The physical meaning of the normalized Laplacian spectrum for the Internet. Graph perturbations and changes in the weighted spectral distribution. Graph perturbations and changes in the multiplicity of the eigenvalue 1. The two spectral metrics are asymptotically independent of network size. The two spectral metrics are a good choice for study on the Internet optimization.,,,
S0140366415003588," Social aware Opportunistic forwarding algorithms are much needed in environments which lack network infrastructure or in those that are susceptible to frequent disruptions . However most of these algorithms are oblivious to both the user s interest in the forwarded content and the limited power resources of the available mobile nodes . This paper proposes PI SOFA a framework for integrating the awareness of both interest and power capability of a candidate node within the forwarding decision process . Furthermore the framework adapts its forwarding decisions to the expected contact duration between message carriers and candidate nodes . The proposed framework is applied to three state of the art social aware opportunistic forwarding algorithms that target mobile opportunistic message delivery . A simulation based performance evaluation demonstrates the improved effectiveness efficiency reduction of power consumption and fair utilization of the proposed versions in comparison to those of the original algorithms . The results show more than 500 extra f measure mainly by disregarding uninterested nodes while focusing on the potentially interested ones . Moreover power awareness preserves up to 8 power with 41 less cost to attain higher utilization fairness by focusing on power capable interested nodes . Finally this paper analyzes the proposed algorithms performance across various environments . These findings can benefit message delivery in opportunistic mobile networks . 
",Integrating interest and power awareness into opp. social forwarding algorithms. PI SOFA integrates with the SocialCast PeopleRank and SCAR algorithms. Our algorithms outperform the Epidemic Profilecast and EBubbleRap algorithms. Performance metrics are effectiveness efficiency power consumption and fairness. Real data simulations show 500 more fmeasure preserve 8 power with 41 less cost.,,,
S0141938215000906," GdVO4 Eu3 Bi3 with tetragonal phase has been successfully synthesized by employing efficient irradiations . The assembly of composites with fine grains based on acoustic energy and microwave radiation requires low temperature and short reaction time . All the compounds exhibited red emissions and they can be sensitized through the doped Bi3 ions . The dependence of pH changes and doping concentration on the fluorescence features has been discussed . The photoluminescence measurements show that the optical properties achieved the best results at pH 9 for GdVO4 Eu3 Bi3 or pH 7 for GdVO4 Eu3 . 
",GdVO4 Eu3 phosphors have been efficiently synthesized by a SMC method. Bi3 can be encapsulated into the phosphor. The doping concentration and pH dependence were discussed.,,,
S0140366414002989," Smart technologies play a key role in sustainable economic growth . They transform houses offices factories and even cities into autonomic self controlled systems acting often without human intervention and thus sparing people routine connected with information collecting and processing . The paper gives an overview of a novel Wi Fi technology currently under development which aims to organize communication between various devices used in such applications as smart grids smart meters smart houses smart healthcare systems smart industry etc . 
",We study carefully the IEEE 802.11ah draft standard published in July 2014. We overview use cases of .11ah especially related to smart cities scenarios. We describe in details novel mechanisms and explain why they are needed in .11ah.,,,
S0140366414002990," Vehicular Ad Hoc Networks will play an important role in Smart Cities and will support the development of not only safety applications but also car smart video surveillance services . Recent improvements in multimedia over VANETs allow drivers passengers and rescue teams to capture share and access on road multimedia services . Vehicles can cooperate with each other to transmit live flows of traffic accidents or disasters and provide drivers passengers and rescue teams rich visual information about a monitored area . Since humans will watch the videos their distribution must be done by considering the provided Quality of Experience even in multi hop multi path and dynamic environments . This article introduces an application framework to handle this kind of services and a routing protocol the DBD that enhances the dissemination of live video flows on multimedia highway VANETs . DBD uses a backbone based approach to create and maintain persistent and high quality routes during the video delivery in opportunistic Vehicle to Vehicle scenarios . It also improves the performance of the IEEE 802.11p MAC layer by solving the Spurious Forwarding problem while increasing the packet delivery ratio and reducing the forwarding delay . Performance evaluation results show the benefits of DBD compared to existing works in forwarding videos over VANETs where main objective and subjective QoE results are measured . 
",We design a framework for video and in general high data rate applications transmission over a VANET. Our framework includes both application and routing layer design. We analyze MAC layer behavior and improve its utilization by solving the Spurious Forwarding problem. We tested our protocols on a real scenario and considered both QoS and QoE including MOS experiments in a real car. We grant a high data rate in ad hoc mode also for long distance through our backbone beaconless approach.,,,
S0140366415003667," Information Centric Networking is regarded as one of the representative network architectures of Future Internet . In this paper an information centric architecture called Content Scent based Architecture will be proposed for mobile ad hoc networks . In CSAR each content has its special content scent and can be found by tracing the scent it spreads over the network . The content scent has the property similar to the natural scent that can spread over air mix with other scents decay with distance and time and strengthen with fresh supplement . Using this property scent based routing and reliable content delivery functionalities are provided for the mobile ad hoc environment . Simulation results show that CSAR has an efficient route discovery procedure with less routing overhead and better in network caching for the mobile ad hoc networks . 
",A Content Scent based Architecture is designed for mobile ad hoc networks. We define content scent Scent Table and operation of Scent Tables. Broadcast based and scent based routing principle is proposed. An in network caching mechanism is designed based on content scent emitting.,,,
S0140366415002558," The wide adoption of smart phones has enabled Online Social Networks to exploit the location awareness capabilities offering users better interaction and context aware content . While these features are very attractive the publication of users location in an OSN exposes them to privacy hazards . Recently various protocols have been proposed for private proximity testing where users are able to check if their online friends are near without disclosing their locations . However the computation cost of the required cryptographic operations utilized in such protocols is not always efficient for mobile devices . In this paper we introduce a lightweight and secure proximity testing protocol suitable for online mobile users . We show that our protocol is provably secure under the well known factoring problem and we analyze its efficiency . Our results show that our approach outperforms other existing protocols by significantly reducing the computational cost and making it practical for devices with limited resources . Finally we demonstrate the applicability of our proposal in an actual OSN location based mobile application . 
",Our paper introduces a novel lightweight protocol for private proximity testing. The proposed protocol clearly outperforms current state of the art. We formally prove that the protocol is secure in the semi honest model. We present experimental results in Python and C and compare it with its peers. We demonstrate an Android application that uses are protocol and proves its efficacy.,,,
S0164121215002010," Having a large number of applications in the marketplace is considered a critical success factor for software ecosystems . The number of applications has been claimed to determine which ecosystems holds the greatest competitive advantage and will eventually dominate the market . This paper investigates the influence of developer multi homing in three leading mobile application ecosystems . Our results show that when regarded as a whole mobile application ecosystems are single homing markets . The results further show that 3 of all developers generate more than 80 of installed applications and that multi homing is common among these developers . Finally we demonstrate that the most installed content actually comprises only a small number of the potential value propositions . The results thus imply that attracting and maintaining developers of superstar applications is more critical for the survival of a mobile application ecosystem than the overall number of developers and applications . Hence the mobile ecosystem is unlikely to become a monopoly . Since exclusive contracts between application developers and mobile application ecosystems are rare multi homing is a viable component of risk management and a publishing strategy . The study advances the theoretical understanding of the influence of multi homing on competition in software ecosystems . 
",Developers multi home by offering apps in competing mobile application ecosystems. If developers multi home several competing ecosystems can survive. We analyzed multi homing in three mobile application ecosystems with 1.3 million apps. 3 of developers generate 80 of installations in a marketplace. The majority of all apps single home but the most popular apps multi home.,,,
S0140366415002510," While direct social ties have been intensely studied in the context of computer mediated social networks indirect ties have seen little attention . Yet in real life we often rely on friends of our friends for recommendations for introduction to a new job opportunity and for many other occasional needs . In this work we attempt to 1 quantify the strength of indirect social ties 2 validate the quantification and 3 empirically demonstrate its usefulness for applications on two examples . We quantify social strength of indirect ties using a measure of the strength of the direct ties that connect two people and the intuition provided by the sociology literature . We evaluate the proposed metric by framing it as a link prediction problem and experimentally demonstrate that our metric accurately predicts link s formation . We show via data driven experiments that the proposed metric for social strength can be used successfully for social applications . Specifically we show that it can be used for predicting the effects of information diffusion with an accuracy of up to 0.753 . We also show that it alleviates known problems in friend to friend storage systems by addressing two previously documented shortcomings reduced set of storage candidates and data availability correlations . 
",We quantify the strength of indirect social ties by de ning a metric named social strength based on various observations from sociology. We evaluate the social strength metric by framing it as a link prediction problem. Social strength metric s usefulness is demonstrated by two applications predicting information diffusion paths and expanding peers storage candidates in Friend to friend storage systems.,,,
S0167839614001113,"Recently it was shown that a bi cubic patch complex with n sided holes can be completed into a curvature continuous surface by n sided caps of degree bi 5 that offer good and flexible shape Karciauskas and Peters 2013 . This paper further explores the space of n sided caps of degree bi 5 but focuses on functionals to set degrees of freedom and to optimally propagate and average out curvature from the bi cubic complex. 
",Multi sided holes in a bicubic patch complex are filled by a curvature continuous cap. The cap is of degree bi 5 and has good and flexible shape. The space of n sided G2 caps of degree bi 5 is explored via functionals.,,,
S0141938213001054," Dynamic images that possess beauty and are user friendly can increase the use of digital technology . In addition to information conveyance dynamic images also act as a communication bridge in the human machine interface . Dynamic images are widely used in the application of digital media . Therefore understanding the visual effects of dynamic images on viewers is a very important issue . From a visual communication design perspective dynamic images influence not only image quality but also the viewers perception and impression of the displayed image . In the contemporary age characterized by universal usage of dynamic images designers should attain synchronized knowledge and understanding of relevant media technology so as to present preferred design quality in the management of digital design such as animation design Web page design multimedia design and so on . The current study noted that psychological effects such as viewers visual attention preferences and understanding were more important than image quality . Therefore this study adopted the viewpoint of visual design and conducted a perceptual evaluation of grating frequency and grating velocity . The pair comparison method and scale method were adopted in the research methodology to simplify perceptual evaluations and enhance their validity . The purpose of this study was To propose recommendations for displaying dynamic images and improving image performance using perceptual evaluation methods . To examine the influence of psychological factors on viewer s comfort when they experience dynamic images . To identify the best grating feature combinations that meet viewer s psychological characteristics and propose recommendations for dynamic images design . The study concludes that it is useful to establish criteria for evaluating users perception and to develop perceptual evaluations of dynamic images . It is recommended that designers find a balance between watching a moving imaging display clearly and watching it comfortably for successful reception by the viewer . 
",To propose visual evaluation methods recommendations for displaying dynamic images. To examine the influence of psychological factors of dynamic images on viewer. To meet the best grating feature combinations of viewer.,,,
S0141938214000031," Visual discomfort is one of the most frequent complaints of the viewers while watching 3D images and videos . Large disparity and large amount of motion are two main causes of visual discomfort . To quantify this influence three objectives are set in this paper . The first one is the comparative analysis on the influence of different types of motion i.e . static stereoscopic image planar motion and in depth motion on visual discomfort . The second one is the investigation on the influence factors for each motion type for example the disparity offset the disparity amplitude and velocity . The third one is to propose an objective model for visual discomfort . Thirty six synthetic stereoscopic video stimuli with different types of motion are used in this study . In the subjective test an efficient paired comparison method called Adaptive Square Design was used to reduce the number of comparisons for each observer and keep the results reliable . The experimental results showed that motion does not always induce more visual discomfort than static conditions . The in depth motion generally induces more visual discomfort than the planar motion . The relative disparity between the foreground and the background and the motion velocity are identified as main factors for visual discomfort . According to the subjective results an objective model for comparing visual discomfort induced by different types of motion is proposed which shows high correlation with the subjective perception . 
",Motion does not always induce more visual discomfort than static conditions. In depth motion generally induces more visual discomfort than the planar motion. Relative disparity and velocity are main factors for visual discomfort. Disparity amplitude didn t impact visual discomfort significantly. An objective visual discomfort model is proposed.,,,
S0045790616300611," Retina recognition is the most stable and reliable biometric system due to its stability uniqueness and non replicable nature of vascular pattern . On the other hand the complexity of vascular pattern in diseased retina makes the extraction of blood vessels very hard which majorally effects the recognition rate . The main aim of this paper is to design a robust retinal recognition system with reduced computational complexity and to explore novel retinal features . This paper presents two different approaches for retinal recognition one is vascular based feature extraction with an improved vessel segmentation algorithm and second is non vascular based feature extraction . Vascular based method uses vessel properties of retinal images and aims to improve the efficiency of retinal recognition system . Whereas non vascular based method intends to analyze non vessel properties of retinal images in order to reduce time complexity . The proposed system is assessed on two local and three public databases . 
",Novel methods for personal identification using retinal images. Vascular based method involves the use of vessel properties of retinal images with improved vessel segmentation algorithm by catering pathological lesions. Non vascular based method uses novel structural features structure to perform person identification.,,,
S0141938214000687," Dysprosium doped di strontium magnesium di silicate namely Sr2MgSi2O7 Dy3 phosphor was prepared by the solid state reaction method . The phase structure surface morphology particle size elemental analysis was analyzed by using XRD TEM EDX and FTIR techniques . The EDX and FTIR spectra confirm the present elements in Sr2MgSi2O7 Dy3 phosphor . The optical properties of Sr2MgSi2O7 Dy3 phosphor was investigated utilizing thermoluminescence photoluminescence long lasting phosphorescence and mechanoluminescence . Under the ultraviolet excitation the emission spectra of Sr2MgSi2O7 Dy3 phosphor are composed of a broad band and the characteristic emission of Dy3 peaking at 470nm 575nm and 678nm originating from the transitions of 4F9 2 6H15 2 4F9 2 6H13 2 and 4F9 2 6H11 2 . CIE color coordinates of Sr2MgSi2O7 Dy3 are suitable as white light emitting phosphor . Decay graph indicate that this phosphor also contains fast decay and slow decay process . The peak of ML intensity increases linearly with increasing impact velocity of the moving piston . The possible mechanism of this white light emitting long lasting phosphor is also investigated . 
",The phase structure of the Sr2MgSi2O7 Dy3 phosphor is consistent with standard tetragonal crystallography. CIE color coordinates of Sr2MgSi2O7 Dy3 are suitable as white light emitting phosphor. Decay graph indicate that this phosphor also contains fast decay and slow decay process. ML intensity increases linearly with increasing impact velocity of moving piston.,,,
S0140366415000729," Mobile phone sensing is a new paradigm which takes advantage of smart phones to collect and analyze data at large scale but with a low cost . Supporting pervasive communications among mobile devices in such a large scale mobile social network becomes a key challenge for this new mobile sensing system . One possible solution is allowing packet delivery among mobile devices via opportunistic communications during intermittent contacts . However the lack of rich contact opportunities still causes poor delivery ratio and long delay especially for large scale networks . Deployment of additional stationary throwboxes can create a greater number of contact opportunities thus improve the performance of routing . However the locations of deployed throwboxes are critical to such improvement . In this paper we investigate where to deploy throwboxes in a large scale throwbox assisted mobile social DTN . By leveraging the social properties discovered from real life tracing data we propose a set of social based throwbox placement algorithms which smartly pick the location of each throwbox . Extensive simulations are conducted with a real life wireless tracing dataset and a wide range of existing DTN routing methods . The results confirm the efficiency of the proposed methods . 
",Study how to deploy throwboxes in a large scale mobile social DTN. Propose a set of social based throwbox placement algorithms. Simulation results from real life traces confirm the efficiency of proposed methods.,,,
S0167839615000540," The investigation of the umbral calculus based generalization of Bernstein polynomials and B zier curves is continued in this paper First a generalization of the de Casteljau algorithm that uses umbral shift operators is described . Then it is shown that the quite involved umbral shifts can be replaced by a surprisingly simple recursion which in turn can be understood in geometrical terms as an extension of the de Casteljau interpolation scheme . Namely instead of using only the control points of level to generate the points on level r as in the ordinary de Casteljau algorithm one uses also points on level or more previous levels . Thus the unintuitive parameters in the algebraic definition of generalized Bernstein polynomials get geometric meaning . On this basis a new direct method for the design of B zier curves is described that allows to adapt the control polygon as a whole by moving a point of the associated B zier curve . 
",Generalization of the de Casteljau algorithm with new parameters. Geometric interpretation of the new parameters. New direct method of curve design. New results are illustrated with many pictures.,,,
S0167839614001009," Fitting a sparse surface to approximate vast dense data is of interest for many applications reverse engineering recognition and compression etc . The present work provides an approach to fit a Loop subdivision surface to a dense triangular mesh of arbitrary topology whilst preserving and aligning the original features . The natural ridge joined connectivity of umbilics and ridge crossings is used as the connectivity of the control mesh for subdivision so that the edges follow salient features on the surface . Furthermore the chosen features and connectivity characterise the overall shape of the original mesh since ridges capture extreme principal curvatures and ridges start and end at umbilics . A metric of Hausdorff distance including curvature vectors is proposed and implemented in a distance transform algorithm to construct the connectivity . Ridge colour matching is introduced as a criterion for edge flipping to improve feature alignment . Several examples are provided to demonstrate the feature preserving capability of the proposed approach . 
",The natural ridge joined connectivity of umbilics and ridge crossings is used as the connectivity of control mesh for subdivision. Preserving and aligning with salient features. Curvature sensitive distance metric for automatic construction of connectivity.,,,
S0167839615000151," Ck geometrically continuous surface constructions were developed to create surfaces that are smooth also at irregular points where in a quad mesh three or more than four elements come together. Isogeometric elements were developed to unify the representation of geometry and of engineering analysis. We show how matched constructions for geometry and analysis automatically yield isogeometric elements. This provides a formal framework for the existing and any future isogeometric elements based on geometric continuity. 
",A construction principle for engineering analysis elements is formally verified . The principle yields isogeometric Ck elements from Gk manifold constructions . The manifold representations can have irregular extraordinary star points . The principle applies in any number of variables and any number of range dimensions . As a consequence Gk constructions can directly be used to solve PDEs on manifolds,,,
S0167839613000678," In this paper a new method for computing intersection between a ray and a parametric surface is proposed which finds many applications in computer graphics robotics and geometric modeling . The method uses the second order derivative of the surface which can handle inherent problems that Newton Raphson and Halley methods have such as instability caused by inappropriate initial conditions and tangential intersection . Case examples are presented to demonstrate the capability of the proposed method . 
",A novel second order method is proposed to compute the intersection between a ray and a parametric surface using the second order approximation. It is a geometric iteration scheme which is less sensitive to initial conditions than Newton Raphson and Halley methods. It handles cases that Newton Raphson or Halley method fails to do inappropriate initial condition tangential and multiple intersections.,,,
S0140366415002248," Distributed reflective denial of service attacks especially those based on UDP reflection and amplification can generate hundreds of gigabits per second of attack traffic and have become a significant threat to Internet security . In this paper we show that an attacker can further make the DRDoS attack more dangerous . In particular we describe a new DRDoS attack called store and flood DRDoS or SF DRDoS which leverages peer to peer file sharing networks . An attacker can store carefully prepared data on reflector nodes before the flooding phase to greatly increase the amplification factor of an attack . In this way SF DRDoS is more surreptitious and powerful than traditional DRDoS . We present two prototype SF DRDoS attacks on two popular Kademlia based P2P file sharing networks Kad and BT DHT . Experiments in real world environments showed that this attack can achieve an amplification factor of 2400 on average in Kad and reach an upper bound of attack bandwidth at 670 Gbps and 10 Tbps for Kad and BT DHT respectively . We also propose some candidate defenses to mitigate the SF DRDoS threat . 
",We propose a novel reflective amplification DDoS attack called store and flood DRDoS. SF DRDoS gains a high amplification factor by storing prepared data on reflectors. We implement prototypes on two Kademlia networks Kad and BT DHT. Real world experiments achieves an average amplification factor of 2400 in Kad. The upper bound of attack bandwidth could be 670 Gbps and 10 Tbps for Kad and BTDHT.,,,
S0167839615000138," We present a novel deterministic and efficient method to detect whether a given rational space curve is symmetric . By using well known differential invariants of space curves namely the curvature and torsion the method is significantly faster simpler and more general than an earlier method addressing a similar problem . To support this claim we present an analysis of the arithmetic complexity of the algorithm and timings from an implementation in Sage . 
",The paper presents a novel deterministic and efficient method to detect whether a given rational space curve is symmetric. The method is significantly faster simpler and more general than earlier methods addressing similar problems. An analysis of the arithmetic complexity of the algorithm and timings from an implementation in Sage are included.,,,
S0141938214000699," Although the three dimensional television is popular for its stereoscopy the fatigue caused by the prolonged watching of 3DTV should not be underestimated . Electroencephalogram has been widely used for monitoring the brain s functional activities . Based on our previous research of 3DTV fatigue one more objective and effective 3DTV fatigue evaluation model is proposed on gravity frequency of power spectrum and power spectral entropy . As the fatigue changes the gravity frequency reflects the transition of EEG power spectrum and the power spectral entropy describes the level of chaos of EEG . 16 channels of EEG data of twenty five subjects watching 2DTV and 3DTV were collected and gravity frequency of power spectrum and power spectral entropy were then calculated and analyzed . These two parameters of the 3D group changed more significantly comparing with that of the 2D group on several electrodes . There are significant decreases in gravity frequency and power spectral entropy in several brain regions after long time of watching 3DTV which indicates the decline of subjects alertness level . Based on the subjective evaluation and two significant parameters gravity frequency and power spectral entropy an accurate evaluation model for 3DTV fatigue was established using the regression equation . 
",The optimized objective evaluation model for 3DTV fatigue was established. There were significant decreases in gravity frequency and power spectral entropy. More significant changes of these parameters were found in 3D group. The evaluation model for 3DTV fatigue was established with the regression equation.,,,
S0141938214000171," We investigated green phosphorescent organic light emitting diodes with charge control layer to produce high efficiency and improve operational lifetime . Three types of devices were fabricated following the number of CCL within emitting layer maintaining the thickness of whole EML . The CCL and host material which was 4 4 bis biphenyl with bipolar property can control carrier movement in EML . Therefore the electroluminescent performance improvement as efficiency and lifetime was realized with a good charge balance an effective triplet exciton confinement and the reduced triplet exciton quenching effect in EML . Device 2 with a CCL as exciton distribution structure exhibits the remarkable EL performances for the maximum luminous and external quantum efficiency of 65.34cd A and 20.42 respectively . Moreover operational lifetime is nearly improved 2.5 times than the conventional device . 
",Green PHOLEDs have been fabricated using charge control layer CCL . The distributed recombination zone and charge balance are achieved through the CCL. Green PHOLEDs with CCL exhibited efficient carrier and triplet exciton confinement. Electroluminescent performance improvement as efficiency and lifetime was realized.,,,
S0167839614000223," We present an approach to finding the implicit equation of a planar rational parametric cubic curve by defining a new basis for the representation . The basis which contains only four cubic bivariate polynomials is defined in terms of the B zier control points of the curve . An explicit formula for the coefficients of the implicit curve is given . Moreover these coefficients lead to simple expressions which describe aspects of the geometric behaviour of the curve . In particular we present an explicit barycentric formula for the position of the double point in terms of the B zier control points of the curve . We also give conditions for when an unwanted singularity occurs in the region of interest . Special cases in which the method fails such as when three of the control points are collinear or when two points coincide will be discussed separately . 
",We construct a basis for the implicit representation of planar rational cubic B zier curves. We present an explicit formulation of the implicit polynomial coefficients. Barycentric formulas for the double point of the cubic curve are derived. A simple test for when unwanted branches occur is formulated. Conditions for the degeneration of cubics to conics are presented.,,,
S0167819114000337," Linear least squares problems are commonly solved by QR factorization . When multiple solutions need to be computed with only minor changes in the underlying data knowledge of the difference between the old data set and the new can be used to update an existing factorization at reduced computational cost . We investigate the viability of implementing QR updating algorithms on GPUs and demonstrate that GPU based updating for removing columns achieves speed ups of up to 13.5 compared with full GPU QR factorization . We characterize the conditions under which other types of updates also achieve speed ups . 
",The first implementation of QR factorization updating algorithms on GPUs. We achieved speed ups over full QR factorization of over 13 in some cases. The accuracies of our results were comparable with serial computed solutions.,,,
S0140366414003004," Mobile network providers face an ever increasing number of mobile devices requesting similarly increasing amounts of data . In this article we present a two step approach to modeling and simulating the amounts of data produced by mobile devices based on applications that are highly utilized on the network . In the first step we separate the applications on a mobile device into highly utilized and background ones for the overall population to be modeled . With the identified overall application groups we employ a four state Hidden Markov Model to capture the characteristics of the high utilization applications as aggregates per device the characteristics of the background applications are matched to four states dependent on the high utilization aggregates states . Utilizing the Exponential distribution for both we closely match their original user based characteristics . The suitability of our model is lastly corroborated through simulation based comparisons of estimations for the bandwidth requirements of the individual users or our model s estimates are typically within ten percent of the original values . 
",We find the 7 of applications responsible for 75 of data in a mobile device dataset. Active and background applications are aggregated for limited parameters. We simulate a mobile device based on these applications using a 4 state Markov Model. Using exponential distributions simulated results are within 10 of submitted data.,,,
S0167839614000570," A rational curve on a rational surface such that the unit normal vector field of the surface along this curve is rational will be called a curve providing Pythagorean surface normals . These curves represent rational paths on the surface along which the surface possesses rational offset curves . Our aim is to study rational surfaces containing enough PSN curves . The relation with PN surfaces will be also investigated and thoroughly discussed . The algebraic and geometric properties of PSN curves will be described using the theory of double planes . The main motivation for this contribution is to bring the theory of rational offsets of rational surfaces closer to the practical problems appearing in numerical control machining where the milling cutter does not follow continuously the whole offset surface but only certain chosen trajectories on it . A special attention will be devoted to rational surfaces with pencils of PSN curves . One of the basic operations in Computer Aided Design and related technical applications is offsetting . The two sided offset to an irreducible surface is the envelope of the system of spheres centered at the points of with the radius . Because of their high applicability investigating surface offsets has become a popular research field in recent years and many interesting and challenging problems have been thoroughly studied e.g . analysis of their geometric and algebraic properties determining the number and type of components and constructing rational parameterizations see e.g . Arrondo et al . L and Pottmann Sendra and Sendra . Computational techniques for offsets have been surveyed firstly by Pham and later by Maekawa . A serious problem with offsets is that they are often far more complicated than the input generating surfaces . Hence approximation methods are in several real situations not only possible but unavoidable . However offsets to special classes of rational surfaces admit exact rational representations . The notion of rational surfaces with rational offsets the so called Pythagorean Normal vector surfaces was introduced by Pottmann as the surface counterpart to the theory of Pythagorean Hodograph curves which are planar curves possessing rational offset curves introduced by Farouki and Sakkalis . More details about PN surfaces can be found e.g . in papers by Krasauskas L vi ka and Bastl L vi ka and Vr ek Vr ek and L vi ka . For a survey of shapes with Pythagorean normals property see Farouki and for a recent discussion about rational offset surfaces and their modelling application see Krasauskas and Peternell . The aim of this paper is to start an investigation of rational offsets to surfaces from a new point of view and return this research back to one of the practical applications which originally motivated the introduction of PN surfaces in CAGD namely closer to numerical control machining . We recall that PN surfaces were introduced due to their straightforward applicability in CAD CAM as they guarantee that the surface representing the centers of the sphere milling cutter is. 
",An algebraic analysis of components of offsets to irreducible surfaces along rational curves is provided. A simple criterion for deciding if the curve on the surface is proper non proper PSN is formulated. Surfaces with PSN pencils such that a generic fiber is a proper non proper PSN curve are investigated. It is shown that all developable surfaces are PSN pencils but not necessarily PN surfaces . For non developable surfaces a method for generating all real parameterizations respecting PSN pencils is designed.,,,
S0045794915003302," An elliptical ring test method is proposed to replace the circular ring test recommended by ASTM and AASHTO for faster and more reliable assessment of cracking tendency of concrete . Numerical models are also established to simulate stress development and crack initiation propagation in restrained concrete rings . Cracking age position and propagation in various rings are obtained from numerical analyses that agree well with experimental results . Elliptical thin rings of certain geometry can shorten the ring test duration as desirable . In thin rings crack initiation is caused by external restraint effect so that a crack occurs at the inner circumference and propagates towards the outer one . In thick rings crack initiation is mainly due to the self restraint effect so that a crack occurs at the outer circumference and propagates towards their inner one . Therefore thick elliptical concrete rings do not necessarily crack earlier than circular ones as observed from experiment . 
",A numerical model proposed for underpinning the elliptical ring test for concrete. The model considers nonlinear moisture distribution shrinkage within concrete. The multi physics model integrates micromechanics modelling into macro FE model. The multi physics model is able to simulate concrete crack initiation propagation. The numerical approach can be used for analysing other types of concrete structures.,,,
S0141938213000449," Seven compounds with pyridine as the backbone modified by carbazole moiety bromine atom and fluorine atom were synthesized . Compounds 1 2 3 with bromo substitution at the 2 position and carbazole modification at the 5 position of pyridine emit not only a sharp blue singlet fluorescence but also a wide banded excimer based orange emission . The two colors coming from a single molecule can be used to fabricate a simplified white light emitting device . The electroluminescence based on 1 and 2 exhibits white light emission with CIE coordinates of x 0.25 and y 0.30 for 1 and x 0.33 and y 0.37 for 2 at high current densities very close to pure white emission . In addition the role of bromo substitution at pyridine is concluded to be essential to generate molecular interaction thus an excimer emission . 
",Molecules with pyridine as the backbone and modified by carbazoles were synthesized. The molecules exhibit sharp blue fluorescence and excimer orange emission. Structurally bromo was identified as a crucial factor to generate excimer emission. The molecules were made into simple structured OLED devices and emit white emission.,,,
S0141938213000243," A two dimensional tactile display for visually impaired people was fabricated and successfully operated . Character and graphic information is dynamically displayed by an array of pins in up and down positions . The contraction of shape memory alloy micro coil actuators moves the pins up and down when the SMA actuator is heated by an electrical current . A tube fabricated from magnetic material is attached to each pin and a permanent magnet accurately positions the pins in an up or down state without any feedback control . This latch mechanism overcomes problems of heat storage and electrical consumption of the SMA actuator because the current is supplied only when the pins move . The tactile display has a 100 pins array . The tactile information is displayed sequentially every 0.3s and the tactile pins are latched at 0.1N by magnetic force . The pins are arranged at a pin interval of 2.5mm and move 2mm up and down . Furthermore in order to offer tactile graphic information the interval between pins is narrow . A module for the tactile display with pin intervals of 1.27mm has also been developed . 
",The tactile pins are actuated by an SMA coil actuator and latched by magnetic force. The pins remain in place even if the electrical current to the SMA actuator is cut off by the latch mechanism. The 100 pin tactile display consists of 10 stacked modules and latch parts. The magnetic latch mechanism can control the stroke of the pin accurately without any feedback control. The magnetic latch mechanism realizes lower electrical consumption and high density of the tactile pins.,,,
S0167839614000533," By means of bond theory we study Stewart Gough platforms with n dimensional self motions with . It turns out that only architecturally singular manipulators can possess these self motions . Based on this result we present a complete list of all SG platforms which have n dimensional self motions . Therefore this paper also solves the famous Borel Bricard problem for n dimensional motions . We also give some remarks and a new result on SG platforms with 2 dimensional self motions nevertheless a full discussion of this case remains open . 
",We list all Stewart Gough platforms which have n dimensional self motions with Therefore we solve the Borel Bricard problem for n dimensional motions. We also give remarks and a new result on hexapods with 2 dimensional self motions.,,,
S0141938215300160," A new display concept for reproduction of high luminance colors based on a liquid crystal display has been developed using a brighter backlight unit and color mapping algorithms . The new concept is able to display brighter colors close to a peak luminance of a display white than conventional displays so that realistic scene of brighter colors is better reproduced . It may also be one of the future display solutions needed to extend the color gamut in the direction of brighter colors which is a principal limitation in conventional displays even in high dynamic range display systems . With the new concept an xvYCC compatible display can be easily realized . 
",A new display concept for the reproduction of high luminance colors was proposed. A new display was compared with the conventional display using xvYCC and sRGB encoded images. xvYCC compatible display was realized. Impact of sRGB expansion was verified through the subjective experime.,,,
S0141938215000025," A film insert injection compression molding process was introduced to encapsulate cholesteric liquid crystal displays with flexible and rigid lens for full protection of displays to replace the currently used time consuming hand lamination technique . For this purpose a new interchangeable cavity instrumented hot runner mold was designed and constructed . This complex method was carefully optimized considering challenges arising from an insert multilayer display with 80 liquid crystal content as well as different thermal expansion coefficients between the layers and the lens material as a high potential of delamination and warpage . Concerning the desired physical properties including transparency low melt viscosity and melting temperature as well as a wide range of hardness grades from soft to hard three different hardness grades of thermoplastic polyurethanes were found to be the best candidates for this lens application . During proposed lens encapsulation the pressure changes were evaluated with screw and mold movements using position detection via displacement transducers attached to track the mold closure and screw forward motion . The quality of encapsulation and shrinkage related problems as well as their elimination were all discussed . Display substrate material selection criteria for lowered warpage were defined with supporting thermal characterizations . Among the process parameters tested also by applying the design of experiments with Taguchi method mold temperature was found to be the most influential parameter on warpage followed by pin gate opening time packing pressure and cooling time . 
",New technique developed for lens encapsulation of thin and flexible LCDs. In process pressure and thermal history collected with real time instrumentation. Display substrate options characterized according to recorded thermal history. Using PEN substrate and post process annealing found to be effective for part quality. Warpage reduced by application of process optimization technique.,,,
S0097849313000460," Graphical abstract Robustness of the biharmonic distance from a source point which has been computed using the linear FEM mass matrix as weight with respect to tiny and missing triangles noise holes of an irregularly sampled surface with local shape artifacts . 
",We study the discretization of harmonic Laplacian diffusion maps and distances. We generate spectral distances in a unified way. We focus on different Laplacian matrices initial conditions and shapes. The user can perform a large set of experiments on maps and spectral distances.,,,
S0141938213000632," We have grown the crystals of two thiophene phenylene co oligomers composed of in line five and seven alternating thiophene and benzene rings . These TPCOs are characterized by thiophenes located at both molecular terminals . The grown crystals had several pairs of parallel crystal faces that function as optical resonators . These produced interference fringes in their emission and reflectance spectra . We measured the emission spectra in both weak broadband excitation and intense laser excitation regimes . From these spectra we determined the phase refractive index dispersion and the anisotropic group refractive indices . We made field effect transistor devices using the above TPCO crystals and carried out their electrical measurements . These optical and charge transport properties are compared with those of crystals of other alternating TPCOs with phenylenes located at both molecular terminals . 
",We have grown the crystals of two thiophene phenylene co oligomers TPCOs . These TPCOs were thiophene terminated alternating oligomers. We determined their anisotropic group refractive indices and the dispersions. The properties were compared with those of phenylene terminated TPCO crystals.,,,
S0141938213000644," Optical properties of seven regioregular poly with different alkyl side chain lengths which are poly poly poly poly poly poly and poly have been studied in the mid infrared spectral region by means of Fourier Transformation Infrared spectroscopy and IR spectroscopic ellipsometry . Absorbance spectra obtained in this fingerprint region are potential to characterize the structures formed by organic molecules in thin films due to molecular vibrations in detail . In consequence the vibrational absorption bands of these seven samples demonstrated that P3PT P3HT and P3hept exhibited very similar band profiles in contrast the stretching vibration of thiophene rings underwent a blue shift in P3BT P3OT P3DT and P3DDT . The highest value of the real part of the complex dielectric constant was obtained from P3HT on both indium thin oxide and silicon substrates whereas the imaginary part was directly affected by increasing in the alkyl side chain lengths in a frequency range around 3000cm 1 . The optical properties of P3PT in the mid IR region developed an affinity with those of P3HT . Thus P3PT is particularly a suitable polymer active material candidate for high performance devices . 
",We investigate optical properties of seven regioregular P3ATs in mid IR region. We utilize FTIR spectroscopy and IRSE to obtain the optical properties. P3HT on ITO and Si substrates indicates the highest dielectric constant 1 . The imaginary part 2 is affected by increasing in the alkyl side chain lengths. The optical properties of P3PT in mid IR region show affinity with those of P3HT.,,,
S0167839614000090," The problem of designing smoothly rounded right angle corners with Pythagorean hodograph curves is addressed . A corner can be uniquely specified as a single PH cubic segment closely approximating a circular arc . Similarly a corner can be uniquely constructed with a single PH quintic segment having a unimodal curvature distribution . To obtain corners incorporating shape freedoms that permit a fine tuning of the curvature profile PH curves of degree 7 are required . It is shown that degree 7 PH curves define a one parameter family of corners facilitating precise control over the extremum of the unimodal curvature distribution within a certain range of the parameter . As an alternative a corner construction based upon splicing together two PH quintic segments is proposed that provides two free parameters for shape adjustment . The smooth corner shapes constructed through these schemes can exploit the computational advantages of PH curves including exact computation of arc length rational offset curves and real time interpolator algorithms for motion control in manufacturing robotics inspection and similar applications . 
",Pythagorean hodograph PH curves are used to round sharp corners. G2 continuity is possible with planar PH curves of degree 5 and 7. The corners admit exact offset curve and arc length computation. The PH curve corner constructions incur only quadratic equations.,,,
S0141938215300354," Although visually impaired people are generally considered to be dependent and helpless people they actually share the same characteristics as other people . Thanks to the Braille alphabet which has been developed to reduce inequality of opportunity to minimum their lives become a little bit easier . Besides Braille alphabet there are many devices and software developed for visually impaired people . In this study a readable vocalized and refreshable Braille device which can ease the lives of visually impaired people has been developed . The results of the tests on this device with 20 visually impaired people whose ages ranged between 5 and 15 and who do not know the Braille alphabet indicated the usability rate of this device to be 81.8 . The usability rate of the device was found to be 97.16 in the tests with 30 visually impaired people whose ages ranged between 10 and 33 and who know the Braille alphabet . The durability test of the device indicated that Braille cells worked with 100 efficiency in the trials from 50 characters to 4000 characters . The device was superior both in terms of features and 35 cheaper compared to the ones available on the market . Besides it was determined that USB connection transmitted data faster compared to the Bluetooth connection . 
",A new Braille device was developed for blind people. The device features are readable vocalized refreshable and multi functional. The usability rate of the device was found to be 97.16 . Blind people can read all magazines with their hands and listen all magazines with this device.,,,
S0167839615000801," We describe a construction of LR spaces whose bases are composed of locally linearly independent B splines which also form a partition of unity . The construction conforms to given refinement requirements associated to subdomains . In contrast to the original LR paper and similarly to the hierarchical B spline framework the construction of the mesh is based on a priori choice of a sequence of nested tensor B spline spaces . 
",The paper discusses bivariate LR splines with the non nested support N2S property. Conditions on LR mesh refinement for preserving this property are presented. A hierarchical construction for LR meshes with the N2S property is proposed and analyzed. We prove the completeness of LR splines on the resulting hierarchical meshes.,,,
S0141938214000729," Polymer networks are employed in vertical aligned liquid crystal cells to stabilize the LC molecular configuration under the in plane field driving . Two different polymer morphologies respectively produced by the monofunctional and bifunctional acrylate monomers are assembled on the glass substrate surface . The enhanced electro optical performance is observed on the LC cell with bifunctional acrylate polymer networks and the appropriate display cell is developed at an optimum concentration of 2wt . This type of polymer LC cell shows the fast turn off and turn on responses at the low driving voltage which are attributed to the strong anchoring and the stable LC reorientations respectively . Furthermore around 30 improvement in the gray level response on the 2 wt TA 9164 polymer LC cell is successfully achieved as compared to the pure LC cells . 
",The proposed polymer cells have the stable molecular structure at high voltages. The turn off response is improved by TA 9164 polymers with stronger anchoring. 2wt polymer concentration is the optimum selection for TA 9164 VA IPS cells. The fast turn on response at the low operating voltage 12V is demonstrated. The superior gray level response with an improvement of 30 is achieved.,,,
S0045794916300608," Tensile fabric membranes present opportunities for efficient structures combining the cladding and support structure . Such structures must be doubly curved to resist external loads but doubly curved surfaces can not be formed from flat fabric without distorting . Computational methods of patterning are used to find the optimal composition of planar panels to generate the form but are sensitive to the models and techniques used . This paper presents a detailed discussion of and insights into the computational process of patterning . A new patterning method is proposed which uses a discrete model advanced flattening methods dynamic relaxation and re meshing to generate accurate cutting patterns . Comparisons are drawn with published methods of patterning to show the suitability of the method . 
",A discussion of computational patterning is presented. A new patterning method using a discrete model is proposed. Comparisons are drawn with published results. The suitability of discrete models for patterning are demonstrated.,,,
S0140366414003120," Multi tier networks comprising of macro cellular network overlaid with less power short range home base station like femtocells provide an economically feasible solution for meeting the unrelenting traffic demands . However femtocells that use co channel allocation with macrocells result in cross tier interference which eventually degrades the system performance . It is for this reason that cross polarized data transmission is proposed in this paper as a potential approach towards improving the spectral efficiency of cellular systems and at the same time permitting co channel allocation . Here two independent information channels occupying the same frequency band can be transmitted over a single link . The paper evaluates a scenario where femtocell network makes use of right hand circular polarization and macrocell network makes use of left hand circular polarization for signal transmission . The polarizations being orthogonal to each other due to their sense of rotation ensure isolation between the networks and enable both of them to use the same spectral resources simultaneously . Analytical and simulation results prove that this opens the scope for an easily implementable remarkable opportunity in the context of two tier femto macro network that can increase the system capacity . The paper closes by discussing the technical challenges involved in the implementation as well as the possible solutions to overcome the same . 
",Cross polarized transmission for frequency reuse in femto macro networks. Femtocells use Right Hand Circular Polarization RHCP . Macrocells use Left Hand Circular Polarization LHCP . The proposed method promotes interference free operation in both tiers. It also assures better coverage higher spectral efficiency and enhanced throughput on both tiers.,,,
S0097849313000575,"We present an efficient and robust algorithm for the landmark transfer on 3D meshes that are approximately isometric. Given one or more custom landmarks placed by the user on a source mesh our method efficiently computes corresponding landmarks on a family of target meshes. The technique is useful when a user is interested in characterization and reuse of application specific landmarks on meshes of similar shape for example meshes coming from the same class of objects . Consequently across a set of multiple meshes consistency is assured among landmarks regardless of landmark geometric distinctiveness. The main advantage of our method over existing approaches is its low computation time. Differently from existing non rigid registration techniques our method detects and uses a minimum number of geometric features that are necessary to accurately locate the user defined landmarks and avoids performing unnecessary full registration. In addition unlike previous techniques that assume strict consistency with respect to geodesic distances we adopt histograms of geodesic distance to define feature point coordinates in order to handle the deviation of isometric deformation. This allows us to accurately locate the landmarks with only a small number of feature points in proximity from which we build what we call a minimal graph. We demonstrate and evaluate the quality of transfer by our algorithm on a number of Tosca data sets. 
",We present a method for the landmark transfer on 3D meshes that are nearly isometric. One or more custom landmarks are placed by the user on a source mesh. Our method efficiently computes corresponding landmarks on a family of target meshes. For each landmark we construct a minimal graph that accurately locates the landmark. We adopt modified histograms of geodesic distance to consistently locate a landmark.,,,
S0167839614000983," The purpose of this paper is to present algorithms for computing all the differential geometry properties of non transversal intersection curves of three parametric hypersurfaces in Euclidean 4 space . For transversal intersections the tangential direction at an intersection point can be computed by the extension of the vector product of the normal vectors of three hypersurfaces . However when the three normal vectors are not linearly independent the tangent direction can not be determined by this method . If normal vectors of hypersurfaces are parallel we have tangential intersection and if normal vectors of hypersurfaces are not parallel but are linearly dependent we have almost tangential intersection . In each case we obtain unit tangent vector principal normal vector binormal vectors and curvatures of the intersection curve . 
",We study the non transversal intersection of parametric hypersurfaces in 4 space. We classify the non transversal intersection in two different cases called almost tangential intersection and tangential intersection . We obtain all the Frenet apparatus for the almost tangential intersection. We obtain all the Frenet apparatus for the tangential intersection.,,,
S0167839615000837," We introduce the concept of reduced curvature formulae for 3 D space entities . A reduced formula entails only derivatives of the functions involved in the entity s representation and admits no further algebraic simplifications . Although not always the most compact reduced curvature formulae entail only basic arithmetic operators and are more efficient computationally compared to alternative unreduced formulae . Reduced formulae are presented for the normal mean and Gaussian curvatures of a surface and the curvature of curves on a surface where each surface or curve on a surface may be defined parametrically or implicitly . Reduced formulae are also presented for the curvature of surface intersection curves where each of the intersecting surfaces may be a given surface or an offset of a given surface and each given surface may be defined parametrically or implicitly . Known formulae are cited without derivation to form a collection in one place of new and of known results scattered in the literature . Each curve curvature formula is presented together with a formula for the respective binormal vector from which formulae for the Frenet frame and torsion of the curve can be derived . 
",Reduced curvature formulae are presented namely formulae entailing only derivatives of the functions involved in the entity s representation. Reduced curvature formulae are more efficient compared to unreduced formulae. Formulae are presented for curves on a surface and for surface intersection curves in all representation modes of the surfaces. Formulae are also presented for the normal curvature of offset surfaces and for the curvature of intersection curves of offset surfaces.,,,
S0141938213000255," This paper presents a new pixel circuit with all p type TFTs for AMOLED displays . The proposed pixel circuit can effectively compensate for the threshold voltage shift of the driving TFT the degradation of OLED and the parasitic resistance of the power supply line . And thus the brightness uniformity of AMOLED displays can be enhanced . It is shown that the nonuniformity of the OLED current with an average value of 7.3 can be achieved by the proposed pixel circuit while that of the conventional 2T1C pixel circuit is 73 . Moreover a high contrast ratio can be also obtained by the proposed pixel circuit since there is no light emitting except for the emission period . 
",The proposed pixel circuit is realized by all p type TFTs. The brightness uniformity of AMOLED display will be enhanced. The effect of the parasitic parameters is sufficiently analyzed. A high contrast ratio can be achieved by the proposed pixel circuit.,,,
S0167839613001003," An orthonormal frame is rotation minimizing with respect to if its angular velocity satisfies or equivalently the derivatives of and are both parallel to . The Frenet frame along a space curve is rotation minimizing with respect to the principal normal p and in recent years adapted frames that are rotation minimizing with respect to the tangent t have attracted much interest . This study is concerned with rotation minimizing osculating frames incorporating the binormal b and osculating plane vectors f g that have no rotation about b . These frame vectors may be defined through a rotation of t p by an angle equal to minus the integral of curvature with respect to arc length . In aeronautical terms the rotation minimizing osculating frame specifies yaw free rigid body motion along a curved path . For polynomial space curves possessing rational Frenet frames the existence of rational RMOFs is investigated and it is found that they must be of degree 7 at least . The RMOF is also employed to construct a novel type of ruled surface with the property that its tangent planes coincide with the osculating planes of a given space curve and its rulings exhibit the least possible rate of rotation consistent with this constraint . 
",The rotation minimizing osculating frame RMOF on space curves is introduced. RMOFs define yaw free rigid body motion along a space curve. The RMOF can be used to construct a novel type of ruled surface. Polynomial space curves with rational RMOFs must be of degree 7 at least.,,,
S0141938214000055," The present study investigates the effects of relative positions of stereoscopic objects vs. Near vs . Both and seat location on viewers psychological responses . People who watched a movie with Both conditions reported more arousal and satisfaction compared to people who watched a movie with either the Far or Near condition . More importantly interaction effects were reported such as more dizziness with the Near condition if sitting on the left or right side in the cinema and with the Far condition if sitting in the middle of the cinema . Additionally people who wore glasses felt more eye fatigue than those who did not . Secondly people felt less presence sensation of depth and arousal if they had prior experience viewing stereoscopic movies . The results indicate that viewing experience with different PSOs and or seat locations can influence psychological response . 
",We examined the effects of positions of stereoscopic objects and seat location on the viewers. People feel more dizziness in Near to viewers condition with sitting on the sides. People feel more dizziness in Far from viewers condition with sitting on the middle. People feel more arousal and satisfaction in Both condition. Wearing stereoscopic glasses or having prior experiences effects on the viewer.,,,
S0167839614001022," In this work we present a parameter dependent Refine and Smooth subdivision algorithm where the refine stage R consists in the application of a perturbation of Chaikin s Doo Sabin s vertex split while each smoothing stage S performs averages of adjacent vertices like in the Lane Riesenfeld algorithm . This constructive approach provides a unifying framework for univariate bivariate primal and dual subdivision schemes with tension parameter and allows us to show that several existing subdivision algorithms proposed in the literature via isolated constructions can be obtained as specific instances of the proposed strategy . Moreover this novel approach provides an intuitive theoretical tool for the derivation of new non tensor product subdivision schemes that in the regular regions satisfy the property of reproducing bivariate cubic polynomials thus resulting the natural extension of the univariate family presented in Hormann and Sabin . 
",A parameter dependent Refine and Smooth subdivision algorithm. A unifying framework for primal dual univariate and bivariate subdivision schemes with tension parameter. A generalization to quadrilateral meshes of the univariate family of subdivision schemes with cubic precision. A non tensor product extension of the interpolatory 4 point and the dual approximating 4 point subdivision schemes.,,,
S0141938214000481," The TiO2 coated silicate micro spheres core shell particles were synthesized using the sol gel reaction followed by calcination . The SMS @ TiO2 particles were used to enhance the light diffusion property of polycarbonate composites . Our experimental analysis shows that the TiO2 was coated on the SMS particles and the optimum parameters of the reaction were 4 1 of the Si Ti molar ratio and 500 C of the calcination temperature . The UV Vis spectra indicate that SMS @ TiO2 can absorb or hinder the UV light which may prolong the service life of PC light diffusion materials . Compared to that of PC composites physically mixed with SMS TiO2 the haze of the PC SMS @ TiO2 composites was little affected while the transmittance was obviously enhanced which can be increased from 55.5 for PC TiO2 SMS to 70.3 for PC SMS @ TiO2 with only 0.6wt filler loading . 
",The SMS@TiO2 particles were used to enhance the light diffusion property of polycarbonate PC composites. The SMS@TiO2 can absorb or hinder the UV light which may prolong the service life of PC light diffusion materials. The resulting light diffusing agent was well dispersed in PC. The resulting light diffusing agent achieved the balance of transmittance and haze for light diffusion applications.,,,
S0165168414001054," In this paper we propose a musical noise free blind speech extraction method using a microphone array for application to nonstationary noise . In our previous study it was found that optimized iterative spectral subtraction results in speech enhancement with almost no musical noise generation but this method is valid only for stationary noise . The proposed method consists of iterative blind dynamic noise estimation by e.g . independent component analysis or multichannel Wiener filtering and musical noise free speech extraction by modified iterative SS where multiple iterative SS is applied to each channel while maintaining the multichannel property reused for the dynamic noise estimators . Also in relation to the proposed method we discuss the justification of applying ICA to signals nonlinearly distorted by SS . From objective and subjective evaluations simulating a real world hands free speech communication system we reveal that the proposed method outperforms the conventional methods . 
",We propose a musical noise free speech extraction method using a microphone array. We theoretically clarify the degradation of the noise estimation accuracy. We introduce a channel selection strategy to improve the noise estimation accuracy. From experiments the proposed method outperforms the conventional methods,,,
S0098300415300625," Spectral induced polarisation measurements capture the low frequency electrical properties of soils and rocks and provide a non invasive means to access lithological hydrogeological and geochemical properties of the subsurface . The Debye decomposition approach is now increasingly being used to analyse SIP signatures in terms of relaxation time distributions due to its flexibility regarding the shape of the spectra . Imaging and time lapse SIP measurements capturing SIP variations in space and time respectively are now more and more conducted and lead to a drastic increase in the number of spectra considered which prompts the need for robust and reliable DD tools to extract quantitative parameters from such data . We here present an implementation of the DD method for the analysis of a series of SIP data sets which are expected to only smoothly change in terms of spectral behaviour such as encountered in many time lapse applications where measurement geometry does not change . The routine is based on a non linear least squares inversion scheme with smoothness constraints on the spectral variation and in addition from one spectrum of the series to the next to deal with the inherent ill posedness and non uniqueness of the problem . By means of synthetic examples with typical SIP characteristics we elucidate the influence of the number and range of considered relaxation times on the inversion results . The source code of the presented routines is provided under an open source licence as a basis for further applications and developments . 
",Time lapse SIP data are analysed using a Debye decomposition scheme. Smoothness constraints are imposed on spectral and temporal variations. The chosen range of relaxation times must correspond to the frequency range of the data.,,,
S0167839614001010," A generic planar quadrilateral defines a 2 1 bilinear map . We show that by assigning an appropriate weight to one vertex of any planar quadrilateral we can create a map whose inverse is rational linear . 
",We show how to make a quad map be birational by assigning one control point weight. The weight is computed using a simple closed form equation. For convex quads the weight is positive and the pre image is the domain. Any planar quad mesh can be made piecewise birational by using this method.,,,
S0167839614000272,"We present a method for approximating a point sequence of input points by a continuous smooth arc spline with the minimum number of segments while not exceeding a user specified tolerance. Arc splines are curves composed of circular arcs and line segments shortly segments . For controlling the tolerance we follow a geometric approach We consider a simple closed polygon P and two disjoint edges designated as the start s and the destination d. Then we compute a SMAP smooth minimum arc path i.e. a smooth arc spline running from s to d in P with the minimally possible number of segments. In this paper we focus on the mathematical characterization of possible solutions that enables a constructive approach leading to an efficient algorithm. In contrast to the existing approaches we do not restrict the breakpoints of the arc spline to a predefined set of points but choose them automatically. This has a considerably positive effect on the resulting number of segments. 
",A method for approximating point sequences by arc splines is presented. The breakpoints of the arc spline are not restricted to any pre defined set of points. For any user defined accuracy the minimal number of segments is guaranteed. The resulting compact curve representation enables an efficient further processing. Among others it is hence suited for the generation of highly accurate digital maps.,,,
S0098300414002829,"The EverVIEW Data Viewer is a cross platform desktop application that combines and builds upon multiple open source libraries to help users to explore spatially explicit gridded data stored in Network Common Data Form NetCDF . Datasets are displayed across multiple side by side geographic or tabular displays showing colorized overlays on an Earth globe or grid cell values respectively. Time series datasets can be animated to see how water surface elevation changes through time or how habitat suitability for a particular species might change over time under a given scenario. Initially targeted toward Florida s Everglades restoration planning EverVIEW has been flexible enough to address the varied needs of large scale planning beyond Florida and is currently being used in biological planning efforts nationally and internationally. 
",Standards compliant viewer for NetCDF and WMS. Explore and animate time series data as tabular grids or colorized Earth overlays. Intuitive spatially driven user interface for comparing datasets. Model View Presenter software design pattern offers flexibility and extensibility. Custom tools leverage the platform for large scale ecological planning.,,,
S0141938213000759," Visual secret sharing schemes based on visual cryptography or random grids have been proposed in the past decade with the advantages of easy implementation efficiency secret recovering and perfect security . As the concept of multiple secret images has gained more and more attention in academia the novel concept of VC based VSS with cyclic access structure has been discussed recently which is a special case of multiple VSS allowing participants to reconstruct the secret with the one next or last to him her in a cyclic order . To obtain the benefit of RG based VSS compared with VC based VSS this paper proposes the new VSS scheme with cyclic access structure for multiple secret images by random grids . The experimental results and theoretical security analysis demonstrate the feasibility . 
",A VSS scheme with cyclic access structure for multiple images by random grids. The drawbacks from VC based while inheriting all its advantages are removed. The first advantage is loosening limitation of secret number. The second advantage is removing pixel expansion while visual quality is guaranteed.,,,
S0141938214000596," A novel MEMS display device comprising a light separator and MEMS light shutters is introduced . This device is operable both in transmissive mode using internal light source and in reflective mode using external sun light . In transmissive mode the light separator directs internal backlight illuminated on its incident surface into a plural of small openings on the viewing surface . The MEMS light shutters are used to control the color and intensity of light at individual pixels . Internal light utilization efficiency of this display is the greatest compared to any other transmissive displays . The device is capable of having true black background hence a very high contrast ratio . In reflective mode MEMS shutters direct part of the sun light for image display . MEMS shutters have fast response time making the new device suitable for vivid motion picture display and operable at very low voltages suitable for mobile device applications . 
",A new type of MEMS display that is operable at very low voltage is introduced. Its internal back light is directed to exit grooves and controlled by MEMS shutters. A large surface area for building MEMS devices and for electrostatic interaction. True black background high contrast ratio and high light utilization efficiency. It is also operable in reflective mode using external light e.g. bright sunlight.,,,
S0141938215000621," Two phenylanthracene substituted fluorene derivatives 10 9H fluoren 7 yl phenylanthracene and 2 7 di 9 9 spirobi have been designed synthesized and characterized . A device using compound 1 as an emitting material exhibited luminous efficiency power efficiency external quantum efficiency and CIE coordinates of 3.37cd A 1.50lm W 1.87 at 20mA cm2 and at 7V respectively . Furthermore by exploiting this efficient blue fluorescent material as a blue emitting material with the combination of red phosphorescent bisacetylacetonate 2Ir an efficient white OLED with a external quantum efficiency of 1.70 luminous efficiency of 1.38cd A power efficiency of 0.94lm W at 20mA cm2 and the color coordinates of at 14V is demonstrated . 
",We synthesized blue fluorescent materials based on anthracene fluorene hybrid. These materials have the good potentials for efficient blue OLEDs as the emitters. A white OLED using one of these materials as blue emitter was demonstrated.,,,
S0045794914000856," The spatial variation of cell size in a functionally graded cellular structure is achieved using error diffusion to convert a continuous tone image into binary form . Effects of two control parameters greyscale value and resolution on the resulting cell size measures were investigated . Variation in cell edge length was greatest for the Voronoi connection scheme particularly at certain parameter combinations . Relationships between these parameters and cell size were identified and applied to an example where the target was to control the minimum and maximum cell size . In both cases there was an 8 underestimation of cell area for target regions . 
",A novel approach to generate functionally graded cellular structures was evaluated. The relationships between the method parameters and cell size were investigated. Variation in cell edge length was greatest for the Voronoi connection scheme. There was approximately an 8 underestimation of cell area for both target regions. Practical implementation implications were discussed for tackling 3D problems.,,,
S0141938214000912," In this paper we investigated the accuracy of center to center distance perception in near field augmented reality visual targets viewed by stereoscopic glasses . One real and one virtual targets were presented in four layout or target orientations at three different parallax conditions and four levels of scaled between targets distance . The result revealed overall underestimation with an accuracy of about 84 . Interestingly it was noticed that the main effects of layout parallax and center to center distance were significant . Generally accuracy improves when targets put vertical close to observers position and smaller separation of targets . Significant interactions among the three main factors were also reported . The results are of great importance as it provides guide for the developers to decide where to present targets depending on the need for relative accuracy of judgment . Some engineering implications of the result are also discussed in this paper . 
",Accuracy of exocentric distance estimation in augmented reality were studied. Layout parallax and center to center distance influence the accuracy of estimation. Observers generally underestimated center to center distance. Accuracy improved as real and virtual targets presented vertical and closer to observer. Underestimation raised a question whether real environment was dominated by virtual.,,,
S0167839614000260," In Winkel a generalization of Bernstein polynomials and B zier curves based on umbral calculus has been introduced . In the present paper we describe new geometric and algorithmic properties of this generalization including families of polynomials introduced by Stancu and Goldman i.e . families that include both Bernstein and Lagrange polynomial are generalized in a new way a generalized de Casteljau algorithm is discussed an efficient evaluation of generalized B zier curves through a linear transformation of the control polygon is described a simple criterion for endpoint tangentiality is established . 
",Generalization of both Bernstein and Lagrange polynomials. Generalization of the Stancu and Goldman polynomials. Generalization of the de Casteljau algorithm in a new way. Efficient evaluation through a linear transformation of the control polygon. New results are illustrated with many pictures.,,,
S0167839615000400," Toric surface patches are a multi sided generalization of classical rational B zier surface patches which are widely used in free form surface modeling . In this paper we present the first derivatives of toric surface patches along the boundary and study the continuity between adjacent toric surface patches by the toric degenerations . Furthermore some practical sufficient conditions of toric surface patches are developed and the representative examples are given . 
",We present the first derivatives of toric surface patches along the boundary. The conditions for G1 continuity between toric surface patches are analyzed. Some practical sufficient conditions for G1 continuity are developed.,,,
S0141938214000602," Reliability of MEMS devices is a crucial aspect as it can discriminate the successful from partially or totally missed reaching of Microsystem technology based market products . However the topic of MEMS reliability is significantly articulated as it comprises numerous physics of failure and diverse failure mechanisms . Thereafter it requires a pronounced sensitivity related to the actual operation conditions of the Microsystem device within the final application . In other words reliability of MEMS is nowadays regarded as a standalone transversal discipline that must be seriously taken into account already from the early design phase . The purpose of this paper is to provide the reader at first with basic knowledge around the concept of reliability . Thereafter the most relevant physics of failure and failure mechanisms typical of MEMS are grouped and briefly discussed with specific attention to their employment in the field of displays . A synthetic review of valuable solutions to improve specific reliability aspects of MEMS devices for diverse applications is then proposed to the reader . Eventually a brief discussion focused on best practices to address properly reliability during the whole development chain of innovative MEMS based products completes the contribution . It is a belief of the author that the particular blend of topics and aspects reported in the following pages as well as the attitude of considering reliability as a transversal discipline of science contribute to provide this contribution with an important benefit if compared to the reviews on reliability of MEMS previously published in literature . 
",Formal definition of reliability of MEMS. Identification of the main failure mechanisms and of physics of failure. Review of a few solutions published in literature to improve reliability of various MEMS devices. Brief discussion of best practices to be followed in the design of high reliability MEMS.,,,
S0140366415000560," In typical mobile sensing architectures sensing data are collected from users and stored in centralized servers at third parties making it difficult to effectively protect users privacy . A better way to protect privacy is to upload sensing data on personal data stores which are owned and controlled by the users enabling them to supervise and limit personal data disclosure and exercise access control to their data . The problem however remains how data requesters can discover the users who can offer them the data they need . In this paper we suggest a mobile sensing platform that enables data requesters to discover data producers within a specific geographic region and acquire their data . Our platform protects the anonymity of both requesters and producers while at the same time it enables the incorporation of trust frameworks incentive mechanisms and privacy respecting reputation schemes . We also present extensive experimental results that demonstrate the efficiency of our approach in terms of scalability load balancing and performance . 
",We consider privacy for both data consumers and data producers in mobile sensing. We present an integrated platform that considers social aspects of mobile sensing. Data providers can define their own privacy preferences and control how much they share. The platform provider is also considered by ensuring only registered users can request data. We present a distributed solution that requires no trusted third parties.,,,
S0140366415002169," Location based social networks have recently attracted a lot of attention due to the number of novel services they can offer . Prior work on analysis of LBSNs has mainly focused on the social part of these systems . Even though it is important to know how different the structure of the social graph of an LBSN is as compared to the friendship based social networks it raises the interesting question of what kinds of linkages exist between locations and friendships . The main problem we are investigating is to identify such connections between the social and the spatial planes of an LBSN . In particular in this paper we focus on answering the following general question What are the bonds between the social and spatial information in an LBSN and what are the metrics that can reveal them In order to tackle this problem we employ the idea of affiliation networks . Analyzing a dataset from a specific LBSN we make two main interesting observations the social network exhibits signs of homophily with regards to the places venues visited by the users and the nature of the visited venues that are common to users is powerful and informative in revealing the social spatial linkages . We further show that the entropy of a venue can be used to better connect spatial information with the existing social relations . The entropy records the diversity of a venue and requires only location history of users . Finally we provide a simple application of our findings for predicting existing friendship relations based on users historic spatial information . We show that even with simple unsupervised or supervised learning models we can achieve significant improvement in prediction when we consider features that capture the nature of the venue as compared to the case where only apparent properties of the location history are used . 
",We model a location based social network as an affiliation network where the affiliations are the locations visited by the users. We identify clear signs of location based homophily. The type of common locations between users encodes more information about the social connections as compared to the number of common locations visited. Using only location information for users we provide a system that can recover the social ties between them.,,,
S0141938213000620," A main requirement for achieving high efficiency in organic light emitting diodes is that all charges and electrically generated excitons should be employed for emission . We fabricated blue phosphorescent OLEDs with four types electron transporting layers which were doped with lithium quinolate from 0 to 10 . A series of blue devices consisted of indium tin oxide 4 4 bis N phenyl amino biphenyl N N dicarbazolyl 3 5 benzene iridiumbis pyridinato N C2 picolinate doped in mCP 1 3 5 trisbenzene TPBi mixed with Liq Liq aluminum . The blue OLED doped with 5 Liq which demonstrated a maximum luminous efficiency and external quantum efficiency of 17.64cd A and 8.78 respectively were found to be superior to the other blue devices . 
",We study on the effect of the Liq in device performances. We use Liq with one of organometallic compound materials. We fabricated an optimal device with double electron transport layer as the Liq concentration increased. We demonstrated efficient carriers balance in blue PHOLEDs.,,,
S0045790616300593,"In this paper an intelligent fuzzy logic control strategy optimized by genetic algorithm GA has been proposed for uniaxial parallel hybrid electric vehicle PHEV in the fuzzy controller the ratio between the motor target torque and the total demand torque is the first input variable and the state of charge SOC of the battery is the second input variable. The torque distribution coefficient between the engine and the motor is as the output variable. The proposed strategy is compared to the electric auxiliary control strategy. The whole vehicle is modeled based on experimental data and the strategy is verified in automotive simulation software ADVISOR the results show that the proposed control strategy can reduce fuel consumption reduce emissions ensure balanced charging and discharging of the battery effectively avoid the production of the engine peak torque and improve the overall performance of the vehicle. 
",An intelligent fuzzy control strategy is proposed for parallel hybrid electric vehicle. The membership functions of fuzzy controller are optimized by genetic algorithm. Electric vehicle co simulation technology is a good application.,,,
S0165168416000451," In signal processing applications it is often necessary to extract oscillatory components and their properties from time frequency representations e.g . the windowed Fourier transform or wavelet transform . The first step in this procedure is to find an appropriate ridge curve a sequence of amplitude peak positions corresponding to the component of interest and providing a measure of its instantaneous frequency . This is not a trivial issue and the optimal method for extraction is still not settled or agreed . We discuss and develop procedures that can be used for this task and compare their performance on both simulated and real data . In particular we propose a method which in contrast to many other approaches is highly adaptive so that it does not need any parameter adjustment for the signal to be analyzed . Being based on dynamic path optimization and fixed point iteration the method is very fast and its superior accuracy is also demonstrated . In addition we investigate the advantages and drawbacks that synchrosqueezing offers in relation to curve extraction . The codes used in this work are freely available for download . 
",Identifying and following instantaneous ridge frequencies in complex signals. Adaptive universal extraction of ridge frequencies from time frequency representations. Comparison of the performance of different approaches for ridge curve extraction. Relative performance of the synchrosqueezed transforms in terms of curve extraction. Freely available codes implementing the new method.,,,
S0098300415001089," Portable gas analyzers have become a powerful tool for the real time monitoring of volcanic gas composition over the last decade . Gas analyzers make it possible to retrieve in real time the chemical composition of a fumarole system or a plume in an open conduit volcano via periodic field deployments or at permanent stations . The core of a multicomponent volcanic gas analyzer consists of spectroscopic and electrochemical sensors that are used to determine the concentrations of the most abundant volcanic gases in a diluted plume and their mutual molar ratios . Processing such data is often difficult due to the high sensitivity of the sensors to environmental conditions such as humidity gas concentrations and pressure with all involving occasional instrumental drift . Analyses require accurate and time consuming processing by an operator . This paper presents a stand alone program for the processing of chemical data obtained using the MultiGAS called Ratiocalc . The Ratiocalc program has a user friendly interface to enable volcanologists to process large datasets in a simple and rapid manner thereby reducing the processing time associated with volcano monitoring and surveying . 
",Portable gas analyzers for the real time monitoring of volcanic gas composition. Ratiocalc software for the processing of chemical data obtained using the MultiGAS. Electrochemical and infrared sensors for volcanic gas measurements. Calibration cross interference correction georeferencing of MultiGAS data.,,,
S0141938213001066," Interactive displays are becoming an increasingly popular civic engagement mechanism for collecting user feedback in urban settings . However to date no study has investigated how the situatedness of public displays affects the quantity and quality of collected feedback and how public displays compare to traditional paper or web feedback mechanisms . We answer these research questions in a series of lab and field studies . We demonstrate that because people tend to approach this technology with no specific purpose in mind the feedback collected with public displays is noisier than web and paper forms . However we also show that public displays attract much more feedback than web and paper forms and generate much more interest . Furthermore we found that users appropriated our technology beyond its original purpose . Our analysis provides implications on the tradeoffs of using public displays as a feedback mechanism and we discuss ways of improving the collected feedback using public displays . 
",Public displays paper and web forms were compared as feedback mechanisms. We examined the quantity and quality of feedback obtained from each medium. Public displays produced a high quantity of feedback but mostly noise. Paper and web forms generated lower quantity of feedback but better quality. Public displays can be leveraged as significant interest generators.,,,
S0164121215001818," Coping with evolution in automated production systems implies a cross disciplinary challenge along the system s life cycle for variant rich systems of high complexity . The authors from computer science and automation provide an interdisciplinary survey on challenges and state of the art in evolution of automated production systems . Selected challenges are illustrated on the case of a simple pick and place unit . In the first part of the paper we discuss the development process of automated production systems as well as the different type of evolutions during the system s life cycle on the case of a pick and place unit . In the second part we survey the challenges associated with evolution in the different development phases and a couple of cross cutting areas and review existing approaches addressing the challenges . We close with summarizing future research directions to address the challenges of evolution in automated production systems . 
",Automated Production Systems aPS impose specific requirements regarding evolution. We present a classification of how Automated Production Systems evolve. We discuss the state of art and research needs for the development phases of aPS. Model driven engineering and Variability Management are key issues. Cross discipline analysis of non functional requirements must be improved.,,,
S0141938215000979," HEVC is the latest coding standard to improve the coding efficiency by a factor of two over the previous H.264 AVC standard at the cost of the increased complexity of computation rate distortion optimization is one of the computationally demanding operations in HEVC and makes it difficult to process the HEVC compression in real time with a reasonable computing power . This paper aims to present various simplified RDO algorithms with the evaluation of their RD performance and computational complexity . The algorithms for the simplified estimation of the sum of squared error and context adaptive binary arithmetic coding proposed for H.264 AVC are reviewed and then they are applied to the simplification of HEVC RDO . By modifying the previous algorithm for H.264 AVC a new simplified RDO algorithm is proposed for modifying the previous algorithm for H.264 AVC to be optimized for the hierarchical coding structure of HEVC . Further simplification is attempted to avoid the transforms operations in RDO . The effectiveness of the existing H.264 AVC algorithms as well as the proposed algorithms targeted for HEVC is evaluated and the trade off relationship between the RD performance and computational complexity is presented for various simplification algorithms . Experimental results show that reasonable combinations of RDO algorithms reduce the computation by 80 85 at the sacrifice of the BD BR by 3.46 5.93 for low delay configuration . 
",Analysis of the previous research for H.264 AVC RDO and its application to HEVC RDO. Proposing new simplified algorithms for HEVC RDO. Analysis of the relation between computation complexity and quality degradation for proposed algorithms. Hardware estimation for proposed algorithms.,,,
S0141938215300081," Anaglyph video is a recent trend in movies and multimedia the method has also been recently developed for conversion of such videos from traditional 2D screens or by rendering stereoscopic media into 3D video . There have also been many studies regarding movie playing and other types of entertainment that uses olfactory displays or smell generators . These devices are capable of generating a considerable number of different odors with different intensities yet still have some limitations and are not ready for commercial use . In this study a complete solution is presented for a user to experience olfactory sensations with a displayed video on a PC with auditory and 3D visual effects using a novel olfactory display device based on a micro porous piezoelectric film that is capable of digitally producing fine particles of scent material with precision quantity and speed . 
",We implemented an olfactory display based on a novel approach. We developed a novel data hiding technique in 3D video with high robustness. Both novel techniques are combined to provide smell data to the users. A high control over smell data digitization and scene synchronization has been achieved. The system is scalable efficient and applicable to existing 3D video files.,,,
S0141938215300056," The aim of this study is to develop an effective method to analyze regions of interest . Two experiments were conducted at different times using different groups of observers with different images on different displays . Observers eye movement data were collected . Fixation maps showing CIELAB L values were created . The L values between the two maps were used to quantify differences in visual fields counting methods observer variability and repeatability between the two experiments . The results showed that fixation maps can be used to effectively analyze the distribution of eye movements between images . The L value calculated for two fixation maps is easy to understand and computes differences based only on ROIs more effectively than differences based on the entire image . The results from the two experiments were consistent indicating that eye tracking data are robust for evaluating image quality . 
",We develop an effective method to analyze human s regions of interest ROI . Results of two experiments can be compared to test the robustness of the method. The CIEL has been analyze to check the amount of variation between fixation maps. The fixation map can be easy used to analyze the ROIs between images. The delta L value is more effective by computing the fixation map of entire image.,,,
S0167839613001015," A construction of spline spaces suitable for representing smooth parametric surfaces of arbitrary topological genus and arbitrary order of continuity is proposed . The obtained splines are a direct generalization of bivariate polynomial splines on planar partitions . They are defined as composite functions consisting of rational functions and are parametrized by a single parameter domain which is a piecewise planar surface such as a triangulation of a cloud of 3D points . The idea of the construction is to utilize linear rational transformations to endow the piecewise planar surface with a particular differentiable structure appropriate for defining rational splines . 
",A new approach for constructing smooth parametric surfaces is proposed. Bivariate polynomial splines are generalized to rational splines. The genus and order of continuity of the surfaces can be arbitrary. The construction employs linear rational transformations as transition maps. The computational properties of the new splines are similar to those of the classical splines.,,,
S0140366415002613," Graph sampling refers to the process of deriving a small subset of nodes from a possibly huge graph in order to estimate properties of the whole graph from examining the sample . Whereas topological properties can already be obtained accurately by sampling current approaches do not take possibly hidden dependencies between node topology and attributes into account . Especially in the context of online social networks node attributes are of importance as they correspond to properties of the social network s users . Therefore existing sampling algorithms can be extended to attribute sampling but still lack the capturing of structural properties . Analyzing topology and attribute properties jointly can provide valuable insights into the social network and allows for a better understanding of social processes . As major contribution this work proposes a novel sampling algorithm which provides unbiased and reliable estimates of joint topological and attribute based graph properties in a resource efficient fashion . Furthermore the obtained samples allow for the generation of synthetic graphs which show high similarity to the original graph with respect to topology and attributes . The proposed sampling and generation algorithms are evaluated on real world social network graphs for which they demonstrate to be effective . 
",Existing sampling algorithms are extended to node attributes. A novel sampling algorithm is proposed which allows for joint capturing of structure and attribute characteristics. Topological graph similarity measures are extended to additionally assess attribute related similarity. A graph generation method is presented that reproduces topology and attribute related properties of the original graph based on sampling.,,,
S0167839614000545," Ganchev has recently proposed a new approach to minimal surfaces . Introducing canonical principal parameters for these surfaces he has proved that the normal curvature determines the surface up to its position in the space . Here we prove a theorem that permits to obtain equations of a minimal surface in canonical principal parameters and we make some applications on parametric polynomial minimal surfaces . Thus we show that Ganchev s approach implies an effective method to prove the coincidence of two minimal surfaces given in isothermal coordinates by different parametric equations . 
",A differential equation for the transition to canonical parameters is obtained. The class of holomorphic functions generating one minimal surface is determined. Surfaces homothetic to the general Enneper surface are given. The coincidence of some families of polynomial minimal surfaces is proved.,,,
S0141938215300111," This paper presents a new bi side gate driver integrated by indium zinc oxide thin film transistors . Our optimized operate method can achieve high speed performance by employing a lower duty ratio CK2 with its pulse located in the middle of the pulse of CK2L to fully use the bootstrapped high voltage of node Q . In addition the size of devices is optimized by calculation and simulation and the function of the proposed gate driver is predicted by the circuit simulation . Furthermore the proposed gate driver with 20 stages is fabricated by the IZO TFTs process . It is shown that a 2.6 s width pulse with good noise suppressed characteristic can be successfully output at the condition of R load 6k and C load 150pF . The power consumption of the proposed gate driver with 20 stages is measured as 1mW . Hence the proposed gate driver may be applied to the display of 4K resolution at a frame rate of 120Hz . Moreover there is a good stability for the proposed gate driver under 48h operation . 
",The design of high speed gate driver is optimized by calculation and simulation. A 20 stages high speed gate driver is fabricated by IZO TFTs process. A 2.6 s width pulse can be output at the condition of R load 6k and C load 150pF. There is a good stability verified by a 48h test.,,,
S0140366415004211," Publish subscribe is an increasingly popular messaging pattern for distributed systems supporting scalable and extensible programming and optimal spatial temporal and control flow decoupling of distributed components . Publish subscribe middleware and methods were extended towards supporting security in particular confidentiality and increased availability yet a few prior works addressed anonymity of participants . Anonymity of senders and receivers may however be crucial e.g . for supporting freedom of expression in regimes where political repression and censorship prevail . In this article we review basic security and privacy requirements and introduce a new attacker model based on statistical disclosure used to challenge anonymity . We elaborate on design options for privacy preserving publish subscribe systems and present a novel system that leverages peer to peer networking concepts this novel approach protects subscriber anonymity by means of Probabilistic Forwarding and through a novel so called Shell Game algorithm . We verify our solution against the requirements and provide a simulation based analysis of the effectiveness of our approaches in light of our attacker model . The results show that the SG algorithm efficiently protects subscriber anonymity and that anonymity sets can be adjusted via PF . 
",We propose a protocol to construct privacy preserving publish subscribe overlays. Two extensions probabilistic forwarding and the shell game protect anonymity. The construction protects against malicious insiders and global observers. Simulation results show effective privacy protection.,,,
S0141938213000668," Poly poly is the most successful conducting polymer in terms of the practical application . It can be dispersed in water and some polar organic solvents and high quality PEDOT PSS films can be readily prepared through solution processing . In addition PEDOT PSS is highly transparent in the visible range and has excellent thermal stability . Nevertheless PEDOT PSS has a problem of low conductivity . The as prepared PEDOT PSS films from its aqueous solution have a conductivity of lower than 1Scm 1 which severely impedes the application of PEDOT PSS in various aspects . It has been discovered that the conductivity of as prepared PEDOT PSS from its aqueous solution can be significantly enhanced by adding organic compounds like high boiling point polar organic solvents ionic liquids and surfactants or through a post treatment of PEDOT PSS films with organic compounds including high boiling point polar solvents salts zwitterions cosolvents organic and inorganic acids . Conductivity of more than 3000Scm 1 was recently observed on PEDOT PSS films after treated with sulfuric acid . This conductivity is comparable to that of indium tin oxide the conventional transparent electrode material of optoelectronic devices . In addition PEDOT PSS has high mechanical flexibility while ITO is a brittle material . Thus PEDOT PSS is very promising to be the next generation transparent electrode material . This article reviews the methods to enhance the conductivity of PEDOT PSS the mechanisms for the conductivity enhancements and the application of the highly conductive PEDOT PSS films in polymer light emitting diodes and polymer solar cells . 
",PEDOT PSS is promising to be the next generation transparent electrode. This article reviews the secondary doping methods for conductivity enhancement of PEDOT PSS. The conductivity of PEDOT PSS can be comparable to ITO. Highly conductive PEDOT PSS films were used for polymer light emitting diodes and polymer solar cells.,,,
S0141938215000578," Industrial and academic researches have in the last twenty years matured the plasma display panels to the successful product level for commercial flat screen television sets . The sustain driver with resonant operation converting high dc voltage to ac square wave voltage with high frequency for strong visible light emission have great influence on the entire circuit efficiency and cost from driving circuit point . Although circuit losses and costs of PDP drivers are of the utmost importance comparative study of the sustain drivers has been missed in literature despite numerous publications for PDP drivers . This paper in detail presents the theoretical analyses and systematic comparisons of the PDP drivers in terms of switch current and voltage ratings . Based on analytical expressions and comparative features the sustain drivers are evaluated from standpoint of switch ratings . 
",This paper derives theoretical analytic models for switch current and voltage ratings of various PDP sustain drivers. The PDP drivers in terms of switch current and voltage ratings are systematic compared. Based on analytical expressions and comparative features the sustain drivers are evaluated from standpoint of switch ratings.,,,
S0098300415001466,"Metrics to track seasonal transitions are needed for a wide variety of ecological and climatological applications. Here a MATLAB toolkit for calculating spring indices is documented. The spring indices have been widely used in earlier studies to model phenological variability and change through time across a wide range of spatial scales. These indices require only daily minimum and maximum temperature observations e.g. from meteorological records as input along with latitude and produce a day of year value corresponding to the simulated average timing of first leaf and first bloom events among three plant cultivars. Core functions to calculate the spring indices require no external dependencies and data for running several illustrative test cases are included. Instructions and routines for conducing more sophisticated monitoring and modeling studies using the spring indices are also supplied and documented. 
",Spring indices can be used to characterize spring onset across a wide range of spatial scales. These indices are based on phenological models but can be computed from meteorological data. We present software and documentation for calculating spring indices. Some basic examples of the potential uses and extensions of the spring indices are also shown.,,,
S0140366416300068," Adoption of SSL TLS to protect the privacy of web users has become increasingly common . In fact as of September 2015 more than 68 of top 1M websites deploy SSL TLS to encrypt their traffic . The transition from HTTP to HTTPS has brought a new challenge for network operators who need to understand the hostnames of encrypted web traffic for various reasons . To meet the challenge this work develops a novel framework called SFMap which estimates names of HTTPS servers by analyzing precedent DNS queries responses in a statistical way . The SFMap framework introduces domain name graph which can characterize highly dynamic and diverse nature of DNS mechanisms . Such complexity arises from the recent deployment and implementation of DNS ecosystems i.e . canonical name tricks used by CDNs the dynamic and diverse nature of DNS TTL settings and incomplete and unpredictable measurements due to the existence of various DNS caching instances . First we demonstrate that SFMap establishes good estimation accuracies and outperforms a state of the art approach . We also aim to identify the optimized setting of the SFMap framework . Next based on the preliminary analysis we introduce techniques to make the SFMap framework scalable to large scale traffic data . We validate the effectiveness of the approach using large scale Internet traffic . 
",We present the domain name graph DNG which is a formal expression that can keep track of CNAME chains and characterize the dynamic and diverse nature of DNS mechanisms and deployments. We develop a framework called Service Flow map SFMap that works on top of the DNG. SFMap estimates the hostname of an HTTPS server when given a pair of client and server IP addresses. It can statistically estimate the hostname even when associating DNS queries are unobserved due to caching mechanisms etc. Through extensive analysis using real packet traces we demonstrate that the SFMap framework establishes good estimation accuracies and can out perform the state of the art technique called DN Hunter. We also identify the optimized setting of the SFMap framework. The experiment results suggest that the success of the SFMap lies in the fact that it can complement incomplete DNS information by leveraging the graph structure. To cope with large scale measurement data we introduce techniques to make the SFMap framework scalable. We validate the effectiveness of the approach using large scale traffic data collected at a gateway point of Internet access links.,,,
S0165168414003818," Human motion denoising is an indispensable step of data preprocessing for many motion data based applications . In this paper we propose a data driven based human motion denoising method that sparsely selects the most correlated subset of motion bases for clean motion reconstruction . Meanwhile it takes the statistic property of two common noises i.e . Gaussian noise and outliers into account in deriving the objective functions . In particular our method firstly divides each human pose into five partitions termed as poselets to gain a much fine grained pose representation . Then these poselets are reorganized into multiple overlapped poselet groups using a lagged window moving across the entire motion sequence to preserve the embedded spatial temporal motion patterns . Afterward five compacted and representative motion dictionaries are constructed in parallel by means of fast K SVD in the training phase they are used to remove the noise and outliers from noisy motion sequences in the testing phase by solving 1 minimization problems . Extensive experiments show that our method outperforms its competitors . More importantly compared with other data driven based method our method does not need to specifically choose the training data it can be more easily applied to real world applications . 
",A fine grained pose representation model is proposed to boost the performance. We present a data driven based motion denoising method by solving 1 minimization problems. The proposed model selects the most correlated motion bases for motion reconstruction. Our method does not need to choose the training data and can be implemented in parallel mode.,,,
S0141938214000043," It has been common and popular to watch videos in moving vehicles . An important issue in developing comfortable in vehicle video watching systems is to understand how passengers get motion sickness . With this in mind the goals of this paper are to introduce an experimental protocol and a statistical analysis procedure for quantitatively evaluating how motion sickness is developed during car driving and to demonstrate their practical usefulness with a working experimental study . In the experimental protocol motion sickness was induced to subjects by requiring them to watch an in vehicle video during 15 min driving and the time course development of motion sickness was recorded by asking subjects to evaluate their degree of motion sickness every one minute . A main difficulty in analyzing data from these studies is how to incorporate the individual difference in motion sickness susceptibility . Since susceptibilities are markedly different among subjects within subject design experiments are preferred . However it is practically difficult to conduct complete set of trials because subjects who are not willing to continue experiment should be able to withdraw from the subsequent series of trials in accordance with ethical requirement . To cope with this incomplete data issue we introduce a statistical data analysis procedure that enables to estimate and impute missing entries in the within subject design table . Using a working example we demonstrated that the protocol and the procedure are useful for quantitative assessment of the time course motion sickness development . We conducted an in vehicle motion sickness study with 31 subjects where the time course motion sickness developments of video watching book reading and normal riding conditions were compared . The results indicate that video watching brings on 2.7 times more severe motion sickness than normal riding but 25 less severe than book reading . 
",We evaluated how severely in vehicle video watching brings motion sickness. Video watching aggravated motion sickness 2.7 times as much as ordinary car riding. Video watching aggravated motion sickness 25 less severely than book reading. A statistical protocol was introduced to analyze incomplete within subject data. The new model can handle missing entries and capture the time course development.,,,
S0167639315000692," This paper presents an unsupervised method that allows for gradual interpolation between language varieties in statistical parametric speech synthesis using Hidden Semi Markov Models . We apply dynamic time warping using Kullback Leibler divergence on two sequences of HSMM states to find adequate interpolation partners . The method operates on state sequences with explicit durations and also on expanded state sequences where each state corresponds to one feature frame . In an intelligibility and dialect rating subjective evaluation of synthesized test sentences we show that our method can generate intermediate varieties for three Austrian dialects . We also provide an extensive phonetic analysis of the interpolated samples . The analysis includes input switch rules which cover historically different phonological developments of the dialects versus the standard language and phonological processes which are phonetically motivated gradual and common to all varieties . We present an extended method which linearly interpolates phonological processes but uses a step function for input switch rules . Our evaluation shows that the integration of this kind of phonological knowledge improves dialect authenticity judgment of the synthesized speech as performed by dialect speakers . Since gradual transitions between varieties are an existing phenomenon we can use our methods to adapt speech output systems accordingly . 
",We present a method for interpolation between language varieties for speech synthesis. The method is unsupervised based on dynamic time warping. Phone sets and formant analysis for four Austrian language varieties are presented. Subjective evaluation shows that listeners can perceive the intermediate variants. Inclusion of switching rules increases the dialect authenticity of the synthesis.,,,
S0141938215000037," In many virtual environments such as those of video games the scene background moves to give the illusion of movement . In the present study two experiments were designed to investigate the combined impact of lateral background motion and task difficulty on players performance in a target shooting task . Participants had to perform the task on either the moving or the stationary version of a patterned background that was either green or black and white . The difficulty of the task was manipulated by varying the number of visual features shared between the target and distractor items . In accordance with the literature the participants performance was worse and the number and duration of participants fixations increased when the task was difficult . Background motion had an additive negative impact on performance . When the background was black and white background motion had an impact only when the task was easy but not when it was difficult . Design recommendations based on manipulations of the background characteristics are proposed to establish the level of difficulty in simple video games that use lateral background motion . 
",Lateral background motion decreases player performance in shooting tasks. The performance impairment tends to decrease when task difficulty increases. The data may be applied to control challenge in simple shooting video games.,,,
S0141938213000656," The characteristics of MoO3 F8BT ZnO inorganic organic hybrid light emitting diodes fabricated on ZnO Ag ZnO dielectric metal dielectric and conventional ZnO ITO were investigated . The DMD had a low sheet resistance of 9 sq and a high transmittance of 90.7 . The device fabricated on DMD showed similar current density voltage and luminance current density characteristics to that on ZnO ITO indicating the possibility of DMD as a promising transparent conductive layer for IO HyLEDs . The maximum luminous intensity of 237 000cd m2 was demonstrated under pulsed condition for the DMD device . We also investigated the effect of the combination interlayer at the F8BT ZnO interface on the IO HyLEDs . The CIL was composed of a Mg0.52Zn0.48O Mg0.25Zn0.75O bilayer and a self assembled dipole molecule of BA CH3 . The devices with CIL exhibited an approximately threefold enhancement of the luminous intensity and efficiency in comparison with the devices without CIL . This improvement was considered to be brought about by the enhancement of the electron injection efficiency by CIL . 
",MoO3 F8BT ZnO hybrid LEDs were fabricated on ZnO Ag ZnO DMD film. MoO3 F8BT ZnO hybrid LED on DMD showed luminous peak intensity of 237 000cd m2. We report an effect of SADM MgZnO interlayer on emission properties of hybrid LED. Emission efficiency of hybrid LEDs was markedly improved by SADM MgZnO interlayer.,,,
S0097849315001119,"We present an automatic approach for the reconstruction of parametric 3D building models from indoor point clouds. While recently developed methods in this domain focus on mere local surface reconstructions which enable e.g. efficient visualization our approach aims for a volumetric parametric building model that additionally incorporates contextual information such as global wall connectivity. In contrast to pure surface reconstructions our representation thereby allows more comprehensive use first it enables efficient high level editing operations in terms of e.g. wall removal or room reshaping which always result in a topologically consistent representation. Second it enables easy taking of measurements like e.g. determining wall thickness or room areas. These properties render our reconstruction method especially beneficial to architects or engineers for planning renovation or retrofitting. Following the idea of previous approaches the reconstruction task is cast as a labeling problem which is solved by an energy minimization. This global optimization approach allows for the reconstruction of wall elements shared between rooms while simultaneously maintaining plausible connectivity between all wall elements. An automatic prior segmentation of the point clouds into rooms and outside area filters large scale outliers and yields priors for the definition of labeling costs for the energy minimization. The reconstructed model is further enriched by detected doors and windows. We demonstrate the applicability and reconstruction power of our new approach on a variety of complex real world datasets requiring little or no parameter adjustment. 
",Reconstruction of high level parametric building models from indoor scans. Goes beyond mere surface reconstruction of previous reconstruction methods. Plausible configuration of walls determined by means of global optimization. Enables intuitive editing on level of building elements e.g. moving whole walls.,,,
S0167839614000247," This paper describes the application of a structure preserving matrix method to the deconvolution of two Bernstein basis polynomials . Specifically the deconvolution yields a polynomial provided the exact polynomial is a divisor of the exact polynomial and all computations are performed symbolically . In practical situations however inexact forms h and f of respectively and are specified in which case is a rational function and not a polynomial . The simplest method to calculate the coefficients of g is the least squares minimisation of an over determined system of linear equations in which the coefficient matrix is T plitz but the solution is a polynomial approximation of a rational function . It is shown in this paper that an improved result for g is obtained when the T plitz structure of the coefficient matrix is preserved that is a structure preserving matrix method is used . In particular this method guarantees that a polynomial solution to the deconvolution is obtained and thus an essential property of the theoretically exact solution is retained in the computed solution . Computational examples that show the improvement in the solution obtained from the structure preserving matrix method with respect to the least squares solution are presented . 
",A structured matrix method is used to deconvolve two Bernstein basis polynomials. The solution is obtained by solving a constrained optimisation problem. The solution in the paper is better than the solutions from other methods. The computational results are analysed theoretically.,,,
S0141938214000079," Phosphorescent white organic light emitting diodes based on single doped platinum benzene chloride emission layers were investigated in this paper . The devices exhibited electroluminescence spectra composed of bluish and reddish emission bands which corresponding to monomer and excimer emission originated from Pt 4 dopants . With optimized device structures a maximum current efficiency of 11.5cd A was obtained and remained above 10cd A even the brightness was over 6000cd m2 . Furthermore by integrating the fac tris iridium as a complementary emitter and an additional 2 2 2 tris space layer the device efficiency was further improved which exhibited a maximum current efficiency of 20.4cd A at the luminance of 100cd m2 and maintained the mild efficiency roll off that similar to its single Pt 4 doped counterpart . 
",White light emission achieved using the monomer and excimer emission from single Pt 4 dopant. Extremely low efficiency roll off even at luminance exceeding 6000cd m2. Device efficiency has been improved by at least two times with further doping of Ir ppy 3. By utilization of TPBi as space layer the device performance was further improved.,,,
S0141938215300433," The phase behaviors of a blue phase liquid crystal in a planar aligned cell were investigated by means of temperature dependent dielectric spectroscopy . With auxiliary observations of optical transmission spectra and birefringent textures we found that the transition temperatures of two adjacent mesophases including the cholesteric to BPI the BPI to BPII and the BPII to isotropic can clearly be distinguished by the first and second derivatives of the real part dielectric permittivity function to the temperature at a specific frequency . Furthermore an attempt to evaluate the device performance of a polymer stabilized BP derived from the photopolymerization of two monomers was achieved with a new circuital design for the voltage holding ratio measurement . Preliminary results of the VHRs of samples at 180Hz and their temperature dependence were obtained accordingly . 
",Phase behaviors of BP are studied by temperature dependent dielectric spectroscopy. Phase transition temperatures are identified with derivatives of the permittivity. A circuit design with 30 V data line voltage is proposed to evaluate VHR of a PSBP. The VHR of a PSBP cell is primarily determined by the nematic host used.,,,
S0141938214000626," Recombination zone and F rster resonance energy transfer in multilayer organic light emitting diodes were investigated . Basis device architecture is indium tin oxide N N diphenyl N N bis 1 1 biphenyl 4 4 diamine 4 2 tert butyl 6 4H pyran NPB tris aluminum 2 9 dimethyl 4 7 diphenyl 1 10 phenanthroline Al . Exciton recombination zone is located at DCJTB and Alq3 layers . When the NPB spacer is 10 nm thick Alq3 emission governs in electroluminescent spectra owing to absence of FRET between DCJTB and Alq3 . FRET occurs while the NPB spacer is 5 nm thick and thus DCJTB emission is dominant in EL spectra . As the emissive layout of DCJTB Alq3 NPB substitutes for DCJTB NPB Alq3 both DCJTB and NPB emissions are observed due to electron blocking effect of NPB . 
",Study of emissive layer layout dependent recombination zone and energy transfer. Basis emissive layer layout is DCJTB acceptor A NPB spacer Alq3 donor D . Efficiency of energy transfer from D to A is dependent on the thickness of spacer. Recombination zone is located at DCJTB and Alq3 in DCJTB NPB Alq3 layout. Recombination zone of DCJTB Alq3 NPB layout extends to NPB.,,,
S0098300414002532," The depth of valley incision and valley volume are important parameters in understanding the geologic history of early Mars because they are related to the amount sediments eroded and the quantity of water needed to create the valley networks . With readily available digital elevation model data the Black Top Hat transformation an image processing technique for extracting dark features on a variable background has been applied to DEM data to extract valley depth and estimate valley volume . Previous studies typically use a single window size for extracting the valley features and a single threshold value for removing noise resulting in finer features such as tributaries not being extracted and underestimation of valley volume . Inspired by similar algorithms used in LiDAR data analysis to remove above ground features to obtain bare earth topography here we propose a progressive BTH transformation algorithm where the window size is progressively increased to extract valleys of different orders . In addition a slope factor is introduced so that the noise threshold can be automatically adjusted for windows with different sizes . Independently derived VN lines were used to select mask polygons that spatially overlap the VN lines . Volume is calculated as the sum of valley depth within the selected mask multiplied by cell area . Application of the PBTH to a simulated landform achieved an overall relative accuracy of 96 in comparison with only 78 for BTH . Application of PBTH to Ma adim Vallies on Mars not only produced total volume estimates consistent with previous studies but also revealed the detailed spatial distribution of valley depth . The highly automated PBTH algorithm shows great promise for estimating the volume of VN on Mars on global scale which is important for understanding its early hydrologic cycle . 
",We developed an innovative algorithm for estimating valley volume based DEM data. Test on a simulated landform showed 96 relative accuracy in volume estimate. Application to Ma adim Vallis Mars resulted in better volume estimates. Algorithm is highly automated amendable to estimate global valley volume on Mars.,,,
S0141938213000450," A series of the Zn32 Eu3 without or with alkali metal ions doping at a low sintering temperature were synthesized by the solid state reaction method . The XRD pattern shows that all samples exhibit Zn32 crystalline phase . The samples co doped with alkali metal ions have better crystallinity compared with the un compensated ones . The different charge compensation approaches have no influence on the shape and position of the emission and excitation spectra . However the luminescent intensity of samples has been obviously enhanced with different alkali metal ions co doping . The introduction of Li can increase the red emission of Eu3 compared with the others . Thus the volume compensation and the equilibrium of mole number can be taken into consideration by charge compensated approaches . 
",Novel Zn3 BO3 2 Eu3 phosphor has been synthesized by solid state reaction method. The co doped effects of alkali metal ions on the luminescence properties for Zn3 BO3 2 Eu3 phosphors are discussed. The volume compensation and the equilibrium of mole number can be taken into consideration by charge compensation CC approaches.,,,
S0141938215000566," This study explores a new way to integrate advanced display technology into educational activities for children with different disabilities . A free interactive mobile augmented reality application was developed to facilitate the learning of geometry . Twenty one elementary school children participated in an experiment . The results show that the AR system could help the school children to finish puzzle game activities independent of teacher s assistance . With the use of AR display technology the participants demonstrated improved ability to complete puzzle game tasks when compared to the use of traditional paper based methods . Performance data indicated that the use of AR technology could enhance learning motivation and frustration tolerance in children with special needs . 
",Mobile devices with a free application were used to design teaching materials. Augmented reality techniques transform display methods to help children perform tasks independently. The experimental data shows that participants could complete tasks using AR support.,,,
S0141938214000894," New chiral compounds of the steroid type namely 3 alkanoyl and 3 alkyl derivatives of 16 arylidene estrone were synthesized and exhibited high helical twisting power in nematic liquid crystals E63 and LC 1289 . The peculiarities of the molecular structure of chiral compounds and their high degree of chirality were discussed . Quite small concentrations of chiral compounds in both nematic solvents were needed to obtain a short pitch cholesteric helical macrostructure with the selective light reflection in the visible range of the spectrum . The insignificant temperature dependence of the maximum wavelength of the selective light reflection obtained on the whole cholesteric range for all studied liquid crystalline systems could be related to relatively rigid molecular structure of the chiral compounds . The cholesteric LC mixtures comprising 3 alkanoyl and 3 alkyl derivatives of 16 arylidene estrone were characterized by the phase stability and the stability of reflective characteristics in time that makes it possible to consider this series of dopants as appropriate for development of bistable cholesteric reflective LC displays . 
",Dopants are derivatives of industrially available steroid and their synthesis is simple. Dopants own high twisting power and weak temperature shift of reflective wavelength. Low concentrations of dopant in nematic are needed to obtain red green and blue colors. Good solubility of dopants in nematics provides the stability of reflected colors in time.,,,
S0140366415002625," Online social networking websites such as Twitter and Facebook are known to have a wide heterogeneity in the popularity of their users which is counted typically in terms of the number of followers or friends of the users . We add to the large body of work on information diffusion on online social networking websites by studying how the behavior of the small minority of very popular users on Twitter differs from that of the bulk of the population of ordinary users and how these differences may impact information diffusion . Our findings are somewhat counter intuitive . We find that on aggregate metrics such as the tweeting volume and degree of participation on different topics popular users and ordinary users seem similar to each other . We also find that although popular users do seem to command an influential position in driving the popularity of topics on Twitter in practice they do not affect growth rates of user participation and the causality of popular users driving event popularity is hard to establish . Our observations corroborate the findings of other researchers who show that user popularity in terms of number of followers does not translate into driving event popularity but that event popularity may be driven by extraneous factors to do with the importance of the event . 
",Popular users tweet slightly earlier than ordinary users during the growth of Twitter events Popular users engage with a Twitter event for slightly longer than ordinary users Popular users write more original tweets than retweets while the trend is the reverse for ordinary users Tweets by popular users are retweeted more than tweets by ordinary users Popular users are the quickest to retweet tweets The volume of tweeting by popular users is however not different from that of ordinary users Popular users also do not seem to have an influence on the event growth rates Eventual popularity attained by an event is positively correlated with participation by popular users but causality is hard to establish,,,
S0141938215300147," Blue color organic polymeric light emitting diodes are very important because they can be used for tri color display applications fluorescence imaging and exciting yellow phosphor for generating white light for general illumination . But the efficiency of blue organic polymeric light emitting diodes is considerably low due to their large band gap that requires higher energy for effective emission . In this paper we report the enhancement in polyfluorene blue organic light emitting diodes with a polymer nano composite hole transport layer . Blue light emitting diode based on polyfluorene as an emissive layer and poly poly titanium dioxide nanocomposite as the hole transport layer were fabricated and studied . Different concentrations of titanium dioxide nanoparticles were doped in poly poly in the hole transport layer and the performance of the devices were studied . Significant enhancement in the blue peak at 430nm of polyfluorene has been observed with increase in concentration of TiO2 nanoparticles in the hole transport layer . The turn on voltage of the device has also been found to improve significantly with the incorporation of titanium dioxide nanoparticles in the hole transport layer . The optimized concentration of titanium dioxide in the hole transport layer for most efficient device has been found to 15wt . . 
",Polyfluorene based blue OLEDs with polymer nano composite HTL have been fabricated. Significant enhancement in blue peak at 430nm of polyfluorene has been observed. The turn on voltage of the device has also been found to improve significantly. The concentration of TiO2 nanoparticles in the HTL has been optimized.,,,
S0164121215000151," Root cause analysis is a recommended practice in retrospectives and cause effect diagram is a commonly recommended technique for RCA . Our objective is to evaluate whether CED improves the outcome and perceived utility of RCA . We conducted a controlled experiment with 11 student software project teams by using a single factor paired design resulting in a total of 22 experimental units . Two visualization techniques of underlying causes were compared CED and a structural list of causes . We used the output of RCA questionnaires and group interviews to compare the two techniques . In our results CED increased the total number of detected causes . CED also increased the links between causes thus suggesting more structured analysis of problems . Furthermore the participants perceived that CED improved organizing and outlining the detected causes . The implication of our results is that using CED in the RCA of retrospectives is recommended yet not mandatory as the groups also performed well with the structural list . In addition to increased number of detected causes CED is visually more attractive and preferred by retrospective participants even though it is somewhat harder to read and requires specific software tools . 
",Root cause analysis is a recommended practice in retrospectives. We compare the use of cause effect diagram to the use of structural lists in root cause analysis. Cause effect diagram improves the root cause analysis. Cause effect diagram is preferred by participants 75 .,,,
S0140366415002121," Smart grid combines a set of functionalities that can only be achieved through ubiquitous sensing and communication across the electrical grid . The communication infrastructure must be able to cope with an increasing number of traffic types which is as a result of increased control and monitoring penetration of renewable energy sources and adoption of electric vehicles . The communication infrastructure must serve as a substrate that supports different traffic requirements such as QoS across an integrated communication system . This engenders the implementation of middleware systems which considers QoS requirements for different types of traffic in order to allow prompt delivery of these traffic in a smart grid system . A heterogeneous communication applied through the adaptation of the Ubiquitous Sensor Network layered structure to smart grid has been proposed by the International Telecommunication Union . This paper explores the ITU s USN architecture and presents the communication technologies which can be deployed within the USN schematic layers for a secure and resilient communication together with a study of their pro s and con s vulnerabilities and challenges . It also discusses the factors that can affect the selection of communication technologies and suggests possible communications technologies at different USN layers . Furthermore the paper highlights the USN middleware system as an important mechanism to tackle scalability and interoperability problems as well as shield the communication complexities and heterogeneity of smart grid . 
",We explored the adaptation of the USN architecture for SG communication. We reviewed existing communication technologies that can be deployed in SG USN. Vulnerability and challenges of the USN architecture for SG were highlighted. The choice of communication technologies reduces complexity in SG communication. A secure and QoS aware USN middleware can guarantee requirements of SG application.,,,
S0164121213002641," This paper presents an approach for the automated debugging of reactive and concurrent Java programs combining model checking and runtime monitoring . Runtime monitoring is used to transform the Java execution traces into the input for the model checker the purpose of which is twofold . First it checks these execution traces against properties written in linear temporal logic which represent desirable or undesirable behaviors . Second it produces several execution traces for a single Java program by generating test inputs and exploring different schedulings in multithreaded programs . As state explosion is the main drawback to model checking we propose two abstraction approaches to reduce the memory requirements when storing Java states . We also present the formal framework to clarify which kinds of LTL safety and liveness formulas can be correctly analysed with each abstraction for both finite and infinite program executions . A major advantage of our approach comes from the model checker which stores the trace of each failed execution allowing the programmer to replay these executions to locate the bugs . Our current implementation the tool TJT uses Spin as the model checker and the Java Debug Interface for runtime monitoring . TJT is presented as an Eclipse plug in and it has been successfully applied to debug complex public Java programs . 
",We use linear time temporal logic to represent both failures and desirable behaviors regarding a temporally ordered sequence of events and or conditions to be checked along one execution. We use model checking algorithms record the history of the failed execution which we use to implement a controlled replay to locate the bugs. We support the analysis of potentially infinite executions. We use runtime techniques to start the debugging work directly on the programmer s code.,,,
S0141938214000882," This work presents the possibility of the hollow core nanoparticles to improve luminance in an organic light emitting diode device . The finite difference time domain simulation estimates the effect of the hollow core nanoparticles on the external quantum efficiency of the organic light emitting diode device . The efficiency depends on the size and the volume fraction of the hollow core nanoparticles in the polymer layer together with the refractive index and the thickness of the polymer layer . It is shown that the hollow core nanoparticles dispersed in a polymer layer can enhance the external quantum efficiency by a factor of 2.5 . This work also introduces a continuous production method of the hollow core nanoparticles by using the microfluidic self assembly of amphiphilic polymers and the layer formation dispersed with them for the rigorous light scattering . 
",The hollow core nanoparticles can improve the luminance of OLED. The HCNPs can enhance the external quantum efficiency by a factor of 2.5. The HCNPs have been produced continuously by using the microfluidic self assembly. The HCNPs dispersed polymer layer has been also demonstrated successfully.,,,
S0167839614000521," We introduce a novel basis for multivariate hierarchical tensor product spline spaces . Our construction combines the truncation mechanism with the idea of decoupling basis functions . While the first mechanism ensures the partition of unity property which is essential for geometric modeling applications the idea of decoupling allows us to obtain a richer set of basis functions than previous approaches . Consequently we can guarantee the completeness property of the novel basis for large classes of multi level spline spaces . In particular completeness is obtained for the multi level spline spaces defined on T meshes for hierarchical splines of degree p for example with single knots and p adic refinement and with knots of multiplicity and dyadic refinement without any further restriction on the mesh configuration . Both classes include multivariate quadratic hierarchical tensor splines with dyadic refinement . 
",The paper introduces a novel basis for multivariate hierarchical tensor product spline spaces. It uses the truncation mechanism and the idea of decoupling. The truncation mechanism ensures the partition of unity property. The idea of decoupling allows us to obtain a richer set of basis functions than previous approaches. The construction guarantees the completeness property of the novel basis for large classes of multi level spline spaces.,,,
S0141938214000663," Electrowetting display is a reflective display technology in which fluidic pixels can response and switch quickly by electronic control showing the capability for video speed reflective display applications . In this paper a new driver system is proposed and realized for video playing function of active matrix electrowetting display . The hardware system is designed based on Field Programmable Gate Array and the existing electrophoretic display driving integrated chips . A driving logic circuit and FPGA software is introduced for providing the EWD system with driving and timing control . And a set of specific driving waveforms which is loaded to a lookup table of the FPGA in advance is designed to display grayscale on EWDs . 4 level gray scale videos have been successfully performed by applying the driving waveforms . To our knowledge such work has not been reported before . 
",A driver is designed for video playing in active matrix electrowetting displays. The hardware system is designed based on commercial driver integrated chips. A timing controller is designed in a FPGA according to the system characteristics. We design new driving waveforms for playing videos in electrowetting displays. The driving system could work very well for playing 4 gray scale video.,,,
S0167839614000636," We consider isogeometric functions and their derivatives . Given a geometry mapping which is defined by an n dimensional NURBS patch in an isogeometric function is obtained by composing the inverse of the geometry mapping with a NURBS function in the parameter domain . Hence an isogeometric function can be represented by a NURBS parametrization of its graph . We take advantage of the projective representation of the NURBS patch as a B spline patch in homogeneous coordinates . We derive a closed form representation of the graph of a partial derivative of an isogeometric function . The derivative can be interpreted as an isogeometric function of higher degree and lower smoothness on the same piecewise rational geometry mapping hence the space of isogeometric functions is closed under differentiation . We distinguish the two cases and with a focus on in the latter one . As a first application of the presented formula we derive conditions which guarantee and smoothness for isogeometric functions on several singularly parametrized planar and volumetric domains as well as on embedded surfaces . It is interesting to note that the presented conditions depend not only on the general structure of the patch but on the exact representation of the interior of the given geometry mapping . 
",We derive an exact representation of the derivatives of an isogeometric function. The isogeometric function is defined over a domain of arbitrary dimension. We use the formula to derive smoothness results for functions on singular patches. The conditions depend on the exact representation of the interior of the domain. The derivative formula can be used to derive functionals for regularization.,,,
S0141938214000390," This paper presents a new three dimensional display that can display 3 D images at long distances of tens or hundreds of meters in the depth direction and that can control their 3 D positions to meet new requirements for outdoor use . The proposed display uses changing size as a cue to depth perception i.e . the smoothly expanding motion of virtual images formed with optical systems according to the forward movements of the users to display 3 D images at more distant positions in the depth direction than positions where virtual images are formed with optical systems because conventional 3 D displays that use binocular disparity are only able to display 3 D images at short distances in the depth direction . The feasibility of the proposed display was evaluated by subjective tests using a moving minivan in which observers viewed a test pattern that overlapped the real view ahead of the automobile observed through the windshield . The results obtained from the subjective tests revealed that the test pattern was observed at long distances over tens and hundreds of meters in the depth direction and that the position in the depth direction of the test pattern could be controlled by changing the rate at which the motion of the test pattern smoothly expanded . These results demonstrated that the proposed display was feasible . 
",This study presents a new 3 D display that can display 3 D images in wide spaces. The 3 D positions of 3 D images displayed with the proposed display are controlled. The proposed display uses changing size as a cue to depth perception of 3 D images. The feasibility of the proposed display is evaluated with psychophysical experiments. The results of the experiments demonstrate that the proposed display is feasible.,,,
S0140366414000255," A big portion of Internet traffic nowadays is video . A good understanding of user behavior in online Video on Demand systems can help us design configure and manage video content distribution . With the help of a major VoD service provider we conducted a detailed study of user behavior watching streamed videos over the Internet . We engineered the video player at the client side to collect user behavior reports for over 540 million sessions . In order to isolate the possible effect of session quality of experience on user behavior we use only the sessions with perfect QoE and leave out those sessions with QoE impairments . Our main finding is that users spend a lot of time browsing viewing part of one video after another and only occasionally watching a video to its completion . We consider seek as a special form of browsing repeating partial viewing of the same video . Our analysis leads to a user behavior model in which a user transitions through a random number of short views before a longer view and repeats the process a random number of times . This model can be considered an extension and a more detailed alternative to the closed queueing network formulation introduced by Wu et al . . As an application of our user behavior model we use it to measure video popularity . We study the similarity of our approach to subjective evaluation and simple view count based metric and conclude our approach gives results closer to subjective evaluation . 
",We discover a quite consistent user early departure pattern for different sets of view records over different time periods for videos of different popularity. We show the distribution of the number of videos a user transition through. This understanding helps construct a more realistic user behavior model. We show seeks can be considered as part of video browsing and explain what users are doing using seek either emulating fast forwarding or looking for some specific content. We measure a user level video switching and early departure behavior. We explore the online video popularity and users seek behavior.,,,
S0164121215001855," Computing paradigms have shifted towards highly parallel processing and massive replication of data . This entails the efficient distribution of requests and the synchronization of results provided to users . Guaranteeing SLAs requires the ability to evaluate the performance of such systems while taking the effect of non parallel workloads into consideration . This can be achieved with performance models that are able to represent both parallel and sequential workloads . This paper presents a product form stochastic Petri net approximation of fork join queueing networks with interfering requests . We derive the necessary conditions that guarantee the accuracy of the approximations and verify this through examples in comparison to simulation . We apply these approximate models to the performance evaluation of replication in NoSQL cloud datastores and illustrate the composition of large models from smaller models thus facilitating the ability to model a range of deployment scenarios . We show the efficiency of our solution method which finds the product form solution of the models without the representation of the state space of the underlying CTMC . 
",Product form SPN approximation for fork join networks with interfering requests. Conditions for accurate approximation. Validation against simulation. Evaluation of the performance of replication in NoSQL cloud datastores.,,,
S0141938213000267," To obtain a black matrix with high optical density and low dielectric constant graphene oxide was prepared from synthetic graphite oxide and then incorporated into the conventional carbon black pigment to fabricate black matrix films . The introduction of insulating GO effectively lowered the dielectric constant of carbon black based BM but maintained the high optical density . The dielectric constant of the BM film significantly decreased from 26 to 4.5 . This work demonstrates the successful chemical modification and good dispersion of carbon based materials and their physical effects on the BM films . 
",The introduction of insulating graphene oxide in carbon black containing black matrix. Effect of BYK2150 dispersant on dispersion of carbon based materials. Targeting low dielectric constant of the black matrix film while maintaining high optical density.,,,
S0165168414004368," In this paper we propose a novel method for the blind compensation of drift for the asynchronous recording of an ad hoc microphone array . Digital signals simultaneously observed by different recording devices have drift of the time differences between the observation channels because of the sampling frequency mismatch among the devices . On the basis of a model in which the time difference is constant within each short time frame but varies in proportion to the central time of the frame the effect of the sampling frequency mismatch can be compensated in the short time Fourier transform domain by a linear phase shift . By assuming that the sources are motionless and have stationary amplitudes the observation is regarded as being stationary when drift does not occur . Thus we formulate a likelihood to evaluate the stationarity in the STFT domain to evaluate the compensation of drift . The maximum likelihood estimation is obtained effectively by a golden section search . Using the estimated parameters we compensate the drift by STFT analysis with a noninteger frame shift . The effectiveness of the proposed blind drift compensation method is evaluated in an experiment in which artificial drift is generated . 
",Modeling of drift as frame shift. STFT domain compensation as linear phase shift. Probabilistic model of drift. Maximum likelihood estimation of sampling frequency mismatch. Efficient resampling with modified STFT analysis with noninteger frame shift.,,,
S0045790616300714,"This work develops LED Light Emitting Diode image display system that can display patterns on the spokes of a bicycle. The proposed system of LED lights that are mounted on the wheel can provide safety by producing a beautiful pattern. The main control board and LED lighting strips that determine the displayed pattern are developed using embedded system and electrical circuit designs. A mobile application program is developed to control the lighting hardware remotely the system communicates with the main control board via a Wi Fi wireless network interface. The cyclist can change the patterns using the mobile application program or by pushing buttons on the main control board before riding. Six patterns are designed for display using this system and the pattern can be made to change repeatedly at present intervals. Experimental results reveal that the proposed system performs effectively on the wheel up to a maximum speed of 40 km hr. 
",Three LED lighting strips perform the LED imaging function on the wheel. The LED lighting strip is controlled by the mobile APP via a Wi Fi wireless network. It can work normally with wheel speed up to 35km hr.,,,
S0141938214000377," We demonstrated an optimized 8 domain vertical aligned liquid crystal display by minimizing its color washout . The index G is adopted to analyze the degree of color washout through simulation . By using linearly and circularly polarized incident light the optimized regime of the area and applied voltage ratios of sub pixels is obtained . In the experiments with the circularly polarized light the images are sensuously improved in a coupled capacity type LCD by applying the simulated applied voltage ratio of two sub pixels . 
",We optimized an 8 domain VA LCD by minimizing its color washout. We adopted the index G value to analyze the degree of color washout. The color washout is improved in all the oblique viewing direction. In general the quality of LCD is raised.,,,
S0140366414000449," Software Defined Networking has been widely recognized as a promising way to deploy new services and protocols in future networks . The ability to program the network enables applications to create innovative new services inside the network itself . However current SDN programmability comes with downsides that could hinder its adoption and deployment . First in order to offer complete control today s SDN networks provide low level API s on which almost any type of service can be written . Because the starting point is a set of low level API calls implementing high level complex services needed by future network applications becomes a challenging task . Second the set of emerging SDN technologies that are beginning to appear have little in common with one another making it difficult to set up a flow that traverses multiple SDN technologies providers . In this paper we propose a new way to set up SDN networks spanning multiple SDN providers . The key to our approach is a Network Hypervisor service . The Network Hypervisor offers high level abstractions and APIs that greatly simplify the task of creating complex SDN network services . Moreover the Network Hypervisor is capable of internetworking various SDN providers together under a single interface abstraction so that applications can establish end to end flows without the need to see or deal with the differences between SDN providers . 
",A new HyperNet abstraction to help an average user create special purpose SDNs. A HyperNet architecture is developed to support a HyperNet. A Network Hypervisor service that provides high level HyperNet APIs. A MobileNet HyperNet is implemented to demonstrate our design. MobileNet greatly enhances the communication performance for mobile devices.,,,
S0140366413001229," Cognitive radio refers to an intelligent radio with the capability of sensing the radio environment and dynamically reconfiguring the operating parameters . Recent research has focused on using cognitive radios in ad hoc environments . Spectrum sensing is the most important aspect of successful cognitive radio ad hoc network deployment to overcome spectrum scarcity . Multiple cognitive radio users can cooperate to sense the primary user and improve sensing performance . Cognitive radio ad hoc networks are dynamic in nature and have no central point for data fusion . In this paper gradient based fully distributed cooperative spectrum sensing in cognitive radio is proposed for ad hoc networks . The licensed band used for TV transmission is considered the primary user . The gradient field changes with the energy sensed by cognitive radios and the gradient is calculated based on the components which include energy sensed by secondary users and received from neighbors . The proposed scheme was evaluated from the perspective of reliable sensing convergence time and energy consumption . Simulation results demonstrated the effectiveness of the proposed scheme . 
",A novel gradient based fully distributed cooperative sensing scheme is proposed. A consensus based algorithm is used to handle the network model with a fixed graph. The proposed scheme does not require any prior knowledge of network topology. Energy consumption and the number of exchanged messages are reduced significantly. The proposed and consensus based schemes are compared through simulations.,,,
S0141938214000821," Modern liquid crystal displays require novel technologies such as new alignment methods to eliminate alignment layers fast response and long operation time . To this end we report an overview of recent efforts in LCD technologies devoted to realize more display modes having no alignment layer faster switching time and low battery consumption . In particular we overview recent advances on the liquid crystals alignment for display applications which includes superfine nanostructures polymeric microchannels and polymer stabilized LCs . Furthermore we analyze the main optical and electro optical properties of new generation LCDs displays addressing a particular attention to LCs blue phase hosting gold nanoparticles . Moreover we focus on the progress of electrofluidic displays which demonstrates characteristics that are similar to LCDs with attention on various pixel designs operation principles and possible future trends of the technology . 
",Surfactant free LCD based on periodic structures realized though nanoimprint lithography and interference holography. LCD based on LCs blue phase and gold nanoparticles. Electrowetting technology for displays applications.,,,
S0140366415002273," Location aware applications are one of the biggest innovations brought by the smartphone era and are effectively changing our everyday lives . But we are only starting to grasp the privacy risks associated with constant tracking of our whereabouts . In order to continue using location based services in the future without compromising our privacy and security we need new privacy friendly applications and protocols . In this paper we propose a new compact data structure based on Bloom filters designed to store location information . The spatial Bloom filter as we call it is designed with privacy in mind and we prove it by presenting two private positioning protocols based on the new primitive . The protocols keep the user s exact position private but allow the provider of the service to learn when the user is close to specific points of interest or inside predefined areas . At the same time the points and areas of interest remain oblivious to the user . The two proposed protocols are aimed at different scenarios a two party setting in which communication happens directly between the user and the service provider and a three party setting in which the service provider outsources to a third party the communication with the user . A detailed evaluation of the efficiency and security of our solution shows that privacy can be achieved with minimal computational and communication overhead . The potential of spatial Bloom filters in terms of generality security and compactness makes them ready for deployment and may open the way for privacy preserving location aware applications . 
",We define a new data structure for location privacy based on bloom filters. We provide two privacy preserving protocols for location aware services. We prove the security of the protocols and we analyze their efficiency. We provide a thorough evaluation of the data structure and we simulate the protocols.,,,
S0141938215000591," After the appearance of digital broadcasting realistic images such as 3D and high resolution broadcasting are rapidly developing and evolving . And recent trend emphasizes not only viewing the information but also using it through the display media . To keep up with this trend large size display manufacturers are trying to support resolution beyond the full HD in order to display additional information and high definition broadcasting at the same time . In this paper we present a 21 9 cinema mode resolution function using two ICs that support full HD for Plasma Display Panels which is one of the largest flat display panels . In order to achieve this we first set the master and the slave between the two ICs and propose an efficient information exchange and computation scheme using serial communication that links the master and the slave . Furthermore we realize the cinema function with 21 9 resolution through image quality processing and operational parameter exchange and computation in the boundary that is set between the master and the slave . Finally the cinema mode image realized using the hardware circuit board is demonstrated . 
",A 21 9 cinema mode function is implemented by using two ICs that support full HD. We propose efficient information change and computation scheme between two ICs. This scheme has an advantage of reducing the SOC development cost. Not only the 2560 1080 but also can be applied to the 4K 2K resolution.,,,
S0167839613000794," A method to construct arbitrary order continuous curves which pass through a given set of data points is introduced . The method can derive a new family of symmetric interpolating splines with various nice properties such as partition of unity interpolation property local support and second order precision etc . Applying these new splines to construct curves and surfaces one can adjust the shape of the constructed curve and surface locally by moving some interpolating points or by inserting new interpolating points . Constructing closed smooth curves and surfaces and smooth joining curves and surfaces also become very simple in particular for constructing continuous closed surfaces by using the repeating technique . These operations mentioned do not require one to solve a system of equations . The resulting curves or surfaces are directly expressed by the basis spline functions . Furthermore the method can also directly produce control points of the interpolating piecewise B zier curves or tensor product B zier surfaces by using matrix formulas . Some examples are given to support the conclusions . 
",A method to construct interpolating curves and surfaces is introduced. A new family of interpolating splines is obtained. Formulas to obtain B zier control points are given in matrix form. Avoid to solve a system of equations for obtaining closed surfaces.,,,
S0140366415002649," Content Distribution Networks are networks that make intense use of caching and multicast overlay techniques for transmitting video and other stream media a type of traffic that has been growing tremendously in the last years . To cope with this increasing demand several telecommunications companies have been associated to create federated CDNs that involves many different providers and hence distinct domains . Although beneficial in terms of increased capillarity and scalability of service delivery the interaction between FCDN elements from different providers brings many new challenges . Among them one that has received little attention so far refers to security an essential service for preventing the misuse of the FCDN resources . Aiming to tackle this issue this paper presents the Overlay Communication Protocol a security mechanism that allows the secure signaling communication in FCDNs . OCP takes into account all elements involved in the content delivery process addressing Route Forgery attacks and concealing the network structure from potential attackers . Together with the protocol description and security analysis we also present experimental results on its implementation showing that it introduces little impact on the overall network performance . 
",Discussion of security issues in Federated Content Distribution Networks FCDNs . Proposal of security protocol for signaling communications in FCDNs. Security and performance analyses of the proposed protocol.,,,
S0141938214000924," This paper reviews several optical connecting devices that are based on microelectromechanical systems components . In this paper we divide optical connecting devices into two categories . The first category includes MEMS based optical switches developed for optical fiber communication which perform optical switching wavelength division multiplexing routing and or optical cross connection . The other category consists of MEMS based optical interconnects that have been constructed primarily for use in rack to rack board to board chip to chip card to card and or intra chip interface connections . Working principles of these MEMS optical connecting devices will also be discussed in this paper . 
",This paper reviews several optical connecting devices that are based on microelectromechanical systems MEMS components. The reviewed MEMS based optical connecting devices are divided into two categories optical switches and optical interconnects. Working principles of these optical connecting devices are discussed.,,,
S0165168414005520,"Human motion retrieval plays an important role in many motion data based applications. In the past many researchers tended to use a single type of visual feature as data representation. Because different visual feature describes different aspects about motion data and they have dissimilar discriminative power with respect to one particular class of human motion it led to poor retrieval performance. Thus it would be beneficial to combine multiple visual features together for motion data representation. In this article we present an Adaptive Multi view Feature Selection AMFS method for human motion retrieval. Specifically we first use a local linear regression model to automatically learn multiple view based Laplacian graphs for preserving the local geometric structure of motion data. Then these graphs are combined together with a non negative view weight vector to exploit the complementary information between different features. Finally in order to discard the redundant and irrelevant feature components from the original high dimensional feature representation we formulate the objective function of AMFS as a general trace ratio optimization problem and design an effective algorithm to solve the corresponding optimization problem. Extensive experiments on two public human motion database i.e. HDM05 and MSR Action3D demonstrate the effectiveness of the proposed AMFS over the state of art methods for motion data retrieval. The scalability with large motion dataset and insensitivity with the algorithm parameters make our method can be widely used in real world applications. 
",An Adaptive Multi view Feature Selection AMFS algorithm is proposed to fuse multiple features formotion data retrieval. The local regression model isused to learn a datum adaptive graph for each feature to preserve local structure information. The selection matrix is learnt from all local graphs by exploiting the complementary information between different features. An efficient iterative optimization approach is designed to solve objective function represented in trace ratio form.,,,
S0142061514003743," This survey paper is an excerpt of a more comprehensive study on Smart Grid and the role of Advanced Metering Infrastructure in SG . The survey was carried out as part of a feasibility study for creation of a Net Zero community in a city in Ontario Canada . SG is not a single technology rather it is a combination of different areas of engineering communication and management . This paper introduces AMI technology and its current status as the foundation of SG which is responsible for collecting all the data and information from loads and consumers . AMI is also responsible for implementing control signals and commands to perform necessary control actions as well as Demand Side Management . In this paper we introduce SG and its features establish the relation between SG and AMI explain the three main subsystems of AMI and discuss related security issues . 
",Advances in Metering Infrastructure in Smart Grid. Comprehensive survey on control actions for load demand. Distributed Energy Resources and improvement of delivered power quality.,,,
S0140366414003326," Barrier coverage is one of the most important applications of wireless sensor networks . It is used to detect mobile objects are entering into the boundary of a sensor network field . Energy efficiency is one of the main concerns in barrier coverage for wireless sensor networks and its solution can be widely used in sensor barrier applications such as intrusion detectors and border security . In this work we take the energy efficiency as objectives of the study on barrier coverage . The cost in the present paper can be any performance measurement and normally is defined as any resource which is consumed by sensor barrier . In this paper barrier coverage problem is modeled based on stochastic coverage graph first . Then a distributed learning automata based method is proposed to find a near optimal solution to the stochastic barrier coverage problem . The stochastic barrier coverage problem seeks to find minimum required number of sensor nodes to construct sensor barrier path . To study the performance of the proposed method computer simulations are conducted . The simulation results show that the proposed algorithm significantly outperforms the greedy based algorithm and optimal method in terms of number of network barrier paths . 
",Proposing a new approach to barrier coverage in wireless sensor network. Modeling barrier coverage with stochastic edge weighted graph. Finding an optimal solution for the network stochastic edge weighted coverage graph. Comparing the performance of the proposed method with the greedy and optimal methods.,,,
S0167839614001125," The invariant of a rational space curve gives important information about the curve . In this paper we describe the structure of all parameterizations that have the same type what we call a stratum and as well the closure of strata . Many of our results are based on papers by the second author that appeared in the commutative algebra literature . We also present new results including an explicit formula for the codimension of the locus of non proper parametrizations within each stratum and a decomposition of the smallest stratum based on which two dimensional rational normal scroll the curve lies on . 
",We compute dimension of each stratum. The closure of each stratum is explicitly given as a union of smaller strata. Non proper parametrizations are of high codimension in each stratum. The smallest stratum is further stratified by rational normal scrolls.,,,
S0098300416300206," In this study enhancements to the numerical representation of sluice gates and turbines were made to the hydro environmental model Environmental Fluid Dynamics Code and applied to the Severn Tidal Power Group Cardiff Weston Barrage . The extended domain of the EFDC Continental Shelf Model allows far field hydrodynamic impact assessment of the Severn Barrage pre and post enhancement to demonstrate the importance of accurate hydraulic structure representation . The enhancements were found to significantly affect peak water levels in the Bristol Channel reducing levels by nearly 1m in some areas and even affect predictions as far field as the West Coast of Scotland albeit to a far lesser extent . The model was tested for sensitivity to changes in the discharge coefficient C used in calculating discharge through sluice gates and turbines . It was found that the performance of the Severn Barrage is not sensitive to changes to the C value and is mitigated through the continual rather than instantaneous discharge across the structure . The EFDC CSM can now be said to be more accurately predicting the impacts of tidal range proposals and the investigation of sensitivity to C improves the confidence in the modelling results despite the uncertainty in this coefficient . 
",Enhancements made to numerical representation of hydraulic structures in EFDC. New representation has greatly affected the predicted impact of the Severn Barrage. Enhancements allow 1st accurate assessment of far field impacts of Severn Barrage. Sensitivity testing demonstrated that uncertainty in C value is not significant. Uncertainty in the C value can be mitigated through increased sluice capacity.,,,
S0142061514005869," This paper proposes a three phase four leg voltage sourced inverter based load unbalance compensator including its control algorithm which is a component of a microgrid . The purpose of proposed three phase four leg VSI based LUC is to improve power quality of the standalone microgrid . Power quality of the microgrid which was installed in Mara island Korea is analyzed using a real operational data . In this work the microgrid in Mara island which includes a photovoltaic power generation system a diesel generator a battery energy storage system and a power management system is modeled in PSCAD EMTDC and proposed three phase four leg VSI based LUC is also modeled and applies to the modeled microgrid . Power flow and stability of the modeled microgrid with the LUC is analyzed under variable irradiance and unbalance loads . The results show that the proposed LUC helps to improve stability of the stand alone microgrid . The proposed three phase four leg VSI based LUC and its control algorithm can be effectively utilized to the stand alone microgrid which has large unbalance loads . 
",This paper proposes a novel load unbalance compensator LUC for the stand alone microgrid using three phase four leg VSI. The model of a three phase four leg VSI for the LUC and the microgrid are simulated using PSCAD EMTDC. This demonstrates that the proposed LUC increases the stability of stand alone microgrid under unbalance load conditions.,,,
S0167839613000782," T splines are a generalization of NURBS surfaces the control meshes of which allow T junctions . T splines can significantly reduce the number of superfluous control points in NURBS surfaces and provide valuable operations such as local refinement and merging of several B splines surfaces in a consistent framework . In this paper we propose a variant of T splines called Modified T splines . The basic idea is to construct a set of basis functions for a given T mesh that have the following nice properties non negativity linear independence partition of unity and compact support . Due to the good properties of the basis functions the Modified T splines are favorable both in adaptive geometric modeling and isogeometric analysis . 
",We propose a variant of T splines called Modified T splines. A set of basis functions are constructed for a given T mesh. These bases have nice properties non negativity linear independence partition of unity and compact support. Modified T splines are favorable both in adaptive geometric modeling and isogeometric analysis.,,,
S0167839614000491," Salient features in 3D meshes such as small high curvature details in the middle of largely flat regions are easily ignored by most mesh simplification methods . Nevertheless these features can be perceived by human observers as perceptually important in CAD models . Recently mesh saliency has been introduced to identify those visually interesting regions . In this paper we apply view based mesh saliency to a purely visual method for surface simplification from two approaches . In the first one we propose a new simplification error metric that considers polygonal saliency . In the second approach we use viewpoint saliency as a weighting factor of the quality of a viewpoint in the simplification algorithm . Our results show that saliency can improve the preservation of small but visually significant surfaces even in visual algorithms for surface simplification . However this comes at a price because logically some other low saliency regions in the mesh are simplified further . 
",Reduce complexity in a polygonal mesh according to perception. Calculate a view based saliency map from a viewpoint information channel. Simplify the mesh with either polygonal or viewpoint saliency using a visual algorithm for surface simplification. Improvement in the visual appearance of the simplified mesh.,,,
S0140366415002431," Unified communications has enabled seamless data sharing between multiple devices running on various platforms . Traditionally organizations use local servers to store data and employees access the data using desktops with predefined security policies . In the era of unified communications employees exploit the advantages of smart devices and 4G wireless technology to access the data from anywhere and anytime . Security protocols such as access control designed for traditional setup are not sufficient when integrating mobile devices with organization s internal network . Within this context we exploit the features of smart devices to enhance the security of the traditional access control technique . Dynamic attributes in smart devices such as unlock failures application usage location and proximity of devices can be used to determine the risk level of an end user . In this paper we seamlessly incorporate the dynamic attributes to the conventional access control scheme . Inclusion of dynamic attributes provides an additional layer of security to the conventional access control . We demonstrate that the efficiency of the proposed algorithm is comparable to the efficiency of the conventional schemes . 
",We propose robust access control framework for a network which has allowed smart devices to be connected to the internal network in order to enable seamless data sharing. Smart device s sensor data such as location app usage pattern unlock failures are being considered for access control and data confidentiality These sensor data and conventional static credentials are combined to develop a secure framework which verifies the users access privileges in run time The algorithm supports both the access control and data confidentiality simultaneously. Algorithms are validated via simulation and the results show that the performance is comparable with existing schemes. Existing schemes are thoroughly surveyed and different between proposed and existing schemes are clearly highlighted.,,,
S0141938213000760," This study examined the effects of display method text display rate and observation angle on comprehension performance and subjective preferences for Chinese characters presented on an LED display . The factors and levels studied were as follows four text display methods three text display rates and seven observation angles . The results indicated that a display rate of 160cpm was in general superior to 240 and 320cpm for comprehension scores and subjective evaluations . The effects of display method and observation angle were found to be non significant . However there was a significant interaction between display method and display rate . The results of this study were used to make ergonomics recommendations applicable to LED displays for determining optimum dynamic text display methods for Chinese characters . 
",We investigated the optimum dynamic text display methods for delivering Chinese messages on an LED display. The optimal display rate is 160cpm whereas speeds at 320cpm or above are not recommended. Leading display method should be used when the display rate at 320cpm or above is necessary.,,,
S0141938213000693," We have grown crystals in which two kinds of thiophene phenylene co oligomers are hybridized . Biphenyl capped thiophene and biphenyl capped terthiophene were chosen from among the TPCOs . The hybrid crystals were grown in both the vapor phase and the liquid phase . These hybrid crystals showed the emission colors intermediate between the two components . Correspondingly maximum peak positions of the emissions from the hybrid crystals were located halfway between those from single component crystals of BP1T and BP3T . We made hybrid thin films by co deposition of the two TPCOs in vacuum . The thin films exhibited both emission colors and emission peak positions similar to those of the hybrid crystals . The X ray diffraction measurements indicate that the crystallographic structure of the hybrid crystals resembles that of the BP1T crystal . Also we made field effect transistors using the hybrid crystals and measured their hole mobilities . We briefly discuss the implications of the X ray diffraction and electrical data . 
",We have grown the hybrid crystals of two thiophene phenylene co oligomers TPCOs . They were biphenyl capped thiophene BP1T and biphenyl capped terthiophene BP3T . The hybrid crystals showed the emission colors intermediate between the two TPCOs. The crystallographic structure of the hybrid crystals was similar to that of BP1T.,,,
S0165168415000067," Sketch based human motion retrieval is a hot topic in computer animation in recent years . In this paper we present a novel sketch based human motion retrieval method via selected 2 dimensional Geometric Posture Descriptor . Specially we firstly propose a rich 2D pose feature call 2D Geometric Posture Descriptor which is effective in encoding the 2D posture similarity by exploiting the geometric relationships among different human body parts . Since the original 2GPD is of high dimension and redundant a semi supervised feature selection algorithm derived from Laplacian Score is then adopted to select the most discriminative feature component of 2GPD as feature representation and we call it as selected 2GPD . Finally a posture by posture motion retrieval algorithm is used to retrieve a motion sequence by sketching several key postures . Experimental results on CMU human motion database demonstrate the effectiveness of our proposed approach . 
",We proposed a rich 2D pose feature called 2D Geometric Posture Descriptor 2GPD . The 2GPD is effective to the changeable hand drawn sketch. A semi supervised feature selection algorithm based on Laplacian Score is used to simplify our features. We proposed an efficient sketched based human motion retrieval framework.,,,
S0141938215000554," Blue organic light emitting devices combing a composite hole transporting layer and novel homogeneous double emitting layers have been fabricated . The c HTL plays a significant role of rectification in balancing the carriers injection concentration which matches well with the DELs structure . The DELs is consisted of two homogeneous hosts such as 2 methyl 9 10 di anthracene and 9 10 di anthracene . The optimal device presents the maximal current efficiency of 15.9cd A at 4.9mA cm2 and the minor efficiency roll off of 13.4 under the driving voltage varying from 5V to 10V respectively . Meanwhile the device s maximal current efficiency and the corresponding efficiency roll off have been obviously improved by 55.9 and 63.9 compared with those of the conventional device . These results indicate that the homogeneous DELs not only greatly facilitate carriers injection into the emitting layer but also evenly modulate carriers distribution due to natural energy barrier of the interface . The transient photoluminescence decay of double hosts further illustrates that the DELs structure can increase the recombination ratio of electron hole pairs and improve the exciton s utilization . Additionally the optimal device current density is reduced by 44.1 under the same luminance of 25 780cd m2 compared with that of the conventional device . 
",Composite hole transporting layer with novel homogenous double emitting layers. Balance of carrier distribution as well as the expansion of exciton formation zone. Blue OLEDs based on the structure of c HTL and DELs exhibit improved performances.,,,
S0141938214000183," Common projection optics use K hler illumination to achieve a required lighting . These systems always prevent the realization of a compact optical configuration along with a high lumen output . Based on conventional K hler illumination a modified K hler illumination system for LED based projection display is presented in this paper which can significantly reduce the system volume while allowing for adequate and homogeneous illumination . Equipped with the proposed system a pocket sized CF LCoS projector with a physical dimension of 27.4mm 19.4mm 9.6mm is designed simulated and analyzed . Compared to conventional approaches this design could offer an average 43 volume reduction with acceptable tolerance . To the best of our knowledge the screen uniformity of 90.2 and the light efficiency of 56.5 are competitive as compared with those of the currently commercialized pocket sized CF LCoS projectors . 
",An improved design of conventional K hler illumination is proposed for projection display. The modified K hler illumination can significantly reduce the system volume. An actual pocket sized CF LCoS projector is designed simulated and analyzed. High light efficiency and illumination uniformity are both available. Tolerance analysis shows that the system is well acceptable for current machining.,,,
S0141938215000529," Objective It is generally assumed that motion in motion images is responsible for increased postural sway as well as for visually induced motion sickness . However this has not yet been tested . To that end we studied postural sway and VIMS induced by motion and still images . Method 15 Participants were exposed to motion and still images in separate sessions . Motion images consisted of video clips taken from a first person shooter game . Still images consisted of stills taken every 10s from these same clips . Before during and after exposure VIMS was rated and postural sway was measured . Sway path length standard deviation and short and long term scaling components of the centre of pressure were calculated as measures of postural sway . Results VIMS scores obtained during and after exposure to motion images were significantly higher compared to scores obtained before and directly after exposure to still images . The sway path length standard deviation in anteroposterior direction and short term scaling components in mediolateral and anteroposterior direction increased significantly during exposure to motion and still images . Conclusion In this experiment motion and still images caused different levels of VIMS but comparable increases in postural sway . We assume VIMS was caused by a mismatch between visual and vestibular motion cues . The increase in sway during exposure to still images can be explained by visual effects present in still images . The lack of vection in the motion images may explain why sway was not larger when viewing these motion images as compared to viewing the still images . 
",The effect of visual motion on visually induced motion sickness VIMS and postural control was studied. Postural sway is not necessarily increased by visual motion. Visually induced motion sickness VIMS seems to be caused by visual motion. Different from sickness motion in images does not seem to be the necessary condition to increase postural sway.,,,
S0167639314000740," With teleconferencing becoming more accessible as a communication platform researchers are working to understand the consequences of the interaction between human perception and this unfamiliar environment . Given the enclosed space of a teleconference room along with the physical separation between the user microphone and speakers the transmitted audio often becomes mixed with the reverberating auditory components from the room . As a result the audio can be perceived as smeared in time and this can affect the user experience and perceived quality . Moreover other challenges remain to be solved . For instance during encoding compression and transmission the audio and video streams are typically treated separately . Consequently the signals are rarely perfectly aligned and synchronous . In effect timing affects both reverberation and audiovisual synchrony and the two challenges may well be inter dependent . This study explores the temporal integration of audiovisual continuous speech and speech syllables along with a non speech event across a range of asynchrony levels for different reverberation conditions . Non reverberant stimuli are compared to stimuli with added reverberation recordings . Findings reveal that reverberation does not affect the temporal integration of continuous speech . However reverberation influences the temporal integration of the isolated speech syllables and the action oriented event with perceived subjective synchrony skewed towards audio lead asynchrony and away from the more common audio lag direction . Furthermore less time is spent on simultaneity judgements for the longer sequences when the temporal offsets get longer and when reverberation is introduced suggesting that both asynchrony and reverberation add to the demands of the task . 
",Explores perceived audiovisual synchrony in reverberant environments. The temporal smear caused by reverberation can alter a signal s acoustic signature. Reverberating acoustics is a common problem in teleconferencing systems. Reverberation may not have adverse effects on the perceived synchrony for continuous audiovisual speech. The temporal integration of speech syllables and isolated events is affected by the acoustic phenomenon.,,,
S0141938214000419," Previous studies on stereoscopic acuity have shown that the percentage of stereo blind subjects is relevant . Moreover stereoscopic visualization is becoming widely diffused in different fields like e.g . entertainment surgery or VR training where it is necessary an accurate assessment of stereoscopic abilities of the involved subjects . Therefore there might be the need of performing a stereo blindness and stereo acuity test before each visualization session involving stereoscopic images . In this paper we propose a method to assess stereo acuity and stereo blindness directly on the chosen device under the same visualization condition and setup adopted for the tasks to perform in order to have the same perceptual response . We present software based tests suitable for a generic stereoscopic displays and we compare their effectiveness performing a comparison with a standard physical card based test commonly used in assessment of stereo acuity and stereo blindness . We provide to the reader all the details to perform autonomously the tests of which images will be downloadable from web . 
",Stereo acuity and blindness assessment directly on the used display. We present test assessment with users and comparison with a physical test. We provide all the details and the images to perform autonomously the tests.,,,
S0167839615000023," In this study we propose a robust algorithm for reconstructing free form space curves in space using a Non Uniform Rational B Spline snake model . Two perspective images of the required free form curve are used as the input and a nonlinear optimization process is used to fit a NURBS snake on the projected data in these images . Control points and weights are treated as decision variables in the optimization process . The Levenberg Marquardt optimization algorithm is used to optimize the parameters of the NURBS snake where the initial solution is obtained using a two step procedure . This makes the convergence faster and it stabilizes the optimization procedure . The curve reconstruction problem is reduced to a problem that comprises stereo reconstruction of the control points and computation of the corresponding weights . Several experiments were conducted to evaluate the performance of the proposed algorithm and comparisons were made with other existing approaches . 
",A novel approach for the reconstruction of free form space curves is presented. The proposed approach is based on NURBS snake model and quadratic programming. An energy minimization method which is analogous to snake model NURBS snake model is used. Levenberg Marquardt algorithm is used for optimizing the parameters of the energy function. The initial parameters of the NURBS snake are obtained using a two step procedure in which firstly weights are obtained by solving a quadratic programming problem and then control points are evolved by solving a system of linear equation. The proposed method has been tested using various synthetic and real data. The proposed scheme is also applicable when the samplings of image curves vary. The method is easy to implement and can be used to reconstruct complex free form shape curves.,,,
S0141938214000389," The article investigates how the various colors and color pairs used as grouping factors affect the visual search process and direct manipulation activities in the context of toolbar like graphical panels . Red green and blue colors having the same perceptual distance in the CIELab space are used . The results demonstrate significant influence of the examined color related factors on the speed and accuracy . The color preattentive property depends strongly on the grouping pattern layouts with smaller colored areas were operated worse than panels divided into larger parts . Meaningful differences were also observed between panels with single and two colored backgrounds . Preferences were examined by pairwise comparisons before and after performing the search and select tasks . Subjective judgments were significantly differentiated by the toolbar background color pattern both prior to the performance tasks and after them . The initial relative weights structure changed decidedly after the performance experience being more consistent with the search and select results . The location factor was irrelevant for the speed and accuracy as well as for preferences . Objective and subjective findings are compared and discussed . Linear regression models showing the preference structure change and the relationship between mean acquisition times and mean preference weights are developed and discussed . 
",Toolbars with background colors having the same perceptual distance are examined. Smaller uniformly colored areas are operated worse in terms of speed and accuracy than those divided into larger parts. Preferences strongly depend on the toolbar color pattern and decidedly change after the performance tasks. Regression models of preference change and relationship between speed and preferences are developed and discussed.,,,
S0167839613001027," In this paper the dual representation of spatial parametric curves and its properties are studied . In particular rational curves have a polynomial dual representation which turns out to be both theoretically and computationally appropriate to tackle the main goal of the paper spatial rational Pythagorean hodograph curves . The dual representation of a rational PH curve is generated here by a quaternion polynomial which defines the Euler Rodrigues frame of a curve . Conditions which imply low degree dual form representation are considered in detail . In particular a linear quaternion polynomial leads to cubic or reparameterized cubic polynomial PH curves . A quadratic quaternion polynomial generates a wider class of rational PH curves and perhaps the most useful is the ten parameter family of cubic rational PH curves determined here in the closed form . 
",Dual representation of spatial rational PH curves is presented. Connection between the degrees of a dual and a point representation of rational curves is revealed. It is proven that linear quaternion polynomials lead to reparameterized cubic PH curves. Spatial rational PH curves of a class are derived in a closed form having degrees of freedom.,,,
S0140366415002479," Online Social Networks constitute vital communication and information sharing channels . Unfortunately existing coarse grained privacy preferences insufficiently protect the shared information . Although cryptographic techniques provide interesting mechanisms to protect privacy several issues remain problematic such as OSN provider acceptance user adoption key management and usability . To mitigate these problems we propose a practical solution that uses Identity Based Encryption to simplify key management and enforce data confidentiality . Moreover we devise an Identity Based outsider anonymous private sharing scheme to disseminate information among multiple users . Furthermore we demonstrate the viability and tolerable overhead of our solution via an open source prototype . 
",Novel practical solution using Identity based Encryption with multiple untrusted Key Servers PKGs to be used on top of current Online Social Networks OSNs to protect privacy as con dentiality of the content. An outsider anonymous Identity Based broadcast encryption protocol with a multi PKG model to support multiple recipients. A prototype implementation as Firefox extension requiring a relatively small overhead.,,,
S0097849313000472,"There are hundreds of distinct 3D CAD and engineering file formats. As engineering design and analysis has become increasingly digital the proliferation of file formats has created many problems for data preservation data exchange and interoperability. In some situations physical file objects exist on legacy media and must be identified and interpreted for reuse. In other cases file objects may have varying representational expressiveness. We introduce the problem of automated file recognition and classification in emerging digital engineering environments where all design manufacturing and production activities are born digital. The result is that massive quantities and varieties of data objects are created during the product lifecycle. This paper presents an approach to automated identification of engineering file formats. This work operates independent of any modeling tools and can identify families of related file objects as well as variations in versions. This problem is challenging as it cannot assume any a priori knowledge about the nature of the physical file object. Applications for these methods include support for a number of emerging applications in areas such as forensic analysis data translation as well as digital curation and long term data management. 
",Provides support for emerging applications in long term data management. Compression based classification enables specification free format identification. Classification accuracy best when NCD distance and the first 16KB of files used. Classifier is highly effective at distinguishing among very similar formats. Computational time is comparable or better than approaches based on known signatures.,,,
S0140366415002467," A promising development in the design of datacenters is the hybrid network architecture consisting of both optical and electrical elements in which end to end traffic can be routed through either an electrical path or an optical path . The core optical switch is used to dynamically create optical paths between pairs of electrical edge switches in such a datacenter network . In this context the joint problem of bandwidth allocation and VM placement poses new and different challenges not addressed yet in hybrid datacenter . In particular we foresee two issues the number of edge switches that can be simultaneously reached using optical paths from an edge switch is limited by the size of the optical switch the dynamic creation of optical paths can potentially establish a constrained optical network topology leading to poor performance . In this work we abstract the requests of tenants as virtual networks and study the problem of embedding virtual networks on a hybrid datacenter . We formulate the problem as a non linear optimization problem and analyze its complexity . We develop and analyse three algorithms for embedding dynamically arriving virtual network demands on a hybrid optical electrical datacenter . Through simulations we demonstrate the effectiveness of not only exploiting the already established optical paths but also of using electrical network in embedding requests of virtual networks . 
",Joint problem of bandwidth allocation and VM placement in optical electrical DC. Challenges i limited reachability due to the size of the optical switch in the datacenter netework ii dynamic creation of optical paths lead to the creation of constrained optical network topology. The problem is formulated as nonlinear optimization problem that is NP hard in nature. Input requests from users are abstracted as virtual networks where a single node in a virtual network is a cluster of VMs and an edge connecting two nodes is the bandwidth demand between the two clusters of VMs. Three algorithms are developed for embedding input virtual networks on hybrid datacenter network and their performances are evaluated using simulations.,,,
S0140366415002534," The lack of abstraction in a growing semantic virtual and abstract world poses new challenges for assessing security and QoS tradeoffs . For example in Future Internet scenarios where Unified Communications will take place being able to predict the final devices that will form the network is not always possible . Without this information the analysis of the security and QoS tradeoff can only be based on partial information to be completed when more information about the environment is available . In this paper we extend the description of context based parametric relationship model providing a tool for assessing the security and QoS tradeoff based on interchangeable contexts . Our approach is able to use the heterogeneous information produced by scenarios where UC is present . 
",We explain the motivation for using CPRM in UC. We extend the definition of CPRM. We define the requirements for a CPRM based compliant tool. We implement the Security and QoS Tradeoff SQT tool. We test our approach with a use case based on WSN.,,,
S0098300416300498," The static offsets caused by earthquakes are well described by elastostatic models with a discontinuity in the displacement along the fault . A traditional approach to model this discontinuity is to align the numerical mesh with the fault and solve the equations using finite elements . However this distorted mesh can be difficult to generate and update . We present a new numerical method inspired by the Immersed Interface Method for solving the elastostatic equations with embedded discontinuities . This method has been carefully designed so that it can be used on parallel machines on an adapted finite difference grid . We have implemented this method in Gamra a new code for earth modeling . We demonstrate the correctness of the method with analytic tests and we demonstrate its practical performance by solving a realistic earthquake model to extremely high precision . 
",We present Gamra a freely available tool for realistic earthquake modeling. Gamra uses a novel method to model faults without conforming meshes. Gamra scales to the largest problems using parallel adaptive mesh refinement. We document performance with a high precision model of the Mw 7.3 Landers earthquake.,,,
S0141938214000833," We examined the effects of the visual size and the number of digits on reading numerical time information in young adults . Using an adaptive staircase procedure minimal stimulus presentation duration for 80 correct responses was determined for visual sizes ranging from 0.1 to 15 when reading 1 2 or 3 2 digit units of time information . All three time types revealed U shaped relations between MSPD and visual size with the characteristics of the relation depending on the number of time units . Time type had two different effects . First longer time types gave rise to longer MSPDs as more elements needed to be encoded into working memory . Second longer time types gave rise to smaller ranges of optimal visual character size decreasing from 0.2 2 for the 1 unit time type to 0.3 0.5 for the 3 unit time type . The lower boundary of the optimal range of visual size may be understood as resulting from acuity limitations . The shift in the upper boundary of the optimal range of visual size is suggested to reflect the change in size of the visual span associated with larger visual character sizes . 
",Perceptual thresholds for reading numerical time vary with visual size and length. Larger numbers of time units require exponentially more reading time. Range of optimal visual sizes decreases with the number of time units to be read.,,,
S0165168414003855," We present a novel unsupervised fall detection system that employs the collected acoustic signals from an elderly person s normal activities to construct a data description model to distinguish falls from non falls . The measured acoustic signals are initially processed with a source separation technique to remove the possible interferences from other background sound sources . Mel frequency cepstral coefficient features are next extracted from the processed signals and used to construct a data description model based on a one class support vector machine method which is finally applied to distinguish fall from non fall sounds . Experiments on a recorded dataset confirm that our proposed fall detection system can achieve better performance especially with high level of interference from other sound sources as compared with existing single microphone based methods . 
",A data description method is applied to detect falls by collected acoustic signals footstep sound signals from normal activities. A spectral subtraction based binaural dereverberation method is applied to reduce the sound reverberation for pre processing. A time frequency TF mask based blind source separation technique is applied to separate the sound signal with the interferences of noises. One class support vector machine method is employed for the data description of the Mel frequency cepstral coefficient features to distinguish fall and non fall sounds.,,,
S0141938213000425," In this study the effects of ambient illuminance and light source on participants reading performance and visual fatigue during a long reading task were investigated using three electronic paper displays . Reading on electronic paper displays was also compared with reading on paper . In Experiment 1 100 participants performed a reading task where the display area for the text was equated for the displays . The results indicated that participants visual performance and visual fatigue did not differ significantly among different electronic paper displays ambient illuminance conditions or light sources . In Experiment 2 another 60 participants performed the same reading task where the full screen of each electronic paper display was used to present the text . The results showed that reading speed differed significantly across different electronic paper displays and ambient illuminance levels . The reading speed was slower for displays with smaller screens and increased as the ambient illuminance increased . Changes in the critical flicker fusion frequency significantly differed across ambient illuminance levels . Implications of the results for the use of electronic paper displays are discussed . 
",Electronic paper displays with large screen have faster reading speed. Both sun light and fluorescent light are proper for reading on E paper display. Ambient illuminances 1000lx and 1500lx have better visual performance. Ambient illuminances 200lx have more visual fatigue.,,,
S0141938213000929," To answer the question what is 3D good for we reviewed the body of literature concerning the performance implications of stereoscopic 3D displays versus non stereo displays . We summarized results of over 160 publications describing over 180 experiments spanning 51years of research in various fields including human factors psychology engineering human computer interaction vision science visualization and medicine . Publications were included if they described at least one task with a performance based experimental evaluation of an S3D display versus a non stereo display under comparable viewing conditions . We classified each study according to the experimental task of primary interest judgments of positions and or distances finding identifying or classifying objects spatial manipulations of real or virtual objects navigation spatial understanding memory or recall and learning training or planning . We found that S3D display viewing improved performance over traditional non stereo displays in 60 of the reported experiments . In 15 of the experiments S3D either showed a marginal benefit or the results were mixed or unclear . In 25 of experiments S3D displays offered no benefit over non stereo 2D viewing . From this review stereoscopic 3D displays were found to be most useful for tasks involving the manipulation of objects and for finding identifying classifying objects or imagery . We examine instances where S3D did not support superior task performance . We discuss the implications of our findings with regard to various fields of research concerning stereoscopic displays within the context of the investigated tasks . 
",We reviewed the performance implications of stereoscopic 3D displays versus non stereo 2D. We summarized and classified results of over 160 publications. We found that stereo 3D display viewing improved performance in 60 of experiments. In only 25 of experiments S3D displays clearly offered no benefit over 2D viewing. Stereoscopic 3D displays were most helpful for the spatial manipulation of objects.,,,
S0045794915003053," We extend our existing hp finite element framework for non conducting magnetic fluids to the treatment of conducting magnetic fluids including magnetostriction effects in both two and three dimensions . In particular we present to the best of our knowledge the first computational treatment of magnetostrictive effects in conducting fluids . We propose a consistent linearisation of the coupled system of non linear equations and solve the resulting discretised equations by means of the Newton Raphson algorithm . Our treatment allows the simulation of complex flow problems with non homogeneous permeability and conductivity and apart from benchmarking against established analytical solutions for problems with homogeneous material parameters we present a series of simulations of multiphase flows in two and three dimensions to show the predicative capability of the approach as well as the importance of including these effects . 
",The first computational treatment of magnetostritive effects in conducting fluids. Consistent linearisation of the coupled non linear equations. Computational solution using Newton Raphson and hp finite elements. Benchmarking against problems with homogeneous material parameters. Simulation of complex flow problems with non homogeneous material parameters.,,,
S0141938215300561," This paper presents an optimized color characterization model based on Radial Basis Functions . The performance of the proposed model was tested on a number of different mobile devices and compared with the performance of other state of the art color characterization models . We compared the accuracy of models using the CIELAB color difference . Four different models were discussed in detail Piecewise Linear Model Assuming Variation in Chromaticity Polynomial regression Artificial Neural Network and proposed Radial Basis Function model . For training and evaluation of the models we measured a large number of color samples on various mobile device displays . Results have shown that our optimized RBF model has superior accuracy over other models with median color difference of 0.39 . In addition it has particularly good accuracy for colors on the boundary of device s gamut with maximum color difference of 0.87 where other models shown unacceptably high color difference . 
",A novel method for characterization of a mobile device display based on Radial Basis Functions is presented. An overview of other state of the art methods for color characterization is given. Compared methods are tested on a number of various mobile devices. It is found that proposed method for color calibration is flexible and very accurate. Future research perspective for further optimization of the model is given.,,,
S0097849315001806,"Handheld Augmented Reality HAR has the potential to introduce Augmented Reality AR to large audiences due to the widespread use of suitable handheld devices. However many of the current HAR systems are not considered very practical and they do not fully answer to the needs of the users. One of the challenging areas in HAR is the in situ AR content creation where the correct and accurate positioning of virtual objects to the real world is fundamental. Due to the hardware limitations of handheld devices and possible restrictions in the environment the correct 3D positioning of objects can be difficult to achieve we are unable to use AR markers or correctly map the 3D structure of the environment. We present SlidAR a 3D positioning for Simultaneous Localization And Mapping SLAM based HAR systems. SlidAR utilizes 3D ray casting and epipolar geometry for virtual object positioning. It does not require a perfect 3D reconstruction of the environment nor any virtual depth cues. We have conducted a user experiment to evaluate the efficiency of SlidAR method against an existing device centric positioning method that we call HoldAR. Results showed that SlidAR was significantly faster required significantly less device movement and also got significantly better subjective evaluation from the test participants. SlidAR also had higher positioning accuracy although not significantly. 
",The correct in situ 3D positioning of virtual objects with HAR is fundamental. We developed a 3D positioning method for SLAM based handheld AR. We evaluated our method against a conventional device centric method. Our method was significantly better in objective and subjective measurements.,,,
S0141938214000328," The spectral analysis of Heart Rate Variability can be used for assessing the autonomic nervous activities and further the physiological conditions of subjects . This study intended to explore whether or not people would have fatigue faintness and other kinds of uncomfortable conditions after watching a 3D film by using HRV measures as the objective physiological indices in addition to other subjective physiological indices . Twenty men aged 22 2 experienced watching 3D films and 2D films and were served as the controls of themselves . As the controls the subjects had to rest at the same place . All subjects were are randomized for taking different experiences and the electrocardiographic signals were recorded during the whole process . The researchers could obtain the indices of the autonomic nervous activities before and after experiencing 3D and 2D movies with the help of spectral HRV analyses along with the objective physiological information . The subjects were requested to fill out the questionnaire for the subjective feelings after the movie experiences . It was found that the subjects high frequency power representing parasympathetic nervous activities decreases after watching a 3D film . The sympathetic and parasympathetic nerve activities before and after watching a 2D film were not significantly different . The subjects complained that they felt dizzy had headaches and got visual fatigue while watching a 3D film . This study found that the subjects parasympathetic nerve activities were reduced after watching a 3D film indicating that watching a 3D film would make people uncomfortable and tired . This result was the same as that of the questionnaire . Thus HRV analyses could be an objective physiological index for discomfort as viewing 3D films . 
",HRV analysis can be used to assess the various fatigue and uncomfortable. The subjects parasympathetic nerve activity was reduced after watching a 3D film. Watching a polarized 3D film would make people uncomfortable and tired.,,,
S0141938216300312," The impact of aberration on the speckle suppression efficiency is investigated in a laser projector system containing a moving diffractive optical element . The results of a qualitative analysis based on the number of diffraction orders passed through the optical system are presented along with a quantitative analysis built upon the Fresnel approximation and the thin lens model . It is shown that the speckle contrast in the paraxial area of the screen is practically insensitive to aberrations limited to a few percent at most due to the change in angle between diffraction orders . However the speckle contrast in peripheral areas changes stepwise if aberrations change the number of diffraction orders that illuminate the area . 
",Model to evaluate the influence of aberrations on speckle suppression efficiency SS is worked out. SS is practically insensitive to aberrations in paraxial area of screen. SS at periphery changes stepwise when the aberrations change the number of diffraction orders.,,,
S0141938213000917," The color of a displayed image by a projector can be distorted by features of the device the ambient light the projection screen and also the observer . This has raised the need to correct the image during the display to eliminate these effects and to ensure a constancy of the color appearances . In this paper we propose models for controlling the appearance of the displayed image . We argue that depending on the target application the computational color constancy can be specified at different steps of the formation scheme of the sensed image by a human . Based on that observation and the image formation models we reformulate the problem of the color constancy and we show that the resulting transformations can not be explained by von Kries theory . Two compensation algorithms are deduced . The first allows preserving the appearance of the original image and it can be used for the constancy of the acquired image whatever the environment conditions . The second algorithm allows to simulate appearances of a sensed image in a specific conditions . It can be used for the compensation of the screen reflectance or to create special effects or the camouflage . In addition we propose a complementary operation for the contrast compensation which is derived from the Weber s law . Experimental results show the merits of the proposed models and algorithms . 
",We model the sensed image by human eye. We explore the color constancy to reduce the screen effects on the projected image. We argue that the proposed model goes further than the von Kries theory.,,,
S0167839614000648," One major issue in CAGD is to model complex objects using free form surfaces of general topology . A natural approach is curvenet based design where designers directly create and modify feature curves . These are interpolated by smoothly connected multi sided patches which can be represented by transfinite surfaces defined as a combination of side interpolants or ribbons . A ribbon embeds Hermite data i.e . prescribed positional and cross derivative functions along boundary curves . The paper focuses on two transfinite schemes the first is an enhanced and extended variant of a multi sided generalization of the classical Coons patch the second one is based on a new concept of combining doubly curved composite ribbons each one interpolating three adjacent sides . Main contributions include various ribbon parameterizations that surpass former methods in quality and computational efficiency . It is proven that these surfaces smoothly interpolate the prescribed ribbon data . Both formulations are based on non regular convex polygonal domains and distance based rational blending functions . A few examples illustrate the results . 
",Two n sided transfinite surface representations are presented. The new schemes interpolate ribbon surfaces with continuity. One of these is a generalization of the Coons patch based on the Boolean sum scheme. The other combines curved side interpolants. Special distance based blending functions and parameterizations are introduced.,,,
S0098300414000259," In seismology waveform cross correlation has been used for years to produce high precision hypocenter locations and for sensitive detectors . Because correlated seismograms generally are found only at small hypocenter separation distances correlation detectors have historically been reserved for spotlight purposes . However many regions have been found to produce large numbers of correlated seismograms and there is growing interest in building next generation pipelines that employ correlation as a core part of their operation . In an effort to better understand the distribution and behavior of correlated seismic events we have cross correlated a global dataset consisting of over 300 million seismograms . This was done using a conventional distributed cluster and required 42 days . In anticipation of processing much larger datasets we have re architected the system to run as a series of MapReduce jobs on a Hadoop cluster . In doing so we achieved a factor of 19 performance increase on a test dataset . We found that fundamental algorithmic transformations were required to achieve the maximum performance increase . Whereas in the original IO bound implementation we went to great lengths to minimize IO in the Hadoop implementation where IO is cheap we were able to greatly increase the parallelism of our algorithms by performing a tiered series of very fine grained transformations on the data . Each of these MapReduce jobs required reading and writing large amounts of data . But because IO is very fast and because the fine grained computations could be handled extremely quickly by the mappers the net was a large performance gain . 
",A global dataset of over 300 million waveforms has been cross correlated. The algorithms have been adapted to run as MapReduce jobs on a Hadoop cluster. Increased parallelism was required to make best use of mappers. IO was significantly increased but had little impact on performance. A factor of 19 speedup was achieved relative to initial implementation.,,,
S0167642315001288," In order to provide a rigorous foundation for Software Product Lines several fundamental approaches have been proposed to their formal behavioral modeling . In this paper we provide a structured overview of those formalisms based on labeled transition systems and compare their expressiveness in terms of the set of products they can specify . Moreover we define the notion of tests for each of these formalisms and show that our notions of testing precisely capture product derivation i.e . all valid products will pass the set of test cases of the product line and each invalid product fails at least one test case of the product line . 
",Comparing three fundamental models of Software Product Lines. Proving modal transition systems less expressive than product line labeled transition systems. Proving product line labeled transition systems less expressive than featured transition systems. Using Abramsky s test expressions to characterize product derivation.,,,
S0141938213000681," Electroswitching of emission and coloration was achieved by a combination of a luminescent Eu complex and an electrochromic molecule of diheptyl viologen indicating that the complex molecule combination could be used as a display material with dual emissive and reflective modes . The coloration of the material was associated with the electrochromism of HV2 . Emission control was found to be possible due to the electrochromism of HV2 via intermolecular energy transfer from the excited state of the Eu ion to the reduced state of HV . By using this mechanism dual emissive and reflective representation of numerical characters were demonstrated . 
",Electroswitching of emission and coloration was achieved by Eu III complex and viologen. Coloration of the material was associated with the electrochromic reaction. Emission control was induced by intermolecular energy transfer from Eu III ion to viologen. Complex molecule system can be used as display material with dual emissive and reflective modes. Numeric indication and mutli color representation of dual mode display were demonstrated.,,,
S0142061515003324," The increased number of renewable power plants pose threat to power system balance . Their intermittent nature makes it very difficult to predict power output thus either additional reserve power plants or new storage and control technologies are required . Traditional spinning reserve can not fully compensate sudden changes in renewable energy power generation . Using new storage technologies such as flow batteries it is feasible to balance the variations in power and voltage within very short period of time . This paper summarises the controlled use of hybrid flow battery thermal and hydro power plant system to support wind power plants to reach near perfect balance i.e . make the total power output as close as possible to the predicted value . It also investigates the possibility of such technology to take part in the balance of the Lithuanian power system . A dynamic model of flow battery is demonstrated where it evaluates the main parameters such as power energy reaction time and efficiency . The required battery size is tested based on range of thermal and hydro power plant reaction times . This work suggests that power and energy of a reasonable size flow battery is sufficient to correct the load and wind power imbalance . 
",We modelled hybrid power system with flow battery hydro and thermal power plants. The forecasted and actual wind power data were used from Lithuanian power system. An optimal low pass filter time constants are determined using objective function. Results depict the required power and capacity ratings of flow battery.,,,
S0167819113001051," SpiNNaker is a biologically inspired massively parallel computer designed to model up to a billion spiking neurons in real time . A full fledged implementation of a SpiNNaker system will comprise more than 105 integrated circuits . Given this scale it is unavoidable that some components fail and in consequence fault tolerance is a foundation of the system design . Although the target application can tolerate a certain low level of failures important efforts have been devoted to incorporate different techniques for fault tolerance . This paper is devoted to discussing how hardware and software mechanisms collaborate to make SpiNNaker operate properly even in the very likely scenario of component failures and how it can tolerate system degradation levels well above those expected . 
",Discussion of chip level fault tolerance of SpiNNaker s design. The implemented software improves fault tolerance by providing diagnostics and reconfiguration. Exploration of communication level fault tolerance and its effects on system scalability. Wide range of experiments showing that SpiNNaker is highly resilient to failures.,,,
S0141938215000335," Minimum angle of resolution was measured for the grating which consisted of lines of two colors selected from Red Green Blue White and Black . Method of two alternative forced choice was used where the participants were asked to answer the direction of the color grating of the horizontal or vertical directions . From the measured psychometric function of the ratio of the correct answers MAR which corresponded to the threshold of 75 correct answer ratio was determined . MAR of the grating patches with more than one primary color was measured to be affected by the combination of colors and to be 10 30 larger than that of the grating patch of White Black . While the resolving power for Blue pattern had been known to be worse than those for Green and Red patterns MAR of the grating including Blue was not always the worst . 
",Minimum angle of resolution MAR is measured for colored grating. Colored grating consists of two colors from R G B White and Black. MAR is the smallest for grating of White and Black. MAR for grating with Blue is not always smaller than grating with other colors.,,,
S0141938213000437," Cholesteric liquid crystals have been extensively studied due to their unique self organized helical molecular structures and selective Bragg reflection properties which exhibit great potentials for color displays and other practical applications . When functional nanoscale molecular switches are doped in liquid crystals the phases of the LCs or the molecular structures of the Ch LCs can be changed upon the influence of external stimuli such as light and temperature . In this paper the photoresponsive molecular switch based LCs for display applications are reviewed . The progress and effort in developing molecular switches the principles of light tuning photo addressed color displays information processing bistable displays and flexible displays are presented . 
",Functional nanoscale molecular switches used in liquid crystals are reviewed. Molecular switch based liquid crystal displays are discussed. Principles of molecular switch based liquid crystal displays are presented.,,,
S0142061513002676," In this paper an interleaved step up converter with a single capacitor snubber for PV energy conversion applications is proposed . The step up converter adopts two sets of boost converters with an interleaved fashion and coupled inductor technology to reduce output ripple current and increase output power level . To achieve higher conversion efficiency and reduce switching losses of the proposed converter a lossless single capacitor turn off snubber is introduced . Therefore the conversion efficiency can be increased significantly . In order to draw the maximum power from the PV arrays a perturbation and observation method and a microcontroller are associated to implement maximum power point tracking algorithm and power regulating scheme . Finally a prototype of a soft switching interleaved soft switching boost converter with coupled inductors has been built and implemented . Experimental results have obtained to verify the performance and feasibility of the proposed converter for PV arrays applications . 
",This paper presents an interleaved soft switching step up converter for PV arrays applications. The perturbation and observation method is adopted to extract the maximum power of PV arrays. The proposed circuit structure uses a lossless single capacitor turn off snubber to increase efficiency. The proposed power system is suitable for renewable energy conversion applications.,,,
S0141938215300020," This paper presents a novel image restoration algorithm using examples and truncated constrained least squares filter for ultra high definition television systems . The proposed approach consists of three steps generation of the patch dictionary using multiple step image blurring selection of the optimum patch based on the orientation and the amount of blurring and combination of the selected patch in the dictionary and its filtered version by the TCLS restoration filter for reducing the patch mismatch error . In the proposed algorithm a complicated point spread function estimation process is replaced with the generation of multiple differently blurred patches . Furthermore the patch dictionary is made by orientation based classification to reduce the time to search the optimum patch . Experimental results show that the proposed algorithm can restore more natural images with less synthetic artifacts than existing methods . The proposed method provides a significantly improved restoration performance over existing methods in the sense of both subjective and objective measures including peak to peak signal to noise ratio and structural similarity measure . 
",This paper presents restoration method using examples and TCLS filter for UHD image. The proposed method can be used for wide range of consumer imaging systems. In order to reduce the search time patch is classified based on the edge orientation.,,,
S0141938215300494," In this paper we proposed a novel method to embed a series of ternary secret data into a cover image based on an improved Least Significant Bit scheme using the modulo three strategy . Our new method can hide two ternary numbers into each grayscale pixel normally only modify the two LSBs of the pixel while it may cause overflow underflow and a carry borrow . We solve these problems by adding 1 to the pixel or subtracting 1 from the pixel before embedding . The embedding capacity of our method can be 3.1699bpp . At the same time the quality of the stego image of our new method also is better than traditional LSB scheme when the embedding capacity is greater than 3bpp with a Peak Signal to Noise Ratio greater than 37dB . Extensive experimental results indicated that our new method is capable of getting a higher PSNR than traditional LSB scheme when the embedding capacity is greater than 3bpp and it has higher resistance ability against the chosen steganalysis algorithm when the embedding capacity is low . 
",We proposed a novel method to embed a series of ternary secret data based on LSB. The proposed scheme can hide two ternary numbers into each grayscale pixel normally only modify the two LSBs of the pixel. Our proposed scheme has higher resistance ability and achieves a higher PSNR.,,,
S0167839614000752," For an arbitrary degree B zier curve we first establish sufficient conditions for its control polygon to become homeomorphic to via subdivision . This is extended to show a subdivided control polygon that is ambient isotopic to . We provide closed form formulas to compute the corresponding number of iterations for equivalence under homeomorphism and ambient isotopy . The development of these a priori values was motivated by application to high performance computing where providing estimates of total run time is important for scheduling . 
",We establish ambiently isotopic control polygons for B zier curves via subdivision. We provide closed form formulas for the required number of subdivision iterations. The results are being applied in computer animation.,,,
S0141938214000870," This paper discusses state of the art tactile displays fabricated by a micro electronic mechanical system . A tactile display conveys tactile sensations to users by using actuators . Traditional tactile displays consist of large size actuators such as a motor or an ultrasound vibrator to convey tactile feedback by vibration . In addition the tactile sensation of traditional displays has poor resolution . Microelectromechanical system technology which is a miniature fabrication process enables etching sputtering and assembling of miniature structures . Recently the technology was applied to tactile displays . For example shape memory alloy actuators are widely used in tactile displays to convey roughness or vibration . The actuators are fabricated by a sputtering process and then thinned . The displays convey various tactile sensations including feedback and tactile sensations of objects such as paper or wood . This paper is a review of tactile displays fabricated by MEMS technology . We also describe the fabrication processes and stimulation methods to present the potential and applications of the displays . 
",This paper describes tactile displays with MEMS technology. Tactile displays represent tactile sensation with various simulation methods. We classify the displays according to tactile sensation presented on them and describe them.,,,
S0167839613000587," We examine the problem of computing exactly the Voronoi diagram of a set of possibly intersecting smooth convex pseudo circles in the Euclidean plane given in parametric form . Pseudo circles are sites every pair of which has at most two intersecting points . The Voronoi diagram is constructed incrementally . Our first contribution is to propose robust and efficient algorithms under the exact computation paradigm for all required predicates thus generalizing earlier algorithms for non intersecting ellipses . Second we focus on InCircle which is the hardest predicate and express it by a simple sparse polynomial system which allows for an efficient implementation by means of successive Sylvester resultants and a new factorization lemma . The third contribution is our cgal based c software for the case of possibly intersecting ellipses which is the first exact implementation for the problem . Our code spends about a minute to construct the Voronoi diagram of 200 ellipses when few degeneracies occur . It is faster than the cgal segment Voronoi diagram when ellipses are approximated by k gons for 15 and a state of the art implementation of the Voronoi diagram of points when each ellipse is approximated by more than 1250 points . 
",Exact Voronoi diagram of smooth convex pseudo circles via dual Delaunay graph. Geometric and algebraic analysis of required predicates. Complete exact and efficient c implementation for intersecting ellipses. Novel combination of symbolic numeric techniques.,,,
S0141938215300470," Modern interfaces increasingly rely on screens filled with digital text to display information to users . Previous research has shown that even relatively subtle differences in the design of the on screen typeface can influence to device glance time in a measurable and meaningful way . Here we outline a methodology for rapidly and flexibly investigating the legibility of typefaces on digital screens in glance like contexts and apply this method to a comparison of 5 Simplified Chinese typefaces . We find that the legibility of the typefaces measured as the minimum presentation time needed to read character strings accurately and respond to a yes no lexical decision task is sensitive to differences in the typeface s design characteristics . The most legible typeface under study could be read 33.1 faster than the least legible typeface in this glance induced context . A second study examined two different weights of the MT YingHei type family as well as two contrast polarity conditions to investigate how these variations impact legibility thresholds . Results indicate that bold weight text is easier to read in this enforced glance like context and that positive polarity text is easier to read compared to white on black text under the lighting conditions considered . These results are discussed in terms of contextual factors that may mediate glance reading behavior as well as how type design interacts with the practical limitations of a moderate density pixel grid . 
",This study examines aspects of the legibility of Chinese digital typography. Legibility was measured as the amount of time needed to read text accurately. Typeface style weight and contrast polarity were compared across two studies. Hei style type set in bold weight in positive contrast was the most legible. Findings have implications for optimal rendering of text on digital displays.,,,
S0141938214000651," We report a screen printing fabrication process for large area electrowetting display devices using polyimide based materials . The poly was selected as hydrophobic insulator layer and relatively hydrophilic polyimide as grids material . EWD devices that use poly as hydrophobic insulator fabricated with conventional methods showed good and reversible electrowetting performance on both single droplet level and device level which showed its potential application in EWDs . The compatibility of polyimide based materials and hydrophilic polyimide guarantee the good adhesion between two layers and the capability of printable fabrication . To this end the hydrophilic grids have been successfully built on hydrophobic layer by screen printing directly . The resulting EWD devices showed good switch performance and relatively high yield . Compared to conventional method the polyimide based materials and method offer the advantages of simple cheap and fast fabrication and are especially suitable for large area display fabrication . 
",Poly imide siloxane film demonstrated good electrowetting performance. Polyimide based hydrophobic material poly imide siloxane and hydrophilic polyimide showed good compatibility. Hydrophilic polyimide grids could be directly built on the hydrophobic poly imide siloxane film by screen printing. Cheaper materials and simpler fabrication method have been demonstrated for electrowetting displays.,,,
S0140366415001589," With the rapid proliferation of data centers their energy consumption and greenhouse gas emissions have significantly increased . Some efforts have been made to control and lower energy consumption of data centers such as proportional energy consuming hardware dynamic provisioning and virtualization machine techniques . However it is still common that many servers and network resources are often underutilized and idle servers spend a large portion of their peak power consumption . We first build a novel model of virtual network embedding in order to minimize energy usage in data centers for both computing and network resources by taking practical factors into consideration . Due to the NP hardness of the proposed model we develop a heuristic algorithm for virtual network scheduling and mapping . In doing so we specifically take the expected energy consumption at different times virtual network operation and future migration costs and a data center architecture into consideration . Our extensive evaluation results show that our algorithm could reduce energy consumption up to 40 and take up to a 57 higher number of virtual network requests over other existing virtual mapping schemes . 
",We formulate energy efficient virtual network embedding that incorporates energy costs of operation and migration for nodes and links. We prove the NP hardness of the problem and develop a heuristic algorithm to minimize the energy consumption. We consider a practical intra DC architecture to further improve energy efficiency. We conduct extensive evaluations and comparisons with existing algorithms to show that the proposed algorithm substantially saves energy consumption and allows high acceptance ratios.,,,
S0167819114000398," Simulation of in vivo cellular processes with the reaction diffusion master equation is a computationally expensive task . Our previous software enabled simulation of inhomogeneous biochemical systems for small bacteria over long time scales using the MPD RDME method on a single GPU . Simulations of larger eukaryotic systems exceed the on board memory capacity of individual GPUs and long time simulations of modest sized cells such as yeast are impractical on a single GPU . We present a new multi GPU parallel implementation of the MPD RDME method based on a spatial decomposition approach that supports dynamic load balancing for workstations containing GPUs of varying performance and memory capacity . We take advantage of high performance features of CUDA for peer to peer GPU memory transfers and evaluate the performance of our algorithms on state of the art GPU devices . We present parallel efficiency and performance results for simulations using multiple GPUs as system size particle counts and number of reactions grow . We also demonstrate multi GPU performance in simulations of the Min protein system in E. coli . Moreover our multi GPU decomposition and load balancing approach can be generalized to other lattice based problems . 
",Larger and longer simulations of biological RDMEs with multiple GPUs. Spatial decomposition allows for simulation of large cellular volumes. Multi GPU performance allows an increase in particle counts and reactions. Load balancing optimizes for heterogeneity in lattice sites and GPU hardware.,,,
S0141938214000353," We investigate the effect of the metal pattern shape on the starburst phenomenon of touch screen panels based on opaque metallic grids . It is demonstrated that a standalone random metal grid can suppress the starburst phenomenon to a great extent . By way of ray tracing simulation we have found that specular reflection of light on the gentle slope of the patterned edges of metallic grids contributes to the generation of starburst patterns . It is also addressed that employing a light absorbing material and increasing metal grid spacing can reduce the intensity of starburst patterns . 
",We examine the effect of metal meshes on the starburst phenomenon of touch panels. The gentle slope of the patterned edges generates the starburst patterns. The starburst patterns were suppressed substantially by a light absorbing material. A random metal grid shows many but weakened threadlike beams.,,,
S0140366416301141,"A malicious alteration of system provided timeline can negatively affect the reliability of computer forensics. Indeed detecting such changes and possibly reconstructing the correct timeline of events is of paramount importance for court admissibility and logical coherence of collected evidence. However reconstructing the correct timeline for a set of network nodes can be difficult since an adversary has a wealth of opportunities to disrupt the timeline and to generate a fake one. This aspect is exacerbated in cloud computing where host and guest machine time can be manipulated in various ways by an adversary. Therefore it is important to guarantee the integrity of the timeline of events for cloud host and guest nodes or at least to ensure that timeline alterations do not go undetected. This paper provides several contributions. First we survey the issues related to cloud machine time reliability. Then we introduce a novel architecture CURE aimed at providing timeline resilience to cloud nodes. Further we implement the proposed framework and extensively test it on both a simulated environment and on a real cloud. We evaluate and discuss collected results showing the effectiveness of our proposal. 
",Cloud time alteration attack scenarios consequences and countermeasures have been surveyed. A novel cloud simulator has been designed implemented and released as open source. CURE architecture can detect Timeline of Events alterations to aid a forensic investigation process. A real world deployment of CURE has been tested on a public cloud. Measured performance figures support CURE viability.,,,
S0141938214000286," It has been shown that multisensory presentation can improve perception attention and object memory compared with unisensory presentation . Consequently we expect that multisensory presentation of landmarks can improve spatial memory and navigation . In this study we tested the effect of visual auditory and combined landmark presentations in virtual mazes on spatial memory and spatial navigation . Nineteen participants explored four different virtual mazes consisting of nodes with landmarks and corridors connecting them . Each maze was explored for 90s . After each exploration participants performed the following tasks in fixed order draw a map of the maze recall adjacent landmarks for three given landmarks place all landmarks on the map of the maze and find their way through the maze to locate five given landmarks in fixed order . Our study shows significant effects of multisensory versus unisensory landmarks for the maze drawing task the adjacency task and the wayfinding task . Our results suggest that audiovisual landmark presentations improve spatial memory and spatial navigation performance in virtual environments . 
",We compare navigating virtual mazes with visual auditory and combined landmarks. We examine changes in spatial memory tasks and way finding performance. Audiovisual landmark presentation yields the best performance for map drawing landmark recollection and wayfinding.,,,
S0165168415001073," In this paper we address the tasks of audio source counting and separation for a stereo anechoic mixture of audio signals . This will be achieved in two stages . In the first stage a novel approach is introduced for estimating the number of sources as well as the channel mixing coefficients . For this purpose a 2 D spectrum is evaluated against both the phase and amplitude differences of the two channels . Hence obtaining the peak locations of the spectrum yields the number of the sources and the corresponding channel coefficients . In the second stage an extension of a single channel complex matrix factorization method to multichannel is developed to extract the individual source signals . We find primary estimates of the sources via binary masking and then apply the complex factorization to the complex spectrogram of each source . The obtained factors are then utilized as initial values in the complex multichannel factorization model . We also suggest a method for estimating the number of required components for modeling each source . The separation performance improvement over the conventional methods is investigated by calculating BSS evaluation metrics . The comparison is also carried out in terms of source counting and localization with the recently proposed DeMIX Anechoic method . 
",The tasks of audio source counting and separation for a stereo anechoic mixture of audio signals are addressed. A novel approach is introduced for estimating the number of sources as well as the channel mixing coefficients based on evaluating a 2 D spectrum. An extension of a single channel complex matrix factorization method to multichannel is developed to extract the individual source signals. A model order selection method is proposed to infer the optimal number of the components required for modeling each source. The separation performance improvement over the conventional methods is investigated by calculating BSS evaluation metrics.,,,
S0141938213000498," Ca TiO3 Eu3 M and CaTiO3 Pr3 M powders were prepared by combustion synthesis method and the samples were further heated to 1000 C to improve the crystallinity . The structure and morphology of materials were examined by X ray diffraction and a scanning electron microscopy . The morphologies of SrTiO3 Eu3 CaTiO3 Eu3 or CaTiO3 Pr3 powders co doped with other metal ions were very similar . Small and coagulated particles of nearly cubical shapes with small size distribution having smooth and regular surface were formed . Photo luminescence spectra of CaTiO3 Pr3 and co doped either with Li Na K Ag La3 or Gd3 ions showed red emissions at 613nm due to the 1D2 3H4 transition of Pr3 . The variation of intensity of emission peak with different co doping follows the order K Ag Na Li La3 Gd3 . The characteristic emissions of CaTiO3 Eu3 lattices had strong emission at 614 and 620nm for 5D0 7F2 with other weak transitions observed at 580 592 654 705nm for 5D0 7Fn transitions where n 0 1 3 4 respectively in all host lattices . Photoluminescence intensity in SrTiO3 Eu3 is more than CaTiO3 Eu3 lattices . A remarkable increase of photoluminescence intensity was observed if co doped with Li ions in CaTiO3 Eu3 and SrTiO3 Eu3 . 
",Ca or Sr TiO3 Eu3 N N Li Na or K powders were prepared by combustion method. CaTiO3 Pr3 N N Li Na Ag K Gd or La were synthesized by combustion method. PL intensity of CaTiO3 Pr3 increases when co doping with Li Na Ag K Gd or La. Li co doping results in an increase of PL intensity in Ca or Sr TiO3 Eu3 powders.,,,
S0141938214000407," This research investigated whether carrying red colored products enhances female sexual attractiveness . In the first experiment male participants were instructed to observe women carrying laptops in different colors . The results indicated that the women who carried red laptops were perceived to possess a significantly higher level of attractiveness and sex appeal than those who carried laptops in other colors however the red laptop did not affect men s perceptions of the assertiveness and health level of the women . In the second experiment the initial experiment was repeated but female participants observed the women laptop color did not influence how the participants perceived the women s attractiveness sex appeal assertiveness and level of health . In other words women carrying products in red only affected how men but not women perceived them thus women using red products are more attractive and sexually appealing to men . 
",Women carried red colored products can enhance their sexual attractiveness. Women carrying products in red only affected how men but not women perceived them. Red colored products did not affect perceived assertiveness and health level of the women.,,,
S0140366415002583," In Vehicular ad hoc networks one of the challenging tasks is to find an accurate localization information . In this paper we have addressed this problem by introducing a novel approach based on the idea of cooperative localization . Our proposed scheme incorporates different techniques of localization along with data fusion as well as vehicle to vehicle communication to integrate the available data and cooperatively improve the accuracy of the localization information of the vehicles . The simulation results show that sharing the localization information and deploying that of the neighboring vehicles not only assures the vehicles in a vicinity to obtain more accurate localization information but also find the results robust to sensor inaccuracies or even to failures . Moreover further improvement has been achieved by estimating the vehicle prior using unscented transform together with sequential decentralized extended Kalman filtering . 
",The paper deals with challenging localization problem in vehicular ad hoc networks. A novel approach is proposed based on the idea of cooperative localization. Our scheme integrates available data and cooperatively improves location accuracy. Localization is more accurate and robust to sensor inaccuracies or even to failures. The estimation of vehicle prior and sequential decentralized EKF improve further.,,,
S0097849314000661,"We present a robust approach for reconstructing the main architectural structure of complex indoor environments given a set of cluttered 3D input range scans. Our method uses an efficient occlusion aware process to extract planar patches as candidate walls separating them from clutter and coping with missing data and automatically extracts the individual rooms that compose the environment by applying a diffusion process on the space partitioning induced by the candidate walls. This diffusion process which has a natural interpretation in terms of heat propagation makes our method robust to artifacts and other imperfections that occur in typical scanned data of interiors. For each room our algorithm reconstructs an accurate polyhedral model by applying methods from robust statistics. We demonstrate the validity of our approach by evaluating it on both synthetic models and real world 3D scans of indoor environments. 
",We reconstruct an architectural model from a laser scanned indoor environment. Our algorithm can handle complex and highly concave room arrangements. It automatically detects all rooms without knowing the number of rooms in advance. Our pipeline can cope with occlusions and clutter using a robust heat diffusion process. An evaluation on artificial and real world data shows the accuracy of the method.,,,
S0098300413002185,"An image processing software has been developed which allows quantitative analysis of multi and hyperspectral data from oceanic coastal and inland waters. It has been implemented into the Water Colour Simulator WASI which is a tool for the simulation and analysis of optical properties and light field parameters of deep and shallow waters. The new module WASI 2D can import atmospherically corrected images from airborne sensors and satellite instruments in various data formats and units like remote sensing reflectance or radiance. It can be easily adapted by the user to different sensors and to optical properties of the studied area. Data analysis is done by inverse modelling using established analytical models. The bio optical model of the water column accounts for gelbstoff coloured dissolved organic matter CDOM detritus and mixtures of up to 6 phytoplankton classes and 2 spectrally different types of suspended matter. The reflectance of the sea floor is treated as sum of up to 6 substrate types. An analytic model of downwelling irradiance allows wavelength dependent modelling of sun glint and sky glint at the water surface. The provided database covers the spectral range from 350 to 1000 nm in 1 nm intervals. It can be exchanged easily to represent the optical properties of water constituents bottom types and the atmosphere of the studied area. 
",Free image processing software for multi and hyperspectral data. Deep and shallow waters. Wavelength dependent sun and sky glint correction. Easy adaptation to different environments and sensors.,,,
S0141938215300019," Panoramic videos provide a high resolution wide angle field of view to audience . The rendering of such panoramic videos on large multi projector curved displays further enhances the experience by providing them an immersive visual environment . Immersive panoramic projection thus comprise of three steps acquisition of videos stitching of video frames and their projection using multi projector display system . Both panoramic stitching and multi projector rendering require compute intensive geometric and photometric transformations that have to be applied for each frame of a video during stitching as well as rendering process . In this paper we propose a unified scheme that handles these two separate geometric transformations using a single combined Look up Table . The scheme allows projection of planar and cylindrical panoramas on curved cylindrical displays . Experimental results show that the proposed scheme provides a speedup of up to 73 and 83 for projector resolutions of HD and 4K respectively for a two camera two projector panoramic display system . Furthermore for a 4K projector the proposed scheme requires 2.278ms for the geometric transformation of its content thus enabling real time panoramic video projection . 
",Generation of panoramic videos requires alignment and stitching of frames. High resolution immersive projection requires multi projector alignment and rendering. We present a unified panorama stitching and multi projector rendering scheme. The scheme provides considerable speed up over traditional stitching and rendering schemes.,,,
S0045790616300623," The aim of multi focus image fusion is to combine several images taken by different sensors and with different focuses to increase the perception of scene. The proposed methods suffer from some undesirable side effects like blurring artifact and or blocking which decreases the quality of the output image. This paper presents an efficient approach for multi focus image fusion based on variance and spatial frequency calculated in the wavelet domain. The proposed method remarkably reduces the amount of distortion artifacts and contrast loss due to the fact that variance and spatial frequency based fusion significantly enhances reliability in feature selection and data fusion procedures. The algorithm ensures to a great extant the access to the data of the images. The experimental results verify the efficiency of the proposed method in the output image quality as well as its lower complexity in comparison with several recently related works. 
",A new method of fusion using sharpness criteria in wavelet domain is used in this paper. The suggested method uses the two criteria of spatial frequency and variance for fusion. A lot of experiments indicate the efficiency of the recommended method both in quality and complexity.,,,
S0167839615000850," In this article we provide the characterization of analysis suitable T spline spaces as the space of piecewise polynomials with appropriate linear constrains on the subdomain interfaces . We describe AST meshes for which the linear constrains are equivalent to smoothness conditions and provide examples showing that this is not always the case . 
",We study the analysis suitable T spline spaces as a piecewise polynomials space. We characterize the difference between two neighboring polynomial expressions. We study when the space can be described by smoothness conditions.,,,
S0097849313000447,"In this work we propose and experiment an original solution to 3D face recognition that supports face matching also in the case of probe scans with missing parts. In the proposed approach distinguishing traits of the face are captured by first extracting 3D keypoints of the scan and then measuring how the face surface changes in the keypoints neighborhood using local shape descriptors. In particular 3D keypoints detection relies on the adaptation to the case of 3D faces of the meshDOG algorithm that has been demonstrated to be effective for 3D keypoints extraction from generic objects as 3D local descriptors we used the HOG descriptor and also proposed two alternative solutions that develop respectively on the histogram of orientations and the geometric histogram descriptors. Face similarity is evaluated by comparing local shape descriptors across inlier pairs of matching keypoints between probe and gallery scans. The face recognition accuracy of the approach has been first experimented on the difficult probes included in the new 2D 3D Florence face dataset that has been recently collected and released at the University of Firenze and on the Binghamton University 3D facial expression dataset. Then a comprehensive comparative evaluation has been performed on the Bosphorus Gavab and UND FRGC v2.0 databases where competitive results with respect to existing solutions for 3D face biometrics have been obtained. 
",3D face recognition approach deployable in real non cooperative contexts of use. Fully 3D approach based on keypoints detection description and matching. MeshDOG keypoints detector combined with the multi ring GH descriptor. RANSAC algorithm included for outlier removal from matching keypoints. State of the art accuracy for recognizing 3D scans with missing parts.,,,
S0140366416300901," The current age of increased people mobility calls for a better understanding of how people move how many places does an individual commonly visit what are the semantics of these places and how do people get from one place to another . We show that the number of places visited by each person is regulated by some properties that are statistically similar among individuals . Subsequently we present a PoIs classification in terms of their relevance on a per user basis . In addition to the PoIs relevance we also investigate the variables that describe the travel rules among PoIs in particular the spatial and temporal distance . As regards the latter existing works on mobility are mainly based on spatial distance . Here we argue rather that for human mobility the temporal distance and the PoIs relevance are the major driving factors . Moreover we study the semantic of PoIs . This is useful for deriving statistics on people s habits without breaking their privacy . With the support of different datasets our paper provides an in depth analysis of PoIs distribution and semantics it also shows that our results hold independently of the nature of the dataset in use . We illustrate that our approach is able to effectively extract a rich set of features describing human mobility and we argue that this can be seminal to novel mobility research . 
",Visited locations are classified in 3 main categories according to their relevance. People visit regularly just few places where they spend most of their time. People commute between places based on their time not spatial distance. HOME and WORK places are in the set of few places mostly visited. Mostly visited places semantic inference is based on user mobility behavior analysis.,,,
S0141938214000717," MEMS are used in many fields including display applications which are extensively studied both in academia and industry . For practical devices numbers of advanced technologies have been developed based on MEMS concept . For display technologies projection displays reflective displays transmissive displays and other display modes have been achieved by different MEMS modes . In this review the current MEMS based display technologies are introduced and discussed including digital micromirror device laser scanning display interferometric modulator display digital micro shutter time multiplexed optical shutter grating light valve and others . The typical structure and fundamental of each display mode are interpreted . 
",MEME based display technologies are reviewed. Projection reflective transmissive and novel display modes are included. Typical structures and fundamentals of different display modes are interpreted.,,,
S0045790616300726," The compositional and content attributes of images carry information that enhances the performance of image retrieval . Standard images are constructed by following the rule of thirds that divides an image into nine equal parts by placing objects or regions of interest at the intersecting lines of the grid . An image represents regions and objects that are in a spatial semantic relationship with respect to each other . While the Bag of Features representation is commonly used for image retrieval it lacks spatial information . In this paper we present two novel image representation methods based on the histograms of triangles which add spatial information to the inverted index of BoF representation . Histograms of triangles are computed at two levels by dividing an image into two and four triangles that are evaluated separately . Extensive experiments and comparisons conducted on two datasets demonstrate that the proposed image representations enhance the performance of image retrieval . 
",The addition of spatial information to the inverted index of the BoF representation. Image representation in the form of triangular histograms. Three different classifiers are evaluated in order to determine the best performance of the proposed work.,,,
S0167839615000977," Many problems in computer aided geometric design and computer graphics can be turned into a root finding problem of a polynomial equation . Among various solutions clipping methods based on the Bernstein B zier form usually have good numerical stability . A traditional clipping method using polynomials of degree r can achieve a convergence rate of for a single root . It utilizes two polynomials of degree r to bound the given polynomial of degree n where and the roots of the bounding polynomials are used for clipping off the subintervals containing no roots of . This paper presents a rational cubic clipping method for finding the roots of a polynomial within an interval . The bounding rational cubics can achieve an approximation order of 7 and the corresponding convergence rate for finding a single root is also 7 . In addition differently from the traditional cubic clipping method solving the two bounding polynomials in the new method directly constructs the two rational cubics in which can be used for bounding in many cases . Some examples are provided to show the efficiency the approximation effect and the convergence rate of the new method . 
",Use two rational cubics as bounding polynomials. Achieve a convergence rate 7 for a single root. It can achieve linear computational complexity for improved cases.,,,
S0167839614000600," In this paper we introduce triangular subdivision operators which are composed of a refinement operator and several averaging operators where the refinement operator splits each triangle uniformly into four congruent triangles and in each averaging operation every vertex will be replaced by a convex combination of itself and its neighboring vertices . These operators form an infinite class of triangular subdivision schemes including Loop s algorithm with a restricted parameter range and the midpoint schemes for triangular meshes . We analyze the smoothness of the resulting subdivision surfaces at their regular and extraordinary points by generalizing an established technique for analyzing midpoint subdivision on quadrilateral meshes . General triangular midpoint subdivision surfaces are smooth at all regular points and they are also smooth at extraordinary points under certain conditions . We show some general triangular subdivision surfaces and compare them with Loop subdivision surfaces . 
",Smoothness of general triangular midpoint surfaces for arbitrary control meshes. analysis tools for infinite classes of triangular subdivision schemes. New spectral properties of subdivision matrices. Smoothness analysis of Loop subdivision surfaces.,,,
S0167839615000825," A new equivalence notion between non stationary subdivision schemes termed asymptotic similarity which is weaker than asymptotic equivalence is introduced and studied . It is known that asymptotic equivalence between a non stationary subdivision scheme and a convergent stationary scheme guarantees the convergence of the non stationary scheme . We show that for non stationary schemes reproducing constants the condition of asymptotic equivalence can be relaxed to asymptotic similarity . This result applies to a wide class of non stationary schemes . 
",Sufficient condition for convergence of non stationary schemes reproducing constants in terms of difference schemes. Introduction of the relation asymptotical similarity between two non stationary subdivision schemes. Convergence of non stationary subdivision schemes by asymptotical similarity to a convergent stationary scheme.,,,
S0140366415001279," Poor medication adherence is a prevalent medical problem resulting in significant morbidity and mortality especially for elder adults . In this paper we propose a Socialized Prompting System which combines ubiquitous sensors in the smart home and mobile social networks to improve medication adherence . Ubiquitous sensors benefit the seamless monitoring of medication intake behaviors while the mobile social networks contribute to social prompting in a community . The mechanisms of medication monitoring with ubiquitous sensors and the collaborative prompting based on mobile social network are presented . The experimental results showed that the medication adherence of the testing subjects has been improved by using the proposed system . 
",A socialized prompting system improves medication adherence of elders. Ubiquitous sensors benefit the seamless monitoring of medication intake behaviors. The mobile social networks contribute to social prompting in a community.,,,
S0140366414000966," Vehicular safety is an emergent application in inter vehicular communications . As this application is based on fast multi hop message propagation including information such as position direction and speed it is crucial for the data exchange system of the vehicular application to be resilient to security attacks . To make vehicular networks viable and acceptable to consumers we have to design secure protocols that satisfy the requirements of the vehicular safety applications . The contribution of this work is threefold . First we analyze the vulnerabilities of a representative approach named Fast Multi hop Algorithm to the position cheating attack . Second we devise a fast and secure inter vehicular accident warning protocol which is resilient against the position cheating attack . Finally an exhaustive simulation study shows the impact of the attack on the protocol FMBA on delaying the transmission of alert messages . Furthermore we show that our secure solution is effective in mitigating the position cheating attack . 
",We analyze vulnerabilities of an approach for alert message broadcasting to the position cheating attack. We demonstrate how the position cheating attack delays the transmission of alert messages. We propose a solution based on collaboration among neighbor vehicles to detect nodes cheating about their position. We demonstrate how our detection mechanism ensures a correct estimation of vehicles transmission range. Our detection mechanism allows a quick broadcast of the alert message.,,,
S0141938213000735," The current study investigates the effects of 3D displays . People experienced superior fidelity and brightness when they watched 2D still images on a shuttered display rather than on a polarized display . Conversely people experienced greater brightness when they watched 3D still images on a polarized display rather than on a shuttered display . Second people were able to read a smaller font or characters on a shuttered display than on a polarized display . Third people noticed flickering on a shuttered display when they watched 3D images . Fourth people experienced greater brightness when they watched 3D moving images on a shuttered display rather than on a shuttered display . The perceived brightness of the screen positively correlated with enjoyment content satisfaction and 3DTV satisfaction when the viewers watched a 3D movie . The flickering on the other hand has a negative correlation with enjoyment and 3DTV satisfaction . 
",We examined the effects of 3D displays shuttered and polarized displays on the viewers. People experience brightness or fidelity differently depending on the image conditions 2D or 3D . Readability on a shuttered display is greater than a polarized display. Brightness positively correlated with the effects on the viewers. Flickering negatively correlated with the effects on the viewers.,,,
S0167839615001004," The Laplace Beltrami operator is the foundation of describing geometric partial differential equations and it also plays an important role in the fields of computational geometry computer graphics and image processing such as surface parameterization shape analysis matching and interpolation . However constructing the discretized Laplace Beltrami operator with convergent property has been an open problem . In this paper we propose a new discretization scheme of the Laplace Beltrami operator over triangulated surfaces . We prove that our discretization of the Laplace Beltrami operator converges to the Laplace Beltrami operator at every point of an arbitrary smooth surface as the size of the triangular mesh over the surface tends to zero . Numerical experiments are conducted which support the theoretical analysis . 
",We construct a localized discrete Laplace Beltrami operator over triangular mesh. Our algorithm is based on the discretization of the heat kernel defined on the surface. Our discretization scheme is point wise convergent for arbitrary triangulated surfaces. We propose a method to estimate the parameters involved in the algorithm adaptively. Experimental results shows that our method outperforms other discretization schemes.,,,
S0141938213000711," The study investigates the effect of attention on prospective memory performance in programmer multitasking incorporating a taxonomical understanding of prospective memory errors . It also presents an analysis of cognitive workload . The results demonstrate that people are more vulnerable to prospective memory performance failure when there is a situation that one is required to devote more attention in monitoring to identify a cue for an intended action . Omission error is the most prominent among all types of prospective memory error . Analysis reveals that when there is a stimulus driven situation of attention capture a performance decrement observed in terms of prospective memory is not necessarily concerned with an increase in CWL . 
",Attention is an important factor concerning cueing of an intended action. Stimulus driven situation of attention capture has facilitative effect. Performance decrement is not associated with an increased cognitive workload. The results have practical significance concerning human errors in multitasking.,,,
S0141938214000304," A series of Bi3 and Gd3 doped ZnB2O4 phosphors were synthesized with solid state reaction technique . X ray diffraction technique was employed to study the structure of prepared samples . Excitation and emission spectra were recorded to investigate the luminescence properties of phosphors . The doping of Bi3 or Gd3 with a small amount does not change the structure of prepared samples remarkably . Bi3 in ZnB2O4 can emit intense broad band purplish blue light peaking at 428nm under the excitation of a broad band peaking at 329nm . The optimal doping concentration of Bi3 is experimentally ascertained to be 0.5mol . The decay time of Bi3 in ZnB2O4 changes from 0.88 to 1.69ms . Gd3 in ZnB2O4 can be excited with 254nm ultraviolet light and yield intense 312nm emission . The optimal doping concentration of Gd3 is experimentally ascertained to be 5mol . The decay time of Gd3 in ZnB2O4 changes from 0.42 to 1.36ms . 
",ZnB2O4 Bi3 is an effective purplish blue phosphor. ZnB2O4 Gd3 is an effective ultraviolet phosphor. This work indicates that ZnB2O4 is a promising host for display applications.,,,
S0098300415300832," This paper presents a fully automatic method for seismic event classification within a sparse regional seismograph network . The method is based on a supervised pattern recognition technique called the Support Vector Machine . The classification relies on differences in signal energy distribution between natural and artificial seismic sources . We filtered seismic records via 20 narrow band pass filters and divided them into four phase windows P P coda S and S coda . We then computed a short term average value for each filter channel and phase window . The 80 discrimination parameters served as a training model for the SVM . We calculated station specific SVM models for 19 on line seismic stations in Finland . The training data set included 918 positive and 3469 negative examples . An independent test period determined method and rules for integrating station specific classification results into network results . Finally we applied the network classification rules to independent evaluation data comprising 5435 fully automatic event determinations 5404 of which had been manually identified as explosions or noise and 31 as earthquakes . The SVM method correctly identified 94 of the non earthquakes and all but one of the earthquakes . The result implies that the SVM tool can identify and filter out blasts and spurious events from fully automatic event solutions with a high level of accuracy . The tool helps to reduce the work load and costs of manual seismic analysis by leaving only a small fraction of automatic event determinations the probable earthquakes for more detailed seismological analysis . The self learning approach presented here is flexible and easily adjustable to the requirements of a denser or wider high frequency network . 
",Fully automatic method for classification of seismic events. The method is based on Support Vector Machine. Effective in filtering out blasts and spurious events from automatic event bulletins. The method is flexible and easily adjustable to denser or wider networks.,,,
S0141938214000936," Fiber optical microendoscopy has recently been an essential medical diagnostic tool for patients in investigating tissues in vivo due to affordable cost high quality imaging performance compact size high speed imaging and flexible movement . Microelectromechanical systems scanner technology has been playing a key role in shaping the miniaturization and enabling high speed imaging of fiber optical microendoscopy for over 20years . In this article both review of MEMS based fiber optical microendoscopy for optical coherence tomography confocal and two photon imaging will be discussed . These advanced optical endoscopic imaging modalities provide cellular and molecular features with deep tissue penetration enabling guided resections and early cancer assessment . 
",We summarize state of the art in MEMS based fiber optical microendoscopy for early cancer assessment. MEMS scanner technology is a key enabling element in miniaturization of microendoscope. Electrostatic actuation is the most popular mechanism to drive MEMS scanners.,,,
S0167642313000452," The CancerGrid approach to software support for clinical trials is based on two principles careful curation of semantic metadata about clinical observations to enable subsequent data integration and model driven generation of trial specific software artefacts from a trial protocol to streamline the software development process . This paper explains the approach presents four varied case studies and discusses the lessons learned . 
",Summary philosophy and lessons of the CancerGrid project and follow ons. Software support for cancer clinical trials and similar data collection exercises. Metadata support to enable subsequent meta analysis. Model driven generation of software artefacts to run trial. Four case studies.,,,
S0141938214000638," Stereoscopic 3D television is now available in the home . However little published information is available on viewer use or experience . In this study 120 people from 29 households were given a new TV and reported on their television viewing and other screen use on a near daily basis over 8weeks . People reported enjoying S3D TV and cinema more than TV and cinema in general but enjoying S3D video games less than video games in general . S3D TV and video games were both associated with an increased though still low level of adverse effects such as headache and eyestrain . I speculate that this may be because video games present a particularly strong conflict between vergence and accommodative demand . 
",120 people reported on TV viewing and other screen use in their home over 8weeks. Viewer reports were compared for conventional 2D and stereoscopic 3D displays. People reported enjoying 3D TV and cinema more than 2D. People reported slightly more adverse effects e.g. headache with 3D.,,,
S0097849316300504," Graphical abstract Using rigid and non rigid registration to correct misalignments in geometry and texture . Two input textured surfaces are captured by RGB D cameras the camera configuration provides initial alignment . Successive rigid and non rigid steps improve it giving a final surface with high quality textures 
",Using color and geometric to nd correspondences in both rigid and non rigid cases. Using color when determining the transformation deformation in aligning surfaces. Proving our algorithm s effectiveness when aligning richly textured surfaces.,,,
S0167819115000472," Community detection has become a fundamental operation in numerous graph theoretic applications . It is used to reveal natural divisions that exist within real world networks without imposing prior size or cardinality constraints on the set of communities . Despite its potential for application there is only limited support for community detection on large scale parallel computers largely owing to the irregular and inherently sequential nature of the underlying heuristics . In this paper we present parallelization heuristics for fast community detection using the Louvain method as the serial template . The Louvain method is a multi phase iterative heuristic for modularity optimization . Originally developed by Blondel et al . the method has become increasingly popular owing to its ability to detect high modularity community partitions in a fast and memory efficient manner . However the method is also inherently sequential thereby limiting its scalability . Here we observe certain key properties of this method that present challenges for its parallelization and consequently propose heuristics that are designed to break the sequential barrier . For evaluation purposes we implemented our heuristics using OpenMP multithreading and tested them over real world graphs derived from multiple application domains . Compared to the serial Louvain implementation our parallel implementation is able to produce community outputs with a higher modularity for most of the inputs tested in comparable number or fewer iterations while providing absolute speedups of up to 16 using 32 threads . 
",Novel parallel heuristics for community detection in large scale graphs. Multi threaded implementations using OpenMP. Thorough experimental evaluation of parallel heuristics on a platform with 32 cores.,,,
S0141938214000365," This study investigates impacts of oxygen flow during the deposition of amorphous indium gallium zinc oxide channel layer with a radio frequency magnetron sputter on the electrical characteristics of the fabricated thin film transistors . Results indicate that as the film was deposited with a higher oxygen flow the transfer curves are positively shifted while the field effect mobility is significantly decreased . To get more insight about the effects channel resistance and the parasitic source to drain resistance of the fabricated devices are extracted using the total resistance method . The extracted a IGZO channel resistance per unit length and RSD are found to increase while the extracted effective mobility is decreased with increasing oxygen flow during sputtering . These observations are postulated to be related the decrease in the In ratio and the increase in the Zn ratio of the a IGZO films with increasing the oxygen flow rate which lead to higher resistivity and lower carrier concentration . The extracted RSD can be comparable with R CH for the devices prepared with high oxygen flow resulting in the roll off of FE as the channel length is shorter than 20 m . 
",Carrier concentration increased with reducing oxygen flow tends to enhance u FE on current reduce V th SS. ch is higher while E is lower in the films with a higher oxygen flow. Increasing O2 flow decreases In atom and increases Zn atom of IGZO films. Increasing O2 flow increases channel resistance and contact resistance. Visible decline in FE as shorter channel length of devices with higher O2 flow.,,,
S0141938215300032," With the spread of portable smart devices social networking services are gaining popularity . At the same time emoticons which can be used a primary tool to deliver the enriched personal feelings are also gaining popularity in the social networking services . Now emoticon markets are much bigger than before since the territory of emoticons broadens the culture and social issues . However provided emoticons from the service providers are difficult to express the exact personalized feeling . Thus users can not edit what they want to express . In this study we propose a new concept of emoticons an editable visual object to resolve above problems . User can edit the components inside the proposed editable visual object and send it to express exact intention . Further we propose an efficient editable visual object description schema to represent and transmit the editable visual object . To prove the performance and efficiency of proposed technique we implement and test the prototype system for the mobile device . As shown in the test results the proposed description method is at most 100 times superior to the compared screen capturing method in the view of transmission bandwidth . The proposed editable visual object can be exploited not only mobile applications but also various fields such as education and medical field . 
",We analyze the drawbacks of current emoticon system. We introduce an editable visual object which can substitute the emoticon. We propose a schema based EVO description method. We propose a differential coding to transmit the EVO. We implement the prototype mobile application to verify the performance the proposed EVO.,,,
S0140366414000887," In wireless communication systems users compete for communication opportunities through a medium access control protocol . Previous research has shown that selfish behavior in medium access games could lead to inefficient and unfair resource allocation . We introduce a new notion of reciprocity in a medium access game and derive the corresponding Nash equilibrium . Further using mechanism design we show that this type of reciprocity can remove unfair inefficient equilibrium solutions . The best response learning method for the reciprocity game framework is studied . It demonstrates that the game converges to the unique and stable Nash equilibrium if the nodes have low collision costs or high psychological sensitivity . For symmetric games the converged Nash equilibrium turns out to be the fair strategy . 
",A new notion of reciprocity in a medium access game is introduced and the corresponding Nash equilibrium is derived. It has been shown that this type of reciprocity can remove unfair inefficient equilibrium solutions. The best response learning method for the reciprocity game framework is studied and simulated. The game converges to the unique Nash equilibrium if the nodes have low collision costs or high psychological sensitivity. For symmetric games the converged Nash equilibrium turns out to be the fair strategy.,,,
S0167839613000551," This paper proposes a generalization of the ordinary de Casteljau algorithm to manifold valued data including an important special case which uses the exponential map of a symmetric space or Riemannian manifold . We investigate some basic properties of the corresponding B zier curves and present applications to curve design on polyhedra and implicit surfaces as well as motion of rigid body and positive definite matrices . Moreover we apply our approach to construct canal and developable surfaces . 
",A Generalization of the ordinary De Casteljau s algorithm and B zier construction for nonlinear data. Main properties and several applications from Dynamics medical imaging and Geometry curve design on polyhedra motion of rigid body and positive definite matrices. Geometric and numerical aspects of the approach and criteria for implementation.,,,
S0140366415002315," High quality online video streaming both live and on demand has become an essential part of many consumers lives . The popularity of video streaming however places a burden on the underlying network infrastructure . This is because it needs to be capable of delivering significant amounts of data in a time critical manner to users . The Video on Demand distribution paradigm uses a unicast independent flow for each user request . This results in multiple duplicate flows carrying the same video assets that only serve to exacerbate the burden placed upon the network . In this paper we present OpenCache a highly configurable efficient and transparent in network caching service that aims to improve the VoD distribution efficiency by caching video assets as close to the end user as possible . OpenCache leverages Software Defined Networking technology to benefit last mile environments by improving network utilisation and increasing the Quality of Experience for the end user . Our evaluation on a pan European OpenFlow testbed uses adaptive bitrate video to demonstrate that with the use of OpenCache streaming applications play back higher quality video and experience increased throughput higher bitrate and shorter start up and buffering times . 
",High quality video streaming has become an essential part of many consumers lives. We designed OpenCache an OpenFlow assisted in network caching service. OpenCache benefits last mile environments by improving network utilisation. OpenCache increases the Quality of Experience for the end user. OpenCache was evaluated on a large pan European OpenFlow testbed with clear benefits.,,,
S0140366414003697," Resource allocation is an important issue that has not been well resolved in multi relay multi user Decode and Forward OFDM systems with subcarrier pairing at relays in the presence of heterogeneous flows i.e . simultaneous real time and non real time traffic . In this paper we address the issue by first formulating it as a joint optimization problem of relay user pair selection subcarrier pair assignment and power allocation and then solving it through dual decomposition and subgradient methods . Asymptotic optimal iterative algorithms with polynomial complexity are proposed with the objective of maximizing sum transmission rate of NRT traffic while providing quality of service guarantee for RT traffic in two cases with total network power constraint and with individual power constraints . Effective selections of initial dual variables and stepsize for the subgradient method are also presented . Simulation results are provided in multiple scenarios and they show that the proposed algorithms outperform the existing schemes in terms of providing QoS requirements of RT users and maximizing sum network transmission rates algorithms designed under total power constraint exploit the power resource much better than those under individual power constraints . 
",Resource allocation with subcarrier pairing under coexistence of RT and NRT traffic. Optimization of relay user selection subcarrier pair assignment and power allocation. Flexible QoS guarantee of real time traffic by system parameter adjustment adaptively. Total power constraint uses power resource better than individual power constraints.,,,
S0098300413001799," Analysis of geophysical borehole data can often be hampered by too much information and noise in the trace leading to subjective interpretation of layer boundaries . Wavelet analysis of borehole data has provided an effective way of mitigating noise and delineating relevant boundaries . We extend wavelet analysis by providing a complete set of code and functions that will objectively block a geophysical trace based on a derivative operator algorithm that searches for inflection points in the bore log . Layer boundaries detected from the operator output are traced back to a zero width operator so that boundaries are consistently and objectively detected . Layers are then classified based on importance and analysis is completed by selecting either total number of layers a portion of the total number of layers selection of minimum layer thickness or layers detected by a specified minimum operator width . We demonstrate the effectiveness of the layer blocking technique by applying it to a case study for alluvial aquifer detection in the Gascoyne River area of Western Australia . 
",Differential wavelet operator for automated borehole blocking. Hierarchical blocking based on layer importance. Layer blocking based on minimum thickness of layers and of total layers. Full Matlab code freely available under Creative Commons.,,,
S0141938215300093," Icon plays a critical role in computer interface design . Studies on icon taxonomy explain the way in which various types of icon represent the objects and provide designers creation rules by which icons are more in line with users cognitive psychology . However along with larger and larger use of icons the previous classification criterion causes the boundary between categories blur . What s more Single classification standard is not able to well illustrate the icons applied in today s computer applications . The purpose of this paper is to present an objective oriented icon taxonomy which proposes to categorize icons into action icon and knowledge icon . To assess this proposition we analyzed a sample of icons that applied in computer interface and suggest precise application domains to both action icon and knowledge icon categories . The results of this practice manifested that action icon and knowledge icon implied a high relation with applied environment and explicated the development trace of computer icons . This work is one of the first to point out the notion of knowledge icon and to highlight the importance of objective of icon application . Findings in this paper could enrich icon use in computer interface design especially provides possible way to improve online knowledge sharing by visual tool like icon . 
",Proposing an icon taxonomy focusing on applied objective. Defining two terms of computer icons action icon and knowledge icon. Analyzing the applied fields of action icon and knowledge icon. Comparing the proposed icon taxonomy with previous studies. Emphasizing the applied environment and location characteristic of knowledge icon.,,,
S0167819114001148," We introduce a region template abstraction and framework for the efficient storage management and processing of common data types in analysis of large datasets of high resolution images on clusters of hybrid computing nodes . The region template abstraction provides a generic container template for common data structures such as points arrays regions and object sets within a spatial and temporal bounding box . It allows for different data management strategies and I O implementations while providing a homogeneous unified interface to applications for data storage and retrieval . A region template application is represented as a hierarchical dataflow in which each computing stage may be represented as another dataflow of finer grain tasks . The execution of the application is coordinated by a runtime system that implements optimizations for hybrid machines including performance aware scheduling for maximizing the utilization of computing devices and techniques to reduce the impact of data transfers between CPUs and GPUs . An experimental evaluation on a state of the art hybrid cluster using a microscopy imaging application shows that the abstraction adds negligible overhead and achieves good scalability and high data transfer rates . Optimizations in a high speed disk based storage implementation of the abstraction to support asynchronous data transfers and computation result in an application performance gain of about 1.13 . Finally a processing rate of 11 730 4K 4K tiles per minute was achieved for the microscopy imaging application on a cluster with 100 nodes . This computation rate enables studies with very large datasets . 
",Region templates RT data management abstraction and runtime system is introduced. It provides a container for data structures used by spatiotemporal applications. RT supports execution on machines with CPU GPU and unified data access interface. Different data I O implementations targeting multiple memory levels are provided. Example application attains high processing rates and good scalability with RT.,,,
S0141938216300026," A new method is developed for the preparation of a transparent electrically conductive hybrid polymer films which maintain their electrical conductivity when subjected to deformation in two and three dimensions including bending stretching and twisting . These films are made by a hybrid roll to roll process where electrically conductive nanofibers are partially embedded in solution cast dielectric polymer films such as poly polyimide or polyurethanes . Nanofibers partially embedded in films can be laser ablated to obtain in plane anisotropic electrical conductivity created in the form of parallel conductive lines separated by non conductive spaces on the film surface . The patterned films can be used for high resolution pixel addressable liquid crystal displays . There are several situations such as wearable sensors curved displays and advanced flexible OLED lightings where elastic extensibility and or permanent deformability are desired without significant loss of electrical conductivity . To satisfy this demand the multifunctional films can be made elastic thermally deformable with the judicious choice of materials comprising the nanofibers and the matrix they are embedded in . The multifunctional transparent conductive films are easily manufactured through a low cost continuous hybrid roll to roll process . Furthermore they can be used in future displays solar cells wearable electronics and skin attached sensors requiring flexibility as well as stretchability and more desirably subsequent recovery after cessation of stress and strains without any loss of electrical conductivity . 
",Roll to roll line to produce transparent flexible stretchable electrodes is developed. Nanofibers are embedded in the dielectric and conductive phases as embedded structure suppress delamination under flexing and stretching. Any soluble substrate can be made conductive while maintaining its transparency. With elastomeric substrates stretchable electrode are produced. The thermoplastic electrodes can be thermoformable into parts with complex curvatures.,,,
S0141938215000347," Background The use of colors in visual displays can facilitate visual search and perception but their use may not be effective without consideration of differences in the interpretation of their connoted meanings between individuals of different occupational backgrounds . Objective This study examined color concept associations for Hong Kong Chinese students and white collar workers and then compared their associations with those of two other occupational groups to assess whether occupational background is related to color concept associations . Method A questionnaire survey was used to examine color concept associations among 100 university students and 100 white collar workers in Hong Kong . The participants were presented with 16 safety related concepts and were asked to choose one of 10 colors that best represented each concept . The associations reported by four groups with different occupations were compared . A chi square test was applied to determine whether significant color association existed for each concept . Results Our results revealed that each concept was significantly associated with at least one color . The Hong Kong white collar workers and managerial staff in mainland China both expressed the same color associations for 14 of the tested concepts and differed only over which colors were associated with strong and radiation hazard . These findings generally suggest that populations with similar occupations can have similar color perceptions . However the students and the other three employee groups reported different color associations with the concepts normal off and potential hazard . Conclusions Occupational background is associated with how color is interpreted for its connoted meaning . One should consider occupational background carefully when choosing colors in product designs especially for safety . 
",The associations between ten colors and sixteen concepts were tested for four groups of Chinese with different occupations. Nine out of sixteen most frequent color associations are the same for these groups. People with similar occupations yield similar color concept associations.,,,
S0165168414002953," Structured sparsity approaches have recently received much attention in the statistics machine learning and signal processing communities . A common strategy is to exploit or assume prior information about structural dependencies inherent in the data the solution is encouraged to behave as such by the inclusion of an appropriate regularisation term which enforces structured sparsity constraints over sub groups of data . An important variant of this idea considers the tree like dependency structures often apparent in wavelet decompositions . However both the constituent groups and their associated weights in the regularisation term are typically defined a priori . We here introduce an adaptive wavelet denoising framework whereby a sparsity inducing regulariser is modified based on information extracted from the signal itself . In particular we use the same wavelet decomposition to detect the location of salient features in the signal such as jumps or sharp bumps . Given these locations the weights in the regulariser associated to the groups of coefficients that cover these time locations are modified in order to favour retention of those coefficients . Denoising experiments show that not only does the adaptive method preserve the salient features better than the non adaptive constraints but it also delivers significantly better shrinkage over the signal as a whole . 
",A wavelet shrinkage method is introduced that exploits structured sparsity from the DTCWT. Structured sparsity is enforced by using a hierarchical regulariser. The regulariser is adapted according to the analysed signal to promote preservation of salient features. Results in denoising experiments show improved performance with respect to non adaptive approaches.,,,
S0098300413002124," The Google Maps Earth GIS has been integrated with a microscale meteorological model to improve the system s functionality and ease of use . Almost all the components of the model system including the terrain data processing morphological data generation meteorological data gathering and initialization and displaying visualizing the model results have been improved by using this approach . Different from the traditional stand along model system this novel system takes advantages of enormous resources in map and image data retrieving handling four dimensional data visualization overlaying and many other advanced GIS features that the Google Maps Earth platform has to offer . We have developed modular components for all of the model system controls and data processing programs which are glued together with the JavaScript language and KML XML data . We have also developed small modular software using the Google application program interface to convert the model results and intermediate data for visualizations and animations . Capabilities such as high resolution image street view and 3D buildings in the Google Earth Map are also used to quickly generate small scale vegetation and building morphology data that are required for the microscale meteorological models . This system has also been applied to visualize the data from other instruments such as Doppler wind lidars . Because of the tight integration of the internet based GIS and a microscale meteorology model the model system is more versatile intuitive and user friendly than a stand along system we had developed before . This kind of system will enhance the user experience and also help researchers to explore new phenomena in fine scale meteorology . 
",Meteorological model and Google GIS integration. Meteorological model data preprocessing. Meteorological model results and lidar data visualization. Microscale Meteorological model input data generation and gathering. Novel graphical user interface using GIS via internet.,,,
S0167839613001039," In this work an extension has been performed on the analysis basis of spline based meshfree method to stabilize its solution . The potential weakness of the SBMFM is its numerical instability from using regular grid background mesh . That is if an extremely small trimmed element is produced by the trimming curves that represent boundaries of the analysis domain it can induce an excessively large condition number in global system matrix . To resolve the instability problem the extension technique of the weighted extended B spline is implemented in the SBMFM . The basis functions with very small trimmed supports are extrapolated by neighboring basis functions with some special scheme so that those basis functions can be condensed in the solution process . In order to impose essential boundary conditions in the SBMFM with extended basis Nitsche s method is implemented . Using numerical examples the presented SBMFM with extended basis is shown to be valid and effective . Moreover the condition number of the system is well managed guaranteeing the stability of the numerical analysis . 
",Extension is performed to stabilize the basis of spline based meshfree method. Nitsche s method was employed to impose essential boundary conditions. The validity of the proposed method was proved by error analysis.,,,
S0142061513002585," Conventionally monitoring operating conditions of a power transmission line is accomplished by periodic inspections along this line . This monitoring allows corrective maintenance by finding faults during the inspection . But in more efficient maintenance predictive techniques that are characterized by real time monitoring should be employed . Such predictive techniques allow for verifying the working status of the line by using normal working models to detect faults and fault models for diagnosis . This paper presents a study that used a mathematical model appropriate for application to predictive maintenance of transmission line segments at low cost without the need for sensors distributed along the line and presenting a new indicator of transmission line operation conditions . By tracking the leakage current of transmission lines this model allows for estimating the current line insulation status . Once the current line insulation status is known it is possible to compare it against other future status and verify the progress of the insulation conditions of that line . The model uses a new indicator called MCHO which can detect and diagnose both normal and abnormal operating conditions of a power transmission line . This new indicator is the capacitance of the harmonic frequencies of the transmission line leakage current . The model was validated through measurements obtained on a stretch of transmission line . 
",Mathematical model appropriate for predictive maintenance of transmission line. Based on leakage current the model allows estimating the line insulation status. It is not needed sensors distributed along the line. New indicator of transmission line operation conditions.,,,
S0165168414000644," In this paper we design and analyze a Newton like blind equalization algorithm for the APSK system . Specifically we exploit the principle of minimum entropy deconvolution and derive a blind equalization cost function for APSK signals and optimize it using Newton s method . We study and evaluate the steady state excess mean square error performance of the proposed algorithm using the concept of energy conservation . Numerical results depict a significant performance enhancement for the proposed scheme over well established blind equalization algorithms . Further the analytical excess mean square error of the proposed algorithm is verified with computer simulations and is found to be in good conformation . 
",Formulation of a Newton like blind equalization algorithm for APSK system. Discussion on stationary points of the cost in joint impulse response domain. Discussion on equalizer s tracking performance in non stationary environment. Evaluation of moments in closed form using Nuttal Q function.,,,
S0140366414002801," The Smart Grid is expected to utilize a wireless infrastructure for power data collection in its Advanced Metering Infrastructure applications . One of the options to implement such a network infrastructure is to use a wireless mesh network based on IEEE 802.11s mesh standard . However IEEE 802.11s standard relies on MAC based routing and thus requires the availability of MAC addresses of destinations . Due to large size of AMI networks this creates a broadcast storm problem when such information is to be obtained via Address Resolution Protocol broadcast packets . In this paper we propose a mechanism to significantly alleviate such broadcast storm problem in order to improve the scalability of 802.11s and thus make it better suited for Smart Grid AMI applications . Our contribution is adapting 802.11s standard for addressing ARP broadcast storm problem in a secure and efficient manner . Specifically we utilize the proactive Path Request packet and Path Reply of layer 2 path discovery protocol of 802.11s namely HWMP for piggybacking ARP information . In this way the MAC address resolution is handled during routing tree creation maintenance and hence the broadcasting of ARP requests by the smart meters to learn the MAC address of the data collector is completely eliminated . Furthermore since piggybacking the ARP via PREQ may pose vulnerabilities for possible ARP cache poisoning attacks the data collector also authenticates the messages it sends to SMs by using Elliptic Curve Digital Signature Algorithm . We have extensively analyzed the behavior and overhead of the proposed mechanism using implementation of IEEE 802.11s in ns 3 simulator . The evaluations for both UDP and TCP show that compared to the original ARP broadcast operations our approach reduces the end to end delay significantly without negatively impacting the packet delivery ratio and throughput . 
",We identify ARP broadcast problem in 802.11s. We propose a secure piggybacking based ARP for 802.11s based Smart Grid networks. We propose several modifications to Hybrid Wireless Mesh Protocol HWMP . Piggybacking based ARP reduces the end to end delay significantly.,,,
S0167839613000502," We present a fast algorithm for finding a basis for any rational planar curve that has a complex rational parametrization . We begin by identifying two canonical syzygies that can be extracted directly from any complex rational parametrization without performing any additional calculations . For generic complex rational parametrizations these two special syzygies form a basis for the corresponding real rational curve . In those anomalous cases where these two canonical syzygies do not form a basis we show how to quickly calculate a basis by performing Gaussian elimination on these two special syzygies . We also present an algorithm to determine if a real rational planar curve has a complex rational parametrization . Examples are provided to illustrate our methods . 
",Two special syzygies for complex rational curves. A fast algorithm to compute a basis for complex rational curves. An algorithm to determine if a real rational curve has a complex rational parametrization.,,,
S0045790616300696," Graphical abstract Image graphical abstract Blind Source Separation Forward Blind Source Separation Short Fourier Transform Impulse response Symmetric Adaptive Decorrelation Forward Symmetric Adaptive Decorrelation Adaptive Noise Cancellation Least Mean Square Normalized LMS Frequency Least Mean Square Transform Domain LMS Two channel variable step size forward Two sensor Gauss seidel fast affine projection Time Domain Symmetric Adaptive Decorrelation Mean Square Error Signal to Noise Ratio Segmental signal to noise ratio Mean square error Segmental mean square error Cepstral Distance Mean Opinion Score Decibel Voice activity detector Expectation operator Discrete Time index Mean averaging value of the SegSNR Mean averaging value of the SegMSE Mean averaging value of the CD Real and adaptive impulse responses length Mean averaging value of the CD Sampling frequency . source number 1 source number 2 output number 1 output number 2 post filter number 1 post filter number 2 Direct IR path for speech Direct IR path for noise Speech signal Noise real filter vectors Adaptive filter vectors length of the real impulse responses . channel observation number . d 1 2 . number of IR of each paths . i d 1 2 coefficient index cross IR between each channel with i d 1 2 . noisy observations with d 1 2 . diffuse noise components present with d 1 2 . Fixed step sizes Small positive constant Variances STFT of s1 STFT of s2 STFT of s STFT of b STFT of m1 STFT of m2 STFT of w12 STFT of w21 STFT of f12 STFT of f21 STFT of u1 STFT of u2 STFT of p1 STFT of p2 DSP of m1 DSP of m2 Speech detector Silence detector 
",We propose a new Frequency domain adaptive decorrelating algorithm. The proposed algorithm improves convergence speed even with long adaptive filters. The new algorithm is efficient in Speech quality Enhancement and Acoustic Noise Reduction applications. The proposed algorithm distorts less the speech signal at the output.,,,
S0141938214000067," This paper proposes a power efficient drive circuit for plasma display panels . The proposed circuit reduces reactive power consumption by varying the inductance for energy recovery and by separating the grounds of the sustain and data drivers . Power consumption due to discharge current is reduced by using two soft switching inductors for the pull up switches in the bridge circuit . Power consumption for data addressing is reduced by using a dc voltage source to bias the ground for the sustain driver . The proposed circuit was tested on a 50 full HD single scan PDP which had a sustain discharge gap of 80 m total power consumption to display the dynamic broadcasting content of IEC 62087 was 40W less than that required by the conventional drive circuit and the EMI level for 2 9MHz was reduced significantly . The experimental results demonstrate that a high performance power efficient PDP drive circuit can be built using the proposed method . 
",Proposed circuit reduces reactive and discharge power consumption. Reactive power is reduced by decreasing the energy recovery current. Discharge power is reduced by achieving soft switching of the bridge circuit. Biased scan method was used for reducing power consumption for data address. Total power consumption was decreased by 40W 14.5 .,,,
S0167839614000211," We provide explicit representations of three moving planes that form a basis for a standard Dupin cyclide . We also show how to compute bases for Dupin cyclides in general position and orientation from their implicit equations . In addition we describe the role of moving planes and moving spheres in bridging between the implicit and rational parametric representations of these cyclides . 
",Explicit Representations of three basis elements for a standard cyclide. An algorithm of determining the key information of a cyclide from its implicit equation. An algorithm of computing a basis for a cyclide. The role of moving planes and moving spheres played in cyclide.,,,
S0098300413002719," Data semantics play an extremely significant role in spatial data infrastructures by providing semantic specifications to geospatial data and enabling in this way data sharing and interoperability . By applying on the fly composite geospatial processes on the above data it is possible to produce valuable geoinformation over the web directly available and applicable to a wide range of geo activities of significant importance for the research and industry community . Cloud computing may enable geospatial processing since it refers to among other things efficient computing resources providing on demand processing services . In this context we attempt to provide a design and architectural framework for web applications based on open geospatial standards . Our approach includes in addition to geospatial processing data acquisition services that are essential especially when dealing with satellite images and applications in the area of remote sensing and similar fields . As a result by putting in a common framework all data and geoprocesses available in the Cloud it is possible to combine the appropriate services in order to produce a solution for a specific need . 
",Geospatial services in the Cloud for both spatial data acquisition and processing. Interaction UML diagram representing requests for and responses from OGC services. Multi tier architecture with open source software and OGC standards implementations. Challenge to specify potential geospatial processes for future WPS implementations. From desktop and proprietary web applications to open GIS systems in the Cloud.,,,
S0140366415002339," Social exchange theory proposes that social behavior is the result of an exchange process . The purpose of this exchange is to maximize benefits and minimize costs . Online social networks seem to be an ideal platform for social exchange because they provide an opportunity to keep social relations with a relatively low cost compared to offline relations . This theory was verified positively many times for offline social interactions and we decided to examine whether this theory may be also applied to online social networks . Our research is focused on reciprocity which is crucial for social exchanges because humans keep score assign meaning to exchanges and change their subsequent interactions based on a reciprocity balance . The online social network platform of our choice was Facebook which is one of the most successful online social sites allowing users to interact with their friends and acquaintances . In our study we found strong empirical evidence that an increase in the number of reciprocity messages the actor broadcasts in online social networks increases the reciprocity reactions from his or her audience . This finding allowed for positive verification of the social exchange theory in online communities . Hence it can be stated that our work contributes to theories of exchange patterns in online social networks . 
",Our research is focused on reciprocity which is crucial for social exchanges. The online social network platform of our choice was Facebook which is one of the most successful online social sites. In our study we found strong empirical evidence that an increase in the number of reciprocity messages the actor broadcasts in online social networks increases the reciprocity reactions from his or her audience. This finding allowed for positive verification of the social exchange theory in online communities.,,,
S0140366415002364," Large content providers such as Google Yahoo and Microsoft aim to directly connect with consumer networks and place the content closer to end users . Exchanging traffic directly between end users and content providers can reduce the cost of transit services . However direct connection to all end users is simply not feasible . Content providers by and large still rely on transit services to reach the majority of end users . We argue that routing policies are an important factor in considering the selection of ISPs for content providers . Therefore determining which ISP to peer or use as a transit becomes a key question for content providers . In this paper we formulate the policy aware peering problem in which we determine not only which ISP to connect with but also the kind of peering agreement to establish . We prove that such a policy aware peering problem is NP complete and propose a heuristic algorithm to solve the problem . Further we perform a large scale measurement study of the peering characteristics of five large content providers and evaluate the existing peering connections deployed by the content providers . Our measurement results show that changing the existing peering agreements or adding as little as 3 5 new peering connections can enhance the connection between content providers and end users significantly . 
",We investigate the impact of routing policies and peering policies on the selection of peering connections for content providers. We formulate the policy aware peering problem and prove that the problem is NP complete. We provide a heuristic algorithm to solve policy aware peering problem. We examine the lower bound of peering connections to cover end users in today s Internet. We investigate 5 large content providers and find that adding as little as 3 5 new peering connections can enhance their connection to end users significantly.,,,
S0141938213000474," Recently much research has been performed to guarantee viewer s safety in watching stereoscopic 3D videos . Most of them focused on individual content factors causing viewer discomfort . This paper extends the kinds of content factors and focuses more on the composite rather than individual factors . To analyze them a subjective test for discomfort is performed for four stereoscopic 3D videos . Also all the quantitative values of the factors to be considered are extracted from the contents . The quantitative factors we consider are the amount of disparity the frequency and amount of the disparity changes object movement and the color and luminance information . In addition we also include some situational factors such as story of the contents situation or circumstances of a scene and movement position and direction of camera . We analyze qualitatively by comparing the subject test results and the extracted quantitative factors as well as the situational factors to find when and how much those factors in combination or individually cause viewer discomfort . We summarize and substantially show which factors or their combinations strongly or weakly affect viewer discomfort . 
",This paper s purpose is to find as many factors causing visual discomfort as possible. We extracted possible factors and performed subject tests for four 3D contents. We analyzed by comparing temporarily the test results and the extracted factors. Frequency and abruptness of disparity change were the strongest cause of discomfort. Other factors composite with disparity increased or decreased viewer s discomfort.,,,
S0167839613001040," A new binary four point subdivision scheme is presented which keeps the second order divided difference at the old vertices unchanged when the new vertices are inserted . Using the symbol of the subdivision scheme we show that the limit curve is at least continuous . Furthermore the conditions imposed on the initial points are also discussed thus the given limit functions are both monotonicity preserving and convexity preserving 
",We present a new relaxation of binary four point subdivision scheme. The scheme keeps the second order divided difference at the old vertices unchanged when inserting new vertices. Our limit functions are both monotonicity preserving and convexity preserving. Our limit curve is at least continuous when using the symbol of the scheme.,,,
S0140366416300226," Existing flat peer to peer systems based on distributed hash tables perform unsatisfactorily under churn due to their non hierarchical topology . These flat DHTs experience low lookup success ratio high lookup latency and high bandwidth usage as a consequence of the presence of churn . With this we explore the use of hierarchical DHT specifically the superpeer design in mitigating the effects of churn . To the best of our knowledge we are the first to intensively examine HDHTs with and without high churn through simulations . Using the OMNeT simulator and the OverSim framework we analyze flat and hierarchical DHTs with and without churn . Results show that the implemented HDHTs perform more satisfactorily than a flat DHT because of better fault isolation and smaller cluster sizes at the cost of higher superpeer traffic . HDHTs are more stable as they have better lookup success ratios . They are more efficient as evidenced by lower lookup latencies and lower average node bandwidth usage . They are more scalable since their performance do not degrade significantly even at high population . With this the implemented HDHTs can be utilized to alleviate the effects of churn in mobile networks . 
",This research reviews recent works on flat DHTs under high churn. This research evaluates a flat DHT and a hierarchical DHT with or without churn. This research assesses the performance of two HDHTs with or without churn.,,,
S0097849314001411,"Dehydrated core shell fruits such as jujubes raisins and plums show very complex buckles and wrinkles on their exocarp. It is a challenging task to model such complicated patterns and their evolution in a virtual environment even for professional animators. This paper presents a unified physically based approach to simulate the morphological transformation for the core shell fruits in the dehydration process. A finite element method FEM which is based on the multiplicative decomposition of the deformation gradient into an elastic part and a dehydrated part is adopted to model the morphological evolution. In the method the dehydration pattern can be conveniently controlled through physically prescribed parameters according to the geometry and material of the real fruits. The effects of the parameters on the final dehydrated surface patterns are investigated and summarized in detail. Experiments on jujubes wolfberries raisins and plums are given which demonstrate the efficacy of the method. 
",A unified physically based approach for modeling the deformation in a general dehydration process. A nonlinear finite element method to simulate the dehydration induced morphological transformation. Simple parameters based on the geometry and material of the fruits to control the evolution of surface patterns. A looking up table for efficient simulation.,,,
S0167639314000697," A spectral envelope estimation algorithm is presented to achieve high quality speech synthesis . The concept of the algorithm is to obtain an accurate and temporally stable spectral envelope . The algorithm uses fundamental frequency and consists of F0 adaptive windowing smoothing of the power spectrum and spectral recovery in the quefrency domain . Objective and subjective evaluations were carried out to demonstrate the effectiveness of the proposed algorithm . Results of both evaluations indicated that the proposed algorithm can obtain a temporally stable spectral envelope and synthesize speech with higher sound quality than speech synthesized with other algorithms . 
",A spectral envelope estimator is presented for high quality speech synthesis. The proposed algorithm enables us to obtain the temporally stable spectral envelope. Objective results showed the proposed algorithm was better than other algorithms. Subjective results showed that the proposed algorithm can synthesize natural speech.,,,
S0140366415000328," Efficient message dissemination is of utmost importance to propel the development of useful services and applications in Vehicular ad hoc Networks . In this paper we propose a novel adaptive system that allows each vehicle to automatically adopt the most suitable dissemination scheme in order to fit the warning message delivery policy to each specific situation . Our mechanism uses as input parameters the vehicular density and the topological characteristics of the environment where the vehicles are located in order to decide which dissemination scheme to use . We compare our proposal with respect to two static dissemination schemes and three adaptive dissemination systems . Simulation results demonstrate that our approach significantly improves upon these solutions being able to support more efficient warning message dissemination in all situations ranging from low densities with complex maps to high densities in simple scenarios . In particular RTAD improves existing approaches in terms of percentage of vehicles informed while significantly reducing the number of messages sent thus mitigating broadcast storms . 
",We implemented and assessed RTAD our real time adaptive dissemination system. Our approach uses the number of neighbors to estimate the vehicle density. We tested RTAD under four different urban scenarios. We compared our proposal against two static schemes and three adaptive systems.,,,
S0141938214000195," The aim of the study was to investigate the effect of various character and background colour combinations on cognitive performance during onscreen searching tasks and to identify the best combinations with a multimodal approach of physiological subjective and performance data collection . In the absence of proper character and background colour combination the optimum performance for a cognitive task is greatly affected which in turn affects the productivity of the individual worker or communications among the operators working under the same network through information sharing . This study was designed by selecting six colours i.e . white black yellow red blue and green and subsequently combining them to 16 character and background colour combinations for a searching task . Right and left headed arrows were used as the character for the searching maneuver . Forty four volunteers participated in the experiments . Various eye movement variables legibility rating scale NASA TLX questionnaire searching time and percentage of error were recorded . Subjects performed better wherever a good contrast was there because of a high legibility . A poorly contrasted display affected the physiological variables as well as subjective responses to negative directions . Among the combinations of dark character light background blue and red character on white background is highly recommended and while that for light character dark background white is found to be the best character on blue and green backgrounds . 
",Physiological subjective and performance measures were effective for the study. Eye movement parameters play significant role in interpreting the results. Black background should be used cautiously in display design. Combination of two colours with close luminance value should be avoided. Designing with blue either as background or as character is always advantageous.,,,
S0141938214000791," The relationship between the occurrence of simulator sickness and varying latency in a helmet mounted display was explored in this study . Previous work has always considered latency to be a constant . The aim of this research was to determine if a latency that varied over time would impact the experience of SS for HMD users . An object location task was used while viewing real live video scenes via HMD . A planned comparisons approach was utilized with four experimental conditions 2 of them having constant latency and 2 of them having sinusoidally varying latency . These conditions allowed for the assessment of the effects of constant latency vs. varying latency on the experience of SS . The results indicated that a varying latency is associated with greater experience of SS among HMD users than constant latency . Results also indicated as has other recent research that added constant latency on its own does not appear to be associated with the experience of higher levels of SS in an HMD . 
",Explored the relationship between simulator sickness and latency characteristics. No support found for the relationship between raw latency and simulator sickness. 0.2Hz frequency of latency is associated with higher levels of simulator sickness. Varying amplitude of latency is associated with higher levels of simulator sickness.,,,
S0141938215000542," The Spectral Radiance Piecewise Partition Model is a new model for characterizing Liquid Crystal Displays . According to the additive property of the spectral radiances of the primary displayed color the SRPPM model can be divided into three subspace partitions to calculate the transfer coefficient matrixes with cubic polynomial which based on the relationship between the digital input of the LCD and the spectral radiance of displayed color . The color difference was used to evaluate the accuracy of this model . Comparing with other characterization models for LCD the experimental results show that the average color difference of SRPPM is 0.82 E 76 and 80 of the color differences between color samples are less than 1 E 76 . 
",The SRPPM presents a three piecewise partition characterization model for LCD with the display spectral radiance. The characterization is simplified by the cubic polynomial algorithm. The number of color samples for display characterization is reduced by using SRPPM. The experimental results show the good performance of the SRPPM.,,,
S0141938213000383," Iridiumbispyridinato N C2 picolinate is one typical bluish green phosphor widely used in phosphorescent organic light emitting diodes . In order to optimize its electroluminescent performance 3 6 carbazolyl was introduced into the pyridine ring of the 2 4 difluorophenyl pyridine ligand via a non conjugated CH2 linkage . The generated 3 6 di tert butyl 9 pridine 3 yl methyl 9H carbazole was used as cyclometalating ligand to prepare iridium complex 1 2Ir . In comparison with the case to attach carbazole directly on pyridine this non conjugated CH2 linking strategy avoids the unwanted bathochromic shift of the phosphorescence and improves the solubility of the iridium complex . 2Ir was used as doped emitter to fabricate OLEDs by both spin coating and vacuum evaporation methods . Efficient bluish green electrophosphorescence was obtained with maximum luminance efficiency of 22cd A and 26cd A for the solution processed and vacuum deposited devices respectively which far exceed those of the parent Firpic based device . The improved performance for 2Ir was interpreted in terms of improved charge balance brought by the presence of the carbazole groups in the ligands . 
",New iridium complex 1 with non conjugation peripheral carbazole unit was reported. Solution processed and vacuum deposited organic light emitting diodes were fabricated using 1 as doped emitter. The efficiency of solution processed device was improved to 22cd A.,,,
S0167839615000941," Optimal recursive decomposition is crucial for analyzing designing solving or finding realizations of geometric constraint systems . While the optimal DR planning problem is NP hard even for 2D bar joint constraint systems we describe an algorithm for a broad class of constraint systems that are isostatic or underconstrained . The algorithm achieves optimality by using the new notion of a canonical DR plan that also meets various desirable previously studied criteria . In addition we leverage recent results on Cayley configuration spaces to show that the indecomposable systems that are solved at the nodes of the optimal DR plan by recombining solutions to child systems can be minimally modified to become decomposable and have a small DR plan leading to efficient realization algorithms . We show formal connections to well known problems such as completion of underconstrained systems . Well suited to these methods are classes of constraint systems that can be used to efficiently model design and analyze quasi uniform and self similar layered material structures . We formally illustrate by modeling silica bilayers as body hyperpin systems and cross linking microfibrils as pinned line incidence systems . A software implementation of our algorithms and videos demonstrating the software are publicly available online . 
",Every canonical decomposition is optimal for isostatic or underconstrained graphs. An algorithm to find a canonical and optimal decomposition recombination plan. An efficient technique for optimally modifying large indecomposable systems. Applications of the theory to modeling materials. A collection of open problems in the area.,,,
S0141938214000675," A series of Sm3 and Dy3 doped LaBWO6 phosphors were synthesized by high temperature solid state reaction . Recorded XRD patterns proved that the titled compound in a single phase has been obtained . Sm3 and Dy3 doped LaBWO6 could emit orange and white light respectively . The optimal doping concentration of Sm3 or Dy3 was experimentally ascertained to be 6mol . The critical distance of energy transfer for Sm3 or Dy3 doped sample is 1.540nm . In addition there is no cross energy transfer between the Sm3 and Dy3 ions in the co doped samples . The results indicated that the electric dipole dipole interaction is predominant energy transfer mechanism for concentration quenching of Sm3 or Dy3 doped LaBWO6 phosphor . The charge transfer band was observed in the excitation spectra of Sm3 or Dy3 doped LaBWO6 phosphors . Present investigation indicated that Sm3 and Dy3 doped LaBWO6 can be applied in solid state lighting and LaBWO6 is a promising host for display applications . 
",Sm3 and Dy3 doped LaBWO6 could emit intense orange and white light respectively. Concentration dependence of Sm3 and Dy3 luminescence was investigated in detail. Critical distance and mechanism of the energy transfer were experimentally ascertained.,,,
S0167839615000953," In this paper we discuss the bicubic spline spaces over hierarchical T meshes in detail . The topological explanation of the dimension formula is further explored . We provide three necessary conditions for the completeness of the spline spaces . Based on the three conditions the rule of the refinement of the hierarchical T mesh is given and a basis is constructed . The basis functions are linearly independent and complete which provides a solution for the defects in the former literature . 
",We explore the topologic explanation of the dimension formula. The configuration of the T mesh has been given to guarantee the completeness. We construct a basis. Some elementary applications have been given.,,,
S0164121214002702," This paper proposes an architecture and associated methodology to separate front end UI concerns from back end coding concerns to improve the platform flexibility shorten the development time and increase the productivity of developers . Typical UI development is heavily dependent upon the underlying platform framework or tool used to create it which results in a number of problems . We took a separation based UI architecture and modified it with a domain specific language to support the independence of UI creation thereby resolving some of the aforementioned problems . A methodology incorporating this architecture into the development process is proposed . A climate science application was created to verify the validity of the methodology using modern practices of UX DSLs code generation and model driven engineering . Analyzing related work provides an overview of other methods similar to our method . Subsequently we evaluate the climate science application conclude and detail future work . 
",We propose a new DSL and separation based architecture and development methodology. The approach separates front end UI concerns from back end implementation concerns. A climate science application illustrates the approach and verifies its validity. Evaluation of the approach includes comparative analysis and usability studies. The approach increases developer productivity and enhances UI design flexibility.,,,
S0097849315001387," Estimating surface normals from a single image alone is a challenging problem . Previous work made various simplifications and focused on special cases such as having directional lighting known reflectance maps etc . This is problematic however as shape from shading becomes impractical outside the lab . We argue that addressing more realistic settings requires multiple shading cues to be combined as well as generalized to natural illumination . However this requires coping with an increased complexity of the approach and more parameters to be adjusted . Starting from a novel large scale dataset for training and analysis we pursue a discriminative learning approach to shape from shading . Regression forests enable efficient pixel independent prediction and fast learning . The regression trees are adapted to predicting surface normals by using von Mises Fisher distributions in the leaves . Spatial regularity of the normals is achieved through a combination of spatial features including texton as well as novel silhouette features . The proposed silhouette features leverage the occluding contours of the surface and yield scale invariant context . Their benefits include computational efficiency and good generalization to unseen data . Importantly they allow estimating the reflectance map robustly thus addressing the uncalibrated setting . Our method can also be extended to handle perspective projection . Experiments show that our discriminative approach outperforms the state of the art on various synthetic and real world datasets . 
",We estimate shape and reflectance map of an unknown object from a single image. The object is assumed to have uniform diffuse albedo. The model works in uncalibrated illumination with a perspective projection model. We leverage a novel large scale dataset in a discriminative learning approach. Training uses synthetic data rendered given the estimated lighting.,,,
S0141938214000614," This paper presents an optimized fabrication method for developing a freestanding bridge for RF MEMS switches . In this method the sacrificial layer is patterned and hard baked a 220 C for 3min after filling the gap between the slots of the coplanar waveguide . Measurement results by AFM and SEM demonstrate that this technique significantly improves the planarity of the sacrificial layer reducing the uneven surface to less than 20nm and the homogeneity of the Aluminum thickness across the bridge . Moreover a mixture of O2 Ar and CF4 was used and optimized for dry releasing of the bridge . A large membrane was released without any surface bending . Therefore this method not only simplifies the fabrication process but also improves the surface flatness and edge smoothness of the bridge . This fabrication method is fully compatible with standard silicon IC technology . 
",We significantly improved the planarity of the sacrificial layer. We improve the planarity of the membrane. We analyse the effect of different etchant gases for releasing the bridge. We released a large membrane without any surface bending.,,,
S0141938215300159," It is a challenging task to improve the visual comfort of a stereoscopic 3D image with satisfactory viewing experience . In this paper we propose a visual comfort improvement scheme by adjusting zero disparity plane for projection . The degree of visual discomfort is predicted by considering three factors spatial frequency disparity response and visual attention . Then the selection of an optimal ZDP is guided by the predicted visual discomfort map . Finally the disparity ranges of the crossed and uncrossed disparities are automatically adjusted according to the ZDP as requirements . Experiment results show that the proposed scheme is effective in improving visual comfort while preserving the unchanged depth sensation . 
",Factors from spatial frequency disparity energy and visual attention are considered. An optimal ZDP from the derived visual discomfort map is established. We adjust the convergence plane first based on the depth value of the ZDP. We reproject the disparity ranges of the crossed and uncrossed disparities.,,,
S0140366415003825," We present PrivHab a secure geographic routing protocol that learns about the mobility habits of the nodes of the network and uses this information in a secure manner . PrivHab is designed to operate in areas that lack of network using the store carry and forward approach . PrivHab compares nodes and chooses the best choice to carry messages towards a known geographical location . To achieve a high performance and low overhead PrivHab uses information about the usual whereabouts of the nodes to make optimal routing decisions . PrivHab makes use of cryptographic techniques from secure multi party computation to preserve nodes privacy while taking routing decisions . The overhead introduced by PrivHab is evaluated using a proof of concept implementation and its performance is studied under the scope of a realistic application of podcast distribution . PrivHab is compared through simulation with a set of well known delay tolerant routing algorithms in two different scenarios of remote rural areas . 
",PrivHab is a secure geographic routing protocol that learns the habits of the nodes. PrivHab compares nodes and selects the best to carry a message towards a location. PrivHab makes use of cryptographic techniques to preserve nodes privacy. PrivHab performance is studied under the scope of a podcast distribution application.,,,
S0098300413002975," GIS multicriteria decision analysis techniques are increasingly used in landslide susceptibility mapping for the prediction of future hazards land use planning as well as for hazard preparedness . However the uncertainties associated with MCDA techniques are inevitable and model outcomes are open to multiple types of uncertainty . In this paper we present a systematic approach to uncertainty and sensitivity analysis . We access the uncertainty of landslide susceptibility maps produced with GIS MCDA techniques . A new spatially explicit approach and Dempster Shafer Theory are employed to assess the uncertainties associated with two MCDA techniques namely Analytical Hierarchical Process and Ordered Weighted Averaging implemented in GIS . The methodology is composed of three different phases . First weights are computed to express the relative importance of factors for landslide susceptibility . Next the uncertainty and sensitivity of landslide susceptibility is analyzed as a function of weights using Monte Carlo Simulation and Global Sensitivity Analysis . Finally the results are validated using a landslide inventory database and by applying DST . The comparisons of the obtained landslide susceptibility maps of both MCDA techniques with known landslides show that the AHP outperforms OWA . However the OWA generated landslide susceptibility map shows lower uncertainty than the AHP generated map . The results demonstrate that further improvement in the accuracy of GIS based MCDA can be achieved by employing an integrated uncertainty sensitivity analysis approach in which the uncertainty of landslide susceptibility model is decomposed and attributed to model s criteria weights . 
",Sensitivity and uncertainty analysis for GIS Multicriteria Decision Making is underdeveloped in GIScience. Uncertainties associated with GIS Multicriteria for landslide susceptibility mapping are systematically explored. We develop a novel spatially explicit approach and combine it with Dempster Shafer Theory. The results demonstrate that the accuracy of GIS Multicriteria can be improved through this spatially explicit approach.,,,
S0141938215000967," An increasing number of studies on efficient implementation of vivid and realistic displays are being conducted as liquid crystal displays become widely used in TV applications . For vivid displays the specifications such as wide color gamut and high dynamic range should be implemented in LCDs . However the low transmittance rate of the WCG and the significant costs for the peak luminance capability of the HDR are major obstacles . Hence an RGBW LCD which is capable of increasing the transmission efficiency may be a good platform to overcome these problems . In this paper we estimate the perceived brightness effect of the WCG by using the Helmholtz Kohlrausch effect on RGB and RGBW LCDs . The simulation results showed that the RGBW LCD was more suitable for adopting the WCG than the RGB LCD in terms of the brightness balance of achromatic and chromatic colors and the results were also confirmed by subjective tests . In addition we propose an effective method to implement the HDR display based on the RGBW LCD . The data stretch considering a local adaptation characteristic of a human visual system greatly enhanced the details of the dark regions and the local peak dimming using the white channel analysis and the white channel data itself increased the expressiveness of the peak luminance in irradiative or specular regions to 1500nits . 
",We analyze the better suitability of RGBW LCDs for wide color gamut than RGB LCDs. We propose an effective method to implement the HDR display using RGBW LCDs. Our HDR displays based on RGBW LCDs can reproduce 1500nits peak luminance. Our HDR displays have very high potential for mass production.,,,
S0141938213000401," The materials science and engineering related to the fabrication of conducting polymer thin films and the progress in the development of devices integrated with organic transparent electrodes based on conducting polymers for display applications are reviewed . Transparent electrodes are essential components for many display modules . With the evolution of display technologies conducting polymers are recently emerging as important alternative materials for the fabrication of transparent electrodes . Conducting polymers offer some advantages such as light weight low cost mechanical flexibility and excellent compatibility with plastic substrates for the development of next generation display technologies and in particular are expected to play an important role in the development of flexible display technologies . 
",Fundamental materials science of conducting polymers. Materials engineering of conducting polymer thin films. Application of organic transparent electrodes based on conducting polymers in display devices.,,,
S0140366414003661," Internet Protocol Television traffic streaming over vehicular ad hoc networks challenge could be distinguished by the frequent VANETs topology change paired with real time streaming of IPTV traffic which requires high bandwidth and strict performance service quality . For VANETs to deliver better quality IPTV traffic the network must satisfy the compelling Quality of Service requirement of real time traffic streaming such as jitter bandwidth delay and loss . There are numerous metrics defined in the literature for evaluating multimedia streaming traffic QoS such as video quality metric peak signal to noise ratio moving picture quality metric and many more . However these metrics relied upon the objective approach of quality evaluation that requires the original signal as reference and can not isolate the impact of network impairments on the video quality . Therefore fail to provide any mapping between the network QoS parameters and the respective deteriorated quality of the multimedia traffic . Similarly such procedures are not practically applicable to VANETs whose network characteristics make it practically impossible to access the reference video sequence . Hence in this paper we conduct an experiment to determine the feasibility of delivering a qualitative IPTV service over VANETs . We derived analytical model to quantify the IPTV QoS influencing parameters where we establish relationships between the variables of each parameter and their impacts on the IPTV traffic QoS . Through extensive experiment we evaluate the IPTV transmission QoS parameters to assure a priority for handling bandwidth allocation delay and loss control to a negligible level . 
",We experiment via simulation the feasibility of deploying quality IPTV over VANETs. We derived model that quantifies the network parameters as they affect IPTV QoS. We established a connection between each parameter and their impacts on IPTV QoS. Using MDI DF scheme we outline a method for adequate buffer size configuration.,,,
S0141938214000341," In this study we explored how stereoscopic depth affects performance and user experience in a mobile device with an autostereoscopic touch display . Participants conducted a visual search task with an image gallery application on three layouts with different depth ranges . The task completion times were recorded and the participants were asked to rate their experiences . The results revealed that the image search times were facilitated by a mild depth effect and that too great a depth slowed search times and decreased user experience ratings . 
",We studied subjects interactions with a stereoscopic image gallery application. A 3 inch autostereoscopic touch display was used. We examined the effect of stereoscopic depth on image search time and user experience. The results show that a small depth disparity range of 2.3 3.9arc min allows faster image search times. The results show that a medium depth disparity range of 4.7 7.0arc min makes the image search slower.,,,
S0141938213000723," This study investigated the effects of ambient illumination conditions and background color on visual performance with TFT LCD screens . Results of Experiment 1 indicated that all the three independent variables had significant effects on mean percentage of character identification . Mean character identification performance was best under white light 500 lux and blue background . In general the backgrounds with primary colors had better mean character identification performance than the middle point colors and gray . Results of Experiment 2 indicated that the illumination intensity and background color had significant effects on mean text comprehension performance for reading comprehension . Mean text comprehension performance were higher under 500 lux and blue background . The backgrounds with primary colors had better mean correct answers than the gray background . According to the results white light normal ambient illumination and a background with primal colors seemed to be the optimal conditions . If the yellow ambient light is necessary using blue as the TFT LCD background color of will provide better performance . The Pearson product moment correlation coefficient indicated that short term visual task measurement might be suitable to evaluate the visual performance . 
",Visual performance was best under white light 500 lux and blue background. The backgrounds with primary colors had better visual performance than the gray background. If the yellow light is necessary using blue as the background color of will provide better performance.,,,
S0141938216300166," Three dimensional images are perceived as images that float in front of the screens of 3 D displays . Users should be able to interact with these images instantaneously and accurately in applications where their bodies actually seen by them interact with the images . However conventional techniques using just binocular disparity are too slow and inaccurate . Therefore we propose a new technique where the visually perceived positions of images are obtained from the body movements of users . The feasibility of this technique was evaluated in an experiment using the positions obtained from users as they reached out to touch the images . These positions were closer to the visually perceived positions of the images than those calculated from binocular disparity . These findings demonstrate the feasibility of the proposed technique for 3 D interactive applications . 
",This paper presents a new technique for interactive applications of 3 D displays. The positions of 3 D images were obtained from the body movements of users. Interactions occurred when the users bodies were at the obtained positions. The accuracy and precision of the obtained positions were evaluated. The evaluation results demonstrate the feasibility of the proposed technique.,,,
S0140366415003205," Online Social Networks are the most popular applications in todays Internet and they have changed the way people interact with each other . Understanding the structural properties of OSNs and in particular how users behave when they connect to OSNs is crucial for designing user centered systems . Results about OSNs demonstrated that the relationships that an individual maintains with other people can be organized into a set of circles according to the ego network model . The study of the impact of ego networks structure on the availability patterns of users is seriously limited by the lack of information about users availability patterns . In this work we contribute to fill this gap by analysing availability information of a sample of Facebook users . The data reveal a number of strong temporal dependencies which provide insights into the availability pattern that characterize an ego network . 
",We have defined and implemented a Facebook application to log a Facebook dataset. We have studied and validated the structural properties of the whole dataset and of the Dunbar ego networks. We have analyzed the interactions of the users. The availability of the users in the Dunbar ego networks have been investigated. Our results reveal the presence of the temporal homophily property in the Dunbar ego networks.,,,
S0167839614000089," Splines can be constructed by convolving the indicator function of a cell whose shifts tessellate . This paper presents simple geometric criteria that imply that for regular shift invariant tessellations only a small subset of such spline families yield nested spaces primarily the well known tensor product and box splines . Among the many non refinable constructions are hex splines and their generalization to the Voronoi cells of non Cartesian root lattices . 
",Multivariate splines can be derived by convolving indicator functions. Refinability nestedness is an important property of spline spaces. Refinable convolution derived splines on shift invariant tessellations are rare. Hex splines and their generalizations are not refinable. This is proven via simple geometric criteria for refinability.,,,
S0141938214000316," In order to understand the effect of hole injection on the performance of organic light emitting diodes with electron and hole type hosts used in the emissive layer we fabricated OLEDs based on a green fluorescent 10 2 3 6 7 tetrahydro 1 1 7 7 tetramethyl 1H 5H 11H benzopyropyranoquinolizin 11 one with poly poly and MoO3 as the hole injection layer respectively . Here N N di N N diphenyl benzidine and tris aluminum are respectively used as the host in the emissive layer for comparison . It is clearly found that different hole injection layers play different roles in the adjustment of the electron hole injection and the transport balance thus the different hosts are needed in the emissive layer for high electroluminescence efficiency OLEDs . This means that the selection of appropriate hole injection layers for OLEDs according to the different hosts in the emissive layer is especially important in the fabrication of high efficiency single emissive layer OLEDs . 
",We compared the effect of MoO3 and PEDOT PSS as the hole injection layer HIL on the electroluminescence EL performance. We discussed the effect of different host materials. OLEDs with NPB as the host show higher power efficiency when MoO3 was used as the hole injection layer.,,,
S0142061514000027," During the last years wind power has emerged as one of the most important sources in the power generation share . Due to stringent Grid Code requirements wind power plants should provide ancillary services such as fault ride through and damping of power system oscillations to resemble conventional generation . Through an adequate selection of input output signal pairs WPPs can be effectively used to provide electromechanical oscillations damping . In this paper different analysis techniques considering both controllability and observability measures and input output interactions are compared and critically examined . Recommendations are drawn to select the best signal pairs available from WPPs to contribute to power oscillations damping . Control system design approaches including single input single output and multivariable control are considered . The recommendation of analysis techniques is justified through the tools usage in a test system including a WPP . 
",A comparison of various input output signal selection methods is presented. Wind power plant contribution to power system oscillation damping is studied. Recommendations are drawn to select the best signal pairs available from WPPs. SISO and MIMO control system design approaches are considered.,,,
S0141938213000504," In this paper qualitative colours are named by defining intervals on reference systems built on the Hue Saturation and Lightness colour space . The new model for Qualitative Colour Description distinguishes rainbow colours pale light dark colours and colours in the grey scale . For comparing the qualitative colours described a similarity measure based on interval distances is introduced . HSL intervals for each colour label have been defined experimentally using data obtained in a user survey . Preliminary experiments have also been conducted in order to check that the proposed approach is consistent with human appreciations of colour similarity and complementarity . 
",Colours are named by intervals on reference systems on the HSL colour space. Rainbow colours pale light dark colours and colours in the grey scale appear. A similarity measure based on interval distances SimQCDInt is defined. The SimQCDInt is used for comparing the qualitative colours QCs defined. The QC system and the SimQCDInt are tested proving their effectiveness.,,,
S0141938213000905," We have fabricated two kinds of thin film transistors on the glass substrates using polycrystalline silicon thin films with small and large grain sizes processed by Excimer Laser Annealing and carried out a comparative analysis . The grain size was controlled by nickel atom content in amorphous silicon thin films . With increasing grain size of poly Si thin films the carrier mobility increased from 40.8 to 54.4cm2 Vs and the absolute value of threshold voltage reduced from 2.1 to 1.5V . The observed improvements in the electrical characteristics of poly Si TFTs are attributed mainly to the reduction of defect density rendered by large grain size . These observations indicate that the sputtered nickel atom content in a Si layer is a key parameter in determining the characteristics of ELA processed TFTs fabricated on the glass substrates . 
",Poly Si TFTs were fabricated by ELA process using Ni sputtered a Si films. The content of Ni was a key parameter for the performance of poly Si TFTs. Relationship between the characteristics of TFTs and grain was obtained. We present the Ni content needed for the optimized performance of poly Si TFT.,,,
S0165168415003187," The fractional Fourier transform is one of the most useful tools for the nonstationary signal processing . In this paper the randomized nonuniform sampling and approximate reconstruction of the nonstationary random signals in the fractional Fourier domain are developed . The nonuniform samples are treated as random perturbations from a uniform grid . The samples used for the sinc interpolation reconstruction are placed on another nonuniform grid which is not necessarily equal to the samples originally acquired . When considering the second order random statistic characters the nonuniform sampling is equivalent to the uniform sampling of the signal after a pre filter in the FRFD where the frequency response is related to the characteristic function of the perturbations . The effectiveness of the reconstruction is analyzed and the mean square error is computed by utilizing the equivalent filter system . Furthermore the randomized reconstruction of the chirp period stationary random signal is proposed . At last the minimum MSE on the special cases of the randomized sampling and reconstruction is discussed . The effectiveness of the proposed reconstruction method is verified by the simulation . 
",The randomized nonuniform sampling is equivalent to the uniform sampling after a pre filter. The randomized sinc interpolation reconstruction in the FRFD is proposed. The MSE of the reconstruction is analyzed by the equivalent system in the sense of statistics. The randomized reconstruction of the chirp period stationary signal is represented by a finite summation. Special cases of the randomized sampling and reconstruction are discussed.,,,
S0045794915000255," The consistently linearized eigenproblem plays an important role in stability analysis of structures . Solution of the CLE requires computation of the tangent stiffness matrix and of its first derivative with respect to a dimensionless load parameter denoted as . In this paper three approaches of computation of are discussed . They are based on an analytical expression for the derivative of the element tangent stiffness matrix a load based finite difference approximation and a displacement based finite difference approximation . The convergence rate the accuracy and the computing time of the LBFDA and the DBFDA are compared using the analytical solution as the benchmark result . The numerical investigation consists of the analysis of a circular arch subjected to a vertical point load at the vertex and of a thrust line arch under a uniformly distributed load . The main conclusion drawn from this work is that the DBFDA is superior to the LBFDA . 
",The consistently linearized eigenproblem CLE is solved. Derivative of the tangent stiffness matrix w.r.t. load factor is derived. The aforementioned derivative is also approximated numerically. The displacement based approximation is effective and efficient.,,,
S0167839613001064," Geometric partial differential equations for curves and surfaces are used in many fields such as computational geometry image processing and computer graphics . In this paper a few differential operators defined on space curves are introduced . Based on these operators several second order and fourth order geometric flows for evolving space curves are constructed . Some properties of the changing rates of the arc length of the evolved curves and areas swept by the curves are discussed . Short term and long term behaviors of the evolved curves are illustrated . 
",Several second and fourth order geometric partial differential equations for space curves are constructed. Changing rates of the arc length of the evolved curves and areas swept by the curves are discussed. Short term and long term behaviors of the evolved curves are illustrated.,,,
S0140366414000930," Femtocells offer many advantages in wireless networks such as improved cell capacity and coverage in indoor areas . As these femtocells can be deployed in an ad hoc manner by different consumers in the same frequency band the femtocells can interfere with each other . To fully realize the potential of the femtocells it is necessary to allocate resources to them in such a way that interference is mitigated . We propose a distributed resource allocation algorithm for femtocell networks that is modelled after link state routing protocols . Resource allocation using Link State Propagation consists of a graph formation stage where individual femtocells build a view of the network an allocation stage where every femtocell executes an algorithm to assign OFDMA resources to all the femtocells in the network and local scheduling stage where a femtocell assigns resources to all user equipments based on their throughput requirements . Our evaluation shows that RALP performs better than existing femtocell resource allocation algorithms with respect to spatial reuse and satisfaction rate of required throughput . 
",RALP local scheduler RALP uses messages exchanges and a global allocation algorithm to assign allocation units AUs to the various FAPs. In this article we have proposed a local scheduling scheme that each FAP uses to assign AUs to the various user equipments Ues . The algorithm uses the modulation and coding scheme MCS of the AUs to do the assignment. The global allocation algorithm assigns AUs to the various FAPs. However one FAP might require more AUs than what it has been assigned. If the neighboring FAPs are not using all their assigned AUs then this FAP can borrow AUs from the neighboring FAPs and use it to satisfy its requirements. The borrowing algorithm describes how FAPs can borrow AUs from other FAPs. RALP s main disadvantage is its convergence time as the message exchange rounds depend on the neighborhood graph. In this article we introduce the concept of clusters where the data transfer can proceed after a certain number of message exchange rounds.,,,
S0141938215000955," Multi projector displays allow the realization of large and immersive projection environments by allowing the tiling of projections from multiple projectors . Such tiled displays require real time geometrical warping of the content that is being projected from each projector . This geometrical warping is a computationally intensive operation and is typically applied using high end graphics processing units that are able to process a defined number of projector channels . Furthermore this limits the applicability of such multi projector display systems only to the content that is being generated using desktop based systems . In this paper we propose a platform independent FPGA based scalable hardware architecture for geometric correction of projected content that allows addition of each projector channel at a fractional increase in logic area . The proposed scheme provides real time correction of HD quality video streams and thus enables the use of this technology for embedded and standalone devices . 
",Multi projector rendering allows immersive projection on large projection surfaces. Such multi projector schemes require dedicated GPU based systems to perform rendering. Our work proposes an embedded and standalone system for performing multi projector rendering. The system applied the geometric correction in real time without using LUTs.,,,
S0167839614000193," This paper addresses the problem of determining the symmetries of a plane or space curve defined by a rational parametrization . We provide effective methods to compute the involution and rotation symmetries for the planar case . As for space curves our method finds the involutions in all cases and all the rotation symmetries in the particular case of Pythagorean hodograph curves . Our algorithms solve these problems without converting to implicit form . Instead we make use of a relationship between two proper parametrizations of the same curve which leads to algorithms that involve only univariate polynomials . These algorithms have been implemented and tested in the Sage system . 
",We provide a deterministic method to detect symmetries of rational curves. The method detects all the symmetries of plane rational curves. The method also finds all the involutions of space rational curves. In the space case the method detects also rotational symmetry of PHR curves.,,,
S0141938213000413," In a plasma display panel ramp pulse is used for the reset operation . In this case the stability of ramp slope is very important because the variation of ramp slope leads to the reset failure . However the ramp slope is affected by the image load and operation temperature . In this paper the variation of ramp slope is analyzed and a new ramp driver circuit is proposed . In the proposed driver circuit the effect of image load is compensated for by redesigning the current path of the feedback capacitor . The slope variation caused by temperature variation is reduced by compensating for the threshold voltage shift of a MOSFET . The proposed ramp driver circuit provides multiple ramp slopes by adjusting the control signal and guarantees the stability of ramp slope against the changes of image load and temperature . The variation of ramp slope in the proposed method is reduced by 82.7 compared with the previous switched ramp driver circuit . Moreover the additional hardware cost of the proposed driver circuit is minimal . 
",Ramp slope variation by image load and temperature is analyzed. Slope variation caused by image load is compensated for by redesigning feedback current path in the driver circuit. Slope variation caused by VT shift is compensated for by adjusting the gate voltage. A new and cost effective ramp driver circuit for an AC PDP is proposed.,,,
S0140366414002461," A fundamental goal of datacenter networking is to efficiently interconnect a large number of servers in a cost effective way . Inspired by the commodity servers in today s data centers that come with dual port we consider how to design low cost robust and symmetrical network structures for containerized data centers with dual port servers and low end switches . In this paper we propose a family of such network structure called a DCube including H DCube and M DCube . The DCube consists of one or multiple interconnected sub networks each of which is a compound graph made by interconnecting a certain number of basic building blocks by means of a hypercube like graph . More precisely the H DCube and M DCube utilize the hypercube and 1 m bius cube respectively while the M DCube achieves a considerably higher aggregate bottleneck throughput compared to H DCube . Mathematical analysis and simulation results show that the DCube exhibits graceful performance degradation as the failure rate of server or switch increases . Moreover the DCube significantly reduces the required wires and switches compared to the BCube and fat tree . In addition the DCube achieves a higher speedup than the BCube does for the one to several traffic patterns . The proposed methodologies in this paper can be applied to the compound graph of the basic building block and other hypercube like graphs such as Twisted cube Flip MCube and fastcube . 
",We propose DCube for data centers with dual port servers and low end switches. DCube is a compound graph of a basic building block and hypercube or Mobius graph. DCube significantly reduces the wires and switches compared to BCube and fat tree. The basic ideas also apply if Dcube uses other hypercube like graphs.,,,
S0141938215000943," In general to achieve high compression efficiency a 2D image or a 2D block is used as the compression unit . However 2D compression requires a large memory size and long latency when input data are received in a raster scan order that is common in existing TV systems . To address this problem a 1D compression algorithm that uses a 1D block as the compression unit is proposed . 1D set partitioning in hierarchical trees is an effective compression algorithm that fits the encoded bit length to the target bit length precisely . However the 1D SPIHT can have low compression efficiency because 1D discrete wavelet transform can not make use of the redundancy in the vertical direction . This paper proposes two schemes for improving compression efficiency in the 1D SPIHT . First a hybrid coding scheme that uses different coding algorithms for the low and high frequency bands is proposed . For the low pass band a differential pulse code modulation variable length coding is adopted whereas a 1D SPIHT is used for the high pass band . Second a scheme that determines the target bit length of each block by using spatial correlation with a minimal increase in complexity is proposed . Experimental results show that the proposed algorithm improves the average peak signal to noise ratio by 2.97dB compared with the conventional 1D SPIHT algorithm . With the hardware implementation the throughputs of both encoder and decoder designs are 6.15Gbps and gate counts of encoder and decoder designs are 42.8K and 57.7K respectively . 
",In the hybrid coding scheme DPCM VLC is used for the low pass band and 1D SPIHT is used for the high pass band. DPCM VLC is improved by the additional sub schemes. Target bit length of each block is determined by the block based bit allocation scheme. Block complexity is estimated by using the coding information of the upper block. The proposed 1D SPIHT algorithm is implemented in hardware design.,,,
S0142061513003517," Gravitational search algorithm is based on law of gravity and the interaction between masses . In GSA searcher agents are collection of masses and their interactions are based on Newtonian laws of gravity and motion . In this paper to further improve the optimization performance of GSA opposition based learning is employed in opposition based gravitational search algorithm for population initialization and also for generation jumping . In the present work OGSA is applied for the solution of optimal reactive power dispatch of power systems . Traditionally ORPD is defined as the minimization of active power transmission losses by controlling a number of control variables . ORPD is formulated as a non linear constrained optimization problem with continuous and discrete variables . In this work OGSA is used to find the settings of control variables such as generator voltages tap positions of tap changing transformers and amount of reactive compensation to optimize certain objectives . The study is implemented on IEEE 30 57 and 118 bus test power systems with different objectives that reflect minimization of either active power loss or that of total voltage deviation or improvement of voltage stability index . The obtained results are compared to those yielded by the other evolutionary optimization techniques surfaced in the recent state of the art literature including basic GSA . The results presented in this paper demonstrate the potential of the proposed approach and show its effectiveness and robustness for solving ORPD problems of power systems . 
",Gravitational search algorithm GSA is based on law of gravity and the interaction between masses. To further improve the performance of GSA opposition based learning is employed. Opposition based gravitational search algorithm is applied for ORPD problems. The study is implemented on IEEE 30 57 and 118 bus test power systems. The obtained results are compared to those yielded by the other evolutionary optimization techniques.,,,
S0142061514007686," Evidence shows that a small number of line contingencies in power systems may cause a large scale blackout due to the effects of cascading failures . With the development of new technologies and the growing number of heterogeneous participants a modern smart grid should be able to self heal its internal disturbances by continually performing self assessment to deter detect respond to and restore from unpredictable contingencies . Along this line this research focuses on the problem of how to prevent the occurrence of cascading failures through load shedding by considering heterogeneous shedding costs of grid participants . A fair load shedding algorithm is proposed to solve the problem in a decentralized manner where a load shedding participant need only monitor its own operational status and interact with its neighboring participants . Using an embedded feedback mechanism the fair load shedding algorithm can determine a marginal compensation price for each load shedding participant in real time based on the proportional fairness criterion without knowing the shedding costs of the participants . Such fairly determined compensations can help motivate loaders generators to actively participate in the load shedding in the face of internal disturbances . Finally the properties of the load shedding algorithm are evaluated by carrying out an experimental study on the standard IEEE 30 bus system . The study will offer new insights into emergency planning and design improvement of self healing smart grids . 
",We investigate the problem of how to prevent cascading failures in a smart grid. We present a decentralized control algorithm to achieve fair load shedding. The algorithm takes into consideration the heterogeneity of grid participants. The algorithm can help determine compensations for grid participants in real time. We demonstrate and evaluate the algorithm in standard IEEE 30 bus system.,,,
S0097849314000648,"In this paper we propose a method for estimating the camera pose for an environment in which the intrinsic camera parameters change dynamically. In video see through augmented reality AR technology image based methods for estimating the camera pose are used to superimpose virtual objects onto the real environment. In general video see through based AR cannot change the image magnification that results from a change in the camera s field of view because of the difficulty of dealing with changes in the intrinsic camera parameters. To remove this limitation we propose a novel method for simultaneously estimating the intrinsic and extrinsic camera parameters based on an energy minimization framework. Our method is composed of both online and offline stages. An intrinsic camera parameter change depending on the zoom values is calibrated in the offline stage. Intrinsic and extrinsic camera parameters are then estimated based on the energy minimization framework in the online stage. In our method two energy terms are added to the conventional marker based method to estimate the camera parameters reprojection errors based on the epipolar constraint and the constraint of the continuity of zoom values. By using a novel energy function our method can accurately estimate intrinsic and extrinsic camera parameters. We confirmed experimentally that the proposed method can achieve accurate camera parameter estimation during camera zooming. 
",We propose an intrinsic and extrinsic camera parameter estimation method. We extended a conventional marker based method by adding two energy terms. Camera parameters are estimated accurately using new energy function. The effectiveness of our method is shown in simulated and real environments.,,,
S0140366415003230," The capability of accessing multiple channels through multiple interfaces improve network capacity and is desirable for future Mobile Ad Hoc Networks . However due to the presence of jammers as well as mobility and ad hoc features MANETs require distributed and efficient resource management for channel assignment . To address the channel assignment problem which is a non deterministic polynomial time hard problem we propose a heuristic algorithm called Channel Assignment and JAmmer Mitigation . The CA JAM algorithm assigns a distinct channel for every interface of one station and then all stations exchange the assignment information through beacon frames on every individual interface . When one station receives a beacon the station organizes the information into tables . Therefore each station distributively uses the table to reduce the number of neighboring stations using the same channel to avoid interference which in turn improves the throughput . The tables are also used to learn the disconnected neighbors due to jamming so as to mitigate the effect of jamming and maintain connectivity . CA JAM is fully distributed with no use of control channel or central entity thus it improves connectivity and reduces interference by balancing stations over the available channels while mitigating jamming effects from multi channel multi interface MANETs . We confirm that CA JAM outperforms existing protocols using the OPNET simulator . 
",working for multi interface multi channel MANETs being totally distributed i.e. no CCC or central entity enhancing throughput and reducing interference improving connectivity by balancing stations that are in the same vicinity and using the same channel performing adaptive channel assignment against jamming,,,
S0140366415002480," The buffer sizing problem is a big challenge for high speed network routers to reduce buffer cost without throughput loss . The past few years have witnessed debate on how to improve link utilization of high speed networks where the router buffer size is idealized into dozens of packets . Theoretically the buffer size can be shrunk by more than 100 times . Under this scenario widely argued proposals for TCP traffic to achieve acceptable link capacities mandate three necessary conditions over provisioned core link bandwidth non bursty flows and tens of thousands of asynchronous flows . However in high speed networks where these conditions are insufficient TCP traffic suffers severely from routers with tiny buffers . To explore better performance we propose a new congestion control algorithm called Desynchronized Multi Channel TCP that creates a flow with parallel channels . These channels can desynchronize each other to avoid TCP loss synchronization and they can avoid traffic penalties from burst losses . Over a 10 Gb s large delay network ruled by tiny buffer routers our emulation results show that bottleneck link utilization can reach over 80 with much fewer number of flows . Compared with other TCP congestion control variants DMCTCP can also achieve much better performance in high loss rate networks . Facing the buffer sizing challenge our study is a new step towards the deployment of optical packet switching networks . 
",We identified a problem that the current TCP variants suffer from synchronization caused by tiny buffers at high speed networks. We proposed and implement a new TCP that explores parallelism among multiple channels and TCP desynchronization for high speed networks. We evaluated the performance of the new DMCTCP over different networking environment. We analyzed the optimal performance of the proposed DMCTCP based on theoretical analysis.,,,
S0140366414002497," Having in mind multimedia systems applications we propose a novel model based approach to estimate the clock offset between two nodes on the Internet . Different than current clock offset schemes in the literature which are iterative in nature our scheme is aimed at getting a good non iterative clock offset estimation in real time . In our clock offset estimation approach the One Way Delay measurements are modeled with a shifted gamma distribution representing the current state of the probing link . By using the QQ probability plot technique and linear regression model we estimate the minimum value of the gamma distribution with probability zero . This estimated value represents the clock offset plus network propagation and transmission delay for the corresponding receiving path . End nodes exchange their corresponding minimum estimates and get an improved final clock offset estimate considering the network path asymmetries . Based on real experiments we show that our scheme provides an extremely fast clock offset estimation with lower RMSE and superior stability than NTP and current NTP like state of the art methodologies in the literature Jeske and Sampath Choi and Yoo Adhikari et al . Tsuru et al . . Moreover our proposed scheme is non intrusive easy to implement and targeted as part of more complex real time multimedia distribution protocols requiring a fast and reliable OWD estimates . 
",A new model based scheme for clock offset estimationbetween two nodes is presented. The one way delay is modeled with a gamma distribution representing the actual state of the network link. Real and modeled with zero shift one way delay measurements are QQ plot for the estimation of the clock offset. The proposed model is non intrusive the kernel is not modified and can be implemented at the application layer. The model provides accuracy in the order of 3 5 milliseconds under different traffic conditions.,,,
S0167839613000514," This paper is devoted to introducing a new approach for computing the topology of a real algebraic plane curve presented either parametrically or defined by its implicit equation when the corresponding polynomials which describe the curve are known only by values . This approach is based on the replacement of the usual algebraic manipulation of the polynomials appearing in the topology determination of the given curve with the computation of numerical matrices . Such numerical matrices arise from a typical construction in Elimination Theory known as the B zout matrix which in our case is specified by the values of the defining polynomial equations on several sample points . 
",The topology of the curve is obtained directly from Lagrange interpolation data. Roots of polynomial matrix determinants are obtained as generalized eigenvalues. The algorithm is very useful when the explicit polynomial expressions are huge.,,,
S0167819115000095," The human brain is a complex biological neural network characterised by high degrees of connectivity among neurons . Any system designed to simulate large scale spiking neuronal networks needs to support such connectivity and the associated communication traffic in the form of spike events . This paper investigates how best to generate multicast routes for SpiNNaker a purpose built low power massively parallel architecture . The discussed algorithms are an essential ingredient for the efficient operation of SpiNNaker since generating multicast routes is known to be an NP complete problem . In fact multicast communications have been extensively studied in the literature but we found no existing algorithm adaptable to SpiNNaker . The proposed algorithms exploit the regularity of the two dimensional triangular torus topology and the availability of selective multicast at hardware level . A comprehensive study of the parameters of the algorithms and their effectiveness is carried out in this paper considering different destination distributions ranging from worst case to a real neural application . The results show that two novel proposed algorithms can reduce significantly the pressure exerted onto the interconnection infrastructure while remaining effective to be used in a production environment . 
",We implemented 4 multicast routing algorithms for SpiNNaker. The two most complex algorithms allow for different implementation details. We explored the effects of these implementation details. The exploration allowed for more effective implementation of the algorithms. The enhanced implementations are better suited to be used in production.,,,
S0098300413002720," Machine learning algorithms are a powerful group of data driven inference tools that offer an automated means of recognizing patterns in high dimensional data . Hence there is much scope for the application of MLAs to the rapidly increasing volumes of remotely sensed geophysical data for geological mapping problems . We carry out a rigorous comparison of five MLAs Naive Bayes k Nearest Neighbors Random Forests Support Vector Machines and Artificial Neural Networks in the context of a supervised lithology classification task using widely available and spatially constrained remotely sensed geophysical data . We make a further comparison of MLAs based on their sensitivity to variations in the degree of spatial clustering of training data and their response to the inclusion of explicit spatial information . Our work identifies Random Forests as a good first choice algorithm for the supervised classification of lithology using remotely sensed geophysical data . Random Forests is straightforward to train computationally efficient highly stable with respect to variations in classification model parameter values and as accurate as or substantially more accurate than the other MLAs trialed . The results of our study indicate that as training data becomes increasingly dispersed across the region under investigation MLA predictive accuracy improves dramatically . The use of explicit spatial information generates accurate lithology predictions but should be used in conjunction with geophysical data in order to generate geologically plausible predictions . MLAs such as Random Forests are valuable tools for generating reliable first pass predictions for practical geological mapping applications that combine widely available geophysical data . 
",Robust comparison of five machine learning strategies. Random Forests is a good first choice for classifying lithology. Spatial distribution of training data has considerable influence on predictions. Using coordinates and geophysics data generates accurate and plausible predictions.,,,
S0141938216300178," In the product emotional design era the website homepage design especially the aesthetic design has become a key factor that influences users first impressions attitudes and behaviors . Based on the Stimulus Organism Response theory the current study examines the mechanism through which the aesthetic design of a comprehensive job hunting website homepage affects users satisfaction . Emotion and perceived ease of use were used to describe users internal evaluation to the homepage . The Structural Equation Modeling method was used to test the proposed hypotheses . The empirical evidence shows that aesthetic formality does not have a significant impact on positive arousal but it has positively influences on energetic arousal through its impact on aesthetic appeal . The other hypotheses proposed in this study are all been supported . The additional results i.e . the effects of three key design elements on homepage aesthetics are found . The aim of this study was to provide practical recommendation in the establishment of a pattern of webpage aesthetics that influences users satisfaction . 
",Effect of homepage aesthetics on users satisfaction is studied based on S O R. The effects of key design elements on homepage aesthetics are studied. The evidence is from China. Aesthetic formality has a positive impact on aesthetic appeal.,,,
S0142061514004815," A power outage brings in economic losses for both the customers and the utilities . Studying these unwanted events and making solid predictions about the outcomes of the interruptions has been an attractive area of interest for the researchers for the last couple of decades . By making use of a customer survey study conducted in Finland this paper benefits from both the reported cost data collected from customers and from the analytical data that are available and then presents a new hybrid approach to estimate the customer interruption costs of service sector customer segment . Making use of Value Added information of the customers is a common practice for the cost normalization purposes . This paper verifies the approach by comparing the findings of the customer survey and the econometric model suggested here . This study is a unique source in terms of providing a reliable easy to apply and a straightforward model for calculating the economic impacts of power outages . 
",We analyze the customer interruption costs for service sector customers. It is aimed to come up with an objective and easy to apply model. Customer survey and indirect analytical methods approaches are adopted. Customers are grouped regarding their distinct power consumption characteristics.,,,
S0167839615000795," The boundary representations that are used to represent shape in Computer Aided Design systems create unavoidable gaps at the face boundaries of a model . Although these inconsistencies can be kept below the scale that is important for visualisation and manufacture they cause problems for many downstream tasks making it difficult to use CAD models directly for simulation or advanced geometric analysis for example . Motivated by this need for watertight models we address the problem of converting B rep models to a collection of cubic Clough Tocher splines . These splines allow a watertight join between B rep faces provide a homogeneous representation of shape and also support local adaptivity . We perform a comparative study of the most prominent Clough Tocher constructions and include some novel variants . Our criteria include visual fairness invariance to affine reparameterisations polynomial precision and approximation error . The constructions are tested on both synthetic data and CAD models that have been triangulated . Our results show that no construction is optimal in every scenario with surface quality depending heavily on the triangulation and parameterisation that are used . 
",Watertight conversion of boundary representations to Clough Tocher splines. Comparative study of the most prominent Clough Tocher constructions and some novel variants. Comparison based on visual fairness invariance to affine reparameterisations polynomial precision and approximation error.,,,
S0141938215300482," Large displays enable users to perform several tasks simultaneously . Under such circumstances notification information provided through the concept of ambient displays plays a vital role in assisting users to switch among tasks . This paper presents the experimental results of a notification system design in the peripheral region of large displays . The aim is to provide guidance for notification information design by investigating detection and discrimination performance of human observers when visual notification information is presented away from the foveal region and viewed using peripheral vision . The proposed notification system was designed using an array of glyphs . Each glyph is a small gray square with a fixed size of 60 60 pixels . By changing the gray levels of adjacent glyphs dynamically a glyph array presents a particular dynamic pattern . The experiments involved testing factors that comprised the visual angle size and shape of glyph arrays frequency of temporal modulation phase shift of each pattern and number of stimuli . The results show that glyph arrays are detected accurately if they are larger even at wide viewing angles and that the number of glyphs in a glyph array affects the performance more than the shapes of glyph arrays do . Furthermore the discrimination performance is higher when both the frequency and phase are manipulated simultaneously compared with the case when each of these dimensions is varied separately . When the number of stimuli is set at 8 for example users can maintain an accuracy rate of 70 for the multidimensional design whereas the accuracy rate is only approximately 60 for the single dimensional design . 
",We propose an array of glyphs as a notification system using peripheral vision. The system behaviors were controlled by size rhythm frequency and phase shift. The size of the glyph arrays affects detectability more than location or shape. Phase shift is better than frequency as a coding dimension for discriminability. The results are worst case lower bounds for real applications.,,,
S0140366415002327," Wireless sensor networks adopting static data gathering may suffer from unbalanced energy consumption due to non uniform packet relay . Although mobile data gathering provides a reasonable approach to solving this problem it inevitably introduces longer data collection latency due to the use of mobile data collectors . In the meanwhile energy harvesting has been considered as a promising solution to relieve energy limitation in wireless sensor networks . In this paper we consider a joint design of these two schemes and propose a novel two layer heterogeneous architecture for wireless sensor networks which consists of two types of nodes sensor nodes which are static and powered by solar panels and cluster heads that have limited mobility and can be wirelessly recharged by power transporters . Based on this network architecture we present a data gathering scheme called mobility assisted data gathering with solar irradiance awareness where sensor nodes are clustered around cluster heads that adaptively change their positions according to solar irradiance and the sensing data are forwarded to the data sink by these cluster heads working as data aggregation points . We evaluate the performance of the proposed scheme by extensive simulations and the results show that MADG SIA provides significant improvement in terms of balancing energy consumption and the amount data gathered compared to previous work . 
",Consider a joint design of mobile data gathering and energy replenishment. Propose a novel two layer heterogeneous architecture for wireless sensor networks. Present a mobility assisted data gathering scheme with solar irradiance awareness. Simulation results show that MADG SIA provides significant improvement in terms of balancing energy consumption and the amount data gathered compared to previous work.,,,
S0167839614000818," We show how to find four generic interpolants to a Hermite data set in the complex representation using Pythagorean hodograph curves generated as cuts of degree of Laurent series . The developed numerical experiments have shown that two of these interpolants are simple curves and that these have stable shape in the sense that their topologies persist when the direction of the velocity at each end point changes . Our curves are fair but have different shapes to those of other interpolants . Unlike existing methods our technique allows regular PH interpolants to be found for special collinear Hermite data sets . 
",We introduce a new class of PH curves PH cuts of degree of Laurent series. We show how to find PH skew cut interpolants to a Hermite data set. We show that two of these interpolants are short simple curves with stable shape. Our curves are fair with different shapes to those of other interpolants. We can obtain regular PH interpolants for collinear Hermite data sets.,,,
S0140366415003515," Broadcast scheduling for low duty cycle wireless sensor networks has been extensively studied recently . However existing solutions mainly focused on optimizing delay and total energy consumption without considering load distribution among nodes . Due to limited energy supply for sensor nodes heavily loaded sensors often run out of energy quickly reducing the lifetime of the whole network . In this paper we target at minimizing the maximum transmission load of a broadcast schedule for low duty cycle WSNs subject to the constraint that each node should have the minimum end to end delay under the broadcast schedule . We prove that it is NP hard to find the optimal schedule . Then we devise a Load Balanced Parents Assignment Algorithm that achieves approximation ratio where denotes the maximum number of neighbors that are scheduled to wake up at the same time and is typically a small number in low duty cycle WSNs . Further we introduce how to solve this problem in a distributed manner . Our simulation results reveal that compared with the traditional solutions our proposed LBPA A and distributed solution both exhibit much better average performance in terms of energy fairness total energy consumption and delivery ratio . 
",We are the first to investigate the load balanced minimum end to end delay broadcast scheduling problem LB MEBS for low duty cycle WSNs. We prove that LB MEBS problem is NP hard. We propose an approximation algorithm to address LB MEBS problem. We also propose an efficient distributed solution to address LB MEBS problem. Extensive simulation are conducted to validate the effectiveness of our proposed solutions.,,,
S0098300414002660," We present an algorithm developed for GIS applications in order to produce maps of landside susceptibility in postglacial and glacial sediments in Sweden . The algorithm operates on detailed topographic and Quaternary deposit data . We compare our algorithm to two similar computational schemes based on a global visibility operator and a shadow casting algorithm . We find that our algorithm produces more reliable results in the vicinity of stable material than the global visibility algorithm . We also conclude that our algorithm is more computationally efficient than the other two methods which is important when we may want to assess the effects of uncertainty in the data by evaluating many different models . Our method also provides the possibility to take other data into account . We show how different soil types with different geotechnical properties may be modelled . Our algorithm may also take depth information i.e . the thicknesses of the deposits into account . We thus propose that our method may be used to provide more refined maps than the overview maps in areas where more detailed geotechnical geological data have been acquired . The efficiency of our algorithm suggests that it may replace any global visibility operators used in other applications or processing schemes of gridded map data . 
",We describe a novel method to identify slope and soil criteria of landslides in sensitive clays which is computationally fast and efficient. Provides more reliable results close to stable areas. The model can deposit with laterally varying geotechnical properties. Takes the thickness of the deposits into account.,,,
S0140366415002340," The stock market is a popular topic in Twitter . The number of tweets concerning a stock varies over days and sometimes exhibits a significant spike . In this paper we investigate the relationship between Twitter volume spikes and stock options pricing . We start with the underlying assumption of the Black Scholes model the most widely used model for stock options pricing and investigate when this assumption holds for stocks that have Twitter volume spikes . We find that the assumption is less likely to hold in the time period before a Twitter volume spike and is more likely to hold afterwards . In addition the volatility of a stock is significantly lower after a Twitter volume spike than that before the spike . We also find that implied volatility increases sharply before a Twitter volume spike and decreases quickly afterwards . In addition put options tend to be priced higher than call options . Last we find that right after a Twitter volume spike options may still be overpriced . Based on the above findings we propose a put spread selling strategy for stock options trading . Realistic simulation of a portfolio using one year stock market data demonstrates that even in a conservative setting this strategy achieves a 34.3 gain when taking account of commissions and ask bid spread while S P 500 only increases 12.8 in the same period . 
",We show how Twitter volume spikes relate to stock options pricing. After a Twitter volume spike the volatility of a stock drops significantly. Implied volatility rises sharply before Twitter volume spikes and drops fast later. Right after a Twitter volume spike options may still be overpriced especial puts. We propose successful stock options trading strategies based on the findings above.,,,
S0164121215000680," Test case prioritization assigns the execution priorities of the test cases in a given test suite . Many existing test case prioritization techniques assume the full fledged availability of code coverage data fault history or test specification which are seldom well maintained in real world software development projects . This paper proposes a novel family of input based local beam search adaptive randomized techniques . They make adaptive tree based randomized explorations with a randomized candidate test set strategy to even out the search space explorations among the branches of the exploration trees constructed by the test inputs in the test suite . We report a validation experiment on a suite of four medium size benchmarks . The results show that our techniques achieve either higher APFD values than or the same mean APFD values as the existing code coverage based greedy or search based prioritization techniques including Genetic Greedy and ART in both our controlled experiment and case study . Our techniques are also significantly more efficient than the Genetic and Greedy but are less efficient than ART . 
",We extend ART with the search based algorithm for test case prioritization. Our techniques are as effective as the best search based TCP technique. Our techniques are more efficient than genetic and greedy but not ART. It is the first experiment to evaluate input based search based TCP techniques.,,,
S0140366415003655," Underlying network provides infrastructures for cloud computing in data centers . The server centric architectures integrate network and compute which place routing intelligence on servers . However the existing multi port server based architectures suffer from determined scale and large path length . In this paper we propose FleCube a flexibly connected architecture on multi port servers without using any switches . FleCube is recursively constructed on division of multiple ports in a server by means of complete graph . FleCube benefits data center networks by flexible scale and low diameter as well as large bisection width and small bottleneck degree . Furthermore we develop multi path routing to take advantage of parallel paths between any two servers . MPR adopts random forwarding to distribute traffic load and relieve network congestion . Analysis and comparisons with existing architectures show the advantages of FleCube . Evaluations under different degrees of network traffic demonstrate the merits of FleCube and the proposed routings . 
",Propose novel directly connected architectures. Study the properties behind the novel architecture. Design the single path routing and multi path routing algorithm.,,,
S0045790616300672," This work presents two low power Secure Hash Algorithm 3 designs on Field Programmable Gate Array using embedded Digital Signal Processing slice one for area constrained environments and the other for high speed applications . The seven equations of SHA 3 are logically optimized to three and four stage pipelined organizations for our compact and high speed designs respectively . The maximum parallelism between all the bitwise operations of different stages of SHA 3 is explored with respect to the 48 bit structure of DSP slice . Further Logical Cascade Structure design strategy is proposed in accordance with the DSP slice organization . These optimizations result in saving of resources and at the same time achieve low power with high performance . Our compact design results in saving of 79.10 DSP slices and consumes only 1 7 th of power while 1600 bit DSP design provides 23.57 Gbps throughput and consumes only 1 5 th of power as compared to the conventional SHA 3 designs . 
",Two low power SHA 3 designs are provided on UltraScale FPGA using its embedded Digital Signal Processing DSP slice one for the area constrained environments and the other for high speed applications. All bitwise logical operations of SHA 3 are logically grouped in 48 bit wide parallel operations to get maximum benefit of Xilinx DSP48E2 slice structure. Logical Cascade Structure LCS strategy is used to confine maximum SHA 3 logic within same DSP slice column and also to get maximum benefit from its low power dedicated interconnect. The DSP based compact SHA 3 design utilizes 79.10 less DSP slices and consumes only 1 7th of power while high speed 1600 bit design provides 23.57 Gbps with consumption of only 1 5th of power.,,,
S0167839614000302," New bounds on the magnitudes of the first and second order partial derivatives of rational triangular B zier surfaces are presented . Theoretical analysis shows that the proposed bounds are tighter than the existing ones . The superiority of the proposed new bounds is also illustrated by numerical tests . 
",We present new bounds on the magnitudes of the lower derivatives of rational triangular Bezier surfaces. We theoretically prove that the proposed bounds are tighter than the existing ones. The superiority of the proposed new bounds is also illustrated by two types of numerical tests.,,,
S0167839614000296," We present a simple method for C shaped Hermite interpolation by a rational cubic B zier curve with conic precision . For the interpolating rational cubic B zier curve we derive its control points according to two conic B zier curves both matching the Hermite data and one end curvature of the given Hermite data and the weights are obtained by the two given end curvatures . The conic precision property is based on the fact that the two conic B zier curves are the same when the given Hermite data are sampled from a conic . Both the control points and weights of the resulting rational cubic B zier curve are expressed in explicit form . 
",The method reproduces a conic arc if the input data come from such a conic curve. The method can reproduce a whole conic curve even if it has singularities. The B zier control points have well understood geometrical meaning. The final rational B zier control points and weights are given in explicit form. Each of the tangential angles of the given tangent vectors can be arbitrary up to .,,,
S0141938215300196," Objective Visually induced motion sickness and increased postural sway are two adverse side effects that may occur when viewing motion stimuli . However whether these effects are elevated to a greater extent when viewing stereoscopic 3D motion stimuli compared to 2D stimuli on a TV screen has not been investigated under controlled circumstances . Therefore this study aimed at investigating VIMS and postural sway before during and directly after viewing 2D and 3D motion stimuli on a commonly available TV screen . Method 16 Participants were exposed to an aviation documentary shown in 2D and in 3D on separate occasions . Before during and after exposure VIMS and postural sway were measured . VIMS was quantified by a rating scale giving a single number and by a multi symptom questionnaire that assessed multiple VIMS symptoms separately . Sway path length standard deviations and short range and long range scaling components of the center of pressure were calculated as measures of postural sway . Results VIMS symptom severity as obtained with the single rating scale did not show a significant increase to either 2D or 3D exposure . The multi symptom questionnaire did reveal significant increases in VIMS symptom severity to both 2D and 3D exposure . However VIMS was not significantly more increased in case of 3D exposure compared to 2D exposure . All postural sway measures increased significantly as a result of exposure . None of the postural sway measures was differentially affected to 3D as compared to 2D exposure . Conclusion Viewing 3D motion stimuli did not cause more serious VIMS symptoms compared to viewing motion stimuli in 2D . We attribute this lack of difference to the fact that the 3D effects in this documentary were optimized for viewing in a cinema the projection on the TV screen thus causing quarantining of the visual input . The increase in postural sway irrespective of image type may reflect exploratory behavior allowing the participant to gain more information about self orientation with respect to the virtual environment . 
",Effect of 2D and 3D viewing on visually induced motion sickness VIMS and postural sway was studied. Viewing 2D and 3D stimuli both caused significant but equal increases in postural sway. Viewing 3D stimuli did not clearly induce more VIMS symptoms compared to viewing 2D. Quarantining is proposed to underlie the observation that 3D did not induce more sickness than 2D.,,,
S0140366415000572," Currently more and more mobile terminals embed a number of sensors and generate massive data . Effective utilization to such information can enable people to get more personalized services and also help service providers to sell their products accurately . As the information may contain privacy information of people they are typically encrypted before transmitted to the service providers . This however significantly limits the usability of data due to the difficulty of searching over the encrypted data . To address the above issues in this paper we first leverage the secure kNN technique to propose an efficient and privacy preserving multi feature search scheme for mobile sensing . Furthermore we propose an extended scheme which can personalize query based on the historical search information and return more accurate result . Using analysis we prove the security of the proposed scheme on privacy protection of index and trapdoor and unlinkability of trapdoor . Via extensive experiment on real world cloud systems we validate the performance of the proposed scheme in terms of functionalities computation and communication overhead . 
",We propose a secure multi feature search scheme with low cost on the mobile terminals. We propose an extended scheme to personalize query based on historical search information. We prove the security of the proposed scheme on privacy protection of index and trapdoor and unlinkability of trapdoor.,,,
S0141938215000918," A scalable video coding server can simultaneously provide a single bitstream with a fixed maximum service layer for different kinds of devices having different memory capacity network bandwidth and CPU performance requirements . An efficient hybrid 3D video service scheme is proposed without violation of the SVC standard technology for multiple transmission paths . A dynamic local disparity vector estimation algorithm is used to reflect the motion shift component between stereo views in the inter layer prediction stage of the SVC encoder . To improve the coding efficiency an adaptive search scheme based on distortion rates between corresponding and reference macroblocks is used . Based on experimental results up to 1.41dB of quality improvement using JSVM 9.19 reference software is verified . 
",Field An efficient scheme for hybrid 3D video service technology. Core Algorithm An adaptive search range control mechanism for inter layer prediction. Implementation JSVM 9.19 reference software with standard sequences. Performance Up to 1.41dB of quality improvement with some bit saving. Applications Hybrid broadcasting systems for next generation TV technology.,,,
S0167839613000368," A characterization for spatial Pythagorean hodograph curves of degree 7 with rotation minimizing Euler Rodrigues frames is determined in terms of one real and two complex constraints on the curve coefficients . These curves can interpolate initial final positions and and orientational frames and so as to define a rational rotation minimizing rigid body motion . Two residual free parameters that determine the magnitudes of the end derivatives are available for optimizing shape properties of the interpolant . This improves upon existing algorithms for quintic PH curves with rational rotation minimizing frames which offer no residual freedoms . Moreover the degree 7 PH curves with rotation minimizing ERFs are capable of interpolating motion data for which the RRMF quintics do not admit real solutions . Although these interpolants are of higher degree than the RRMF quintics their rotation minimizing frames are actually of lower degree since they coincide with the ERF . This novel construction of rational rotation minimizing motions may prove useful in applications such as computer animation geometric sweep operations and robot trajectory planning . 
",A characterization of degree 7 PH curves with rotation minimizing Euler Rodrigues frames is developed. The characterization is used to formulate a system of equations for interpolating initial final positions and orientations of a rigid body by a rational rotation minimizing motion. The system incorporates two free parameters for shape optimization of the solutions. Solutions exist for data sets that admit no RRMF quintic interpolants. Although these curves are of slightly higher degree their rational rotation minimizing frames are of lower degree than for RRMF quintics.,,,
S0141938215300391," The development of mobile devices nowadays shows an increasing trend toward interacting with 3D digital content on a 2D touch screen . However many issues regarding the appropriateness of the control mode require further exploration . The experimental design in this study designates displays of two sizes five inches and seven inches with three groups of hand gestures controlling the X Y and Z axis respectively . The three groups of gestures are compared in terms of how they interact with the 3D content . In the experiment 30 adult research subjects twice completed a task that involved rotating three 3D immersive heritage models . Their characteristics completion time subjective evaluation and frequency of gesture change were measured and examined . The results from the experiment and the statistics from a two way Analysis of Variance indicate the display size and the task completion time are inversely related . Under the effect of the Control Display ratio using a smaller display results in a shorter completion time while using a larger display results in a longer completion time tasks with obvious characteristics for the 3D objects require a shorter time to complete but those with no obvious characteristics require more time and using familiar hand gestures leads to a shorter task completion time while using unfamiliar hand gestures leads to a longer completion time . The findings of this study show that the Control Display ratio is an important factor that affects the operational performance of the 3D immersive heritage model s rotation tasks completed with hand gestures on small displays . In addition adaptability and familiarity should be taken into consideration when introducing new hand gestures . Hence the suggestions in this study constitute important guidelines for museums designing technology for the interaction between mobile devices and 3D immersive heritage models . 
",The display size and the task completion time are inversely related. Using a smaller display results in a shorter completion time. Tasks with obvious characteristics for the 3D objects require a shorter time to complete. Using familiar hand gestures leads to a shorter task completion time. The Control Display ratio is an important factor that affects the performance of the 3D rotation tasks. Adaptability and familiarity should be taken into consideration when introducing new hand gestures.,,,
S0140366414002825," The massive integration of renewable energy sources in the power grid ecosystem with the aim of reducing carbon emissions must cope with their intrinsically intermittent and unpredictable nature . Therefore the grid must improve its capability of controlling the energy demand by adapting the power consumption curve to match the trend of green energy generation . This could be done by scheduling the activities of deferrable and or interruptible electrical appliances . However communicating the users needs about the usage of their appliances also leaks sensitive information about their habits and lifestyles thus arising privacy concerns . This paper proposes a framework to allow the coordination of energy consumption without compromising the privacy of the users the service requests generated by the domestic appliances are divided into crypto shares using Shamir Secret Sharing scheme and collected through an anonymous routing protocol by a set of schedulers which schedule the requests by directly operating on the shares . We discuss the security guarantees provided by our proposed infrastructure and evaluate its performance comparing it with the optimal scheduling obtained by means of an Integer Linear Programming formulation . 
",We design a protocol for scheduling appliances when renewable energy is available. User identities and appliance consumption patterns are not exposed to the schedulers. We provide formal proofs of the security guarantees of the proposed protocol. The privacy friendly scheduler yields a scheduling delay similar to the optimal one.,,,
S0045794915001418," This paper focuses on the application of structural topology optimisation technique to design steel perforated I sections as a first attempt to replace the traditional cellular beams and better understand the mechanisms involved when subjected to bending and shear actions . An optimum web opening configuration is suggested based on the results of parametric studies . A FE analysis is further employed to determine the performance of the optimised beam in comparison to the conventional widely used cellular type beam . It is found that the optimised beam overperforms in terms of load carrying capacities deformations and stress intensities . Barriers to the implementation of the topology optimisation technique to the routine design of beam web are highlighted . 
",Structural topology optimisation to design steel I section beam web openings. Numerical analysis and parametric study of local beam sections. Initial suggestion of web opening shapes by the optimal topology. Shear buckling characteristics of optimised beam webs using a nonlinear FEA. Barriers to the implementation of the topology optimisation techniques.,,,
S0141938213000486," The present work describes indium zinc oxide sputtering depositions onto several types of papers using radio frequency and direct current sputtering with a ceramic IZO target . The electrical and optical properties of the resulting materials were optimized by studying the argon and oxygen gas flow rates and the sputtering power effects . At optimal deposition conditions thin films of IZO were achieved with a low sheet resistance and an optical transmittance of ca . 80 in the visible spectrum range . These materials retained these properties for more than 8months . Electrochromic devices with several configurations were built with those conductive papers and life cycling and contrast were measured for the ECDs . These devices exhibited a very good color contrast and electrochromic cyclability up to 30 000 redox cycles . 
",We deposited IZO by DC Sputtering on heat sensitive substrates such as paper. The materials displayed high electric conductivity and optical transparency. Electrochromic materials were deposited using screen printing. Solid state electrochromic devices were assembled showing an excellent performance.,,,
S0045790616300738,"Several methods for collecting psychophysiological data from humans have been developed including galvanic skin response GSR electromyography EMG electroencephalography EEG and the electrocardiogram ECG . This paper proposes a feature extraction method for emotion recognition in EEG based human brain signals. In this research emotions were elicited from subjects using emotion related stimuli from the International Affective Picture System IAPS database. We selected four kinds of emotional stimuli in the arousal valence domain. Raw brain signals were preprocessed using independent component analysis ICA to remove artifacts. We introduced a feature extraction method using LPP and implemented a benchmark based on statistical and frequency domain features. The LPP based results show the highest accuracy when using SVM in the all selected feature set. The results also provide evidence and suggest a way for further developing a more specialized emotion recognition system using brain signals. 
",The use of stimulation strategy may help to enhance the emotion recognition from human brain signals. The late positive potential LPP was analyzed in order to select the features for emotion classification. The LPP based electroencephalography EEG features were selected under multiple frequency bands. The emotion classification was performed by using support vector machine SVM and k nearest neighbors KNN . These findings offer experimental evidence that the LPP components may be possible features for emotion recognition.,,,
S0141938213000462," Menus are commonly employed within user interfaces but are not necessarily a suitable solution for emerging new contexts . For instance for in vehicle displays the use of visually oriented menus creates a clear distraction burden . To investigate how the visual demand of menus varied as a function of their breadth depth and structure a study was conducted following the ISO occlusion protocol . Participants were asked to find and select target words on a touchscreen by navigating menus of varying breadths and depths when options were arranged either in alphabetical order or randomly . Tasks were achieved either with full vision or whilst wearing occlusion goggles enabling only brief opportunities to visually access the touchscreen . Preliminary equations were derived from the data indicating fundamentally different relationships for the visual demand of an interface dependent on whether anticipation can be used during the task . For structured menus the visual demand was a logarithmic function of the breadth of the menus whereas for unstructured menus the relationship was quadratic . Moreover results indicated that for structured menus breadth was favoured over depth as the lowest visual demand was associated with 16 3 menu hierarchies . Conversely for unstructured menus compromise hierarchies were associated with the least visual demand . Conclusions are drawn regarding the setting of boundary conditions for alternative menu structures for use within vehicles . 
",We investigate the design of menu hierarchies for in vehicle user interfaces. Breadth is favoured over depth for structured menus. Breadth and depth are equally favoured for unstructured menus. Guidelines are given regarding acceptable combinations of breadth and depth for menus used in a driving context.,,,
S0167839615000163," We present an efficient adaptive refinement procedure that preserves analysis suitability of the T mesh that is the linear independence of the T spline blending functions . We prove analysis suitability of the overlays and boundedness of their cardinalities nestedness of the generated T spline spaces and linear computational complexity of the refinement procedure in terms of the number of marked and generated mesh elements . 
",We present a new algorithm for the adaptive refinement of T meshes. The algorithm preserves Analysis Suitability and produces nested T spline spaces. Any two generated meshes have an analysis suitable overlay. We prove linear complexity of the refinement algorithm.,,,
S0141938215000931," This paper proposes a fast mode decision algorithm for 3D High Efficiency Video Coding depth intra coding . In the current 3D HEVC design it is observed that for most of the cases full rate distortion cost search of Bi Partition mode could be skipped since most coding units of depth map are very flat or smooth while Bi Partition modes are designed for CUs with edge or sharp transition . Using the rough RD cost value calculated by HEVC Rough Mode Decision as a selection threshold we propose a fast Bi Partition modes selection algorithm to speed up the encoding process . The test result for the proposed fast algorithm reports a 34.4 encoding time saving with a 0.3 bitrate increase on synthesized views for the All Intra test case and negligible impact under the random access test case . Moreover by simply varying the selection threshold we can make a tradeoff between encoding time saving and bitrate loss based on the requirement of different applications . 
",We can skip most of Bi Partition mode full RD cost value calculation. We use the Smallest Rough RD Cost Value SRCV to speedup the encoder. More than 34 encoding time saving is achieved under all intra configuration. Small encoding time saving is also achieved under random access configuration. Flexibility is provided to tradeoff between encoding time saving and BD rate loss.,,,
S0098300413002951,"The internal orientation of fossil mass occurrences can be exploited as useful source of information about their primary depositional conditions. A series of studies using different kinds of fossils especially those with elongated shape e.g. elongated gastropods deal with their orientation and the subsequent reconstruction of the depositional conditions e.g. paleocurrents and transport mechanisms . However disk shaped fossils like planispiral cephalopods or gastropods were used up to now with caution for interpreting paleocurrents. Moreover most studies just deal with the topmost surface of such mass occurrences due to the easier accessibility. Within this study a new method for three dimensional reconstruction of the internal structure of a fossil mass occurrence and the subsequent calculation of its spatial shell orientation is established. A 234 million years old Carnian Triassic monospecific mass occurrence of the ammonoid Kasimlarceltites krystyni from the Taurus Mountains in Turkey embedded in limestone is used for this pilot study. Therefore a 150 45 140 mm3 block of the ammonoid bearing limestone bed has been grinded to 70 slices with a distance of 2 mm between each slice. By using a semi automatic region growing algorithm of the 3D visualization software Amira ammonoids of a part of this mass occurrence were segmented and a 3D model reconstructed. Landmarks trigonometric and vector based calculations were used to compute the diameters and the spatial orientation of each ammonoid. The spatial shell orientation was characterized by dip and dip direction and aperture direction of the longitudinal axis as well as by dip and azimuth of an imaginary sagittal plane through each ammonoid. The exact spatial shell orientation was determined for a sample of 675 ammonoids and their statistical orientation analyzed i.e. NW SE . The study combines classical orientation analysis with modern 3D visualization techniques and establishes a novel spatial orientation analyzing method which can be adapted to any kind of abundant solid matter. 
",First 3D visualization of an ammonoid mass occurrence from the Upper Triassic. Classical orientation analyses combined with modern 3D visualization techniques. A modern method as template for any kind of abundant fossil or solid matter. Starting points for a discussion on the orientation in fossil deposits during various environmental conditions.,,,
S0141938215300238," In this paper a new data hiding method is proposed based on secret sharing scheme with the DNA exclusive or operator for color images . The DNA XOR secret sharing scheme uses a DNA XOR truth table . Each input value of truth table is evaluated and according to that evaluation highest PSNR value is selected for secret sharing . These selected values are embedded into cover image . Cover image is used as an encryption key in the proposed secret sharing process . In this study the hidden data are divided into three secret shares and embedded into the red green and blue channels of a cover image respectively . In here the DNA XOR operator has been firstly used as secret sharing operator in data hiding literature . Our proposed data hiding method was compared with previous methods . The comparison of these methods shows that our proposed method gives the most successful result . 
",We present a new probabilistic secret sharing scheme by using the DNA XOR n n . We suggest a new reversible data hiding algorithm based on the probabilistic secret sharing scheme for color images. We create an optimum data hiding algorithm by using the DNA XOR secret sharing scheme.,,,
S0097849313000484,"In this paper we present a new approach for generic 3D shape retrieval based on a mesh partitioning scheme. Our method combines a mesh global description and mesh partition descriptions to represent a 3D shape. The partitioning is useful because it helps us to extract additional information in a more local sense. Thus part descriptions can mitigate the semantic gap imposed by global description methods. We propose to find spatial agglomerations of local features to generate mesh partitions. Hence the definition of a distance function is stated as an optimization problem to find the best match between two shape representations. We show that mesh partitions are representative and therefore it helps to improve the effectiveness in retrieval tasks. We present exhaustive experimentation using the SHREC 09 Generic Shape Retrieval Benchmark. 
",We propose a simple and effective partitioning algorithm for 3D meshes. The use of part descriptions enhances the use of global descriptors. We define a distance as an optimization problem including both linear and quadratic constraints. Our experiments show that the partitioning algorithm has a great influence in the final effectiveness of retrieval tasks.,,,
S0141938215000608," 3D video has recently seen a massive increase in exposure in our lives . However differences between the viewing and shooting conditions for a film lead to disparities between the reformed media and the original three dimensional effect which cause severe visual fatigue to viewers and result in headaches and dizziness . In this paper a series of image processing algorithms are introduced to overcome these problems . The image processing pipeline is composed of four steps eye pupil detection stereo correspondence computation saliency map generation and 3D warping . Each step is implemented in an S3DS 3D rendering system and its time complexity is measured . From the results it was found that real time stereoscopic 3D rendering is impossible using only a software implementation because SIFT and optical flow calculation requires a significant amount of time . Therefore these two algorithm blocks should be implemented with hardware acceleration . Fortunately active research is being conducted on these issues and real time processing is expected to become available soon for applications beyond full HD TV screens . In addition it was found that saliency map generation and 3D warping blocks also need to be implemented in hardware for full HD display although they do not have significant time complexity compared to SIFT and optical flow algorithm blocks . 
",We present a real time stereoscopic 3D rendering pipeline to reduce visual fatigue when watching 3D TV. We implement the rendering pipeline in a system based on Linux and Windows. Each algorithm of the pipeline is programmed into the system. Computational complexity of each algorithm blocks are evaluated with sample 720p and 1080p stereo frames. It is found that the real time stereoscopic 3D rendering is impossible with software implementation only.,,,
S0167839613000800," Smooth freeform skins from simple panels constitute a challenging topic arising in contemporary architecture . We contribute to this problem area by showing how to approximate a negatively curved surface by smoothly joined rational bilinear patches . The approximation problem is solved with help of a new computational approach to the hyperbolic nets of Huhnen Venedey and R rig and optimization algorithms based on it . We also discuss its limits which lie in the topology of the input surface . Finally freeform deformations based on Darboux transformations are used to generate smooth surfaces from smoothly joined Darboux cyclide patches in this way we eliminate the restriction to surfaces with negative Gaussian curvature . 
",We approximate negatively curved surfaces by smooth unions of rational bilinear patches. We prove vertex consistency of hyperbolic nets using a CAGD approach. We generate smooth surfaces composed of Darboux cyclide patches.,,,
S0097849313001052," Graphical abstract In this paper we present a novel image based framework capturing and rendering with the lighting found in real world scenes . The framework is based on recent developments in HDR video capture enabling efficient capture of the full dimensionality of the illumination in large environments exhibiting both complex spatial and angular variations . The image shows a comparison between traditional image based lighting and our method . It is evident that the lighting complexity enabled by HDR video based scene capture increases the realism and visual interest in the resulting renderings significantly . 
",We present a production ready systems pipeline for image based capture and processing of real world scenes. High dynamic range video is used for scene capture. We show algorithms for reconstruction and modeling of the geometric and radiometric properties of the scene. We show how our methods can be used to create highly realistic computer graphics renderings.,,,
S0141938214000584," Large screens have become more popular in recent years . Because of the increasing size of displays the amount of information presented in the peripheral visual field has gained importance in many tasks on visual display units . Users of displays are often exposed to some change or difference in luminance in or between different areas of a display which in turn may produce a phenomenon known as discomfort glare . Discomfort glare is likely to affect cognitive performance . The performance in the visual periphery is more susceptible to disturbances as is the case for the performance in the central visual field . This study explored the effects of discomfort glare on detecting and processing peripheral visual information in a complex visual task . The task consisted of comparing the orientation of arrows presented in the central visual field and at 18 in the periphery . The arrows were superimposed on a background video and presented by a projection system in virtual reality . 50 of the presentations were preceded by a mild glare scene with a luminance of 25cd m2 flashed prior to the stimulus . Experimental results of 56 participants were analyzed using the theory of signal detection . A significant difference in detectability of stimuli was obtained when comparing the performance in the two situations of glare . Results show that discomfort glare impairs peripheral visual performance in attending stimuli in a virtual reality environment . We therefore propose to consider discomfort glare as a factor affecting performance in detecting peripheral visual information . Discomfort glare should be included as a quality criterion in rating visual information as is done in present standards of displays and lighting . 
",Glare experiments are achievable for evaluating visual attention in display media. Discomfort glare results in reduced visual performance in all ages. Discomfort glare reduces performance in detecting peripheral information. Discomfort glare as a distracter shares the resource for processing information. To consider discomfort glare as a serious issue in a complex visual environment.,,,
S0167839615000783," We consider a cubic spline space defined over a triangulation with Powell Sabin refinement . The space has some local super smoothness and can be seen as a close extension of the classical cubic Clough Tocher spline space . In addition we construct a suitable normalized B spline representation for this spline space . The basis functions have a local support they are nonnegative and they form a partition of unity . We also show how to compute the B zier control net of such a spline in a stable way . 
",We consider a cubic spline space defined over a Powell Sabin refined triangulation. The space has some local super smoothness and can be seen as a close extension of the classical cubic Clough Tocher spline space. We construct a suitable normalized B spline representation for this spline space. We also show how to compute the B zier control net of such a spline in a stable way.,,,
S0140366415002546," Online social networks such as Twitter have emerged as an important mechanism for individuals to share information and post user generated content . However filtering interesting content from the large volume of messages received through Twitter places a significant cognitive burden on users . Motivated by this problem we develop a new automated mechanism to detect personalised interestingness and investigate this for Twitter . Instead of undertaking semantic content analysis and matching of tweets our approach considers the human response to content in terms of whether the content is sufficiently stimulating to get repeatedly chosen by users for forwarding . This approach involves machine learning against features that are relevant to a particular user and their network to obtain an expected level of retweeting for a user and a tweet . Tweets observed to be above this expected level are classified as interesting . We implement the approach in Twitter and evaluate it using comparative human tweet assessment in two forms through aggregated assessment using Mechanical Turk and through a web based experiment for Twitter users . The results provide confidence that the approach is effective in identifying the more interesting tweets from a user s timeline . This has important implications for reduction of cognitive burden the results show that timelines can be considerably shortened while maintaining a high degree of confidence that more interesting tweets will be retained . In conclusion we discuss how the technique could be applied to mitigate possible filter bubble effects . 
",Twitter information streams present a lot of unfiltered noisy information. The cognitive burden on users viewing these streams is increased. We propose a method for identifying and ranking interesting information. We implement the method by applying it to Twitter timelines. Our crowdsourced validations give confidence that the approach is effective.,,,
S0141938215300421," Electronic visual displays have shown a rapid technological evolution in the last two decades . With reference to the ergonomic requirements for video display terminal workstations at an international level attention is focused on the human system interaction . With reference to visual ergonomics the aim of this study is to assess luminance conditions through in field measurements in order to evaluate luminance and contrast ratios luminance and contrast non uniformities . The assessment was applied to widespread flat screen displays and repeated for fourteen combinations of Contrast Brightness . The analysis carried out by the Authors shows the importance of realizing a simple and quick procedure to determine the performance levels of displays used in VDT workstations . The proposed assessment could be used as a practical tool for staff assigned to assess the risks arising from VDT use in the workplace within the Occupational Health and Safety Assessment Procedure . 
",We have analyzed the International Standards on ergonomics of human system interaction. We have assessed luminance conditions of displays through in field measurements. Displays were rated on 14 different Contrast Brightness combinations. Displays have met the requirements only for some Contrast Brightness combinations. Analysis has shown the importance of a rapid assessment of display performance.,,,
S0167819114000842," Data scientists have applied various analytic models and techniques to address the oft cited problems of large volume high velocity data rates and diversity in semantics . Such approaches have traditionally employed analytic techniques in a streaming or batch processing paradigm . This paper presents CRUCIBLE a first in class framework for the analysis of large scale datasets that exploits both streaming and batch paradigms in a unified manner . The CRUCIBLE framework includes a domain specific language for describing analyses as a set of communicating sequential processes a common runtime model for analytic execution in multiple streamed and batch environments and an approach to automating the management of cell level security labelling that is applied uniformly across runtimes . This paper shows the applicability of CRUCIBLE to a variety of state of the art analytic environments and compares a range of runtime models for their scalability and performance against a series of native implementations . The work demonstrates the significant impact of runtime model selection including improvements of between 2.3 and 480 between runtime models with an average performance gap of just 14 between CRUCIBLE and a suite of equivalent native implementations . 
",A domain specific language and runtime models for on and off line data analytics. Detailed analysis of CRUCIBLE s runtime performance in state of the art environments. Development and detailed analysis of a set of runtime models for new environments. Performance comparison with native implementations demonstrating a 14 average performance gap. Formulation of a primitive in the DSL that permits an analytic to be run over multiple data sources.,,,
S0098300415000928," The internal structure and petrophysical property distribution of fault zones are commonly exceedingly complex compared to the surrounding host rock from which they are derived . This in turn produces highly complex fluid flow patterns which affect petroleum migration and trapping as well as reservoir behavior during production and injection . Detailed rendering and forecasting of fluid flow inside fault zones require high resolution explicit models of fault zone structure and properties . A fundamental requirement for achieving this is the ability to create volumetric grids in which modeling of fault zone structures and properties can be performed . Answering this need a method for generating volumetric fault zone grids which can be seamlessly integrated into existing standard reservoir modeling tools is presented . The algorithm has been tested on a wide range of fault configurations of varying complexity providing flexible modeling grids which in turn can be populated with fault zone structures and properties . 
",The generation of discrete grids for explicit fault zone modeling is presented. The method provides robust handling of grids with pillar faults. The grid accommodates handling of 3D properties and structures in fault zones. The method is compatible with existing reservoir modeling and simulation tools.,,,
S0167639314000156," The RSR2015 database designed to evaluate text dependent speaker verification systems under different durations and lexical constraints has been collected and released by the Human Language Technology department at Institute for Infocomm Research in Singapore . English speakers were recorded with a balanced diversity of accents commonly found in Singapore . More than 151h of speech data were recorded using mobile devices . The pool of speakers consists of 300 participants between 17 and 42years old making the RSR2015 database one of the largest publicly available database targeted for text dependent speaker verification . We provide evaluation protocol for each of the three parts of the database together with the results of two speaker verification system the HiLAM system based on a three layer acoustic architecture and an i vector PLDA system . We thus provide a reference evaluation scheme and a reference performance on RSR2015 database to the research community . The HiLAM outperforms the state of the art i vector system in most of the scenarios . 
",We give a survey of 24 speech databases for text dependent speaker verification. The RSR2015 corpus contains 151h of speech for text dependent speaker verification. 300 balanced gender speakers were recorded in English on mobile devices. Evaluation protocols for the 3 parts of the corpus with different text constraints. Survey of classifiers and results of 2 state of the art systems on the RSR2015.,,,
S0167839614000107," We study a particular class of planar four bar mechanisms FBM which are based on a given quadrilateral . The self motion of FBM consists of two different parts one is the motion of an anti parallelogram whilst the other one is a pure translation with circular paths . We will refer to this translatoric part in this paper only and demonstrate that this translatoric self motion has the following property At any moment the positions of the corresponding four coupler points form quads homothetic to Q . This property can be used to define spatial one parametric motions of an extruded version of the four bar mechanism which again generate quads of coupler points homothetic to Q . As the next step we take an arbitrary saturated chain of quads in space and define the corresponding one parametric spatial motions . Then all can be parametrized by the same parameter t. We will show that these partial motions can be interlinked by spherical 2R joints without locking the one parametric self motion . This way the construction delivers a series of new mechanisms which generalize results on so called Fulleroid linkages . An example based on four quads in space is worked out in detail . 
",Particular four bars have coupler quads homothetic throughout the motion. This property can be transferred to 3 space and defines a spatial mechanism. Any saturated chain of quads defines chains of interlinked spatial mechanisms. This construction delivers new overconstrained mechanisms. They generalize the well known Fulleroid linkages.,,,
S0167839614000764," This paper presents a biarc based subdivision scheme for space curve interpolation . Given a sequence of space points or a sequence of space points and tangent vectors the scheme produces a smooth curve interpolating all input points by iteratively inserting new points and computing new tangent vectors . For each step of subdivision the newly inserted point corresponding to an existing edge is a specified joint point of a biarc curve which interpolates the two end points and the tangents . A provisional tangent is also assigned to the newly inserted point . Each of the tangents for the following subdivision step is further updated as a linear blending of the provisional tangent and the tangent at the respective point of a circle passing through the local point and its two adjacent points . If adjacent four initial points and their initial reference tangent vectors are sampled from a spherical surface the limit curve segment between the middle two initial points exactly lies on the same spherical surface . The biarc based subdivision scheme is proved to be continuous with a nice convexity preserving property . Numerical examples also show that the limit curves are continuous and fair . Several examples are given to demonstrate the excellent properties of the scheme . 
",This article presents a biarc based interpolatory subdivision scheme. The scheme produces convexity preserving spatial limit curves in 3D. The limit curves are proved to be continuous while numerical examples show that they are also smooth and fair. The scheme reproduces circular arcs and spherical curves with control data sampled on a circle and a sphere respectively.,,,
S0165168414000814," Mobile device localization in wireless sensor networks is a challenging task . It has already been addressed when the WiFi propagation maps of the access points are modeled deterministically or estimated using an offline human training calibration . However these techniques do not take into account the environmental dynamics . In this paper the maps are assumed to be made of an average indoor propagation model combined with a perturbation field which represents the influence of the environment . This perturbation field is embedded with a distribution describing the prior knowledge about the environmental influence . The device is localized with Sequential Monte Carlo methods and relies on the estimation of the propagation maps . This inference task is performed online using the observations sequentially with a new online Expectation Maximization based algorithm . The performance of the algorithm is illustrated with Monte Carlo experiments using both simulated data and a true data set . 
",We propose an algorithm to solve the simultaneous localization and mapping problem in wireless sensor networks. We introduce a semiparametric statistical model to take random perturbations into account. The algorithm is based on a newly proposed online Expectation Maximization procedure and on sequential Monte Carlo methods. The performance of the algorithm is illustrated with both simulated data and a true data set.,,,
S0167839614000995," A swept surface is generated from a profile curve and a sweep curve by employing the latter to define a continuous family of transformations of the former . By using polynomial or rational curves and specifying the homogeneous coordinates of the swept surface as bilinear forms in the profile and sweep curve homogeneous coordinates the outcome is guaranteed to be a rational surface compatible with the prevailing data types of CAD systems . However this approach does not accommodate many geometrically intuitive sweep operations based on differential or integral properties of the sweep curve such as the parametric speed tangent normal curvature arc length and offset curves since they do not ordinarily have a rational dependence on the curve parameter . The use of Pythagorean hodograph sweep curves surmounts this limitation and thus makes possible a much richer spectrum of rational swept surface types . A number of representative examples are used to illustrate the diversity of these novel swept surface forms including the oriented translation sweep offset translation sweep generalized conical sweep and oriented involute sweep . In many cases of practical interest these forms also have rational offset surfaces . Considerations related to the automated CNC machining of these surfaces using only their high level procedural definitions are also briefly discussed . 
",A novel family of swept surfaces based on differential integral sweep curve properties is introduced. For Pythagorean hodograph PH sweep curves the swept surfaces admit exact rational representations. Several examples involving the sweep curve parametric speed arc length tangent normal curvature and offsets are presented. Preliminary requirements for automated machining of these surfaces using only their high level procedural definitions are addressed.,,,
S0167819114000659," In this paper we develop a rank mapping algorithm for an icosahedral grid system on a massive parallel computer with the 3 D torus network topology specifically on the K computer . Our aim is to improve the weak scaling performance of the point to point communications for exchanging grid point values between adjacent grid regions on a sphere . We formulate a new rank mapping algorithm to reduce the maximum number of hops for the point to point communications . We evaluate both the new algorithm and the standard ones on the K computer using the communication kernel of the Nonhydrostatic Icosahedral Atmospheric Model a global atmospheric model with an icosahedral grid system . We confirm that unlike the standard algorithms the new one achieves almost perfect performance in the weak scaling on the K computer even for 10 240 nodes . Results of additional experiments imply that the high scalability of the new rank mapping algorithm on the K computer is achieved by reducing network congestion in the links between adjacent nodes . 
",Rank mapping algorithm for an icosahedral grid system is developed. The new algorithm is applicable to the computer with a 3 D torus network topology. Using the new algorithm number of hops does not increase with the number of nodes. The new algorithm achieves almost perfect weak scaling on the K computer. The new algorithm seems to reduce the communication congestion on the K computer.,,,
S0140366415003175,"Some social networks such as LinkedIn and ResearchGate allow user endorsements for specific skills. In this way for each skill we get a directed graph where the nodes correspond to users profiles and the arcs represent endorsement relations. From the number and quality of the endorsements received an authority score can be assigned to each profile. In this paper we propose an authority score computation method that takes into account the relations existing among different skills. Our method is based on enriching the information contained in the digraph of endorsements corresponding to a specific skill and then applying a ranking method admitting weighted digraphs such as PageRank. We describe the method and test it on a synthetic network of 1493 nodes fitted with endorsements. 
",For expertise retrieval purposes we can rely on the endorsements received by members of a social network with respect to some skill. Skills are correlated which may affect individual rankings. An endorsement deduction method is proposed which improves data consistency and completeness. Endorsement deduction makes use of the known correlation among skills. The method is validated in a synthetic network resembling LinkedIn.,,,
S0141938215300068," This study evaluates the effect of presentation media on the performance level of visual fatigue and subjective preference of those taking visuospatial tests . Fifty university students participated and performed three visuospatial short term memory tests and three visuospatial ability tests by using both types of display media . The display medium substantially affected all of the measured variables . On average the paper pencil test scores of the visuospatial short term memory tests were about 10 higher and the answer time was about 20 shorter than those of the PC tablet tests . The average paper pencil test score of the visuospatial ability tests was about 35 higher than the average test score of the PC tablet test . The visuospatial performance was substantially decreased under the PC tablet condition compared with that under the pencil paper condition . In addition visual fatigue was greater when participants used the PC tablet than when they used a pencil and paper . 
",The display medium affects the visuospatial short term memory performance. The visuospatial ability of paper pencil test was higher than the tablet PC test. The visual fatigue of paper pencil test was lower than tablet PC test.,,,
S0140366414000929," This paper examines the problem of rate allocation for multicasting over slow Rayleigh fading channels using network coding . In the proposed model the network is treated as a collection of Rayleigh fading multiple access channels . In this model rate allocation scheme that is based solely on the statistics of the channels is presented . The rate allocation scheme is aimed at minimizing the outage probability . An upper bound is presented for the probability of outage in the fading multiple access channel . A suboptimal solution based on this bound is given . A distributed primal dual gradient algorithm is derived to solve the rate allocation problem . 
",We propose a distributed rate allocation scheme for multicasting with network coding. The network is modeled as a collection of slow Rayleigh fading MACs. The rate allocation scheme is based solely on the statistics of the channels. An upper bound is presented for the probability of outage in the fading MAC. The distributed rate allocation scheme is aimed at minimizing the outage probability.,,,
S0141938213000607," Two novel carbazole anthracene hybrided molecules namely 2 9 ethyl 9H carbazole and 2 7 di 9 ethyl 9H carbazole were designed and synthesized via palladium catalyzed coupling reaction . The anthracene was attached either at the 2 site or at both 2 7 sites of the central carbazole core to tune the conjugation state and the optoelectronic properties of the resultant molecules . Both of them show good solubility in common organic solvents . They also possess relatively high HOMO levels that would facilitate efficient hole injection and be favorable for high power efficiencies when used in organic light emitting devices . AnCz and 2AnCz were used as non doped emitter to fabricate OLEDs by vacuum evaporation . Good performance was achieved with maximum luminance efficiency of 2.61cdA 1 and CIE coordinates of for AnCz and 9.52cdA 1 and for 2AnCz . 
",Synthesis and luminescent properties of carbazole anthracene hybrided molecules were studied. Organic light emitting diodes were fabricated by vacuum evaporation using these materials as non doped emitter. Deep blue electroluminescence with good performance was obtained.,,,
S0140366415000584," At present in mobile sensing environment almost all the existing secure large data objects dissemination algorithms are centralized . The centralized servers publicize the sensing tasks and are also the authorized parties to initiate sensed data dissemination . This paper proposes a novel social role and network coding based security distributed data dissemination algorithm referred as PRXeluge to overcome the shortcomings of existing centralized data dissemination algorithms . Unlike the existing participatory sensing applications in PRXeluge service provider just publicizes the sensing tasks and utilizes a conditional proxy re signature technique to authorize different social roles such as authorized smartphone users to be utilized as a contracted picture reporters which sense the data and directly disseminate the sensed large data . Furthermore PRXeluge proposes the XOR network coding scheme on the basis of Seluge security framework . To maximize the number of successfully decoded packets PRXeluge introduces a neighbor node table to determine the optimal coding scheme . Experimental results reveal that the proposed PRXeluge shows better performance in terms of lower data packet transmission and dissemination delay compared to that of Seluge . Furthermore it is observed from the experiment that the proposed algorithm is stronger as compared to that of centralized scheme and performs the fine grain access control without giving any additional load to subscriber nodes . 
",Social role based distributed Large Data Object dissemination framework is proposed. XOR network coding method is introduced in the proposed algorithm. XOR network coding and security framework are seamlessly integrated into PRXeluge. PRXeluge does not bring any additional load to the subscriber nodes.,,,
S0141938215300172," Data hiding also known as information hiding plays an important role in information security for various purposes . Reversible data hiding is a technique that allows distortion free recovery of both the cover image and the secret information . In this paper we propose a new reversible data hiding scheme that is based on the Sudoku technique and can achieve higher embedding capacity . The proposed scheme allows embedding more secret bits into a pair of pixels while guaranteeing the good quality of the stego image . The experimental results showed that the proposed scheme obtained higher embedding capacity than some other previous schemes . In addition our proposed scheme maintained the good visual quality of the stego image which outperforms some existing schemes . 
",Sudoku technique is used for data embedding. A reversible data hiding scheme that is based on the Sudoku technique and can achieve the higher embedding capacity. The reference matrix is built and applied to obtain better embedding capacity. The experimental results showed that the proposed scheme obtained higher embedding capacity than some other previous schemes. The proposed scheme also achieved more consistent results for the different test images.,,,
S0140366415002492," Software as a service is gaining momentum for all business applications including Unified Communications as a Service . In this context user identity will play a key role in connecting the future fragmented communication suites in both corporations and cloud SaaS providers . However SaaS solutions impose strong security challenges to the enterprise s Identity Management since cloud services need to be provided with the employees identities . UCaaS solutions should therefore enforce security properties such as trust relationship anonymity or control on information disclosure . WebRTC is reinforcing the trend towards cloud based UC by adding real time voice and video capabilities into browsers . WebRTC does not tackle IdM and hence it is not evident how WebRTC based cloud services can meet the corporate requirements on IdM . In this paper we discuss various IdM models for cloud based corporate services and we introduce the major requirements for managing user identities in UCaaS . We assess the impacts of these requirements on WebRTC based UC services . We finally propose a slight modification of WebRTC to meet the corporate requirements on IdM . 
",Dissertation about identity centric UCaaS Proposed models for Identity and Access Management IAM in corporate SaaS List of requirements regarding identity for UC Adaptation of WebRTC to meet the identity requirements of future identity centric UCaaS,,,
S0141938214000298," This paper proposes a novel pixel circuit for high resolution high frame rate and low power AMOLED displays that is implemented with one driving n channel TFT six switching n channel poly Si TFTs and a storage capacitor . The proposed pixel circuit adopts the voltage programming scheme for threshold voltage compensation . Because the whole line time is in use only for charging the data voltage this pixel circuit is applicable to high resolution and frame rate displays . In addition it compensates voltage variation of OLEDs and voltage drop of supply lines at lower power consumption . On the average the non uniformity of a proposed circuit is reduced to 2.5 compared to 7.1 of the previous one at a 240Hz full HD display . On the other hand the compensation voltage error which is caused by feed through and charge injection noises from falling control signals of switching TFTs is much less in the proposed scheme than in the previous 5T2C structure . The average error of the proposed circuit is reduced to 0.18V compared to 0.75V of the previous one . The initialization power consumption of the 7T1C circuit is reduced to 98mW compared to 530mW of the 5T2C circuit and the average dynamic power saving ratio of data drivers is estimated in the 7T1C pixel as 98.7 over the 5T2C one for 24 test images . 
",A novel 7T1C pixel circuit is proposed for high resolution high frame rate and low power AMOLED displays. The whole line time is in use only for charging the data voltage. The current non uniformity is reduced to 2.5 at a 240Hz full HD AMOLED display. The average compensated voltage error is reduced to 0.18V compared to 0.75V of 5T2C. The initialization power consumptions are 98mW and the dynamic power saving is 98.7 over 5T2C.,,,
S0167642314000112," We compare different algorithms for computing eigenvalues and eigenvectors of a symmetric band matrix across a wide range of synthetic test problems . Of particular interest is a comparison of state of the art tridiagonalization based methods as implemented in Lapack or Plasma on the one hand and the block divide and conquer algorithm as well as the block twisted factorization method on the other hand . The BD C algorithm does not require tridiagonalization of the original band matrix at all and the current version of the BTF method tridiagonalizes the original band matrix only for computing the eigenvalues . Avoiding the tridiagonalization process sidesteps the cost of backtransformation of the eigenvectors . Beyond that we discovered another disadvantage of the backtransformation process for band matrices In several scenarios a lot of gradual underflow is observed in the accumulation of the transformation matrix and in the backtransformation step . According to the IEEE 754 standard for floating point arithmetic this implies many operations with subnormal numbers which causes severe slowdowns compared to the other algorithms without backtransformation of the eigenvectors . We illustrate that in these cases the performance of existing methods from Lapack and Plasma reaches a competitive level only if subnormal numbers are disabled . Overall our performance studies illustrate that if the problem size is large enough relative to the bandwidth BD C tends to achieve the highest performance of all methods if the spectrum to be computed is clustered . For test problems with well separated eigenvalues the BTF method tends to become the fastest algorithm with growing problem size . 
",Single core performance of symmetric band eigensolvers is compared experimentally. Tridiagonalizing a band matrix produces subnormal numbers and degrades performance. Methods without tridiagonalizing the full band matrix have performance advantages. For clustered eigenvalues the block divide and conquer BD C method is fastest. For separated eigenvalues and narrow bands the BTF method is fastest.,,,
S0141938215300202," A series of Eu3 activated Li2Mg23 materials were synthesized by high temperature solid state reactions . The phosphor can be effectively excited by 394nm near ultraviolet light and emit intense red light with high color purity . Prepared phosphors can be indexed to LMW with particular lyonsite structure . The occupation of Eu3 in LMW is selective . Most of Eu3 comes into 1A sites without inversion symmetry . The present research suggests that LMW is a suitable host for luminescence applications and Eu3 activated LMW is a promising phosphor for phosphor converted white light emitting diodes . 
",Eu3 doped Li2Mg2 WO4 3 is a promising red phosphor. Eu3 doped Li2Mg2 WO4 3 can be effectively excited with multiple excitation wavelength. The occupation of Eu3 in Li2Mg2 WO4 3 was discussed in detail.,,,
S0140366414003648," The widespread use of mobile and high definition video devices is changing Internet traffic with a significant increase in multimedia content especially video on demand and Internet protocol television . However the success of these services is strongly related to the video quality perceived as by the user also known as quality of experience . This paper reviews current methodologies used to evaluate the quality of experience in a video streaming service . A typical video assessment diagram is described and analyses of the subjective objective and hybrid approaches are presented . Finally considering the moving target scenario of mobile and high definition devices the text outlines challenges and future research directions that should be considered in the measurement and assessment of the quality of experience for video streaming services . 
",Concise and up to date review of quality assessment for video streaming services. Description of a typical video assessment process. Analysis of current research on subjective objective and hybrid QoE assesment. Discussion of future trends and challenges for QoE in video streaming services.,,,
S0140366414000875," Traditional wireless sensor networks are constrained by limited battery energy that powers the sensor nodes which impedes the large scale deployment of WSNs . Wireless power transfer technology provides a promising way to solve this problem . With such novel technology recent works propose to use a single mobile charger traveling through the network fields to replenish energy to every sensor node so that none of the nodes will run out of energy . These algorithms work well in small scale networks . In large scale networks these algorithms however do not work efficiently especially when the amount of energy the MC can provide is limited . To address this issue multiple MCs can be used . In this paper we investigate the minimum MCs problem for two dimensional wireless rechargeable sensor networks i.e . how to find the minimum number of energy constrained MCs and design their recharging routes in a 2D WRSN such that each sensor node in the network maintains continuous work assuming that the energy consumption rate for all sensor nodes are identical . By reduction from the Distance Constrained Vehicle Routing Problem we prove that MinMCP is NP hard . Then we propose approximation algorithms for this problem . Finally we conduct extensive simulations to validate the effectiveness of our algorithms . 
",We are the first to consider the minimum mobile charger problem for 2D wireless rechargeable sensor networks. We prove that MinMCP is NP hard. We propose approximation algorithms to address MinMCP. We conduct extensive simulations to verify our analytical findings.,,,
S0167839613000538," Spiral segments are useful in the design of fair curves . They are important in CAD CAM applications the design of highway and railway routes trajectories of mobile robots and other similar applications . The quintic Pythagorean hodograph curve discussed in this article is polynomial it has the attractive properties that its arc length is a polynomial of its parameter and the formula for its offset is a rational algebraic expression . This paper generalises earlier results on planar PH quintic spiral segments and examines techniques for designing fair curves using the new results . 
",Earlier characterisations of PH quintic spirals are generalised to avoid an endpoint of zero curvature. The analysis allows the design of C shaped PH quintic curves without flatspots. Techniques for designing fair curves using these new spiral segments are presented.,,,
S0167839615000448," The purpose of this article is the construction of a normalized basis for a quadratic condensed Powell Sabin 12 macro element space introduced by Alfeld et al . . The basis functions have a local support they are nonnegative and they form a partition of unity . The construction of this basis is adopted from Dierckx and Speleers and is based on the determination of a set of triangles that must contain a specific set of points . The proposed basis can only be constructed on triangulations with a maximal angle less than 
",B spline basis of the quadratic condensed Powell Sabin 12 spline space. Determination of control points. Discrete and differentiable quasi interpolants.,,,
S0140366415002571," Companies and particularly their Chief Security Officers want to ensure that their Security Policies are followed but this becomes a difficult goal to achieve at the point employees are able to use or bring their personal devices at work in a practice that has been named Bring Your Own Device . Since this BYOD philosophy is being adopted by many companies everyday a number of solutions have appeared in the market so that it can be implemented in a secure way and comply with the Security Policies mentioned above . In this paper we propose a taxonomy to classify the features of BYOD systems . This taxonomy is used to present an overview of BYOD security solutions . Also we describe a novel adaptive and free software system named MUSES able to securely manage BYOD environments . MUSES has been developed to cope with security issues with regard to enterprise security policies but as a user centric tool . It considers users behavior in order to adapt improve and even increase the defined set of security rules . To do this the system applies Machine Learning and Computational Intelligence techniques being also able to predict future security incidences produced by these users . The MUSES framework which has released its first prototype in early 2015 is compared with the most relevant solutions offered by other companies to deal with the same issues remarking the advantages that our system offers with respect to them . 
",Survey on BYOD solutions state of the art . Background in corporate security. Novel user centric self adaptive system. Open source multiplatform system. Architecture description. Self adaptation. Applications of Data Mining in this scope. Applications of Machine Learning in this scope. Applications of Evolutionary Algorithms in this scope. Beyond the state of the art. With respect to existing tools. With respect to the scientific part.,,,
S0167839613000575," Recently a new approach to piecewise polynomial spaces generated by B spline has been presented by T. Dokken T. Lyche and H.F. Pettersen namely Locally Refined splines . In their recent work they define the LR B spline collection and provide tools to compute the space dimension . Here different properties of the LR splines are analyzed in particular the coefficients for polynomial representations and their relation with other properties such as linear independence and the number of B splines covering each element . 
",A polynomial representation formula for the LR B spline collection is shown. The number of functions of the LR B spline collection whose support contains a point is studied. The close relation between the above properties is asserted.,,,
S0140366416300883," We observe that the laborious collection of location fingerprints that could also potentially change with time and space remains a hurdle towards the widespread deployment of indoor and outdoor positioning systems using the Wi Fi . Motivated by this we focus on the error bound analysis of indoor Wi Fi fingerprint based positioning for intelligent Access Point placement optimization by using the Fisher Information Matrix to characterize the relationship between the positioning errors and signal distributions . We first derive the closed form error bounds with respect to different system parameters . Second we use the Simulated Annealing algorithm to conduct the Wi Fi AP placement optimization towards the lowest error bounds . Finally we present the detailed discussion extensive simulation and experiment results . 
",The system relies on the fisher information matrix to derive the error bounds of Wi Fi fingerprint based positioning. The theoretical relationship between the positioning errors and signal distributions is presented. The error bound of Wi Fi fingerprint based positioning is an effective criterion for Wi Fi access point deployment. The intelligent access point optimization approach performs well in time consumption.,,,
S0141938215300135," To study the interface effects on the device performance we fabricated indium gallium zinc oxide thin film transistors with a two stack gate insulator structure . The two stack gate insulator was composed of a thick main insulator and a thin interfacial insulator the main insulator determines the effective permittivity of the gate insulator and the interfacial insulator regulates the gate active interface properties . The a IGZO TFTs had about 10cm2 V 1 s 1 field effect mobility values and 107 108 switching ratios . The dependences of FE and threshold voltage V TH on the channel width to length ratio were different according to the electron affinity of the interfacial insulator . The contact resistance between the source drain electrode and the active layer and the electron injection barrier height from the active layer to the interfacial gate insulator layer could explain this finding . In this work we successfully demonstrated the method to distinguish the interface related phenomena from the insulator permittivity related phenomena . 
",Design and demonstration of a two stack gate insulator structure for analysis of interface effects. Dependence of field effect mobility threshold voltage on contact resistance electron affinity. High k gate dielectrics such as HfO2 and ZrO2 have been found to have inferior interface properties.,,,
S0098300414002866," X ray micro tomography is increasingly used for the quantitative analysis of the volumes of features within the 3D images . As with any measurement there will be error and uncertainty associated with these measurements . In this paper a method for quantifying both the systematic and random components of this error in the measured volume is presented . The systematic error is the offset between the actual and measured volume which is consistent between different measurements and can therefore be eliminated by appropriate calibration . In XMT measurements this is often caused by an inappropriate threshold value . The random error is not associated with any systematic offset in the measured volume and could be caused for instance by variations in the location of the specific object relative to the voxel grid . It can be eliminated by repeated measurements . It was found that both the systematic and random components of the error are a strong function of the size of the object measured relative to the voxel size . The relative error in the volume was found to follow approximately a power law relationship with the volume of the object but with an exponent that implied unexpectedly that the relative error was proportional to the radius of the object for small objects though the exponent did imply that the relative error was approximately proportional to the surface area of the object for larger objects . In an example application involving the size of mineral grains in an ore sample the uncertainty associated with the random error in the volume is larger than the object itself for objects smaller than about 8 voxels and is greater than 10 for any object smaller than about 260 voxels . A methodology is presented for reducing the random error by combining the results from either multiple scans of the same object or scans of multiple similar objects with an uncertainty of less than 5 requiring 12 objects of 100 voxels or 600 objects of 4 voxels . As the systematic error in a measurement can not be eliminated by combining the results from multiple measurements this paper introduces a procedure for using volume standards to reduce the systematic error especially for smaller objects where the relative error is larger . 
",Methodology for quantifying random and systematic errors in microCT images presented. Unexpected power law scaling for error in small particle volume as threshold changes. Random component of error in volume insensitive to threshold value.,,,
S0140366415002297," In online systems of videos music or books users behaviors are disclosed to the recommender systems to learn their interests . Such a disclosure raises a serious concern in the public for the leak of users privacy . Meanwhile some algorithms are proposed to obfuscate users historical behavior records to protect users privacy at the cost of degradation of recommendation accuracy . It is a common belief that such tradeoff is inevitable . In this paper however we break this pessimistic belief based on the fact that people s interests are not necessarily limited to items which are geared to a certain gender age or profession . Based on this idea we propose a recommendation friendly privacy preserving framework by introducing a privacy preserving module between a recommender system and user side . For instance to obfuscate a female user s gender information the privacy preserving module adds a set of extra factitious ratings of movies not watched by the given user . These added movies are selected to be those mostly watched by male viewers but interesting the given female user . Extensive experiments show that our algorithm obfuscates users privacy information e.g . gender efficiently but also maintains or even improves recommendation accuracy . 
",Highlight Preserve privacy without loss of recommendation accuracy. Add factitious ratings to blur the privacy but intensify interests. Propose a new similarity metric of ratings.,,,
S0169260714003496," Virtual colon flattening is a minimally invasive viewing mode used to detect colorectal polyps on the colonic inner surface in virtual colonoscopy . Compared with conventional colonoscopy inspecting a flattened colonic inner surface is faster and results in fewer uninspected regions . Unfortunately the deformation distortions of flattened colonic inner surface impede the performance of VF . Conventionally the deformation distortions can be corrected by using the colonic inner surface . However colonic curvatures and haustral folds make correcting deformation distortions using only the colonic inner surface difficult . Therefore we propose a VF method that is based on the colonic outer surface . The proposed method includes two novel algorithms namely the colonic outer surface extraction algorithm and the colonic outer surface based distortion correction algorithm . Sixty scans involving 77 annotated polyps were used for the validation . The flattened colons were independently inspected by three operators and then compared with three existing VF methods . The correct detection rates of the proposed method and the three existing methods were 79.6 67.1 71.9 and 72.7 respectively and the false positives per scan were 0.16 0.32 0.21 and 0.26 respectively . The experimental results demonstrate that our proposed method has better performance than existing methods that are based on the colonic inner surface . 
",We do virtual colon flattening based on colonic outer surface. We conduct both of quantitative and visual evaluations to the results. Using colonic outer surface can improve the outcome of virtual colon flattening.,,,
S0169260714001515," This paper proposes new combined methods to classify normal and epileptic seizure EEG signals using wavelet transform phase space reconstruction and Euclidean distance based on a neural network with weighted fuzzy membership functions . WT PSR ED and statistical methods that include frequency distributions and variation were implemented to extract 24 initial features to use as inputs . Of the 24 initial features 4 minimum features with the highest accuracy were selected using a non overlap area distribution measurement method supported by the NEWFM . These 4 minimum features were used as inputs for the NEWFM and this resulted in performance sensitivity specificity and accuracy of 96.33 100 and 98.17 respectively . In addition the area under Receiver Operating Characteristic curve was used to measure the performances of NEWFM both without and with feature selections . 
",The wavelet transform the phase space reconstruction and the Euclidean distances. Four minimum features to classify normal and epileptic seizure signals in EEG signals. The area under Receiver Operating Characteristic ROC curve was used.,,,
S0169260714003514," This research examines the precision of an adaptive neuro fuzzy computing technique in estimating the anti obesity property of a potent medicinal plant in a clinical dietary intervention . Even though a number of mathematical functions such as SPSS analysis have been proposed for modeling the anti obesity properties estimation in terms of reduction in body mass index body fat percentage and body weight loss there are still disadvantages of the models like very demanding in terms of calculation time . Since it is a very crucial problem in this paper a process was constructed which simulates the anti obesity activities of caraway a traditional medicine on obese women with adaptive neuro fuzzy inference method . The ANFIS results are compared with the support vector regression results using root mean square error and coefficient of determination . The experimental results show that an improvement in predictive accuracy and capability of generalization can be achieved by the ANFIS approach . The following statistical characteristics are obtained for BMI loss estimation RMSE 0.032118 and R 0.9964 in ANFIS testing and RMSE 0.47287 and R 0.361 in SVR testing . For fat loss estimation RMSE 0.23787 and R 0.8599 in ANFIS testing and RMSE 0.32822 and R 0.7814 in SVR testing . For weight loss estimation RMSE 0.00000035601 and R 1 in ANFIS testing and RMSE 0.17192 and R 0.6607 in SVR testing . Because of that it can be applied for practical purposes . 
",Adaptive neuro fuzzy computing technique in estimating the anti obesity. Property of a potent medicinal plant in a clinical dietary intervention. Reduction in body mass index BMI body fat percentage and body weight loss. Anti obesity activities of caraway Carum carvi a traditional medicine on obese women.,,,
S0169260714001497," Breast cancer continues to be a significant public health problem in the world . Early detection is the key for improving breast cancer prognosis . Mammogram breast X ray is considered the most reliable method in early detection of breast cancer . However it is difficult for radiologists to provide both accurate and uniform evaluation for the enormous mammograms generated in widespread screening . Micro calcification clusters and masses are the two most important signs for the breast cancer and their automated detection is very valuable for early breast cancer diagnosis . The main objective is to discuss the computer aided detection system that has been proposed to assist the radiologists in detecting the specific abnormalities and improving the diagnostic accuracy in making the diagnostic decisions by applying techniques splits into three steps procedure beginning with enhancement by using Histogram equalization and Morphological Enhancement followed by segmentation based on Otsu s threshold the region of interest for the identification of micro calcifications and mass lesions and at last classification stage which classify between normal and micro calcifications patterns and then classify between benign and malignant micro calcifications . In classification stage three methods were used the voting K Nearest Neighbor classifier with prediction accuracy of 73 Support Vector Machine classifier with prediction accuracy of 83 and Artificial Neural Network classifier with prediction accuracy of 77 . 
",Noise and background removed using histogram equalization and morphological filtering. Local thresholding and Otsu s technique were used to segment masses from the background. Feature extraction of mammograms images was proposed using Gray Level co occurrence matrix GLCM . Three classifications techniques of mammogram images were applied.,,,
S0169260714003241," Current electrocardiogram signal quality assessment studies have aimed to provide a two level classification clean or noisy . However clinical usage demands more specific noise level classification for varying applications . This work outlines a five level ECG signal quality classification algorithm . A total of 13 signal quality metrics were derived from segments of ECG waveforms which were labeled by experts . A support vector machine was trained to perform the classification and tested on a simulated dataset and was validated using data from the MIT BIH arrhythmia database . The simulated training and test datasets were created by selecting clean segments of the ECG in the 2011 PhysioNet Computing in Cardiology Challenge database and adding three types of real ECG noise at different signal to noise ratio levels from the MIT BIH Noise Stress Test Database . The MITDB was re annotated for five levels of signal quality . Different combinations of the 13 metrics were trained and tested on the simulated datasets and the best combination that produced the highest classification accuracy was selected and validated on the MITDB . Performance was assessed using classification accuracy and a single class overlap accuracy which assumes that an individual type classified into an adjacent class is acceptable . An Ac of 80.26 and an OAc of 98.60 on the test set were obtained by selecting 10 metrics while 57.26 and 94.23 were the numbers for the unseen MITDB validation data without retraining . By performing the fivefold cross validation an Ac of 88.07 0.32 and OAc of 99.34 0.07 were gained on the validation fold of MITDB . 
",A five level ECG signal quality classification algorithm using a machine learning approach was proposed. 13 signal quality metrics were derived from segments of ECG waveforms. The algorithm was trained and tested on a simulated dataset and was validated using data from the MIT BIH arrhythmia database. A classification accuracy of 80.26 and a single class overlap accuracy of 98.60 on the test set were obtained by selecting 10 metrics model. A classification accuracy of 88.07 0.32 and a single class overlap accuracy of 99.34 0.07 were gained on the validation fold of MITDB.,,,
S0167931713005042," In this work the direct transfer of nanopatterns into titanium is demonstrated . The nanofeatures are imprinted at room temperature using diamond stamps in a single step . We also show that the imprint properties of the titanium surface can be altered by anodisation yielding a significant reduction in the required imprint force for pattern transfer . The anodisation process is also utilised for curved titanium surfaces where a reduced imprint force is preferable to avoid sample deformation and damage . We finally demonstrate that our process can be applied directly to titanium rods . 
",Direct nanopatterning of titanium using nanopatterned diamond based stamps. Quantify nanopillar matrix imprint depth with regards to feature size and density. An account of a novel method of reducing the imprint load required to emboss titanium. TEM and EELS analysis following our load reduction treatment. The first demonstration of nanopatterning curved bulk titanium.,,,
S0167947313002326," The detection of change points in heterogeneous sequences is a statistical challenge with applications across a wide variety of fields . In bioinformatics a vast amount of methodology exists to identify an ideal set of change points for detecting Copy Number Variation . While considerable efficient algorithms are currently available for finding the best segmentation of the data in CNV relatively few approaches consider the important problem of assessing the uncertainty of the change point location . Asymptotic and stochastic approaches exist but often require additional model assumptions to speed up the computations while exact methods generally have quadratic complexity which may be intractable for large data sets of tens of thousands points or more . A hidden Markov model with constraints specifically chosen to correspond to a segment based change point model provides an exact method for obtaining the posterior distribution of change points with linear complexity . The methods are implemented in the R package postCP which uses the results of a given change point detection algorithm to estimate the probability that each observation is a change point . The results include an implementation of postCP on a publicly available CNV data set 120 . Due to its frequentist framework postCP obtains less conservative confidence intervals than previously published Bayesian methods but with linear complexity instead of quadratic . Simulations showed that postCP provided comparable loss to a Bayesian MCMC method when estimating posterior means specifically when assessing larger scale changes while being more computationally efficient . On another high resolution CNV data set 14 241 the implementation processed information in less than one second on a mid range laptop computer . 
",We describe a method to assess uncertainty in a set of prespecified change points. It obtains exact estimates of posterior probability of locations without resampling. A constrained hidden Markov model estimates probabilities in linear time. Methods are implemented in the R package postCP available on CRAN. Simulations showed comparable loss to Bayesian implementation in estimating means.,,,
S0169260715000309," Today more and more biological laboratories use 3D cell cultures and tissues grown in vitro as a 3D model of in vivo tumours and metastases . In the last decades it has been extensively established that multicellular spheroids represent an efficient model to validate effects of drugs and treatments for human care applications . However a lack of methods for quantitative analysis limits the usage of spheroids as models for routine experiments . Several methods have been proposed in literature to perform high throughput experiments employing spheroids by automatically computing different morphological parameters such as diameter volume and sphericity . Nevertheless these systems are typically grounded on expensive automated technologies that make the suggested solutions affordable only for a limited subset of laboratories frequently performing high content screening analysis . In this work we propose AnaSP an open source software suitable for automatically estimating several morphological parameters of spheroids by simply analyzing brightfield images acquired with a standard widefield microscope also not endowed with a motorized stage . The experiments performed proved sensitivity and precision of the segmentation method proposed and excellent reliability of AnaSP to compute several morphological parameters of spheroids imaged in different conditions . AnaSP is distributed as an open source software tool . Its modular architecture and graphical user interface make it attractive also for researchers who do not work in areas of computer vision and suitable for both high content screenings and occasional spheroid based experiments . 
",We present a new software suite to analyze brightfield images of spheroids. AnaSP estimates several morphological parameters in a very limited time. We proved the high accuracy of the segmentation method proposed. Both AnaSP source code and a standalone executable version are freely available. The GUI developed makes AnaSP effective even without expertise in computer vision.,,,
S0169260714002909," Background The use of open source software in health informatics is increasingly advocated by authors in the literature . Although there is no clear evidence of the superiority of the current open source applications in the healthcare field the number of available open source applications online is growing and they are gaining greater prominence . This repertoire of open source options is of a great value for any future planner interested in adopting an electronic medical health record system whether selecting an existent application or building a new one . The following questions arise . How do the available open source options compare to each other with respect to functionality usability and security Can an implementer of an open source application find sufficient support both as a user and as a developer and to what extent Does the available literature provide adequate answers to such questions This review attempts to shed some light on these aspects . Objective The objective of this study is to provide more comprehensive guidance from an implementer perspective toward the available alternatives of open source healthcare software particularly in the field of electronic medical health records . Methods The design of this study is twofold . In the first part we profile the published literature on a sample of existent and active open source software in the healthcare area . The purpose of this part is to provide a summary of the available guides and studies relative to the sampled systems and to identify any gaps in the published literature with respect to our research questions . In the second part we investigate those alternative systems relative to a set of metrics by actually installing the software and reporting a hands on experience of the installation process usability as well as other factors . Results The literature covers many aspects of open source software implementation and utilization in healthcare practice . Roughly those aspects could be distilled into a basic taxonomy making the literature landscape more perceivable . Nevertheless the surveyed articles fall short of fulfilling the targeted objective of providing clear reference to potential implementers . The hands on study contributed a more detailed comparative guide relative to our set of assessment measures . Overall no system seems to satisfy an industry standard measure particularly in security and interoperability . The systems as software applications feel similar from a usability perspective and share a common set of functionality though they vary considerably in community support and activity . Conclusion More detailed analysis of popular open source software can benefit the potential implementers of electronic health medical records systems . The number of examined systems and the measures by which to compare them vary across studies but still rewarding insights start to emerge . Our work is one step toward that goal . Our overall conclusion is that open source options in the medical field are still far behind the highly acknowledged open source products in other domains e.g . operating systems market share . 
",The categorization of literature articles suggests the lack of adequate information from an implementer perspective. A more elaborated hands on study of individual OSS can reveal useful information to potential implementers yielding better informed decisions. OSS in healthcare informatics still lag behind established open source systems in several aspects in particular security usability and developers support.,,,
S0169260715000930," Purpose Clinical pathways fall under the process perspective of health care quality . For care providers clinical pathways can be compared to improve health care quality . The objective of this study was to design a convenient physician order set comparison system based on claim records from the National Health Insurance Research Database of Taiwan . Methods Data were retrieved from the NHIRD for the period of 2003 2007 for frequent physician order sets found in hospital surgical hernia repair inpatient claim records . The derived frequent physician order sets were divided into five frequency thresholds 80 85 90 95 and 100 . A consistency index was defined and calculated to understand each care providers adherence to clinical pathways . In addition the average count of physician orders average amount of cost Charlson comorbidity index and recurrence rate were calculated these variables were considered in frequent physician order sets comparison . Results Records for 3262 patients from 257 hospitals were retrieved . The frequent physician order sets of various frequency thresholds Charlson comorbidities and recurrence rates were extracted and computed for comparison among hospitals . A recurrence rate threshold of 2 was established to separate low and high quality of herniorrhaphy at each hospital . Univariable analysis showed that low recurrence rate was associated with high consistency index few surgeons at each hospital and non medical center facility type . A multivariable Cox regression analysis indicated an association of low recurrence rates with consistency index only . Conclusions The proposed system leveraged the claim records to generate frequent physician order sets at hospitals thus solving the difficulty in obtaining clinical pathway data . This allows medical professionals and management to conveniently and effectively compare and query similarities and differences in clinical pathways among hospitals . 
",A system that provides a convenient way for physicians to retrieve and compare clinical pathways among health care providers about herniorrhaphy. The frequent physician order sets were derived from the National Health Database. A higher consistency index leads to lower recurrence rates.,,,
S0167865515001622," Pattern classification methods assign an object to one of several predefined classes categories based on features extracted from observed attributes of the object . When L discriminatory features for the pattern can be accurately determined the pattern classification problem presents no difficulty . However precise identification of the relevant features for a classification algorithm to be able to categorize real world patterns without errors is generally infeasible . In this case the pattern classification problem is often cast as devising a classifier that minimizes the misclassification rate . One way of doing this is to consider both the pattern attributes and its class label as random variables estimate the posterior class probabilities for a given pattern and then assign the pattern to the class category for which the posterior class probability value estimated is maximum . More often than not the form of the posterior class probabilities is unknown . The so called Parzen Window approach is widely employed to estimate class conditional probability densities for a given pattern . These probability densities can then be utilized to estimate the appropriate posterior class probabilities for that pattern . However the Parzen Window scheme can become computationally impractical when the size of the training dataset is in the tens of thousands and L is also large . Over the years various schemes have been suggested to ameliorate the computational drawback of the Parzen Window approach but the problem still remains outstanding and unresolved . In this paper we revisit the Parzen Window technique and introduce a novel approach that may circumvent the aforementioned computational bottleneck . The current paper presents the mathematical aspect of our idea . Practical realizations of the proposed scheme will be given elsewhere . 
",We revisit the Parzen Window approach widely employed in pattern recognition. The Parzen Window approach can suffer from a severe computational bottleneck. This manuscript introduces a new scheme to ameliorate this computational drawback.,,,
S0169260715002448," The deposits of fat on the surroundings of the heart are correlated to several health risk factors such as atherosclerosis carotid stiffness coronary artery calcification atrial fibrillation and many others . These deposits vary unrelated to obesity which reinforces its direct segmentation for further quantification . However manual segmentation of these fats has not been widely deployed in clinical practice due to the required human workload and consequential high cost of physicians and technicians . In this work we propose a unified method for an autonomous segmentation and quantification of two types of cardiac fats . The segmented fats are termed epicardial and mediastinal and stand apart from each other by the pericardium . Much effort was devoted to achieve minimal user intervention . The proposed methodology mainly comprises registration and classification algorithms to perform the desired segmentation . We compare the performance of several classification algorithms on this task including neural networks probabilistic models and decision tree algorithms . Experimental results of the proposed methodology have shown that the mean accuracy regarding both epicardial and mediastinal fats is 98.5 with a mean true positive rate of 98.0 . In average the Dice similarity index was equal to 97.6 . 
",Proposing an accurate intersubject registration for cardiac CT images. Proposing and analyzing a hybrid similarity measure that was applied within the registration procedure. Corroborating on the appliance of classification algorithms for image segmentation. Analyzing the performance and accuracy of various classifiers for the problem. Proposing a unified and fully automatic segmentation method for both epicardial and mediastinal fats on cardiac CT images.,,,
S0169260715002266," Breast cancer is the most commonly occurring type of cancer among women and it is the major cause of female cancer related deaths worldwide . Its incidence is increasing in developed as well as developing countries . Efficient strategies to reduce the high death rates due to breast cancer include early detection and tumor removal in the initial stages of the disease . Clinical and mammographic examinations are considered the best methods for detecting the early signs of breast cancer however these techniques are highly dependent on breast characteristics equipment quality and physician experience . Computer aided diagnosis systems have been developed to improve the accuracy of mammographic diagnosis usually such systems may involve three steps segmentation parameter extraction and selection of the segmented lesions and lesions classification . Literature considers the first step as the most important of them as it has a direct impact on the lesions characteristics that will be used in the further steps . In this study the original contribution is a microcalcification segmentation method based on the geodesic active contours technique associated with anisotropic texture filtering as well as the radiologists knowledge . Radiologists actively participate on the final step of the method selecting the final segmentation that allows elaborating an adequate diagnosis hypothesis with the segmented microcalcifications presented in a region of interest . The proposed method was assessed by employing 1000 ROIs extracted from images of the Digital Database for Screening Mammography . For the selected ROIs the rate of adequately segmented microcalcifications to establish a diagnosis hypothesis was at least 86.9 according to the radiologists . The quantitative test based on the area overlap measure yielded a mean of 0.52 0.20 for the segmented images when all 2136 segmented microcalcifications were considered . Moreover a statistical difference was observed between the AOM values for large and small microcalcifications . The proposed method had better or similar performance as compared to literature for microcalcifications with maximum diameters larger than 460 m. For smaller microcalcifications the performance was limited . 
",Microcalcification segmentation method based on geodesic active contours. Method incorporates radiologists knowledge in its final stage. 86.9 of microcalcifications were adequately segmented. Performance satisfactory for microcalcifications larger than 460 m. Modest performance when segmenting microcalcifications smaller than 460 m.,,,
S0167839616300206," Generalized quantum splines are piecewise polynomials whose generalized quantum derivatives agree up to some order at the joins . Just like classical and quantum splines generalized quantum splines admit a canonical basis with compact support the generalized quantum B splines . Here we study generalized quantum B spline bases and generalized quantum B spline curves using a very general variant of the blossom the generalized quantum blossom . Applying the generalized quantum blossom we develop algorithms and identities for generalized quantum B spline bases and generalized quantum B spline curves including generalized quantum variants of the de Boor algorithms for recursive evaluation and generalized quantum differentiation knot insertion procedures for converting from generalized quantum B spline to piecewise generalized quantum B zier form and a generalized quantum variant of Marsden s identity . 
",Quantum splines are extended to splines whose divided differences agree at the joins. Generalized quantum blossoms are used to derive properties of generalized quantum splines. Algorithms and formulas are extended from quantum splines to generalized quantum splines. Generalized quantum B spline bases and generalized quantum B spline curves are also studied.,,,
S0169260715001583," Background and objective Understanding the causes of disagreement among experts in clinical decision making has been a challenge for decades . In particular a high amount of variability exists in diagnosis of retinopathy of prematurity which is a disease affecting low birth weight infants and a major cause of childhood blindness . A possible cause of variability that has been mostly neglected in the literature is related to discrepancies in the sets of important features considered by different experts . In this paper we propose a methodology which makes use of machine learning techniques to understand the underlying causes of inter expert variability . Methods The experiments are carried out on a dataset consisting of 34 retinal images each with diagnoses provided by 22 independent experts . Feature selection techniques are applied to discover the most important features considered by a given expert . Those features selected by each expert are then compared to the features selected by other experts by applying similarity measures . Finally an automated diagnosis system is built in order to check if this approach can be helpful in solving the problem of understanding high inter rater variability . Results The experimental results reveal that some features are mostly selected by the feature selection methods regardless the considered expert . Moreover for pairs of experts with high percentage agreement among them the feature selection algorithms also select similar features . By using the relevant selected features the classification performance of the automatic system was improved or maintained . Conclusions The proposed methodology provides a handy framework to identify important features for experts and check whether the selected features reflect the pairwise agreements disagreements . These findings may lead to improved diagnostic accuracy and standardization among clinicians and pave the way for the application of this methodology to other problems which present inter expert variability . 
",Inter expert variability in clinical decision making is an important problem. Retinopathy of prematurity is a disease that suffers from inter expert variability. We propose a methodology for understanding the causes of disagreement. The methodology provides a framework to identify important features for experts. An automatic system was also developed to deal with this problem.,,,
S0167947314002953," The tri linear PLS2 iterative procedure an algorithm pertaining to the NIPALS framework is considered . It was previously proposed as a first stage to estimate parameters of the multi way PLS regression method . It is shown that the tri linear PLS2 procedure is convergent . The procedure generates a sequence of parameters which can be described as increasing or decreasing two specific criteria . Furthermore a hidden tensor is described allowing tri linear PLS2 to search its best rank one approximation . This tensor highlights the link between multi way PLS regression and the well known PARAFAC model . The parameters of the multi way PLS regression method can be computed using three alternative procedures . 
",It is shown that the tri linear PLS2 procedure is convergent. The sequences generated by the tri linear PLS2 can be described as increasing or decreasing two specific criteria. A hidden tensor is described allowing tri linear PLS2 to search its best rank one approximation. A link between multi way PLS regression and the well known PARAFAC model is highlighted.,,,
S0169260714001837," Objective Many regional programs of the countries educate asthmatic children and their families to manage healthcare data . This study aims to establish a Web based self management system eAsthmaCare to promote the electronic healthcare services for the asthmatic children in Taiwan . The platform can perform real time online functionality based upon a five tier infrastructure with mutually supportive components to acquire asthma diaries quality of life assessments and health educations . Methods We have designed five multi disciplinary portions on the interactive interface functioned with the analytical diagrams online asthma diary remote asthma assessment instantaneous asthma alert diagrammatical clinic support and asthma health education . The Internet based asthma diary and assessment program was developed for patients to process self management healthcare at home . In addition the online analytical charts can help healthcare professionals to evaluate multi domain health information of patients immediately . Results eAsthmaCare was developed by Java Servlet JSP technology upon Apache Tomcat web server and Oracle database . Forty one voluntary asthmatic children were intervened to examine the proposed system . Seven domains of satisfiability assessment by using the system were applied for approving the development . The average scores were scaled in the acceptable range for each domain to ensure feasibility of the proposed system . Conclusion The study revealed the details of system infrastructure and developed functions that can help asthmatic children in self management for healthcare to enhance communications between patients and hospital professionals . 
",Five tier infrastructure with mutually supportive components on e Healthcare system. Five multi disciplinary portions on interactive interface with online diagrams. Internet based asthma diary and remote QOL assessment for self management at home. Online diagrammatical clinic support for evaluating real time health information. Instantaneous asthma alert and health education for healthcare improvement.,,,
S0169260715001947," Heat Shock Proteins are the substantial ingredients for cell growth and viability which are found in all living organisms . HSPs manage the process of folding and unfolding of proteins the quality of newly synthesized proteins and protecting cellular homeostatic processes from environmental stress . On the basis of functionality HSPs are categorized into six major families namely HSP20 or sHSP HSP40 or J proteins types HSP60 or GroEL ES HSP70 HSP90 and HSP100 . Identification of HSPs family and sub family through conventional approaches is expensive and laborious . It is therefore highly desired to establish an automatic robust and accurate computational method for prediction of HSPs quickly and reliably . Regard a computational model is developed for the prediction of HSPs family . In this model protein sequences are formulated using three discrete methods namely Split Amino Acid Composition Pseudo Amino Acid Composition and Dipeptide Composition . Several learning algorithms are utilized to choice the best one for high throughput computational model . Leave one out test is applied to assess the performance of the proposed model . The empirical results showed that support vector machine achieved quite promising results using Dipeptide Composition feature space . The predicted outcomes of proposed model are 90.7 accuracy for HSPs dataset and 97.04 accuracy for J protein types which are higher than existing methods in the literature so far . 
",We develop a predictor for classification of Heat Shock Proteins and J proteins. It is the combination of Dipeptide Composition and SVM. Two datasets were evaluated using jackknife test. Best results are reported so far in the literature.,,,
S0169260715000279," Background and objective Insulin bolus calculators are simple decision support software tools incorporated in most commercially available insulin pumps and some capillary blood glucose meters . Although their clinical benefit has been demonstrated their utilisation has not been widespread and their performance remains suboptimal mainly because of their lack of flexibility and adaptability . One of the difficulties that people with diabetes clinicians and carers face when using bolus calculators is having to set parameters and adjust them on a regular basis according to changes in insulin requirements . In this work we propose a novel method that aims to automatically adjust the parameters of a bolus calculator . Periodic usage of a continuous glucose monitoring device is required for this purpose . Methods To test the proposed method an in silico evaluation under real life conditions was carried out using the FDA accepted Type 1 diabetes mellitus UVa Padova simulator . Since the T1DM simulator does not incorporate intra subject variability and uncertainty a set of modifications were introduced to emulate them . Ten adult and ten adolescent virtual subjects were assessed over a 3 month scenario with realistic meal variability . The glycaemic metrics mean blood glucose percentage time in target percentage time in hypoglycaemia risk index low blood glucose index and blood glucose standard deviation were employed for evaluation purposes . A t test statistical analysis was carried out to evaluate the benefit of the presented algorithm against a bolus calculator without automatic adjustment . Results The proposed method statistically improved all glycemic metrics evaluating hypoglycaemia on both virtual cohorts percentage time in hypoglycaemia and low blood glucose index . A statistically significant improvement was also observed on the blood glucose standard deviation . Apart from a small increase in mean blood glucose on the adult cohort the rest of the evaluated metrics despite showing an improvement trend did not experience a statistically significant change . Conclusions A novel method for automatically adjusting the parameters of a bolus calculator has the potential to improve glycemic control in T1DM diabetes management . 
",A novel method that aims to automatically adjust the parameters of a bolus calculator is presented. A novel in silico evaluation under realistic intra day variability was carried out. The proposed method has the potential to significantly improve glycemic control in diabetes.,,,
S0167947315000730," Cross validation methodologies have been widely used as a means of selecting tuning parameters in nonparametric statistical problems . In this paper we focus on a new method for improving the reliability of cross validation . We implement this method in the context of the kernel density estimator where one needs to select the bandwidth parameter so as to minimize risk . This method is a two stage subsampling extrapolation bandwidth selection procedure which is realized by first evaluating the risk at a fictional sample size sample size and then extrapolating the optimal bandwidth from to . This two stage method can dramatically reduce the variability of the conventional unbiased cross validation bandwidth selector . This simple first order extrapolation estimator is equivalent to the rescaled bagging CV bandwidth selector in Hall and Robinson if one sets the bootstrap size equal to the fictional sample size . However our simplified expression for the risk estimator enables us to compute the aggregated risk without any bootstrapping . Furthermore we developed a second order extrapolation technique as an extension designed to improve the approximation of the true optimal bandwidth . To select the optimal choice of the fictional size given a sample of size we propose a nested cross validation methodology . Based on simulation study the proposed new methods show promising performance across a wide selection of distributions . In addition we also investigated the asymptotic properties of the proposed bandwidth selectors . 
",A two stage subsampling extrapolation bandwidth selection procedure is proposed. An automatic nested cross validation method is developed to select the subsample size. The extrapolated bandwidth selectors achieve a smaller mean square error. The second order extrapolated bandwidth selector has a relative convergence rate,,,
S0169260714003836," Neuropsychological assessment tests have an important role in early detection of dementia . Therefore we designed and implemented a test battery for mobile devices that can be used for mobile cognitive screening . This battery consists of 33 questions from 14 type of tests for the assessment of 8 different cognitive functions Arithmetic orientation abstraction attention memory language visual and executive functions . This test battery is implemented as an application for mobile devices that operates on Android OS . In order to validate the effectiveness of the neuropsychological test battery it was applied on a group of 23 elderly persons . Within this group 9 were healthy and 14 were already diagnosed with dementia . The education level of the control group and dementia group were comparable as they spent 13.66 5.07 and 13.71 4.14 years at school respectively . For comparison a validated paper and pencil test was applied along with the proposed MCS battery . The proposed test was able to differentiate the individuals in the control and dementia groups for executive visual memory attention orientation functions with statistical significance . Results of the remaining functions language abstraction and arithmetic were statistically insignificant . The results of MCS and MoCA were compared and the scores of individuals from these tests were correlated . 
",A novel mobile cognitive screening MCS test is designed for mobile devices. MCS has easier interface for elderly participants compared to the personal computers. MCS can offer a wider range of multimedia components compared to the paper tests. MCS can compare cognitive functions against the average of healthy individuals . MCS can differentiate healthy and impaired cognitive abilities.,,,
S0169260715001133," We provide a continuation of the existing Activity Table Modeling methodology with a modular spreadsheets simulation . The simulation model developed is comprised of 28 modeling elements for the abdominal surgery cycle process . The simulation of a two week patient flow in an abdominal clinic with 75 beds demonstrates the applicability of the methodology . The simulation does not include macros thus programming experience is not essential for replication or upgrading the model . Unlike the existing methods the proposed solution employs a modular approach for modeling the activities that ensures better readability the possibility of easily upgrading the model with other activities and its easy extension and connectives with other similar models . We propose a first in first served approach for simulation of servicing multiple patients . The uncertain time duration of the activities is modeled using the function rand . The patients movements from one activity to the next one is tracked with nested if functions thus allowing easy re creation of the process without the need of complex programming . 
",A framework for an abdominal surgery process simulation is proposed. The framework uses Activity table modelling technique and spreadsheets. The framework gives the state of each process activity at any point of time. It is easy read possibly to upgrading and extend with other activities and models.,,,
S0167839615001223," In the paper two new approaches for construction of parametric polynomial approximants of a unit circle are presented . The obtained approximants have better approximation properties than those given by other methods i.e . smaller radial error symmetry and exponential error decay . 
",Two new approaches for construction of parametric polynomial approximants of a unit circle are presented. The approximants have good approximation properties small radial error symmetry exponential error decay.,,,
S0169260714002995," The cure fraction models have been widely used to analyze survival data in which a proportion of the individuals is not susceptible to the event of interest . In this article we introduce a bivariate model for survival data with a cure fraction based on the three parameter generalized Lindley distribution . The joint distribution of the survival times is obtained by using copula functions . We consider three types of copula function models the Farlie Gumbel Morgenstern Clayton and Gumbel Barnett copulas . The model is implemented under a Bayesian framework where the parameter estimation is based on Markov Chain Monte Carlo techniques . To illustrate the utility of the model we consider an application to a real data set related to an invasive cervical cancer study . 
",We introduce a cure fraction model based on the generalized Lindley distribution. The joint distribution of the survival times was obtained by using copula functions. Parameters were estimated by Bayesian inference using MCMC simulation.,,,
S0167947314001467," Multiple testing problems have received much attention . Different strategies have been considered in order to deal with this problem . The false discovery rate is probably the most studied criterion . On the other hand the sequential goodness of fit is a recently proposed approach . Most of the developed procedures are based on the independence among the involved tests however in spite of being a reasonable proviso in some frameworks independence is not realistic for a number of practical cases . Therefore one of the main problems in order to develop appropriate methods is precisely the effect of the dependence among the different tests on decisions making . The consequences of the correlation on the z values distribution in the general multitesting problem are explored . Some different algorithms are provided in order to approximate the distribution of the expected rejection proportions . The performance of the proposed methods is evaluated in a simulation study in which for comparison purposes the Benjamini and Hochberg method to control the FDR the Lehmann and Romano procedure to control the tail probability of the proportion of false positives and the Beta Binomial SGoF procedure are considered . Three different dependence structures are considered . As usual for a better understanding of the problem several practical cases are also studied . 
",This paper studies the impact of correlation on the multitesting problem. An approximation for the distribution of the proportion rejections is provided. This approximation is used to develop multitesting adjusting procedures. The obtained results suggest that the proposed methodology gets a good compromise between Type I and Type II errors.,,,
S0169260714001503," Identifying the abnormal changes of mental workload over time is quite crucial for preventing the accidents due to cognitive overload and inattention of human operators in safety critical human machine systems . It is known that various neuroimaging technologies can be used to identify the MWL variations . In order to classify MWL into a few discrete levels using representative MWL indicators and small sized training samples a novel EEG based approach by combining locally linear embedding support vector clustering and support vector data description techniques is proposed and evaluated by using the experimentally measured data . The MWL indicators from different cortical regions are first elicited by using the LLE technique . Then the SVC approach is used to find the clusters of these MWL indicators and thereby to detect MWL variations . It is shown that the clusters can be interpreted as the binary class MWL . Furthermore a trained binary SVDD classifier is shown to be capable of detecting slight variations of those indicators . By combining the two schemes a SVC SVDD framework is proposed where the clear cut cluster is detected by SVC first and then a subsequent SVDD model is utilized to divide the overlapped cluster into two classes . Finally three class MWL levels can be identified automatically . The experimental data analysis results are compared with those of several existing methods . It has been demonstrated that the proposed framework can lead to acceptable computational accuracy and has the advantages of both unsupervised and supervised training strategies . 
",Recognition of mental workload is crucial to improve the safety of human machine system. A simulated process control task was used to elicit different levels of mental workload. Mental workload was assessed by using multichannel EEG signals. Locally linear embedding was used to reduce the dimensionality of EEG power features. Support vector clustering and support vector data description methods were combined to classify EEG data.,,,
S0167923614002036," It has become increasingly important for companies to utilize electronic word of mouth in their marketing campaigns for desired product sales . Identifying key eWOM disseminators among consumers is a challenge for companies . WOM is an interpersonal communication in which a sender spreads a message to receivers . Previously researchers and practitioners have searched for opinion leaders by examining senders and receivers due to limited records on WOM message . Our study identifies three types of opinion leaders through eWOM using a message based approach that elicits more accurate and comprehensive information on opinion leadership than sender based and receiver based approaches . We demonstrate that eWOM of opinion leaders drives product sales due to their product experience and knowledge background . Our findings suggest that companies can increase product sales via effective use of eWOM of such opinion leaders . Managerial and marketing implications are addressed . 
",We identify disseminators for effective marketing campaign by using eWOM message. Our approach identifies more accurate and comprehensive opinion leadership. Opinion leaders eWOM influences product sales through product experience effects. Opinion leaders eWOM influences product sales through knowledge background effects. Firms need disseminators with individually broad and collectively focused knowledge.,,,
S0169260714003903," The interest in image dermoscopy has been significantly increased recently and skin lesion images are nowadays routinely acquired for a number of skin disorders . An important finding in the assessment of a skin lesion severity is the existence of dark dots and globules which are hard to locate and count using existing image software tools . In this work we present a novel methodology for detecting segmenting and count dark dots and globules from dermoscopy images . Segmentation is performed using a multi resolution approach based on inverse non linear diffusion . Subsequently a number of features are extracted from the segmented dots globules and their diagnostic value in automatic classification of dermoscopy images of skin lesions into melanoma and non malignant nevus is evaluated . The proposed algorithm is applied to a number of images with skin lesions with known histo pathology . Results show that the proposed algorithm is very effective in automatically segmenting dark dots and globules . Furthermore it was found that the features extracted from the segmented dots globules can enhance the performance of classification algorithms that discriminate between malignant and benign skin lesions when they are combined with other region based descriptors . 
",Dark dots and globules may occur in benign and malignant skin lesions and contain significant diagnostic value. A methodology for detecting segmenting dark dots and globules from dermoscopy images is proposed based on inverse non linear diffusion. A set of dot globule related features is proposed. The proposed algorithm is applied to a number of images with skin lesions with known histo pathology. The new features can enhance the performance of classification algorithms that discriminate between malignant and benign skin lesions.,,,
S0168169915000459," The electronic identification of sheep and goats has been obligatory in the European Union since 2010 by means of low frequency radio frequency identification systems . The identification of pigs and cattle is currently based on a visual ear tag but electronic animal identification is gaining in importance . The European Union already offers the additional use of electronic identification systems for cattle in their council regulation . Besides the low frequency radio frequency identification an ultra high frequency ear tag is a possibility for electronic animal identification . The benefits of the latter frequency band are the high range the possibility of quasi simultaneous reading and a high data transmission rate . First systematic laboratory tests were carried out before testing the ear tags in practice . Therefore a dynamic test bench was built . The aim of the experiments presented in this study was to compare different ear tags under standardised conditions and select the most suitable for practical use . The influence of different parameters was tested and a standard test procedure to evaluate the quality of the transponder ear tag was developed . The experiments showed that neither the transponder holder material nor the reader settings examined had a significant influence on the average of readings of the different transponder types . The parameter number of rounds did not show a significant effect either . However significant differences between speed transponder orientation and the fourteen transponder types were found . The two most suitable transponder ear tags for cattle and pigs have been determined by comparison . 
",Dynamic test bench. UHF transponders comparison. UHF cattle pig ear tag development.,,,
S0167923613001875," A keyword auction is conducted by Internet search engines to sell advertising slots listed on the search results page . Although much of the literature assumes the dynamic bidding strategy that utilizes the current bids of other advertisers such information is in practice not available for participants in the auction . This paper explores the bidding behavior of advertisers in a sealed bid environment where each bidder does not know the current bids of others . This study considers secure bidding with a trial bid as the bid adjustment process used by the advertisers which is functional in a sealed bid environment . It is shown that the SBT bid adjustment process converges to some equilibrium point in a one shot game irrespective of the initial bid profile . Simulation results verify that a sealed bid environment would be beneficial to search engines . 
",Bidding behaviors in a sealed bid keyword auction is analyzed. Even in a sealed bid environment bidding behavior with a minimal search for others bids leads to a unique fixedpoint. A computer simulation shows that a sealed bid environment is beneficial to a search engine.,,,
S0169260714002545," Background and objective The degeneration of the balance control system in the elderly and in many pathologies requires measuring the equilibrium conditions very often . In clinical practice equilibrium control is commonly evaluated by using a force platform in a clinical environment . In this paper we demonstrate how a simple movement analysis system based on a 3D video camera and a 3D real time model reconstruction of the human body can be used to collect information usually recorded by a physical stabilometric platform . Methods The algorithm used to reconstruct the human body model as a set of spheres is described and discussed . Moreover experimental measurements and comparisons with data collected by a physical stabilometric platform are also reported . The measurements were collected on a set of 6 healthy subjects to whom a change in equilibrium condition was stimulated by performing an equilibrium task . Results The experimental results showed that more than 95 of data collected by the proposed method were not significantly different from those collected by the classic platform thus confirming the usefulness of the proposed system . Conclusions The proposed virtual balance assessment system can be implemented at low cost and for this reason can be considered a home use medical device . On the contrary astabilometric platform has a cost of about 10 000 and requires periodical calibration . The proposed system does not require periodical calibration as is necessary for stabilometric force platforms and it is easy to use . In future the proposed system with little integration can be used besides being an emulator of a stabilometric platform also to recognize and track in real time head legs arms and trunk that is to collect information actually obtained by sophisticated optoelectronic systems . 
",Equilibrium conditions control is very important in the elderly and in many pathologies. Equilibrium control is commonly evaluated by using a stabilometric platform. A low cost virtual stabilometric platform is presented. Results demonstrate that the virtual stabilometric platform is equivalent to a physical one.,,,
S0167947315001607," A Moment Ratio estimator is proposed for an AR model of the errors in an OLS regression that provides standard errors with far less median bias and confidence intervals with far better coverage than conventional alternatives . A unit root and therefore the absence of cointegration does not necessarily mean that a correlation between the variables is spurious . The estimator is applied to a quadratic trend model of real GDP . The rate of change of GDP growth is negative with finite standard error but is insignificant . The output gap often used as a guide to monetary policy has an infinite standard error and is therefore a statistical illusion . 
",A new Moment Ratio estimator is proposed for an AR model of regression errors. The new estimator gives CIs with less size distortion than popular alternatives. Newey West HAC standard errors can far overstate regressor precision. Lack of cointegration does not necessarily imply a regression is spurious. The new estimator is applied to a quadratic trend model of US GDP.,,,
S0169260715000966," Background and objectives Continuous subcutaneous insulin infusion pump is widely considered a convenience and promising way for type 1 diabetes mellitus subjects who need exogenous insulin infusion . In the standard insulin pump therapy there are two modes for insulin infusion basal and bolus insulin . The basal bolus therapy should be individualized and optimized in order to keep one subject s blood glucose level within the normal range however the optimization procedure is troublesome and it perturb the patients a lot . Therefore an automatic adjustment method is needed to reduce the burden of the patients and run to run control algorithm can be used to handle this significant task . Methods In this study two kinds of high order R2R control methods are presented to adjust the basal and bolus insulin simultaneously . For clarity a second order R2R control algorithm is first derived and studied . Furthermore considering the differences between weekdays and weekends a seventh order R2R control algorithm is also proposed and tested . Results In order to simulate real situation the proposed method has been tested with uncertainties on measurement noise drifts meal size meal time and snack . The proposed method can converge even when there are 60min random variations in meal timing or 50 random variations in meal size . Conclusions According to the robustness analysis one can see that the proposed high order R2R has excellent robustness and could be a promising candidate to optimize insulin pump therapy . 
",Two kinds of high order R2R control methods are presented to optimize the insulin pump therapy. Using the proposed method both the basal and the bolus insulin are adjusted simultaneously. The proposed method can converge even when there are 60min random variations in meal timing or 40 random variations in meal size.,,,
S0169260715000024," Background and objective Biofilms are receiving increasing attention from the biomedical community . Biofilm like growth within human body is considered one of the key microbial strategies to augment resistance and persistence during infectious processes . The Biofilms Experiment Workbench is a novel software workbench for the operation and analysis of biofilms experimental data . The goal is to promote the interchange and comparison of data among laboratories providing systematic harmonised and large scale data computation . Methods The workbench was developed with AIBench an open source Java desktop application framework for scientific software development in the domain of translational biomedicine . Implementation favours free and open source third parties such as the R statistical package and reaches for the Web services of the BiofOmics database to enable public experiment deposition . Results First we summarise the novel free open XML based interchange format for encoding biofilms experimental data . Then we describe the execution of common scenarios of operation with the new workbench such as the creation of new experiments the importation of data from Excel spreadsheets the computation of analytical results the on demand and highly customised construction of Web publishable reports and the comparison of results between laboratories . Conclusions A considerable and varied amount of biofilms data is being generated and there is a critical need to develop bioinformatics tools that expedite the interchange and comparison of microbiological and clinical results among laboratories . We propose a simple open source software infrastructure which is effective extensible and easy to understand . The workbench is freely available for non commercial use at http sing.ei.uvigo.es bew under LGPL license . 
",There is a critical need to develop bioinformatics tools that expedite the interchange and comparison of microbiological and clinical results among laboratories. Biofilms are a prominent subject within Biomedical research. The Biofilms Experiment Workbench BEW is a novel software workbench for the operation and analysis of biofilms experimental data. The Biofilms Markup Language BML is a new data representation format for modelling biofilms experiments. BEW is able to manage information of microbiological studies with various purposes.,,,
S0169260714002041," The three parameter Rayleigh damping model applied to time harmonic Magnetic Resonance Elastography has potential to better characterise fluid saturated tissue systems . However it is not uniquely identifiable at a single frequency . One solution to this problem involves simultaneous inverse problem solution of multiple input frequencies over a broad range . As data is often limited an alternative elegant solution is a parametric RD reconstruction where one of the RD parameters is globally constrained allowing accurate identification of the remaining two RD parameters . This research examines this parametric inversion approach as applied to in vivo brain imaging . Overall success was achieved in reconstruction of the real shear modulus that showed good correlation with brain anatomical structures . The mean and standard deviation shear stiffness values of the white and gray matter were found to be 3 0.11kPa and 2.2 0.11kPa respectively which are in good agreement with values established in the literature or measured by mechanical testing . Parametric results with globally constrained indicate that selecting a reasonable value for the distribution has a major effect on the reconstructed image and concomitant damping ratio . More specifically the reconstructed image using a realistic 333Pa value representative of a greater portion of the brain tissue showed more accurate differentiation of the ventricles within the intracranial matter compared to 1000Pa and reconstruction with 333Pa accurately captured the higher damping levels expected within the vicinity of the ventricles . Parametric RD reconstruction shows potential for accurate recovery of the stiffness characteristics and overall damping profile of the in vivo living brain despite its underlying limitations . Hence a parametric approach could be valuable with RD models for diagnostic MRE imaging with single frequency data . 
",Rayleigh damping RD model is structurally non identifiable with single frequency data without extensive a priori information. We propose an alternative approach to overcome non identifiability issue of the model. Parametric RD model was mooted in application to in vivo mechano brain imaging. Results indicate that parametric RD approach shows potential for diagnostic MRE imaging with single frequency data.,,,
S0169260715002291," Group independent component analysis is a well established blind source separation technique that has been widely applied to study multi subject functional magnetic resonance imaging data . The group independent components represent the commonness of all of the subjects in the group . Similar to independent component analysis on the single subject level the performance of GICA can be improved for multi subject fMRI data analysis by incorporating a priori information however a priori information is not always considered while looking for GICs in existing GICA methods especially when no obvious or specific knowledge about an unknown group is available . In this paper we present a novel method to extract the group intrinsic reference from all of the subjects of the group and then incorporate it into the GICA extraction procedure . Comparison experiments between FastICA and GICA with intrinsic reference are implemented on the group level with regard to the simulated hybrid and real fMRI data . The experimental results show that the GICs computed by GICA IR have a higher correlation with the corresponding independent component of each subject in the group and the accuracy of activation regions detected by GICA IR was also improved . These results have demonstrated the advantages of the GICA IR method which can better reflect the commonness of the subjects in the group . 
",We presented a novel method to extract group intrinsic reference from all subjects in a group. A new group ICA model with intrinsic reference GICA IR was further proposed for fMRI data analysis. GICA IR was shown to better reflect the commonness of subjects in the group.,,,
S0169260715000899," In this paper a new automatic approach to determine the accurate measure of human vertebrae is proposed . The aim is to speed up the measurement process and to reduce the uncertainties that typically affect the measurement carried out by traditional approaches . The proposed method uses a 3D model of the vertebra obtained from CT scans or 3D scanning from which some characteristic dimensions are detected . For this purpose specific rules to identify morphological features from which to detect dimensional features unambiguously and accurately are put forward and implemented in original software . The automatic method which is here proposed is verified by analysing real vertebrae and is then compared with the state of the art methods for vertebra measurement . 
",This paper proposes a new approach to determine the measure of human vertebrae. Typical approaches perform measurements with lack of repeatability and reproducibility. The proposed method is based on morphological features recognition from 3D high point density model of the vertebrae. The paper proposes unambiguous rules to identify geometric references and the associated dimensions. Compared to typical approaches the proposed method proved to be more repeatable and reproducible.,,,
S0169260714003058," This study developed a computerised method for fovea centre detection in fundus images . In the method the centre of the optic disc was localised first by the template matching method the disc fovea axis was then determined by searching the vessel free region and finally the fovea centre was detected by matching the fovea template around the centre of the axis . Adaptive Gaussian templates were used to localise the centres of the optic disc and fovea for the images with different resolutions . The proposed method was evaluated using three publicly available databases which consisted of a total of 1419 fundus images with different resolutions . The proposed method obtained the fovea detection accuracies of 93.1 92.1 and 97.8 for the DIARETDB0 DIARETDB1 and MESSIDOR databases respectively . The overall accuracy of the proposed method was 97.0 in this study . 
",We developed a method for automated detection of fovea in fundus images based on vessel free zone and adaptive Gaussian template. The proposed method constrained the search region based on the vessel free region and did not need the segmentation of vessels. The proposed method was evaluated using three publicly available databases DIARETDB0 DIARETDB1 and MESSIDOR . The proposed method obtained the fovea detection accuracy of 97.0 .,,,
S0167865515000744," Vision is one of the most important of the senses and humans use it extensively during navigation . We evaluated different types of image and video frame descriptors that could be used to determine distinctive visual landmarks for localizing a person based on what is seen by a camera that they carry . To do this we created a database containing over 3 km of video sequences with ground truth in the form of distance travelled along different corridors . Using this database the accuracy of localization both in terms of knowing which route a user is on and in terms of position along a certain route can be evaluated . For each type of descriptor we also tested different techniques to encode visual structure and to search between journeys to estimate a user s position . The techniques include single frame descriptors those using sequences of frames and both color and achromatic descriptors . We found that single frame indexing worked better within this particular dataset . This might be because the motion of the person holding the camera makes the video too dependent on individual steps and motions of one particular journey . Our results suggest that appearance based information could be an additional source of navigational data indoors augmenting that provided by say radio signal strength indicators . Such visual information could be collected by crowdsourcing low resolution video feeds allowing journeys made by different users to be associated with each other and location to be inferred without requiring explicit mapping . This offers a complementary approach to methods based on simultaneous localization and mapping algorithms . 
",A new dataset of videos from wearable handheld cameras with location ground truth. Benchmarking of position localization accuracy via associating images between users. Description and comparisons of 4 new descriptor variants for visual localization. Suggested metrics of localization performance using error distance distributions. Intra inter camera image query and retrieval comparisons wearable vs hand held .,,,
S0167931714003347," In order to advance flexible electronic technologies it is important to study the electrical properties of thin metal films on polymer substrates under mechanical load . At the same time the observation of film deformation and fracture as well as the stresses that are present in the films during straining are also crucial to investigate . To address both the electromechanical and deformation behavior of metal films supported by polymer substrates in situ 4 point probe resistance measurements were performed with in situ atomic force microscopy imaging of the film surface during straining . The 4 point probe resistance measurements allow for the examination of the changes in resistance with strain while the surface imaging permits the visualization of localized thinning and crack formation . Furthermore in situ synchrotron tensile tests provide information about the stresses in the film and show the yield stress where the deformation initiates and the relaxation of the film during imaging . A thin 200nm Cu film on 23 m thick PET substrate will be used to illustrate the combined techniques . The combination of electrical measurements surface imaging and stress measurements allow for a better understanding of electromechanical behavior needed for the improvement and future success of flexible electronic devices . 
",Electromechanical properties of metal films are studied with in situ AFM 4PP tensile straining. AFM characterizes the surface damage and 4PP measures the resistance under tensile strain. The difference between local film necking and through thickness cracks can be made. It will be shown when the strain is removed cracks bridge and the resistance decreases.,,,
S0169260715000577," Classifying imbalanced data in medical informatics is challenging . Motivated by this issue this study develops a classifier approach denoted as BSMAIRS . This approach combines borderline synthetic minority oversampling technique and artificial immune recognition system as global optimization searcher with the nearest neighbor algorithm used as a local classifier . Eight electronic medical datasets collected from University of California Irvine machine learning repository were used to evaluate the effectiveness and to justify the performance of the proposed BSMAIRS . Comparisons with several well known classifiers were conducted based on accuracy sensitivity specificity and G mean . Statistical results concluded that BSMAIRS can be used as an efficient method to handle imbalanced class problems . To further confirm its performance BSMAIRS was applied to real imbalanced medical data of lung cancer metastasis to the brain that were collected from National Health Insurance Research Database Taiwan . This application can function as a supplementary tool for doctors in the early diagnosis of brain metastasis from lung cancer . 
",Developing a new classifier approach by combining borderline SMOTE and AIRS. Eight medical datasets from UCI machine learning repository were evaluated. Outperform well known classifiers. The proposed method can handle imbalanced class problems efficiently. Applying successfully to the prediction of lung cancer to brain metastasis.,,,
S0167931713006904," Copper electro chemical deposition of through silicon via is a key challenge of 3D integration . This paper presents a numerical modeling of TSV filling concerning the influence of the accelerator and the suppressor . The diffusion adsorption model was used in the simulation and effects of the additives were incorporated in the model . The boundary conditions were derived from a set of experimental Tafel curves with different concentrations of additives which provided a quick and accurate way for copper ECD process prediction without complicated surface kinetic parameters fitting . The level set method was employed to track the copper and electrolyte interface . The simulation results were in good agreement with the experiments . For a given feature size the current density for superfilling could be predicted which provided a guideline for ECD process optimization . 
",Influence of additives is considered by a set of experimental Tafel curves. TSV filling experiments have been carried out to validate the modeling results. Superfilling of through silicon via can be predicted with this numerical modeling.,,,
S0169260715000590," The assessment of the state of the acrosome is a priority in artificial insemination centres since it is one of the main causes of function loss . In this work boar spermatozoa present in gray scale images acquired with a phase contrast microscope have been classified as acrosome intact or acrosome damaged after using fluorescent images for creating the ground truth . Based on shape prior criteria combined with Otsu s thresholding regional minima and watershed transform the spermatozoa heads were segmented and registered . One of the main novelties of this proposal is that unlike what previous works stated the obtained results show that the contour information of the spermatozoon head is important for improving description and classification . Other of this work novelties is that it confirms that combining different texture descriptors and contour descriptors yield the best classification rates for this problem up to date . The classification was performed with a Support Vector Machine backed by a Least Squares training algorithm and a linear kernel . Using the biggest acrosome intact damaged dataset ever created the early fusion approach followed provides a 0.9913 F Score outperforming all previous related works . 
",A new early fusion approach for acrosome integrity classification is proposed. Specific segmentation based on shape priors was carried out. The biggest acrosome intact damaged dataset ever created was created and published. Acrosome contour is important for improving description and classification. The best result up to date combining shape and texture features has been obtained.,,,
S0168169915002069," An advanced proof of concept real time plant discrimination system is presented that employs two visible laser diodes and one near infrared laser diode . The lasers sequentially illuminate the target ground area and a linear sensor array measures the intensities of the reflected laser beams . The spectral reflectance measurements are then processed by an embedded microcontroller running a discrimination algorithm based on dual Normalised Difference Vegetation Indices . Pre determined plant spectral signatures are used to define unique regions of classification for use by the discrimination algorithm . Measured aggregated NDVI values that fall within a region of classification representing an unwanted plant generate a spray control signal that activates an external spray module thus allowing for a targeted spraying operation . Dynamic outdoor evaluation of the advanced proof of concept real time plant discrimination system using three different plant species and control data determined under static laboratory conditions shows that the system can perform green from green plant detection and accomplish practical discrimination for a vehicle speed of 3km h . 
",We present an advanced real time green from green plant discrimination system. Uniform power multi spot beams are used to illuminate the target vegetation. Spectral reflectance measurements are captured using a line scan sensor. Three laser diodes are used to compute dual NDVI discrimination parameters. Reliable plant detection and discrimination is attained for a 3km h vehicle speed.,,,
S0167947313003381," In several empirical applications analyzing customer by product choice data it may be relevant to partition individuals having similar purchase behavior in homogeneous segments . Moreover should individual and or product specific covariates be available their potential effects on the probability to choose certain products may be also investigated . A model for joint clustering of statistical units and variables is proposed in a mixture modeling framework and an appropriate EM type algorithm for ML parameter estimation is presented . The model can be easily linked with similar proposals appeared in various contexts such as co clustering of gene expression data clustering of words and documents in web mining data analysis . 
",A two level finite mixture model for clustering customers and products is proposed. Clusters of products nested in segments of customers are determined. Customer product features influence the allocation to segments clusters. An appropriate EM algorithm is presented for the special case of purchase counts. The application shows high purchase rate segments linked with clusters of products.,,,
S0169260714002405," Introduction Paroxysmal versus persistent atrial fibrillation can be distinguished based on differences in the spectral parameters of fractionated atrial electrograms . Maximization of these differences would improve characterization of the arrhythmogenic substrate . A novel spectral estimator has been shown previously to provide greater distinction in AF spectral parameters as compared with the Fourier transform estimator . Herein it is described how the differences in NSE spectral parameters can be further improved . Method In 10 persistent and 9 paroxysmal AF patients undergoing electrophysiologic study fractionated electrograms were acquired from the distal bipolar ablation electrode . A total of 204 electrograms were recorded from the pulmonary vein antra and from the anterior and posterior left atrial free wall . The following spectral parameters were measured the dominant frequency which reflects local activation rate the DF amplitude and the mean spectral profile which represents background electrical activity . To optimize differences in parameters between paroxysmal versus persistent AF patients the NSE was varied by selectively removing subharmonics using a threshold . The threshold was altered in steps to determine the optimal subharmonics removal . Results At the optimal threshold level mean differences in persistent versus paroxysmal AF spectral parameters were DA 0.371mV DF 0.737Hz and MP 0.096mV . When subharmonics were not removed the differences were substantially less DA 0.301mV DF 0.699Hz and MP 0.063mV . Conclusions NSE optimization produces greater spectral parameter difference between persistent versus paroxysmal AF data . Quantifying spectral parameter differences can be assistive in characterizing the arrhythmogenic substrate . 
",A novel spectral estimator is used to discern paroxysmal versus persistent atrial fibrillation fractionated electrograms. The estimator is improved by selectively imparting antisymmetry to basis vectors used to construct the power spectrum. This preprocessing step leads to improved differentiation of spectral parameters in paroxysmal versus persistent atrial fibrillation fractionated electrograms. The improvement is potentially useful to better discern regions of atrial fibrillation substrate that should be targeted for catheter ablation. This method has been shown to provide better discernment of atrial fibrillation type as compared with the Fourier transform estimator.,,,
S0169260714002065," In this paper we propose a class of flexible weight functions for use in comparison of two cumulative incidence functions . The proposed weights allow the users to focus their comparison on an early or a late time period post treatment or to treat all time points with equal emphasis . These weight functions can be used to compare two cumulative incidence functions via their risk difference their relative risk or their odds ratio . The proposed method has been implemented in the R CIFsmry package which is readily available for download and is easy to use as illustrated in the example . 
",The proposed class of flexible weight functions allows emphasis on different time periods when comparing cumulative incidence functions between two groups. The comparisons include the risk difference the relative risk and the odds ratio. A large improvement in power was shown when an early weight function was used to compare two cumulative incidence functions with an early difference. The improvement in power under a late difference scenario was more moderate. The companion R package CIFsmry is available for download at CRAN.,,,
S0169260715002461," This paper presents an integrated system for the automatic analysis of mammograms to assist radiologists in confirming their diagnosis in mammography screening . The proposed automated confirmatory system can process a digitalized mammogram online and generates a high quality filtered segmentation of an image for biological interpretation and a texture feature based diagnosis . We use a serial of image pre processing and segmentation techniques including 2D median filtering seeded region growing algorithm image contrast enhancement to remove noise delete radiopaque artifacts and eliminate the projection of the pectoral muscle from a digitalized mammogram . We also develop an entire image texture feature based classification method by combining a Rough set approach to extract five fundamental texture features from images and then an Artificial Neural Network technique to classify a mammogram as normal indicating the presence of a benign lump or representing a malignant tumor . Here 222 random images from the Mammographic Image Analysis Society database are used for the offline ACS training . Once the system is tuned and trained it is ready for the automated use for the analysis and diagnosis of new mammograms . To test the trained system a separate set of 100 random images from the MIAS and another set of 100 random images from the independent BancoWeb database are selected . The proposed ACS is shown to be successful in confirming diagnosis of mammograms from the two independent databases . 
",This paper presents an integrated system for the automatic analysis of mammograms to assist radiologists in confirming their diagnosis in mammography screening. The proposed system consists of four off line stages Image pre processing and segmentation Feature extraction Selection of fundamental features using Rough Set theory Training of an Artificial Neural Network. Once the system is trained and tuned is ready for its on line use. Here the system is successfully tested on two independent databases.,,,
S0169260714003526," Mechanical stimuli play a significant role in the process of long bone development as evidenced by clinical observations and in vivo studies . Up to now approaches to understand stimuli characteristics have been limited to the first stages of epiphyseal development . Furthermore growth plate mechanical behavior has not been widely studied . In order to better understand mechanical influences on bone growth we used Carter and Wong biomechanical approximation to analyze growth plate mechanical behavior and explore stress patterns for different morphological stages of the growth plate . To the best of our knowledge this work is the first attempt to study stress distribution on growth plate during different possible stages of bone development from gestation to adolescence . Stress distribution analysis on the epiphysis and growth plate was performed using axisymmetric finite element analysis in a simplified generic epiphyseal geometry using a linear elastic model as the first approximation . We took into account different growth plate locations morphologies and widths as well as different epiphyseal developmental stages . We found stress distribution during bone development established osteogenic index patterns that seem to influence locally epiphyseal structures growth and coincide with growth plate histological arrangement . 
",Growth plate characteristics affect SOC development. Mechanical environment within growth plate varies according to epiphyseal ossification state. Mechanical stimuli may affect similarly growth plate and epiphyseal ossification.,,,
S0169260714003939," The prediction of the number of clusters in a dataset in particular microarrays is a fundamental task in biological data analysis usually performed via validation measures . Unfortunately it has received very little attention and in fact there is a growing need for software tools libraries dedicated to it . Here we present ValWorkBench a software library consisting of eleven well known validation measures together with novel heuristic approximations for some of them . The main objective of this paper is to provide the interested researcher with the full software documentation of an open source cluster validation platform having the main features of being easily extendible in a homogeneous way and of offering software components that can be readily re used . Consequently the focus of the presentation is on the architecture of the library since it provides an essential map that can be used to access the full software documentation which is available at the supplementary material website . The mentioned main features of ValWorkBench are also discussed and exemplified with emphasis on software abstraction design and re usability . A comparison with existing cluster validation software libraries mainly in terms of the mentioned features is also offered . It suggests that ValWorkBench is a much needed contribution to the microarray software development algorithm engineering community . For completeness it is important to mention that previous accurate algorithmic experimental analysis of the relative merits of each of the implemented measures carried out specifically on microarray data gives useful insights on the effectiveness of ValWorkBench for cluster validation to researchers in the microarray community interested in its use for the mentioned task . 
",Cluster validation for microarray data analysis is an essential task in bioinformatics and biomedicine that is not receiving enough attention. The state of the art does not offer software systems able to help the development of much needed new measures. We propose ValWorkBench and describe its internal modules and classes in order to provide the much needed software platform for the development and testing of new validation measures as well as clustering algorithms. From the programmer point of view no other platform in the same area offers what ValWorkBench provides. From the user point of view the measures contained in it have been the object of rigorous scientific studies i.e. in BMC Bioinformatics 9 2008 462 Algorithms for Molecular Biology 6 2011 1 Natural Computing pages 2014 in press .,,,
S0169260714003010," Analyzing the acceleration photoplethysmogram is becoming increasingly important for diagnosis . However processing an APG signal is challenging especially if the goal is to detect its small components . Accurate detection of c d and e waves is an important first step for any clinical analysis of APG signals . In this paper a novel algorithm that can detect c d and e waves simultaneously in APG signals of healthy subjects that have low amplitude waves contain fast rhythm heart beats and suffer from non stationary effects was developed . The performance of the proposed method was tested on 27 records collected during rest resulting in 97.39 sensitivity and 99.82 positive predictivity . 
",A robust algorithm to detect c d and e waves in the acceleration photoplethysmogram signals. To our knowledge this is the first algorithm published for this purpose. The accurate detection of c d and e waves will enable researchers to monitor serious abnormalities and health problems.,,,
S0167947313001266," A new algorithm is proposed for OLS estimation of linear models with multiple high dimensional category variables . It is a generalization of the within transformation to arbitrary number of category variables . The approach unlike other fast methods for solving such problems provides a covariance matrix for the remaining coefficients . The article also sets out a method for solving the resulting sparse system and the new scheme is shown by some examples to be comparable in computational efficiency to other fast methods . The method is also useful for transforming away groups of pure control dummies . A parallelized implementation of the proposed method has been made available as an R package lfe on CRAN . 
",We generalize the linear within estimator to two or more category variables. We describe a general procedure for projecting out dummy encoded category variables. Parameters for dummy variables can optionally be estimated.,,,
S0167923614001250," Identification of intrinsic characteristics and structure of high dimensional data is an important task for financial analysis . This paper presents a kernel entropy manifold learning algorithm which employs the information metric to measure the relationships between two financial data points and yields a reasonable low dimensional representation of high dimensional financial data . The proposed algorithm can also be used to describe the characteristics of a financial system by deriving the dynamical properties of the original data space . The experiment shows that the proposed algorithm can not only improve the accuracy of financial early warning but also provide objective criteria for explaining and predicting the stock market volatility . 
",A kernel entropy manifold learning algorithm for financial data MLFD MLFD employs the information metric to measure the relationships between two financial data points. MLFD yields reasonable and accurate low dimensional embedding of the original financial data set. The accuracy of the financial early warning is improved by MLFD.,,,
S0169260714003009," This study was performed to evaluate the influences of the myocardial bridges on the plaque initializations and progression in the coronary arteries . The wall structure is changed due to the plaque presence which could be the reason for multiple heart malfunctions . Using simplified parametric finite element model of the coronary artery having myocardial bridge and analyzing different mechanical parameters from blood circulation through the artery we investigated the prediction of the best position for plaque progression . We chose six patients from the angiography records and used data from DICOM images to generate FE models with our software tools for FE preprocessing solving and post processing . We found a good correlation between real positions of the plaque and the ones that we predicted to develop at the proximal part of the myocardial bridges with wall shear stress oscillatory shear index and residence time . This computer model could be additional predictive tool for everyday clinical examination of the patient with myocardial bridge . 
",We evaluated the influences of the myocardial bridges on the plaque localization in the coronary arteries. We investigated the prediction of the best position for plaque progression using WSS OSI and residence time. A good correlation between positions of the plaque at the proximal part of MB with WSS OSI and residence time. This computer model could be additional predictive tool for everyday clinical examination.,,,
S0167947314002345," The lifetime of subjects in reliability and survival analysis in the presence of several causes of failure has attracted attention in the literature . Most studies have simplified the computations by assuming that the causes are independent though this does not hold . Dependent competing risks under progressively hybrid censoring condition using a Marshall Olkin bivariate Weibull distribution is investigated . Maximum likelihood and approximated maximum likelihood estimators are developed for estimating the unknown parameters . Asymptotic distributions of the maximum likelihood estimators are used to construct approximate confidence intervals using the observed Fisher information matrix . Based on a simulation and real applications it is illustrated that when a parametric distributional assumption is nearly true a close approximation could be achieved by deliberately censoring the number of subjects and the study duration using Type II progressively hybrid censoring which might help to save time and money in research studies . 
",Dependent competing risks are investigated. Type II progressively hybrid censoring is considered to save time and costs. Marshall Olkin bivariate Weibull distribution is applied. Simulation study has shown the performances of estimators under the Type II PHCS. Two illustrative examples based on real datasets are provided.,,,
S0169260715002382," Enhancing 2D angiography while maintaining a low radiation dose has become an important research topic . However it is difficult to enhance images while preserving vessel structure details because X ray noise and contrast blood vessels in 2D angiography have similar intensity distributions which can lead to ambiguous images of vessel structures . In this paper we propose a novel and fast vessel enhancement method for 2D angiography . We apply filtering in the principal component analysis domain for vessel regions and background regions separately using assumptions based on energy compaction . First we identify an approximate vessel region using a Hessian based method . Vessel and non vessel regions are then represented sparsely by calculating their optimal bases separately . This is achieved by identifying periodic motion in the vessel region caused by the flow of the contrast medium through the blood vessels when viewed on the time axis . Finally we obtain noise free images by removing noise in the new coordinate domain for the optimal bases . Our method was validated for an X ray system using 10 low dose sets for training and 20 low dose sets for testing . The results were compared with those for a high dose dataset with respect to noise free images . The average enhancement rate was 93.11 0.71 . The average processing time for enhancing video comprising 50 70 frames was 0.80 0.35s which is much faster than the previously proposed technique . Our method is applicable to 2D angiography procedures such as catheterization which requires rapid and natural vessel enhancement . 
",A novel and fast vessel enhancement method for 2D angiography method is proposed. The results showed that we obtain nature enhanced X ray images using our method and the processing time is very small. Our method is applicable to 2D angiography procedures such as catheterization which requires rapid and natural vessel enhancement.,,,
S0169260715000280," Alternative splicing plays a key role in the regulation of the central dogma . Four major types of alternative splicing have been classified as intron retention exon skipping alternative 5 splice sites or alternative donor sites and alternative 3 splice sites or alternative acceptor sites . A few algorithms have been developed to detect splice junctions from RNA Seq reads . However there are few tools targeting at the major alternative splicing types at the exon intron level . This type of analysis may reveal subtle yet important events of alternative splicing and thus help gain deeper understanding of the mechanism of alternative splicing . This paper describes a user friendly R package extracting annotating and analyzing alternative splicing types for sequence alignment files from RNA Seq . SplicingTypesAnno can provide annotation for major alternative splicing at exon intron level . By comparing the annotation from GTF GFF file it identifies the novel alternative splicing sites offer a convenient two level analysis genome scale annotation for users with high performance computing environment and gene scale annotation for users with personal computers generate a user friendly web report and additional BED files for IGV visualization . SplicingTypesAnno is a user friendly R package for extracting annotating and analyzing alternative splicing types at exon intron level for sequence alignment files from RNA Seq . It is publically available at https sourceforge.net projects splicingtypes files or http genome.sdau.edu.cn research software SplicingTypesAnno.html . 
",User friendly R package for identifcation annotating and analyzing alternative splicing events from RNA Seq. Provide annotation for major alternative splicing at exon intron level and identifies novel alternative splicing events using the GTF GFF file. Generates user friendly web report and additional BED files for IGV visualization. Provides comprehensive genome scale analysis for users with High Performance Computing HPC and Gene level analysis for memory efficient personal computers.,,,
S0167947314000619," A judgment post stratified sample is used in order to develop statistical inference for population quantiles and variance . For the th order of the population quantile a test is constructed an estimator is developed and a distribution free confidence interval is provided . An unbiased estimator for the population variance is also derived . For finite sample sizes it is shown that the proposed inferential procedures for quantiles are more efficient than corresponding simple random sampling procedures but less efficient than corresponding ranked set sampling procedures . The variance estimator is less efficient as efficient as or more efficient than a simple random sample variance estimator for small moderately small and large sample sizes respectively . Furthermore it is shown that JPS sample quantile estimators and tests are asymptotically equivalent to RSS estimators and tests in their efficiency comparison . 
",A quantile inference is developed based on a judgment post stratified JPS sample. A variance estimator is constructed for population variance based on a JPS sample. The procedures have higher efficiencies than the ones in a simple random sample. The procedures have slightly lower efficiencies than the ones in a ranked set sample. Inference in a JPS sample is asymptotically equal to the one in a ranked set sample.,,,
S0169260715002321," Background and objective Wireless Capsule Endoscopy can image the portions of the human gastrointestinal tract that were previously unreachable for conventional endoscopy examinations . A major drawback of this technology is that a large volume of data are to be analyzed in order to detect a disease which can be time consuming and burdensome for the clinicians . Consequently there is a dire need of computer aided disease detection schemes to assist the clinicians . In this paper we propose a real time computationally efficient and effective computerized bleeding detection technique applicable for WCE technology . Methods The development of our proposed technique is based on the observation that characteristic patterns appear in the frequency spectrum of the WCE frames due to the presence of bleeding region . Discovering these discriminating patterns we develop a texture feature descriptor based algorithm that operates on the Normalized Gray Level Co occurrence Matrix of the magnitude spectrum of the images . A new local texture descriptor called difference average that operates on NGLCM is also proposed . We also perform statistical validation of the proposed scheme . Results The proposed algorithm was evaluated using a publicly available WCE database . The training set consisted of 600 bleeding and 600 non bleeding frames . This set was used to train the SVM classifier . On the other hand 860 bleeding and 860 non bleeding images were selected from the rest of the extracted images to form the test set . The accuracy sensitivity and specificity obtained from our method are 99.19 99.41 and 98.95 respectively which are significantly higher than state of the art methods . In addition the low computational cost of our method makes it suitable for real time implementation . Conclusion This work proposes a bleeding detection algorithm that employs textural features from the magnitude spectrum of the WCE images . Experimental outcomes backed by statistical validations prove that the proposed algorithm is superior to the existing ones in terms of accuracy sensitivity specificity and computational cost . 
",An automated GI hemorrhage detection scheme from WCE images is proposed.. Features from the NGLCM of the spectrum of the frames and SVM are employed. We introduce difference average a new feature that operates on NGLCM. Validity of the method is confirmed by statistical and graphical analyses. The performance of the proposed scheme compared to the existing ones is promising.,,,
S0167947315003163," The generalized Pareto distribution has been widely used in modelling heavy tail phenomena in many applications . The standard practice is to fit the tail region of the dataset to the GPD separately a framework known as the peaks over threshold in the extreme value literature . In this paper we propose a new GPD parameter estimator under the POT framework to estimate common tail risk measures the Value at Risk and Conditional Tail Expectation for heavy tailed losses . The proposed estimator is based on a nonlinear weighted least squares method that minimizes the sum of squared deviations between the empirical distribution function and the theoretical GPD for the data exceeding the tail threshold . The proposed method properly addresses a caveat of a similar estimator previously advocated and further improves the performance by introducing appropriate weights in the optimization procedure . Using various simulation studies and a realistic heavy tailed model we compare alternative estimators and show that the new estimator is highly competitive especially when the tail risk measures are concerned with extreme confidence levels . 
",A new GPD parameter estimator is proposed. It is based on a nonlinear weighted least squares method. Under the POT framework we estimate tail risk measures. Extensive simulation studies show the new method works well.,,,
S0169260714002077," Active contours are image segmentation methods that minimize the total energy of the contour to be segmented . Among the active contour methods the radial methods have lower computational complexity and can be applied in real time . This work aims to present a new radial active contour technique called pSnakes using the 1D Hilbert transform as external energy . The pSnakes method is based on the fact that the beams in ultrasound equipment diverge from a single point of the probe thus enabling the use of polar coordinates in the segmentation . The control points or nodes of the active contour are obtained in pairs and are called twin nodes . The internal energies as well as the external one Hilbertian energy are redefined . The results showed that pSnakes can be used in image segmentation of short axis echocardiogram images and that they were effective in image segmentation of the left ventricle . The echo cardiologist s golden standard showed that the pSnakes was the best method when compared with other methods . The main contributions of this work are the use of pSnakes and Hilbertian energy as the external energy in image segmentation . The Hilbertian energy is calculated by the 1D Hilbert transform . Compared with traditional methods the pSnakes method is more suitable for ultrasound images because it is not affected by variations in image contrast such as noise . The experimental results obtained by the left ventricle segmentation of echocardiographic images demonstrated the advantages of the proposed model . The results presented in this paper are justified due to an improved performance of the Hilbert energy in the presence of speckle noise . 
",To propose a new method called pSnakes applied and evaluated in the segmentation of the left ventricle from echocardiographic images. To make a comparative study of methods based on active contour methods. To apply the Hilbert transform to calculate the external energy of the proposed pSnakes method.,,,
S0167947315001747," In the dependency rule mining the goal is to discover the most significant statistical dependencies among all possible collapsed contingency tables . Fisher s exact test is a robust method to estimate the significance and it enables efficient pruning of the search space . The problem is that evaluating the required value can be very laborious and the worst case time complexity is where is the data size . The traditional solution is to approximate the significance with the measure which can be estimated in a constant time . However the measure can produce unreliable results . Furthermore it does not support efficient pruning of the search space . As a solution a family of tight upper bounds for Fisher s is introduced . The new upper bounds are fast to calculate and approximate Fisher s value accurately . In addition the new approximations are not sensitive to the data size distribution or smallest expected counts like the based approximation . In practice the execution time depends on the desired accuracy level . According to experimental evaluation the simplest upper bounds are already sufficiently accurate for dependency rule mining purposes and they can be estimated in 0.004 0.1 of the time needed for exact calculation . For other purposes one may need more accurate approximations but even they can be calculated in less than 1 of the exact calculation time . 
",A family of new tight upper bounds to approximate Fisher s is introduced. The new approximations suit for data mining purposes because they are much faster to evaluate than the exact value. Theoretical analysis and empirical evaluation show that the new approximations are very accurate for all practical purposes. The approximations are not sensitive to the data size distribution or small expected counts.,,,
S0167947315001437," Multivariate methods often rely on a sample covariance matrix . The conventional estimators of a covariance matrix require complete data vectors on all subjects an assumption that can frequently not be met . For example in many fields of life sciences that are utilizing modern measuring technology such as mass spectrometry left censored values caused by denoising the data are a commonplace phenomena . Left censored values are low level concentrations that are considered too imprecise to be reported as a single number but known to exist somewhere between zero and the laboratory s lower limit of detection . Maximum likelihood based covariance matrix estimators that allow the presence of the left censored values without substituting them with a constant or ignoring them completely are considered . The presented estimators efficiently use all the information available and thus based on simulation studies produce the least biased estimates compared to often used competing estimators . As the genuine maximum likelihood estimate can be solved fast only in low dimensions it is suggested to estimate the covariance matrix element wise and then adjust the resulting covariance matrix to achieve positive semi definiteness . It is shown that the new approach succeeds in decreasing the computation times substantially and still produces accurate estimates . Finally as an example a left censored data set of toxic chemicals is explored . 
",ML based covariance matrix estimator for left censored data is introduced. Computation times are decreased considerably with parallelized pairwise estimation. The proposed estimators produce unbiased estimates with reasonable variation.,,,
S0168169913002135," Supply chains are increasingly virtualised in response to market challenges and to opportunities offered by nowadays affordable new technologies . Virtual supply chain management does no longer require physical proximity which implies that control and coordination can take place in other locations and by other partners . This paper assesses how the Internet of Things concept can be used to enhance virtualisation of supply chains in the floricultural sector . Virtualisation is expected to have a big impact in this sector where currently still most products physically pass through auction houses on their fixed routes from national growers to national customers . The paper defines the concept of virtualisation and describes different perspectives on virtualisation in literature i.e . the organisational team information technology virtual reality and virtual things perspectives . Subsequently it develops a conceptual framework for analysis of virtualisation in supply chains . This framework is applied in the Dutch floriculture to investigate the existing situation and to define future challenges for virtualisation in this sector . 
",We assess how the Internet of Things can enhance virtualisation in floriculture. We define the concept of virtualisation and different perspectives in literature. A conceptual framework for analysis of virtual supply chains is developed. We analyse the existing situation and future challenges in Dutch floriculture.,,,
S0169260714003708," The purpose of this study was to develop automatic classifiers to simplify the clinical use and increase the accuracy of the forced oscillation technique in the categorisation of airway obstruction level in patients with chronic obstructive pulmonary disease . The data consisted of FOT parameters obtained from 168 volunteers . The first part of this study showed that FOT parameters do not provide adequate accuracy in identifying COPD subjects in the first levels of obstruction as well as in discriminating between close levels of obstruction . In the second part of this study different supervised machine learning techniques were investigated including k nearest neighbour random forest and support vector machines with linear and radial basis function kernels . These algorithms were applied only in situations where high categorisation accuracy 0.9 was not achieved with the FOT parameter alone . It was observed that KNN and RF classifiers improved categorisation accuracy . Notably in four of the six cases studied an AUC 0.9 was achieved . Even in situations where an AUC 0.9 was not achieved there was a significant improvement in categorisation performance . In conclusion machine learning classifiers can help in the categorisation of COPD airway obstruction . They can assist clinicians in tracking disease progression evaluating the risk of future disease exacerbations and guiding therapy . 
",Our aim was to develop automatic classifiers to help the categorisation of airway obstruction level in patients with chronic obstructive pulmonary disease COPD . We used different techniques including k nearest neighbour KNN random forest RF and support vector machines with linear and radial basis function kernels. Feature selection methods were also used to improve the performance of the classifiers. Our findings revealed that KNN and RF classifiers improved categorisation accuracy. Our study and findings will contribute to improvement of tracking COPD progression and guiding therapy.,,,
S0169260714003034," Studies on health domain have shown that health websites provide imperfect information and give recommendations which are not up to date with the recent literature even when their last modified dates are quite recent . In this paper we propose a framework which assesses the timeliness of the content of health websites automatically by evidence based medicine . Our aim is to assess the accordance of website contents with the current literature and information timeliness disregarding the update time stated on the websites . The proposed method is based on automatic term recognition relevance feedback and information retrieval techniques in order to generate time aware structured queries . We tested the framework on diabetes health web sites which were archived between 2006 and 2013 by Archive it using American Diabetes Association s guidelines . The results showed that the proposed framework achieves 65 and 77 accuracy in detecting the timeliness of the web content according to years and pre determined time intervals respectively . Information seekers and web site owners may benefit from the proposed framework in finding relevant and up to date diabetes web sites . 
",The proposed method predicts the time period that a given content belongs to. The archives of high quality diabetes web sites between 2006 and 2013 were utilized. The method s accuracy is 65 in detecting the timeliness according to years. The accuracy increases when detecting timeliness according to time intervals.,,,
S0169260715002308," A confocal microscope provides a sequence of images of the corneal layers and structures at different depths from which medical clinicians can extract clinical information on the state of health of the patient s cornea . A hybrid model based on snake and particle swarm optimisation is proposed in this paper to analyse the confocal endothelium images . The proposed system is able to pre process images detect cells measure cell densities and identify abnormalities in the analysed data sets . Three normal corneal data sets acquired using a confocal microscope and three abnormal confocal endothelium images associated with diseases have been investigated in the proposed system . Promising results are presented and the performance of this system is compared with manual and two morphological based approaches . The average differences between the manual and the automatic cell densities calculated using S PSO and two other morphological based approaches is 5 7 and 13 respectively . The developed system will be deployable as a clinical tool to underpin the expertise of ophthalmologists in analysing confocal corneal images . 
",A new intelligent system to tackle the challenges of confocal corneal endothelium images is developed. This system underpins the expertise of ophthalmologists. It provides clinically useful factors saves a useful amount of clinician time in the process. It is able to model endothelial cells for better evaluation and fast analysis. Useful system for busy clinic and patient care.,,,
S0167931713002025," Anomalous modulation characteristics of Josephson current I c through niobium tunnel junctions at liquid helium temperature were first measured after applying the external magnetic field in perpendicular direction . Josephson current I c was modulated by applying the external magnetic fields parallel and Hz vertical to the junction plane . Modulation characteristics of the I c value upon Hz had hysteresis . Before applying the vertical field modulation characteristics I c were the product of the two Fraunhofer diffraction patterns in Hx and Hy directions which were parallel to the junction edges of the square shape junction respectively . Under the perpendicular magnetic field as much as 4000A m the maximum I c value did not appear at point . After removing this vertical field the I c modulation pattern changed from the product of the two Fraunhofer diffraction patterns to the deformed I c characteristics whose anomalous shape was explained by assuming the extremely low current density at four edges of the square shape except the four corner regions . Some magnetic flux would be trapped perpendicularly inside the junction electrodes within the junction area . After the junction was heated to the room temperature and was again cooled to the liquid He temperature these modulation characteristics again became the normal modulation pattern in which the trapped flux would be released by this thermal cycle . 
",Through Nb superconducting junction with Al oxide current flow without voltage drop. Josephson current I c dependence on Hx Hy and Hz magnetic field was obtained. c dependence on Hx Hy before applying Hz was Fraunhofer pattern. c dependence on Hx Hy after applying Hz was not Fraunhofer pattern. Numerical simulation explained the measured dependence.,,,
S0167839616000029," Let be a cell with a single interior vertex and n boundary vertices . Say that has the interpolation property if for every there is a spline such that for all i . We investigate under what conditions does a cell fail the interpolation property . The question is related to an open problem posed by Alfeld Piper and Schumaker in 1987 about characterization of unconfinable vertices . For hexagonal cells we obtain a geometric criterion characterizing the failure of the interpolation property . As a corollary we conclude that a hexagonal cell such that its six interior edges lie on three lines fails the interpolation property if and only if the cell is projectively equivalent to a regular hexagonal cell . Along the way we obtain an explicit basis for the vector space for 
",Geometric characterization of hexagonal cells without the interpolation property. New examples of unconfinable interior vertices. Interpolation properties of splines depend on the geometry of cells.,,,
S0167839615001417," This paper provides a general formula for the dimension of spline space over general planar T meshes by using the smoothing cofactor conformality method . We introduce a new notion the diagonalizable T mesh where the dimension formula is only associated with the topological information of the T mesh . A necessary and sufficient condition for characterization of the diagonalizable T mesh is also provided . By this new notion we obtain some new dimension results for the spline spaces over T meshes . 
",A general formula for the dimension of the spline space over the T mesh is provided. The diagonalizable T mesh is introduced. A necessary and sufficient condition to characterize the diagonalizable T meshes is proved. Some new dimension results for the spline space over T meshes that do not have a nested structure is provided.,,,
S0169260715000887," We present a software system called Polyp Alert to assist the endoscopist find polyps by providing visual feedback during colonoscopy . Polyp Alert employs our previous edge cross section visual features and a rule based classifier to detect a polyp edge an edge along the contour of a polyp . The technique employs tracking of detected polyp edge to group a sequence of images covering the same polyp as one polyp shot . In our experiments the software correctly detected 97.7 of polyp shots on 53 randomly selected video files of entire colonoscopy procedures . However Polyp Alert incorrectly marked only 4.3 of a full length colonoscopy procedure as showing a polyp when they do not . The test data set consists of about 18h worth of video data from Olympus and Fujinon endoscopes . The technique is extensible to other brands of colonoscopes . Furthermore Polyp Alert can provide as high as ten feedbacks per second for a smooth display of feedback . The performance of our system is by far the most promising to potentially assist the endoscopist find more polyps in clinical practice during a routine screening colonoscopy . 
",We surveyed recent research in polyp detection in screening colonoscopy. We introduced a fast polyp detection algorithm cable of running at 10 frames per second on an off the shelf workstation. It correctly detects 97.7 of polyp shots in our data set. We evaluated our technique on 53 video files of procedures performed using Olympus and Fujinon scopes. Real time polyp detection and feedback potentially help to reduce polyp miss rates to improve quality of care and quality of documentation. The proposed polyp detection algorithm is fast effective and potentially useful for improving quality of colonoscopy.,,,
S0169260714002478," This paper demonstrates the utility of a differencing technique to transform surface EMG signals measured during both static and dynamic contractions such that they become more stationary . The technique was evaluated by three stationarity tests consisting of the variation of two statistical properties i.e . mean and standard deviation and the reverse arrangements test . As a result of the proposed technique the first difference of EMG time series became more stationary compared to the original measured signal . Based on this finding the performance of time domain features extracted from raw and transformed EMG was investigated via an EMG classification problem on data from 18 subjects . The results show that the classification accuracies of all features extracted from the transformed signals were higher than features extracted from the original signals for six different classifiers including quadratic discriminant analysis . On average the proposed differencing technique improved classification accuracies by 2 8 . 
",The stationarity of surface EMG signals during upper limb motions is investigated. The differencing technique can transform EMG signals to become more stationary. The proposed differencing technique improves classification accuracies by 2 8 . Difference absolute mean value exhibits the best performance with an accuracy of 95.82 . EMG derivative features are recommended for use in the classification of dynamic motions.,,,
S0169260714002934," We propose a fast seed detection for automatic tracking of coronary arteries in coronary computed tomographic angiography . To detect vessel regions Hessian based filtering is combined with a new local geometric feature that is based on the similarity of the consecutive cross sections perpendicular to the vessel direction . It is in turn founded on the prior knowledge that a vessel segment is shaped like a cylinder in axial slices . To improve computational efficiency an axial slice which contains part of three main coronary arteries is selected and regions of interest are extracted in the slice . Only for the voxels belonging to the ROIs the proposed geometric feature is calculated . With the seed points which are the centroids of the detected vessel regions and their vessel directions vessel tracking method can be used for artery extraction . Here a particle filtering based tracking algorithm is tested . Using 19 clinical CCTA datasets it is demonstrated that the proposed method detects seed points and can be used for full automatic coronary artery extraction . ROC curve analysis shows the advantages of the proposed method . 
",The seed points are detected for automatic coronary artery tracking from CCTA. The computational efficiency is improved using prior knowledge of vessel structure. Using proposed method a particle filtering based tracking algorithm is tested. ROC curve analysis shows the advantage of the proposed method.,,,
S0167839615001016," This paper presents a novel 3D shape descriptor which explicitly captures the local geometry and encodes it using an efficient representation. Our method consists of multiple evolving fronts which are realized by a set of growing spheres on the surface. At the core of this method is a simple intersection operator between the spheres and the shape s surface. Intersection curves yield a discrete sampling of the surface at different positions and scales. Our key idea is to define a shape descriptor that captures the continuous local geometry of the surface in an efficient and consistent representation by intersecting the surface with multiple spheres and transforming the intersection curve to frequency domain. To evaluate our descriptor we define shape similarity metric and perform shape matching on the SHREC11 non rigid benchmark and other classes. 
",Descriptive and compact feature descriptor based on sphere intersection operator. Fast and meaningful feature detector based on the descriptor. Shape retrieval framework for evaluation against state of the art benchmarks.,,,
S0169260714003873," Registration of pre clinical images to physical space is indispensable for computer assisted endoscopic interventions in operating rooms . Electromagnetically navigated endoscopic interventions are increasingly performed at current diagnoses and treatments . Such interventions use an electromagnetic tracker with a miniature sensor that is usually attached at an endoscope distal tip to real time track endoscope movements in a pre clinical image space . Spatial alignment between the electromagnetic tracker and pre clinical images must be performed to navigate the endoscope to target regions . This paper proposes an adaptive marker free registration method that uses a multiple point selection strategy . This method seeks to address an assumption that the endoscope is operated along the centerline of an intraluminal organ which is easily violated during interventions . We introduce an adaptive strategy that generates multiple points in terms of sensor measurements and endoscope tip center calibration . From these generated points we adaptively choose the optimal point which is the closest to its assigned the centerline of the hollow organ to perform registration . The experimental results demonstrate that our proposed adaptive strategy significantly reduced the target registration error from 5.32 to 2.59 mm in static phantoms validation as well as from at least 7.58mm to 4.71mm in dynamic phantom validation compared to current available methods . 
",We proposed an adaptive mark free registration method for aligning electromagnetic trackers and pre operative images. An endoscope center calibration on the basis of a 2D printed model was presented. A multiple point strategy was proposed to address an assumption of moving endoscopes along the centerline of the intraluminal organ. The registration error was significantly reduced from 7.58 to 4.71mm under dynamic phantom validation.,,,
S0169260715001595," With the aim of controlling drug resistant Plasmodium falciparum a computational attempt of designing novel adduct antimalarial drugs through the molecular docking method of combining chloroquine with five alkaloids individually is presented . These alkaloids were obtained from the medicinal plant Adhatoda vasica . From the obtained individual docking values of important derivatives of quinine and chloroquine as well as individual alkaloids and adduct agents of chloroquine with Adhatoda alkaloids as ligands it was discernible that the adduct agent 1 with chloroquine and adhatodine combination had the minimum energy of interaction as the docking score value of 11.144kcal mol against the target protein triosephosphate isomerase the key enzyme of glycolytic pathway . Drug resistance of P. falciparum is due to a mutation in the polypeptide of TIM . Moratorium of mutant TIM would disrupt the metabolism during the control of the drug resistant P. falciparum . This in silico work helped to locate the adduct agent 1 with chloroquine and adhatodine which could be taken up by pharmacology for further development of this compound as a new drug against drug resistant Plasmodium . 
",Malaria is endemic in the majority of tropical countries and to control strains of Plasmodium falciparum resistant to chloroquine and quinine newer agents are needed. Today artemisinin based drugs are in use but artemisinin resistance in P. falciparum has been reported in several Asian countries. The molecular docking method of combining chloroquine with 5 alkaloids individually of the medicinal plant Adhatoda vasica is presented for the control drug resistant P. falciparum. This in silico work helped to locate the adduct agent 1with chloroquine and adhatodine for further use as a new drug against drug resistant Plasmodium.,,,
S0169260715001777," Premature ventricular contraction is a common type of abnormal heartbeat . Without early diagnosis and proper treatment PVC may result in serious harms . Diagnosis of PVC is of great importance in goal directed treatment and preoperation prognosis . This paper proposes a novel diagnostic method for PVC based on Lyapunov exponents of electrocardiogram beats . The methodology consists of preprocessing feature extraction and classification integrated into the system . PVC beats can be classified and differentiated from other types of abnormal heartbeats by analyzing Lyapunov exponents and training a learning vector quantization neural network . Our algorithm can obtain a good diagnostic result with little features by using single lead ECG data . The sensitivity positive predictability and the overall accuracy of the automatic diagnosis of PVC is 90.26 92.31 and 98.90 respectively . The effectiveness of the new method is validated through extensive tests using data from MIT BIH database . The experimental results show that the proposed method is efficient and robust . 
",Based on the non uniformity of RR interval the premature beats can be diagnosed. The features with Lyapunov exponents of ECG beat are extracted. LVQ neural network is used to identify PVC beats.,,,
S0169260715002199," Setting The infection with Mycobacterium tuberculosis gives a delayed immune response measured by the tuberculine skin test . We present a new technique for evaluation based on automatic detection and measurement of skin temperature due to infrared emission . Design 34 subjects with suspected tuberculosis disease were examined with an IR thermal camera 48h after tuberculin skin injection . Results In 20 subjects IR analysis was positive for tuberculine test . Mean temperature of injection area was higher around 1 C for the positive group . Conclusion IR image analysis achieves similar estimation of tuberculin reaction as the visual evaluation based on higher temperature due to increased heat radiation from the skin lesion . 
",First paper devoted to tuberculine reaction measurement based on thermal imaging. There is good agreement in area length of tuberculin reaction between visual and thermographic evaluation. Automatic generation of positive and negative results of the tuberculin skin test evaluated by infrared imaging.,,,
S0167947313003678," Simulation based forecasting methods for a non Gaussian noncausal vector autoregressive model are proposed . In noncausal autoregressions the assumption of non Gaussianity is needed for reasons of identifiability . Unlike in conventional causal autoregressions the prediction problem in noncausal autoregressions is generally nonlinear implying that its analytical solution is unfeasible and therefore simulation or numerical methods are required in computing forecasts . It turns out that different special cases of the model call for different simulation procedures . Monte Carlo simulations demonstrate that gains in forecasting accuracy are achieved by using the correct noncausal VAR model instead of its conventional causal counterpart . In an empirical application a noncausal VAR model comprised of U.S. inflation and marginal cost turns out superior to the best fitting conventional causal VAR model in forecasting inflation . 
",Forecasting methods for a noncausal VAR model are proposed. Due to the nonlinearity of the prediction problem simulation methods are required. Monte Carlo simulations are used to illustrate the methods. An empirical application to inflation forecasting is presented.,,,
S0167839616300164,"In this paper we propose a novel unsupervised algorithm for automatically segmenting a single 3D shape or co segmenting a family of 3D shapes using deep learning. The algorithm consists of three stages. In the first stage we pre decompose each 3D shape of interest into primitive patches to generate over segmentation and compute various signatures as low level shape features. In the second stage high level features are learned in an unsupervised style from the low level ones based on deep learning. Finally either segmentation or co segmentation results can be quickly reported by patch clustering in the high level feature space. The experimental results on the Princeton Segmentation Benchmark and the Shape COSEG Dataset exhibit superior segmentation performance of the proposed method over the previous state of the art approaches. 
",We propose a novel segmentation and co segmentation approach for 3D shapes. We introduce deep learning into 3D shape segmentation and co segmentation. Our method is data driven but does not need a tedious labeling process. Our algorithm achieves better or comparable performance when compared with the state of the art methods.,,,
S0169260714002491," Multiple statistics show that heart diseases are one of the main causes of mortality in our highly developed societies today . These diseases lead to a change of the physiology of the heart which gives useful information about characteristic and severity of the defect . A fast and reliable diagnosis is the base for successful therapy . As a first step towards recognition of such heart remodeling processes this work proposes a fully automatic processing pipeline for regional classification of the left ventricular wall in ultrasound images of small animals . The pipeline is based on state of the art methods from computer vision and pattern classification . The myocardial wall is segmented and its motion is estimated . A feature extraction using the segmented data is realized to automatically classify the image regions into normal and abnormal myocardial tissue . The performance of the proposed pipeline is evaluated and a comparison of common classification algorithms on ultrasound data of living mice before and after artificially induced myocardial infarction is given . It is shown that the results of this work reaching a maximum accuracy of 91.46 are an encouraging base for further investigation . 
",A fully automatic processing pipeline is proposed for regional classification of the left ventricular wall in ultrasound images of small animals. The pipeline is implemented using state of the art methods from computer vision and pattern classification. Good performance of the processing pipeline is demonstrated on ultrasound data of living mice before and after artificially induced myocardial infarction.,,,
S0169260713002435," This study aimed to focus on medical knowledge representation and reasoning using the probabilistic and fuzzy influence processes implemented in the semantic web for decision support tasks . Bayesian belief networks and fuzzy cognitive maps as dynamic influence graphs were applied to handle the task of medical knowledge formalization for decision support . In order to perform reasoning on these knowledge models a general purpose reasoning engine EYE with the necessary plug ins was developed in the semantic web . The two formal approaches constitute the proposed decision support system aiming to recognize the appropriate guidelines of a medical problem and to propose easily understandable course of actions to guide the practitioners . The urinary tract infection problem was selected as the proof of concept example to examine the proposed formalization techniques implemented in the semantic web . The medical guidelines for UTI treatment were formalized into BBN and FCM knowledge models . To assess the formal models performance 55 patient cases were extracted from a database and analyzed . The results showed that the suggested approaches formalized medical knowledge efficiently in the semantic web and gave a front end decision on antibiotics suggestion for UTI . 
",Bayesian belief networks BBNs and fuzzy cognitive maps FCMs as dynamic influence graphs were applied to handle the task of medical knowledge formalization for decision support. Both approaches were implemented in semantic web framework. For reasoning on these knowledge models a general purpose reasoning engine EYE with the necessary plug ins was developed in the semantic web. To assess the formal models performance the UTI therapy problem was selected as a proof of concept example. The validation results showed that using probabilistic networks and fuzzy cognitive maps in semantic web framework is reliable and efficient for decision support tasks.,,,
S0169260715001510," The histopathological examination of tissue specimens is necessary for the diagnosis and grading of colon cancer . However the process is subjective and leads to significant inter intra observer variation in diagnosis as it mainly relies on the visual assessment of histopathologists . Therefore a reliable computer aided technique which can automatically classify normal and malignant colon samples and determine grades of malignant samples is required . In this paper we propose a novel colon cancer diagnostic system which initially classifies colon biopsy images into normal and malignant classes and then automatically determines the grades of colon cancer for malignant images . To this end various novel structural descriptors which mathematically model and quantify the variation among the structure of normal colon tissues and malignant tissues of various cancer grades have been employed . Radial basis function kernel of support vector machines has been employed as classifier in order to classify grade colon samples based on these descriptors . The proposed system has been tested on 92 malignant and 82 normal colon biopsy images . The classification performance has been measured in terms of various performance measures and quite promising performance has been observed . Compared with previous techniques the proposed system has demonstrated better cancer detection and grading capability . Therefore the proposed CCD system can provide a reliable second opinion to the histopathologists . 
",A novel colon cancer detection and grading system has been proposed. The system exploits structural variation among various colon tissue types. The system mathematically quantifies the variation in terms of some novel features. The shape of lumen is the most discriminating factor among different tissue types. RBF kernel of SVM shows promising results for classification of colon cancer data.,,,
S0167839615001211," In this paper we devise a new algorithm for completing surface with missing geometry and topology founded upon the theory and techniques of sparse signal recovery . The key intuition is that any meaningful 3D shape represented as a discrete mesh almost certainly possesses a low dimensional intrinsic structure which can be expressed as a sparse representation in some transformed domains . Instead of estimating the missing geometry directly our novel method is to find this low dimensional representation which describes the entire original shape . More specifically we find that for many shapes the vertex coordinate function can be well approximated by a very sparse coefficient representation with respect to the dictionary comprising its Laplacian eigenfunctions and it is then possible to recover this sparse representation from partial measurements of the original shape . Taking advantage of the sparsity cue we advocate a novel variational approach for surface inpainting integrating data fidelity constraints on the shape domain with coefficient sparsity constraints on the transformed domain . Because of the powerful properties of Laplacian eigenbasis the inpainting results of our method tend to be smooth and globally coherent with the remaining shape . We demonstrate the performance of our new method via various examples in geometry restoration shape repair and hole filling . 
",A surface inpainting framework based on representations in the transformed domain. Sparsity as a prior to estimate reconstruction coefficients. Laplacian eigenvectors are utilized for constructing reconstruction dictionary.,,,
S0169260715002011," Background and objective The discrimination of Alzheimer s disease and its prodromal stage known as mild cognitive impairment from normal control is important for patients timely treatment . The simultaneous use of multi modality data has been demonstrated to be helpful for more accurate identification . The current study focused on extending a multi modality algorithm and evaluating the method by identifying AD MCI . Methods In this study sparse representation based classification a well developed method in pattern recognition and machine learning was extended to a multi modality classification framework named as weighted multi modality SRC . Data including three modalities of volumetric magnetic resonance imaging fluorodeoxyglucose positron emission tomography and florbetapir PET from the Alzheimer s disease Neuroimaging Initiative database were adopted for AD MCI classification . Results Adopting wmSRC the classification accuracy achieved 94.8 for AD vs. NC 74.5 for MCI vs. NC and 77.8 for progressive MCI vs. stable MCI superior to or comparable with the results of some other state of the art models in recent multi modality researches . Conclusions The wmSRC method is a promising tool for classification with multi modality data . It could be effective for identifying diseases from NC with neuroimaging data which could be helpful for the timely diagnosis and treatment of diseases . 
",Multi modality classification on 113 AD 110 MCI patients and 117 normal controls. Originally single modality SRC was extended as a multi modality framework wmSRC . The wmSRC performed better than each single modality based SRC method. The wmSRC performed better or equally well compared to MKL RF and JRC.,,,
S0169260714002351," Pressure ulcers are considered as one of the most challenging problems that Nursing professionals have to deal with in their daily practice . Nowadays the education on PrUs is mainly based on traditional lecturing seminars and face to face instruction sometimes with the support of photographs of wounds being used as teaching material . This traditional educational methodology suffers from some important limitations which could affect the efficacy of the learning process . This current study has been designed to introduce information and communication technologies in the education on PrU for undergraduate students with the main objective of evaluating the advantages an disadvantages of using ICT by comparing the learning results obtained from using an e learning tool with those from a traditional teaching methodology . In order to meet this major objective a web based learning system named ePULab has been designed and developed as an adaptive e learning tool for the autonomous acquisition of knowledge on PrU evaluation . This innovative system has been validated by means of a randomized controlled trial that compares its learning efficacy with that from a control group receiving a traditional face to face instruction . Students using ePULab gave significantly better learning acquisition scores to post test mean 15.83 than those following traditional lecture style classes to post test mean 11.6 . In this article the ePULab software is described in detail and the results from that experimental educational validation study are also presented and analyzed . 
",A web tool is developed for adaptive self learning of pressure ulcer evaluation. We evaluate the effectiveness of an e learning tool for pressure ulcer diagnosis. The effectiveness of the tool is compared with that from a traditional training. The e learning strategy gives significantly better knowledge acquisition results.,,,
S0167947313002818," Challenges in the analyses of growth mixture models include missing data outliers estimation and model selection . Four non ignorable missingness models to recover the information due to missing data and three robust models to reduce the effect of non normality are proposed . A full Bayesian method is implemented by means of data augmentation algorithm and Gibbs sampling procedure . Model selection criteria are also proposed in the Bayesian context . Simulation studies are then conducted to evaluate the performances of the models the Bayesian estimation method and selection criteria under different situations . The application of the models is demonstrated through the analysis of education data on children s mathematical ability development . The models can be widely applied to longitudinal analyses in medical psychological educational and social research . 
",Four non ignorable missingness models are proposed. Three robust models to deal with outliers are proposed. A full Bayesian method is implemented. Model selection criteria are proposed in a Bayesian context. Three simulation studies and one real data case study are conducted.,,,
S0167931715002488," Transfer printing has been reported recently as a viable route for electronics on flexible substrates . The method involves transferring micro macrostructures such as wires or ultra thin chips from Si wafers to the flexible substrates by using elastomeric transfer substrates such as poly . A major challenge in this process is posed by the residues of PDMS which are left over on Si surface after the nanostructures have been transferred . As insulator PDMS residues make it difficult to realize metal connections and hence pose challenge in the way of using nanostructures as the building blocks for active electronics . This paper presents a method for PDMS residues free transfer of Si micro macrostructures to flexible substrates such as polyimide . The PDMS residues are removed from Si surface by immersing the transferred structures in a solution of quaternary ammonium fluoride such as TBAF and non hydroxylic aprotic solvent such as PMA . The residues are removed at a rate which is about five times faster than the traditional dry etch methods . Unlike traditional alternatives the presented method removes PDMS without attacking the flexible PI substrates . 
",PDMS residues free micro macrostructures. Transfer printing the micro nanostructures to ultra flexible substrates. Removing PDMS using chemical etch and dispersion. Disintegrating and dissolving the PDMS.,,,
S0169260714003915," In this paper the problem of predicting blood glucose concentrations for the treatment of patients with type 1 diabetes is addressed . Predicting BG is of very high importance as most treatments which consist in exogenous insulin injections rely on the availability of BG predictions . Many models that can be used for predicting BG are available in the literature . However it is widely admitted that it is almost impossible to perfectly model blood glucose dynamics while still being able to identify model parameters using only blood glucose measurements . The main contribution of this work is to propose a simple and identifiable linear dynamical model which is based on the static prediction model of standard therapy . It is shown that the model parameters are intrinsically correlated with physician set therapy parameters and that the reduction of the number of model parameters to identify leads to inferior data fits but to equivalent or slightly improved prediction capabilities compared to state of the art models a sign of an appropriate model structure and superior reliability . The validation of the proposed dynamic model is performed using data from the UVa simulator and real clinical data and potential uses of the proposed model for state estimation and BG control are discussed . 
",New glucose prediction model for the treatment of type 1 diabetes. Strongly correlation of model parameters with therapy parameters. Equivalent predictions to models with 50 more parameters. Validation on clinical data as well as UVa simulation data.,,,
S0167839615001375," Rational curves and surfaces are powerful tools for shape representation and geometric modeling . However the real weights are generally difficult to choose except for a few special cases such as representing conics . This paper presents an extension of rational curves and surfaces by replacing the real weights with matrices . The matrix weighted rational curves and surfaces have the same structures as the traditional rational curves and surfaces but the matrix weights can be defined in geometric ways . In particular the weight matrices for the extended rational B zier NURBS or the generalized subdivision curves and surfaces are computed using the normal vectors specified at the control points . Similar to the effects of control points the specified normals can be used to control the curve or the surface s shape efficiently . It is also shown that matrix weighted NURBS curves and surfaces can pass through their control points thus curve or surface reconstruction by the extended NURBS model no longer needs solving any large system but just choosing control points and control normals from the input data . 
",We extend rational curves and surfaces that have real weights to rational curves and surfaces permitting matrix weights. Matrix weighted rational curves and surfaces maintain many similar properties or algorithms of traditional rational curves and surfaces. The matrix weights permit novel geometric means for shape control of rational curves and surfaces. Matrix weighted NURBS curves and surfaces are suitable for curve or surface reconstruction with no need of solving large systems.,,,
S0169260714003472," Many children with motor impairments can not participate in games and jokes that contribute to their formation . Currently commercial computer games there are few options of software and sufficiently flexible access devices to meet the needs of this group of children . In this study a peripheral access device and a 3D computerized game that do not require the actions of dragging clicking or activating various keys at the same time were developed . The peripheral access device consists of a webcam and a supervisory system that processes the images . This method provides a field of action that can be adjusted to various types of motor impairments . To analyze the sensitivity of the commands a virtual course was developed using the scenario of a path of straight lines and curves . A volunteer with good ability in virtual games performed a short training with the virtual course and after 15min of training obtained similar results with a standard keyboard and the adapted peripheral device . A 3D game in the Amazon forest was developed using the Blender 3D tool . This free software was used to model the characters and scenarios . To evaluate the usability of the 3D game the game was tested by 20 volunteers without motor impairments and 13 volunteers with severe motor limitations of the upper limbs . All the volunteers could easily execute all the actions of the game using the adapted peripheral device . The majority positively evaluated the questions of usability and expressed their satisfaction . The computerized game coupled to the adapted device will offer the option of leisure and learning to people with severe motor impairments who previously lacked this possibility . It also provided equality in this activity to all the users . 
",Webcam with supervisory system was developed to allow computer access for people with disabilities. The developed system converts webcam images in command to control a computer game. Virtual game that provides fun without requiring click and drag action. The system provides fun for children with motor difficulties without causing discomfort or embarrassment.,,,
S0167947316000232," Mahalanobis distance may be used as a measure of the disparity between an individual s profile of scores and the average profile of a population of controls . The degree to which the individual s profile is unusual can then be equated to the proportion of the population who would have a larger Mahalanobis distance than the individual . Several estimators of this proportion are examined . These include plug in maximum likelihood estimators medians the posterior mean from a Bayesian probability matching prior an estimator derived from a Taylor expansion and two forms of polynomial approximation one based on Bernstein polynomial and one on a quadrature method . Simulations show that some estimators including the commonly used plug in maximum likelihood estimators can have substantial bias for small or moderate sample sizes . The polynomial approximations yield estimators that have low bias with the quadrature method marginally to be preferred over Bernstein polynomials . However the polynomial estimators sometimes yield infeasible estimates that are outside the 0 1 range . While none of the estimators are perfectly unbiased the median estimators match their definition in simulations their estimates of the proportion have a median error close to zero . The standard median estimator can give unrealistically small estimates and an adjustment is proposed that ensures estimates are always credible . This latter estimator has much to recommend it when unbiasedness is not of paramount importance while the quadrature method is recommended when bias is the dominant issue . 
",We seek an estimate of the population proportion with extreme Mahalanobis index. Nine point estimates are examined in an extensive simulation study. Maximum likelihood estimates have substantial bias. Methods based on polynomial approximations give low bias but can be out of range. An adapted median estimator that always gives sensible estimates is proposed.,,,
S0169260714003228," In conjunction with the advance in computer technology virtual screening of small molecules has been started to use in drug discovery . Since there are thousands of compounds in early phase of drug discovery a fast classification method which can distinguish between active and inactive molecules can be used for screening large compound collections . In this study we used Support Vector Machines for this type of classification task . SVM is a powerful classification tool that is becoming increasingly popular in various machine learning applications . The data sets consist of 631 compounds for training set and 216 compounds for a separate test set . In data pre processing step the Pearson s correlation coefficient used as a filter to eliminate redundant features . After application of the correlation filter a single SVM has been applied to this reduced data set . Moreover we have investigated the performance of SVM with different feature selection strategies including SVM Recursive Feature Elimination Wrapper Method and Subset Selection . All feature selection methods generally represent better performance than a single SVM while Subset Selection outperforms other feature selection methods . We have tested SVM as a classification tool in a real life drug discovery problem and our results revealed that it could be a useful method for classification task in early phase of drug discovery . 
",Our aim was to classify small molecule compounds as drugs and nondrugs using Support Vector Machines SVM . We used three feature selection methods to improve the performance of the SVM classifier. Our findings revealed that data pre processing and feature selection enhance the performance of the classifiers. Our study and findings will contribute to improvement of our understanding of the early phase drug design.,,,
S0167839615001454," In this note we study the regularity of generalized Hermite interpolation and compare it to that of classical Hermite interpolation . While every Hermite interpolation scheme is regular in one variable the classical Hermite interpolation schemes in several variables are regular if and only if they are supported at one point . In this note we exhibit some regular generalized Hermite interpolation schemes supported at two points and study some limitation of existence of such schemes . The existence of such schemes provides a class of counterexamples to a conjecture of Jia and Sharma . 
",We compare regularity properties of generalized and classical Hermite interpolation. Classical Hermite interpolation in several variables is regular iff it interpolates at one point. We exhibit regular generalized Hermite interpolation schemes supported at two points. We study some limitation of existence of such schemes. These schemes provide a class of counterexamples to a conjecture of Jia and Sharma.,,,
S0167947314002448," A Gini based statistical test for a unit root is suggested . This test is based on the well known Dickey Fuller test where the ordinary least squares regression is replaced by the semi parametric Gini regression in modeling the AR process . A residual based bootstrap is used to find critical values . The Gini methodology is a rank based methodology that takes into account both the variate values and the ranks . Therefore it provides robust estimators that are rank based while avoiding loss of information . Furthermore the Gini methodology relies on first order moment assumptions which validates its use for a wide range of distributions . Simulation results validate the Gini based test and indicate its superiority in some design settings in comparison to other available procedures . The Gini based test opens the door for further developments such as a Gini based cointegration test . 
",A Gini based unit root test is developed. The test relies on the semi parametric Gini regression. Includes an in depth numerical comparison of the Gini based test and existing tests. Simulations indicate the superiority of the Gini based test in some design settings.,,,
S0167839616300218," This paper shows that generic 2D Free Form Deformations of degree can be made birational by a suitable assignment of weights to the B zier or B spline control points . An FFD that is birational facilitates operations such as backward mapping for image warping and Extended Free Form Deformation . While birational maps have been studied extensively in the classical algebraic geometry literature this paper is the first to present a family of non linear birational maps that are flexible enough to be of practical use in geometric modeling . 
",A method is proposed for creating a planar free form deformation of degree that has a rational inverse. The control points can be freely positioned and weights are computed that produce a birational map. Weight computation is simple and can be performed in real time as the control points are moved.,,,
S0169260715001376," Background and objective Periodontitis involves progressive loss of alveolar bone around the teeth . Hence automatic alveolar bone loss measurement in periapical radiographs can assist dentists in diagnosing such disease . In this paper we propose an effective method for ABL area localization and denote it as ABLIfBm . Method ABLIfBm is a threshold segmentation method that uses a hybrid feature fused of both intensity and texture measured by the H value of fractional Brownian motion model where the H value is the Hurst coefficient in the expectation function of a fBm curve and is directly related to the value of fractal dimension . Adopting leave one out cross validation training and testing mechanism ABLIfBm trains weights for both features using Bayesian classifier and transforms the radiograph image into a feature image obtained from a weighted average of both features . Finally by Otsu s thresholding it segments the feature image into normal and bone loss regions . Results Experimental results on 31 periodontitis radiograph images in terms of mean true positive fraction and false positive fraction are about 92.5 and 14.0 respectively where the ground truth is provided by a dentist . The results also demonstrate that ABLIfBm outperforms the threshold segmentation method using either feature alone or a weighted average of the same two features but with weights trained differently a level set segmentation method presented earlier in literature and segmentation methods based on Bayesian K NN or SVM classifier using the same two features . Conclusion Our results suggest that the proposed method can effectively localize alveolar bone loss areas in periodontitis radiograph images and hence would be useful for dentists in evaluating degree of bone loss for periodontitis patients . 
",Present an effective threshold segmentation method for localizing alveolar bone loss areas in periodontitis images using a hybrid feature 0.15 fBm H 0.85 1 I both fBm H and I are normalized. TPF FPF of 31 tested radiograph images used and unused in weight training 92.5 14 . 14 lower FPF than level set Bayesian KNN SVM classification using the same two features. The method would be useful for dentists in evaluating degree of bone loss for periodontitis patients.,,,
S0169260714003204," Background and objective Parkinson s disease is the second most common neurodegenerative disease affecting significant portion of elderly population . One of the most frequent hallmarks and usually also the first manifestation of PD is deterioration of handwriting characterized by micrographia and changes in kinematics of handwriting . There is no objective quantitative method of clinical diagnosis of PD . It is thought that PD can only be definitively diagnosed at postmortem which further highlights the complexities of diagnosis . Methods We exploit the fact that movement during handwriting of a text consists not only from the on surface movements of the hand but also from the in air trajectories performed when the hand moves in the air from one stroke to the next . We used a digitizing tablet to assess both in air and on surface kinematic variables during handwriting of a sentence in 37 PD patients on medication and 38 age and gender matched healthy controls . Results By applying feature selection algorithms and support vector machine learning methods to separate PD patients from healthy controls we demonstrated that assessing the in air on surface hand movements led to accurate classifications in 84 and 78 of subjects respectively . Combining both modalities improved the accuracy by another 1 over the evaluation of in air features alone and provided medically relevant diagnosis with 85.61 prediction accuracy . Conclusions Assessment of in air movements during handwriting has a major impact on disease classification accuracy . This study confirms that handwriting can be used as a marker for PD and can be with advance used in decision support systems for differential diagnosis of PD . 
",We exploit the fact that movement during handwriting of a text consists of on surface and in air movements and use it for differential diagnosis of PD. By applying feature selection algorithms and support vector machine learning methods to separate PD from HC we demonstrated 85 classification accuracy. Decision support system based on handwriting analysis can be complementary method to diagnosis made by clinician or other decision support systems.,,,
S0169260714002442," The purpose of this study was the development of a clustering methodology to deal with arterial pressure waveform parameters to be used in the cardiovascular risk assessment . One hundred sixteen subjects were monitored and divided into two groups . The first one was analyzed using APW and biochemical parameters while the remaining 93 healthy subjects were only evaluated through APW parameters . The expectation maximization and k means algorithms were used in the cluster analysis and the risk scores the Systematic COronary Risk Evaluation project the Assessing cardiovascular risk using Scottish Intercollegiate Guidelines Network and the PROspective Cardiovascular M nster commonly used in clinical practice were selected to the cluster risk validation . The result from the clustering risk analysis showed a very significant correlation with ASSIGN and a significant correlation with FRS . The results from the comparison of both groups also allowed to identify the cluster with higher cardiovascular risk in the healthy group . These results give new insights to explore this methodology in future scoring trials . 
",We developed a clustering methodology to deal with arterial pressure waveform APW parameters. A new non invasive device is being evaluated. We collect a database of 116 subjects. This study compares different methods used in the APW analysis towards cardiovascular diseases. It is possible to assess potential cardiovascular risk in healthy subjects using cluster analysis.,,,
S0169260715001492," Background and objective The evaluation of the clinical status of a patient is frequently based on the temporal evolution of some parameters making the detection of temporal patterns a priority in data analysis . Temporal abstraction is a methodology widely used in medical reasoning for summarizing and abstracting longitudinal data . Methods This paper describes JTSA a framework including a library of algorithms for time series preprocessing and abstraction and an engine to execute a workflow for temporal data processing . The JTSA framework is grounded on a comprehensive ontology that models temporal data processing both from the data storage and the abstraction computation perspective . The JTSA framework is designed to allow users to build their own analysis workflows by combining different algorithms . Thanks to the modular structure of a workflow simple to highly complex patterns can be detected . The JTSA framework has been developed in Java 1.7 and is distributed under GPL as a jar file . Results JTSA provides a collection of algorithms to perform temporal abstraction and preprocessing of time series a framework for defining and executing data analysis workflows based on these algorithms and a GUI for workflow prototyping and testing . The whole JTSA project relies on a formal model of the data types and of the algorithms included in the library . This model is the basis for the design and implementation of the software application . Taking into account this formalized structure the user can easily extend the JTSA framework by adding new algorithms . Results are shown in the context of the EU project MOSAIC to extract relevant patterns from data coming related to the long term monitoring of diabetic patients . Conclusions The proof that JTSA is a versatile tool to be adapted to different needs is given by its possible uses both as a standalone tool for data summarization and as a module to be embedded into other architectures to select specific phenotypes based on TAs in a large dataset . 
",JTSA provides a collection of algorithms to perform temporal abstraction and preprocessing of time series. JTSA provides a framework for defining and executing data analysis workflows based on time series processing techniques. JTSA provides a GUI for workflow prototyping and testing.,,,
S0169260714001692," Intestinal abnormalities and ischemia are medical conditions in which inflammation and injury of the intestine are caused by inadequate blood supply . Acute ischemia of the small bowel can be life threatening . Computed tomography is currently a gold standard for the diagnosis of acute intestinal ischemia in the emergency department . However the assessment of the diagnostic performance of CT findings in the detection of intestinal abnormalities and ischemia has been a difficult task for both radiologists and surgeons . Little effort has been found in developing computerized systems for the automated identification of these types of complex gastrointestinal disorders . In this paper a geostatistical mapping of spatial uncertainty in CT scans is introduced for medical image feature extraction which can be effectively applied for diagnostic detection of intestinal abnormalities and ischemia from control patterns . Experimental results obtained from the analysis of clinical data suggest the usefulness of the proposed uncertainty mapping model . 
",An original work on automated identification of bowel ischemia is reported. The computerized detection can greatly assist surgeons in emergency medicine. Testing on clinical data shows the potential application of the proposed approach.,,,
S0169260715001546," Background and objective Meaningful targeting of brain structures is required in a number of experimental designs in neuroscience . Current technological developments as high density electrode arrays for parallel electrophysiological recordings and optogenetic tools that allow fine control of activity in specific cell populations provide powerful tools to investigate brain physio pathology . However to extract the maximum yield from these fine developments increased precision reproducibility and cost efficiency in experimental procedures is also required . Methods We introduce here a framework based on magnetic resonance imaging and digitized brain atlases to produce customizable 3D environments for brain navigation . It allows the use of individualized anatomical and or functional information from multiple MRI modalities to assist experimental neurosurgery planning and in vivo tissue processing . Results As a proof of concept we show three examples of experimental designs facilitated by the presented framework with extraordinary applicability in neuroscience . Conclusions The obtained results illustrate its feasibility for identifying and selecting functionally and or anatomically connected neuronal population in vivo and directing electrode implantations to targeted nodes in the intricate system of brain networks . 
",Framework based on magnetic resonance imaging and digitized 3D brain atlases for brain navigation. It allows the use of individualized information from multiple MRI modalities to assist experimental neurosurgery planning and in vivo tissue processing. Three examples of experimental designs in neuroscience are presented. The results illustrate its feasibility for identifying and selecting connected neuronal population in vivo and directing electrode implantations to targeted nodes in the intricate system of brain networks.,,,
S0169260715000048," Background There is a growing demand for women to be classified into different risk groups of developing breast cancer . The focus of the reported work is on the development of an integrated risk prediction model using a two level fuzzy cognitive map model . The proposed model combines the results of the initial screening mammogram of the given woman with her demographic risk factors to predict the post screening risk of developing BC . Methods The level 1 FCM models the demographic risk profile . A nonlinear Hebbian learning algorithm is used to train this model and thus to help on predicting the BC risk grade based on demographic risk factors identified by domain experts . The risk grades estimated by the proposed model are validated using two standard BC risk assessment models viz . Gail and Tyrer Cuzick . The level 2 FCM models the features of the screening mammogram concerning normal benign and malignant cases . The data driven Hebbian learning algorithm is used to train this model in order to predict the BC risk grade based on these mammographic image features . An overall risk grade is calculated by combining the outcomes of these two FCMs . Results The main limitation of the Gail model of underestimating the risk level of women with strong family history is overcome by the proposed model . IBIS is a hard computing tool based on the Tyrer Cuzick model that is comprehensive enough in covering a wide range of demographic risk factors including family history but it generates results in terms of numeric risk score based on predefined formulae . Thus the outcome is difficult to interpret by naive users . Besides these models are based only on the demographic details and do not take into account the findings of the screening mammogram . The proposed integrated model overcomes the above described limitations of the existing models and predicts the risk level in terms of qualitative grades . The predictions of the proposed NHL FCM model comply with the Tyrer Cuzick model for 36 out of 40 patient cases . With respect to tumor grading the overall classification accuracy of DDNHL FCM using 70 real mammogram screening images is 94.3 . The testing accuracy of the proposed model using 10 fold cross validation technique outperforms other standard machine learning based inference engines . Conclusion In the perspective of clinical oncologists this is a comprehensive front end medical decision support system that assists them in efficiently assessing the expected post screening BC risk level of the given individual and hence prescribing individualized preventive interventions and more intensive surveillance for high risk women . 
",Development of an integrated breast cancer risk prediction model using a two level fuzzy cognitive map FCM model. The proposed model combines the demographic risk factors with the findings of the initial screening mammogram to elicit the hidden and impeding risk of developing breast cancer. Hebbian based learning capabilities of FCM were used to improve the modeling issue and contribute to classification of tumor grading and risk prediction. The accuracy of the system is compared with benchmark machine learning methods showing its superiority.,,,
S0169260714003435," Cell counting is one of the basic needs of most biological experiments . Numerous methods and systems have been studied to improve the reliability of counting . However at present manual cell counting performed with a hemocytometer still represents the gold standard despite several problems limiting reproducibility and repeatability of the counts and at the end jeopardizing their reliability in general . We present our own approach based on image processing techniques to improve counting reliability . It works in two stages first building a high resolution image of the hemocytometer s grid then counting the live and dead cells by tagging the image with flags of different colours . In particular we introduce GridMos a fully automated mosaicing method to obtain a mosaic representing the whole hemocytometer s grid . In addition to offering more significant statistics the mosaic freezes the culture status thus permitting analysis by more than one operator . Finally the mosaic achieved can thus be tagged by using an image editor thus markedly improving counting reliability . The experiments performed confirm the improvements brought about by the proposed counting approach in terms of both reproducibility and repeatability also suggesting the use of a mosaic of an entire hemocytometer s grid then labelled trough an image editor as the best likely candidate for the new gold standard method in cell counting . 
",We present a new two stage image based approach to perform cell counting. Counting is performed on big high resolution images of the whole hemocytometer s grid. Counting the cells by also tagging the big image improves counting reliability. Big images are built using GridMos our fully automated mosaicing software. Both GridMos source code and a standalone executable version are freely available.,,,
S0167839616300127," Hilbert Huang Transform has proven to be extremely powerful for signal processing and analysis in 1D time series and its generalization to regular tensor product domains has also demonstrated its widespread utilities in image processing and analysis . Compared with popular Fourier transform and wavelet transform the most prominent advantage of Hilbert Huang Transform is that it is a fully data driven adaptive method especially valuable for handling non stationary and nonlinear signals . Two key technical elements of Hilbert Huang transform are Empirical Mode Decomposition and Hilbert spectra computation . HHT s uniqueness results from its capability to reveal both global information enabled by EMD and local information and phase information enabled by Hilbert spectra computation from input signals . Despite HHT s rapid advancement in the past decade its theory and applications on surfaces remain severely under explored due to the current technical challenge in conducting Hilbert spectra computation on surfaces . To ameliorate this paper takes a new initiative to compute the Riesz transform on 3D surfaces a natural generalization of Hilbert transform in higher dimensional cases with a goal to make the theoretic breakthrough . The core of our theoretic and computational framework is to fully exploit the relationship between Riesz transform and fractional Laplacian operator which can enable the computation of Riesz transform on surfaces via eigenvalue decomposition of Laplacian matrix . Moreover we integrate the techniques of EMD and our newly proposed Riesz transform on 3D surfaces by monogenic signals to compute Hilbert spectra which include the space frequency energy distribution of signals defined over 3D surfaces and characterize key local feature information . Experiments and applications in spectral geometry processing and prominent feature detection illustrate the effectiveness of the current computational framework of HHT on 3D surfaces which could serve as a solid foundation for upcoming more serious applications in graphics and geometry computing fields . 
",A method to compute Riesz transform RT on surfaces is proposed. The computation of RT depends on the eigenvalue decomposition of Laplacian matrix. EMD and RT on surfaces are integrated by monogenic signals to get Hilbert spectra. The graphics applications based on Hilbert spectra illustrate HHT s potential.,,,
S0167947314002102," An algorithm for the evaluation of the exact Gaussian likelihood of an dimensional vector autoregressive moving average process of order with time dependent coefficients including a time dependent innovation covariance matrix is proposed . The elements of the matrices of coefficients and those of the innovation covariance matrix are deterministic functions of time and assumed to depend on a finite number of parameters . These parameters are estimated by maximizing the Gaussian likelihood function . The advantages of that approach is that the Gaussian likelihood function can be computed exactly and efficiently . The algorithm is based on the Cholesky decomposition method for block band matrices . It is shown that the number of operations as a function of and the size of the series is barely doubled with respect to a VARMA model with constant coefficients . A detailed description of the algorithm followed by a data example is provided . 
",We consider VARMA models with time dependent coefficients. These coefficients and the error variance can be deterministic functions of time. We present an efficient algorithm for computing their exact Gaussian likelihood. It is based on an algorithm for traditional VARMA models. An illustration on financial data is provided.,,,
S0169260715002631," Because of the increased volume of information available to physicians from advanced medical technology the obtained information of each symptom with respect to a disease may contain truth falsity and indeterminacy information . Since a single valued neutrosophic set consists of the three terms like the truth membership indeterminacy membership and falsity membership functions it is very suitable for representing indeterminate and inconsistent information . Then similarity measure plays an important role in pattern recognition and medical diagnosis . However existing medical diagnosis methods can only handle the single period medical diagnosis problem but can not deal with the multi period medical diagnosis problems with neutrosophic information . Hence the purpose of this paper was to propose similarity measures between SVNSs based on tangent function and a multi period medical diagnosis method based on the similarity measure and the weighted aggregation of multi period information to solve multi period medical diagnosis problems with single valued neutrosophic information . Then we compared the tangent similarity measures of SVNSs with existing similarity measures of SVNSs by a numerical example about pattern recognitions to indicate the effectiveness and rationality of the proposed similarity measures . In the multi period medical diagnosis method we can find a proper diagnosis for a patient by the proposed similarity measure between the symptoms and the considered diseases represented by SVNSs and the weighted aggregation of multi period information . Then a multi period medical diagnosis example was presented to demonstrate the application of the proposed diagnosis method and to indicate the effectiveness of the proposed diagnosis method by the comparative analysis . The diagnosis results showed that the developed multi period medical diagnosis method can help doctors make a proper diagnosis by the comprehensive information of multi periods . 
",We proposed similarity measures of SVNSs based on tangent function. We developed a multi period medical diagnosis method using the similarity measure. A diagnosis example showed the application of the proposed diagnosis method.,,,
S0167947314002473," A resolution of the Fisher effect puzzle in terms of statistical inference is attempted . Motivation stems from empirical evidence of time varying coefficients in the data generating process of both the interest rates and inflation rates for 19 OECD countries . These time varying dynamics crucially affect the behaviour of all the co integration estimators considered especially in small samples . When employing simulated critical values instead of asymptotic ones the results provide ample evidence supporting the existence of a long run Fisher effect in which interest rates move one to one with inflation rates in all countries under scrutiny except for Ireland and Switzerland . 
",We introduce a time varying Data Generating Process for the Fisher Effect. We examine the effect of time variation on the behaviour of cointegration estimators. We argue that inference should be based on simulated critical values. We provide evidence that support the validity of the Fisher hypothesis.,,,
S0167947314000462," We propose a frequency domain generalized likelihood ratio test for testing nonstationarity in time series . The test is constructed in the frequency domain by comparing the goodness of fit in the log periodogram regression under the varying coefficient fractionally exponential models . Under such a locally stationary specification the proposed test is capable of detecting dynamic changes of short range and long range dependences in a regression framework . The asymptotic distribution of the proposed test statistic is known under the null stationarity hypothesis and its finite sample distribution can be approximated by bootstrap . Numerical results show that the proposed test has good power against a wide range of locally stationary alternatives . 
",We propose a frequency domain test against nonstationarity in time series. The test is based on comparing the goodness of fit in log periodogram regression. The regression model is the varying coefficient fractionally exponential model. The test is applicable for dynamic changes of both short and long range dependences. The finite sample test distribution is approximated by bootstrap.,,,
S0167947314003569," Retrospective clinical datasets are often characterized by a relatively small sample size and many missing data . In this case a common way for handling the missingness consists in discarding from the analysis patients with missing covariates further reducing the sample size . Alternatively if the mechanism that generated the missing allows incomplete data can be imputed on the basis of the observed data avoiding the reduction of the sample size and allowing methods to deal with complete data later on . Moreover methodologies for data imputation might depend on the particular purpose and might achieve better results by considering specific characteristics of the domain . The problem of missing data treatment is studied in the context of survival tree analysis for the estimation of a prognostic patient stratification . Survival tree methods usually address this problem by using surrogate splits that is splitting rules that use other variables yielding similar results to the original ones . Instead our methodology consists in modeling the dependencies among the clinical variables with a Bayesian network which is then used to perform data imputation thus allowing the survival tree to be applied on the completed dataset . The Bayesian network is directly learned from the incomplete data using a structural expectation maximization procedure in which the maximization step is performed with an exact anytime method so that the only source of approximation is due to the EM formulation itself . On both simulated and real data our proposed methodology usually outperformed several existing methods for data imputation and the imputation so obtained improved the stratification estimated by the survival tree . 
",Retrospective clinical datasets have often small sample size and many missing data. We use Bayesian networks to impute missing data enhancing survival tree analysis. The Bayesian network is learned from incomplete data and used for the imputation. Our method generally achieved more accurate predictions than widely used approaches.,,,
S0169260715000255," The goal of our study is to develop a fast parallel implementation of group independent component analysis for functional magnetic resonance imaging data using graphics processing units . Though ICA has become a standard method to identify brain functional connectivity of the fMRI data it is computationally intensive especially has a huge cost for the group data analysis . GPU with higher parallel computation power and lower cost are used for general purpose computing which could contribute to fMRI data analysis significantly . In this study a parallel group ICA on GPU mainly consisting of GPU based PCA using SVD and Infomax ICA is presented . In comparison to the serial group ICA the proposed method demonstrated both significant speedup with 6 11 times and comparable accuracy of functional networks in our experiments . This proposed method is expected to perform the real time post processing for fMRI data analysis . 
",A parallel group ICA PGICA on GPU was proposed for fMRI data analysis. PGICA demonstrated significant speedup in comparison with serial group ICA. PGICA demonstrated comparable accuracy of functional networks detection.,,,
S0167931715301167," We present a new chlorine free dry etching process which was used to successfully etch indium antimonide grown on gallium arsenide substrates while keeping the substrate temperature below 150 C. By use of a reflowed photoresist mask a sidewall with 60 degree positive slope was achieved whereas a nearly vertical one was obtained when hard masks were used. Long etch tests demonstrated the non selectivity of the process by etching through the entire multi layer epitaxial structure. Electrical and optical measurements on devices fabricated both by wet and dry etch techniques provided similar results proving that the dry etch process does not cause damage to the material. This technique has a great potential to replace the standard wet etching techniques used for fabrication of indium antimonide devices with a non damaging low temperature plasma process. 
",Chlorine free low temperature dry etch of MBE grown InSb on GaAs 60 positive sloped or nearly vertical etch depending on mask Non selective process etching through Insb GaSb and GaAs Low damage process with potential to replace standard InSb wet etch,,,
S0169260714002053," In this paper a passive planar micromixer with ellipse like micropillars is proposed to operate in the laminar flow regime for high mixing efficiency . With a splitting and recombination concept the diffusion distance of the fluids in a micromixer with ellipse like micropillars was decreased . Thus space usage for micromixer of an automatic sample collection system is also minimized . Numerical simulation was conducted to evaluate the performance of proposed micromixer by solving the governing Navier Stokes equation and convection diffusion equation . With software for computational fluid dynamics we simulated the mixing of fluids in a micromixer with ellipse like micropillars and basic T type mixer in a laminar flow regime . The efficiency of the proposed micromixer is shown in numerical results and is verified by measurement results . 
",The SAR structures result in the reduction of the diffusion distance of two fluids. The mixing strength is increased when one stream is injected into the other. Fluids flow faster in SAR mixer for rapid mixing and mixing efficiency.,,,
S0167865513004625," This work addresses the problem of detecting human behavioural anomalies in crowded surveillance environments . We focus in particular on the problem of detecting subtle anomalies in a behaviourally heterogeneous surveillance scene . To reach this goal we implement a novel unsupervised context aware process . We propose and evaluate a method of utilising social context and scene context to improve behaviour analysis . We find that in a crowded scene the application of Mutual Information based social context permits the ability to prevent self justifying groups and propagate anomalies in a social network granting a greater anomaly detection capability . Scene context uniformly improves the detection of anomalies in both datasets . The strength of our contextual features is demonstrated by the detection of subtly abnormal behaviours which otherwise remain indistinguishable from normal behaviour . 
",We propose a novel context aware anomaly detection process applied to human behaviour. We evaluate our method upon 2 different datasets. We utilise a novel social connections metric and a scene model as contextual information. The use of scene contextual information improves the detection of subtle anomalies. The use of social contextual information improves the detection of abnormal behaviour.,,,
S0169260715002175," The aims of this study are summarized in the following items first to investigate the class discrimination power of long term heart rate variability features for risk assessment in patients suffering from congestive heart failure second to introduce the most discriminative features of HRV to discriminate low risk patients and high risk patients and third to examine the influence of feature dimension reduction in order to achieve desired accuracy of the classification . We analyzed two public Holter databases 12 data of patients suffering from mild CHF labeled as LRPs and 32 data of patients suffering from severe CHF labeled as HRPs . A K nearest neighbor classifier was used to evaluate the performance of feature set in the classification . Moreover to reduce the number of features as well as the overlap of the samples of two classes in feature space we used generalized discriminant analysis as a feature extraction method . By applying GDA to the discriminative nonlinear features we achieved sensitivity and specificity of 100 having the least number of features . Finally the results were compared with other similar conducted studies regarding the performance of feature selection procedure and classifier besides the number of features used in training . 
",Investigating the discrimination power of long term HRV for risk assessment in CHF patients. Introducing the significant features of HRV for risk assessment of CHF patients. To examine the influence of GDA in order to achieve desired accuracy of the classification. We achieved sensitivity and specificity of 100 having the least number of features. The results are far better than any other previously reported ones.,,,
S0169260715001960," This study discussed a computer aided program development that meets the requirements of people with physical disabilities . A number of control modes such as electrode signal recorded on the scalp and blink control were combined with the scanning human machine interface to improve the external input output device . Moreover a novel and precise algorithm which filters noise and reduces misrecognition of the system was proposed . A convenient assistive device can assist people with physical disabilities to meet their requirements for independent living and communication with the outside . The traditional scanning keyboard is changed and only the phonetic notations are typed instead of characters thus the time of tone and function selection could be saved and the typing time could be also reduced . Barrier free computer assistive devices and interface for people with physical disabilities in typing or speech could allow them to use a scanning keyboard to select phonetic symbols instead of Chinese characters to express their thoughts . The human machine interface controls can obtain more reliable results as 99.8 connection success rate and 95 typing success rate . 
",A scanning interface with electrical signal detection of the head is designed. A algorithm filters noise and reduces misrecognition of the system. The traditional scanning keyboard is modified. The typing time could be reduced. It can obtain more reliable results as 95 success rate.,,,
S0169260715000036," The importance of evaluating complications and toxicity during and following treatment has been stressed in many publications . In most studies these endpoints are presented descriptively and summarized by numbers and percentages but descriptive methods are rarely sufficient to evaluate treatment related complications . Pepe and Lancar developed Prevalence and Weighted Prevalence functions which take into account the duration and the severity of complication unlike conventional methods of survival analysis or competing risks which are limited to the time to first event . The purpose of this paper is to describe features and use of two R functions main.preval.func and main.wpreval.func which were designed for the analysis of survival adjusted for quality of life . These functions compute descriptive statistics survival and competing risks analysis and especially Prevalence and Weighted Prevalence estimations with confidence intervals and associated test statistics . The use of these functions is illustrated by several examples . 
",The importance of evaluating complications has been stressed in many publications. Descriptive analysis may not be sufficient to evaluate long term complications. This paper provides two R functions using Prevalence and Weighted Prevalence. This method considers duration and repetitions unlike conventional methods.,,,
S0169260714003502," Background Developing countries are confronting a steady growth in the prevalence of the infectious diseases . Mobile technologies are widely available and can play an important role in health care at the regional community and individual levels . Although labs usually able to accomplish the requested blood test and produce the results within two days after receiving the samples but the time for the results to be delivered back to clinics is quite variable depending on how often the motorbike transport makes trips between the clinic and the lab . Objective In this study we seek to assess factors facilitating as well as factors hindering the adoption of mobile devices in the Swazi healthcare through evaluating the end users of the LabPush system . Methods A qualitative study with semi structured and in depth one on one interviews were conducted over two month period July August 2012 . Purposive sampling was used participants were those operating and using the LabPush system at the remote clinics at the national laboratory and the supervisors of users at Swaziland . Interview questions were focused on perceived of ease of use and usefulness of the system . All interviews were recorded and then transcribed . Results This study had aimed its primary focus on reducing TAT prompt patient care reducing bouncing of patients and defaulting of patients which were challenges that the clinicians have always had . Therefore the results revealed several barriers and facilitators to the adoption of mobile device by healthcare providers in the Swaziland . The themes Shortens TAT Technical support Patient centered care Mindset Improved communication Missing Reports Workload Workflow Security of smart phone Human error and Ownership are sorted by facilitators to barriers . Conclusion Thus the end users perspective prompt patient care reduced bouncing of patients technical support better communication willing participant and social influence were facilitators of the adoption m health in the Swazi healthcare . 
",This study seeks to assess factors facilitating as well as factors hindering the adoption of mobile devices in the Swazi healthcare. This study evaluated the end users of the LabPush system in Swaziland. This study provides end users perspective prompt patient care reduced bouncing of patients technical support better communication willing participant and social influence were facilitators of the adoption m health in the Swazi healthcare.,,,
S0167923613001814," Networks permitting anonymous contributions continue to expand and flourish . In some networks the reliability of a contribution is not of particular importance . In other settings however the development of a network is driven by specific purposes which make the reliability of information exchanged of significant importance . One such situation involves the use of information markets for aggregating individuals preferences on new or emerging technologies . At this point there remains skepticism concerning the reliability of the preference revelations in such markets and thus the resulting preference aggregations and rankings of emerging technologies . In this paper we study the reliability of on line preference revelation using a series of controlled laboratory experiments . Our analysis includes individuals pre and post experiment rankings of technologies individual trading and accumulation activities during an electronics market experiment the final experimental market outcomes and a ranking of the same technologies by a panel of experts from a Fortune 5 company . In addition as a final step we allowed each participant to actually select and keep a unit of one of the technologies at zero price . That is we were able to observe each participant s actual final true preference from the set of technologies . 
",Outcome from information market is much consistent with expert panel. Information market does influence individual preferences. There is considerable inconsistency between survey and individual true preferences.,,,
S0169260715000206," The aim of this study is to design a robust feature extraction method for the classification of multiclass EEG signals to determine valuable features from original epileptic EEG data and to discover an efficient classifier for the features . An optimum allocation based principal component analysis method named as OA PCA is developed for the feature extraction from epileptic EEG data . As EEG data from different channels are correlated and huge in number the optimum allocation scheme is used to discover the most favorable representatives with minimal variability from a large number of EEG data . The principal component analysis is applied to construct uncorrelated components and also to reduce the dimensionality of the OA samples for an enhanced recognition . In order to choose a suitable classifier for the OA PCA feature set four popular classifiers least square support vector machine naive bayes classifier k nearest neighbor algorithm and linear discriminant analysis are applied and tested . Furthermore our approaches are also compared with some recent research work . The experimental results show that the LS SVM 1v1 approach yields 100 of the overall classification accuracy improving up to 7.10 over the existing algorithms for the epileptic EEG data . The major finding of this research is that the LS SVM with the 1v1 system is the best technique for the OA PCA features in the epileptic EEG signal classification that outperforms all the recent reported existing methods in the literature . 
",Development of a novel feature extraction method denoted as OA PCA. Introducing Optimum allocation approach that in our innovative concept to get most representative data points from a time window. Better performances than some existing methods.,,,
S0169260714002533," Background The report from the Institute of Medicine To Err Is Human Building a Safer Health System in 1999 drew a special attention towards preventable medical errors and patient safety . The American Reinvestment and Recovery Act of 2009 and federal criteria of Meaningful use stage 1 mandated e prescribing to be used by eligible providers in order to access Medicaid and Medicare incentive payments . Inappropriate prescribing has been identified as a preventable cause of at least 20 of drug related adverse events . A few studies reported system related errors and have offered targeted recommendations on improving and enhancing e prescribing system . Objective This study aims to enhance efficiency of the e prescribing system by shortening the medication list reducing the risk of inappropriate selection of medication as well as in reducing the prescribing time of physicians . Method 103.48 million prescriptions from Taiwan s national health insurance claim data were used to compute Diagnosis Medication association . Furthermore 100 000 prescriptions were randomly selected to develop a smart medication recommendation model by using association rules of data mining . Results and conclusion The important contribution of this model is to introduce a new concept called Mean Prescription Rank of prescriptions and Coverage Rate of prescriptions . A proactive medication list was computed using MPR and CR . With this model the medication drop down menu is significantly shortened thereby reducing medication selection errors and prescription times . The physicians will still select relevant medications even in the case of inappropriate selection . 
",This model is to introduce a new concept called Mean Prescription Rank of prescriptions. The physicians will able to select relevant medications even in the case of inappropriate unintentional selection. To enhance efficiency of the e prescribing system.,,,
S0167947316300184," At about the same time R. Liu introduced the notion of simplicial depth and R. Randles the notion of interdirections . These completely independent and seemingly unrelated initiatives serving different purposes in nonparametric multivariate analysis have spawned significant activity within their quite different respective domains . A surprising and fruitful connection between the two notions is shown . Exploiting the connection statistical procedures based on interdirections can be modified to use simplicial depth instead at considerable reduction of computational burden in the case of dimensions 2 3 and 4 . Implications regarding multivariate sign test statistics are discussed in detail and several other potential applications are noted . 
",An important connection between Liu s simplicial depth and Randles interdirections. Computational advantage of simplicial depth over interdirections. Basis for profitably modifying statistical procedures based on interdirections. New perspectives on multivariate sign tests.,,,
S0167865513003322," In this paper we propose a new method for skin detection in color images which consists in spatial analysis using the introduced texture based discriminative skin presence features . Color based skin detection has been widely explored and many skin color modeling techniques were developed so far . However efficacy of the pixel wise classification is limited due to an overlap between the skin and non skin pixels reported in many color spaces . To increase the discriminating power of the skin classification schemes textural and spatial features are often exploited for skin modeling . Our contribution lies in using the proposed discriminative feature space as a domain for spatial analysis of skin pixels . Contrary to existing approaches we extract the textural features from the skin probability maps rather than from the luminance channel . Presented experimental study confirms that the proposed method outperforms alternative skin detection techniques which also involve analysis of textural and spatial features . 
",The skin probability maps are refined using discriminative features DSPFs . Spatial analysis of skin pixels is more successful when performed in DSPF skin maps. The textural features are extracted from skin maps rather than from the grayscale.,,,
S0169260715002400," Automated fracture detection is an essential part of a computer aided tele medicine system . In this paper we have proposed a unified technique for the detection and evaluation of orthopaedic fractures in long bone digital X ray image . We have also developed a software tool that can be conveniently used by paramedics or specialist doctors . The proposed tool first segments the bone region of an input digital X ray image from its surrounding flesh region and then generates the bone contour using an adaptive thresholding approach . Next it performs unsupervised correction of bone contour discontinuities that might have been generated because of segmentation errors and finally detects the presence of fracture in the bone . Moreover the method can also localize the line of break for easy visualization of the fracture identify its orientation and assess the extent of damage in the bone . Several concepts from digital geometry such as relaxed straightness and concavity index are utilized to correct contour imperfections and to detect fracture locations and type . Experiments on a database of several long bone digital X ray images show satisfactory results . 
",Input Bone X ray image. Output Segmentation of bone contour from the surrounding region. Correct false discontinuities in the bone contour based on digital geometry. Generate an isothetic outer cover of the bone contour. Identify fracture points and the line of fracture.,,,
S0169260715000875," Background and objective Cardiac arrhythmias are disorders in terms of speed or rhythm in the heart s electrical system . Atrial fibrillation is the most common sustained arrhythmia that affects a large number of persons . Electrophysiologic study procedures are used to study fibrillation in patients they consist of inducing a controlled fibrillation in surgical room to analyze electrical heart reactions or to decide for implanting medical devices . Nevertheless the spontaneous induction may generate an undesired AFib which may induce risk for patient and thus a critical issue for physicians . We study the unexpected AFib onset aiming to identify signal patterns occurring in time interval preceding an event of spontaneous fibrillation . Profiling such signal patterns allowed to design and implement an AFib prediction algorithm able to early identify a spontaneous fibrillation . The objective is to increase the reliability of EPS procedures . Methods We gathered data signals collected by a General Electric Healthcare s CardioLab electrophysiology recording system . We extracted superficial and intracavitary cardiac signals regarding 50 different patients studied at the University Magna Graecia Cardiology Department . By studying waveform of intracavitary signals before the onset of the arrhythmia we were able to define patterns related to AFib onsets that are side effects of an inducted fibrillation . Results A framework for atrial fibrillation prediction during electrophysiological studies has been developed . It includes a prediction algorithm to alert an upcoming AFib onset . Tests have been performed on an intracavitary cardiac signals data set related to patients studied in electrophysiological room . Also results have been validated by the clinicians proving that the framework can be useful in case of integration with the polygraph helping physicians in managing and controlling of patient status during EPS . 
",HRV analysis designed to work with R wave also works on A waves. pre AFib signal portions are predictive of spontaneous AFib onset. An automatic device to alert the physician about an incoming AFib onset can be built.,,,
S0169260714002089," Interpenetrated polymer networks composed by two independent polymeric networks that spatially interpenetrate are considered as valuable systems to control permeability and mechanical properties of hydrogels for biomedical applications . Specifically poly poly IPNs have been explored as good hydrogels for mimicking articular cartilage . These lattices are proposed as matrix implants in cartilage damaged areas to avoid the discontinuity in flow uptake preventing its deterioration . The permeability of these implants is a key parameter that influences their success by affecting oxygen and nutrient transport and removing cellular waste products to healthy cartilage . Experimental try and error approaches are mostly used to optimize the composition of such structures . However computational simulation may offer a more exhaustive tool to test and screen out biomaterials mimicking cartilage avoiding expensive and time consuming experimental tests . An accurate and efficient prediction of material s permeability and internal directionality and magnitude of the fluid flow could be highly useful when optimizing biomaterials design processes . Here we present a 3D computational model based on Sussman Bathe hyperelastic material behaviour . A fluid structure analysis is performed with ADINA software considering these materials as two phases composites where the solid part is saturated by the fluid . The model is able to simulate the behaviour of three non biodegradable hydrogel compositions where percentages of PEA and PHEA are varied . Specifically the aim of this study is to verify the validity of the Sussman Bathe material model to simulate the response of the PEA PHEA biomaterials to predict the fluid flux and the permeability of the proposed IPN hydrogels and to study the material domains where the passage of nutrients and cellular waste products is reduced leading to an inadequate flux distribution in healthy cartilage tissue . The obtained results show how the model predicts the permeability of the PEA PHEA hydrogels and simulates the internal behaviour of the samples and shows the distribution and quantification of fluid flux . 
",A 3D computational model to study the behaviour of interpenetrated polymer networks IPNs for cartilage repairing is proposed. The model can predict the permeability of the poly ethyl acrylate PEA poly 2 hydroxyethyl acrylate PHEA IPNs. The model is able to simulate the internal distribution and quantification of fluid flux of the two phases composite. The model enables us to study the nutrient supply within the different biomaterial regions.,,,
S0169260715001522," Background and objective The accurate identification of fat droplets is a prerequisite for the automatic quantification of steatosis in histological images . A major challenge in this regard is the distinction between clustered fat droplets and vessels or tissue cracks . Methods We present a new method for the identification of fat droplets that utilizes adjacency statistics as shape features . Adjacency statistics are simple statistics on neighbor pixels . Results The method accurately identified fat droplets with sensitivity and specificity values above 90 . Compared with commonly used shape features adjacency statistics greatly improved the sensitivity toward clustered fat droplets by 29 and the specificity by 17 . On a standard personal computer megapixel images were processed in less than 0.05s . Conclusions The presented method is simple to implement and can provide the basis for the fast and accurate quantification of steatosis . 
",A new method for identifying fat droplets in histological images is presented. Adjacency statistics are utilized as shape features. Fat droplets are identified with high sensitivity and specificity. Adjacency statistics greatly improve the identification of clustered fat droplets. The method can be quickly executed on standard computers.,,,
S0167926015000231," The increasing complexity of VLSI digital systems has dramatically supported system level representations in modeling and design activities . This evolution makes often necessary a compliant rearrangement of the modalities followed in validation and analysis tasks as in the case of power performances estimation . Nowadays transaction level paradigms are having a wider and wider consideration in the research on electronic system level design techniques . With regard to the available modeling resources the most relevant framework is probably the transaction level extension of the SystemC language which therefore represents the best platform for defining transaction level design techniques . In this paper we present a macro modeling power estimation methodology valid for SystemC TLM prototypes and of general applicability . The present discussion illustrates the implementation modalities of the proposed approach verifying its effectiveness through a comparison with RTL estimation techniques . 
",The paper deals with power analysis on transaction level models of complex digital systems. We propose an estimation methodology strictly based on the modeling constructs of SystemC TLM and of general applicability. We consider the comparison with RTL power estimation techniques on several design aspects estimation accuracy simulation time and modeling effort The experimental results show the design advantages of the proposed methodology with respect to RTL estimation techniques.,,,
S0169260715002369," The visualization of multiple 3D objects has been increasingly required for recent applications in medical fields . Due to the heterogeneity in data representation or data configuration it is difficult to efficiently render multiple medical objects in high quality . In this paper we present a novel intermixing scheme for fusion rendering of multiple medical objects while preserving the real time performance . First we present an in slab visibility interpolation method for the representation of subdivided slabs . Second we introduce virtual zSlab which extends an infinitely thin boundary into a slab with a finite thickness . Finally based on virtual zSlab and in slab visibility interpolation we propose a slab based visibility intermixing method with the newly proposed rendering pipeline . Experimental results demonstrate that the proposed method delivers more effective multiple object renderings in terms of rendering quality compared to conventional approaches . And proposed intermixing scheme provides high quality intermixing results for the visualization of intersecting and overlapping surfaces by resolving aliasing and z fighting problems . Moreover two case studies are presented that apply the proposed method to the real clinical applications . These case studies manifest that the proposed method has the outstanding advantages of the rendering independency and reusability . 
",A novel intermixing scheme is proposed for fusion rendering of multiple medical objects. High quality intermixing results are acquired for intersecting and overlapping surfaces. Our method resolved aliasing and z fighting problems.,,,
S0169260715002163," Cardiopulmonary resuscitation is a first aid key survival technique used to stimulate breathing and keep blood flowing to the heart . Its effective administration can significantly increase the chances of survival for victims of cardiac arrest . LISSA is a serious game designed to complement CPR teaching and also to refresh CPR skills in an enjoyable way . The game presents an emergency situation in a 3D virtual environment and the player has to save the victim applying the CPR actions . In this paper we describe LISSA and its evaluation in a population composed of 109 nursing undergraduate students enrolled in the Nursing degree of our university . To evaluate LISSA we performed a randomized controlled trial that compares the classical teaching methodology composed of self directed learning for theory plus laboratory sessions with a mannequin for practice with the one that uses LISSA after self directed learning for theory and before laboratory sessions with a mannequin . From our evaluation we observed that students using LISSA gave significantly better learning acquisition scores than those following traditional classes . To evaluate the differences between students of these groups we performed a paired samples t test between Group 1 and 2 and between students of Group 1 and 3 . From these tests we observed that there are significant differences in both cases . We also evaluated student performance of main steps of CPR protocol . Students that use LISSA performed better than the ones that did not use it . 
",LISSA is a serious game designed to complement CPR teaching in an enjoyable way. LISSA has been evaluated in a population 109 nursing undergraduate students. Students using LISSA gave significantly better learning acquisition scores than those following traditional classes. Students that use LISSA performed the CPR protocol better than the ones that did not use it.,,,
S0169260715001996," Glaucoma is an optic neuropathy which is one of the main causes of permanent blindness worldwide . This paper presents an automatic image processing based method for detection of glaucoma from the digital fundus images . In this proposed work the discriminatory parameters of glaucoma infection such as cup to disc ratio neuro retinal rim area and blood vessels in different regions of the optic disc has been used as features and fed as inputs to learning algorithms for glaucoma diagnosis . These features which have discriminatory changes with the occurrence of glaucoma are strategically used for training the classifiers to improve the accuracy of identification . The segmentation of optic disc and cup based on adaptive threshold of the pixel intensities lying in the optic nerve head region . Unlike existing methods the proposed algorithm is based on an adaptive threshold that uses local features from the fundus image for segmentation of optic cup and optic disc making it invariant to the quality of the image and noise content which may find wider acceptability . The experimental results indicate that such features are more significant in comparison to the statistical or textural features as considered in existing works . The proposed work achieves an accuracy of 94.11 with a sensitivity of 100 . A comparison of the proposed work with the existing methods indicates that the proposed approach has improved accuracy of classification glaucoma from a digital fundus which may be considered clinically significant . 
",The optic nerve head region is analyzed for glaucoma diagnosis. Adaptive threshold based segmentation of optic disc and cup is implemented. CDR NRR and blood vessel area as per ISNT rule are selected as features. An accuracy of 94.11 with sensitivity of 100 is achieved using SVM classifier.,,,
S0167839615001041," Extended Chebyshev spaces that also comprise the constants represent large families of functions that can be used in real life modeling or engineering applications that also involve important integral or rational curves and surfaces . Concerning CAGD the unique normalized B bases of such vector spaces ensure optimal shape preserving properties important evaluation or subdivision algorithms and useful shape parameters . Therefore we propose global explicit formulas for the entries of those transformation matrices that map these normalized B bases to the traditional bases of the underlying vector spaces . Then we also describe general and ready to use control point configurations for the exact representation of those traditional integral parametric curves and surfaces that are specified by coordinate functions given as linear combinations of ordinary basis functions . The obtained results are also extended to the control point and weight based exact description of the rational counterpart of these integral parametric curves and surfaces . The universal applicability of our methods is presented through polynomial trigonometric hyperbolic or mixed extended Chebyshev vector spaces . 
",We consider extended Chebyshev spaces that also comprise the constant functions. We transform the unique normalized B bases of these spaces to their ordinary ones. The proposed general basis transformation is based only on endpoint derivatives. Ordinary curves surfaces can be described by explicit control point based formulas. Our formulas can be more efficient than other numerical methods up to dimension 16.,,,
S0169260715002655," It is desirable to reduce the excessive radiation exposure to patients in repeated medical CT applications . One of the most effective ways is to reduce the X ray tube current or tube voltage . However it is difficult to achieve accurate reconstruction from the noisy measurements . Compared with the conventional filtered back projection algorithm leading to the excessive noise in the reconstructed images the approaches using statistical iterative reconstruction with low mAs show greater image quality . To eliminate the undesired artifacts and improve reconstruction quality we proposed in this work an improved SIR algorithm for low dose CT reconstruction constrained by a modified Markov random field regularization . Specifically the edge preserving total generalized variation which is a generalization of total variation and can measure image characteristics up to a certain degree of differentiation was introduced to modify the MRF regularization . In addition a modified alternating iterative algorithm was utilized to optimize the cost function . Experimental results demonstrated that images reconstructed by the proposed method could not only generate high accuracy and resolution properties but also ensure a higher peak signal to noise ratio in comparison with those using existing methods . 
",Proposed a novel regularization scheme MMRF for CT reconstruction. Present a new objective function and deduce its iterative equation. Optimize the objective function by a modified alternative iterative algorithm. Design experiments on different phantoms to validate the validity of the approach. Analyze the results by visual evaluation and quantitative evaluation.,,,
S0169260715000942," We propose the signal processing technique of calculating a cross correlation function and an average deviation between the continuous blood glucose and the interpolation of limited blood glucose samples to evaluate blood glucose monitoring frequency in a self aware patient software agent model . The diabetic patient software agent model is a 24 h circadian self aware stochastic model of a diabetic patient s blood glucose levels in a software agent environment . The purpose of this work is to apply a signal processing technique to assist patients and physicians in understanding the extent of a patient s illness using a limited number of blood glucose samples . A second purpose of this work is to determine an appropriate blood glucose monitoring frequency in order to have a minimum number of samples taken that still provide a good understanding of the patient s blood glucose levels . For society in general the monitoring cost of diabetes is an extremely important issue and these costs can vary tremendously depending on monitoring approaches and monitoring frequencies . Due to the cost and discomfort associated with blood glucose monitoring today patients expect monitoring frequencies specific to their health profile . The proposed method quantitatively assesses various monitoring protocols in nine predefined categories of patient agents in terms of risk factors of health status and age . Simulation results show that sampling 6 times per day is excessive and not necessary for understanding the dynamics of the continuous signal in the experiments . In addition patient agents in certain conditions only need to sample their blood glucose 1 time per week to have a good understanding of the characteristics of their blood glucose . Finally an evaluation scenario is developed to visualize this concept in which appropriate monitoring frequencies are shown based on the particular conditions of patient agents . This base line can assist people in determining an appropriate monitoring frequency based on their personal health profile . 
",A cross correlation function and an average deviation between the continuous blood glucose and the interpolation of limited blood glucose samples are calculated. This model evaluates blood glucose monitoring frequency in a self aware patient software agent model. This model provides diabetes patients and physicians a good understanding of the illness from limited blood glucose samples. We propose a new way to determine an appropriate blood glucose monitoring frequency to reduce the cost and discomfort associated with blood glucose monitoring. We propose a base line to assist people to determine an individual appropriate monitoring frequency based on their situations.,,,
S0167947314003338," The intraclass correlation coefficient in a two way analysis of variance is a ratio involving three variance components . Two recently developed methods for constructing confidence intervals for the ICC are the Generalized Confidence Interval and Modified Large Sample methods . The resulting intervals have been shown to maintain nominal coverage . But methods for determining sample size for GCI and MLS intervals are lacking . Sample size methods that guarantee control of the mean width for GCI and MLS intervals are developed . In the process two variance reduction methods are employed called dependent conditioning and inverse Rao Blackwellization . Asymptotic results provide lower bounds for mean CI widths and show that MLS and GCI widths are asymptotically equivalent . Simulation studies are used to investigate the new methods . A real data example is used and application issues discussed . The new methods are shown to result in adequate sample size estimates the asymptotic estimates are accurate and the variance reduction techniques are effective . A sample size program is developed . R program can be downloaded at http dobbinuga.com . Future extensions of these results are discussed . 
",Sample size methods and programs for reliability studies are developed. The two way balanced analysis of variance model without interaction is the focus. The sample size guarantees a user specified mean confidence interval width. Modified large sample and generalized confidence interval methods are used. Novel computational algorithms are developed studied and implemented.,,,
S0167839616300292," A Quasi Extended Chebyshev space is a space of sufficiently differentiable functions in which any Hermite interpolation problem which is not a Taylor problem is unisolvent . On a given interval the class of all spaces which contains constants and for which the space obtained by differentiation is a QEC space has been identified as the largest class of spaces which can be used for design . As a first step towards determining the largest class of splines for design we consider a sequence of QEC spaces on adjacent intervals all of the same dimension we join them via connection matrices so as to maintain both the dimension and the unisolvence . The resulting space is called a Quasi Extended Chebyshev Piecewise space . We show that all QECP spaces are inverse images of two dimensional Chebyshev spaces under piecewise generalised derivatives associated with systems of piecewise weight functions . We show illustrations proving that QECP spaces can produce interesting shape effects . 
",Introduce Quasi Extended Chebyshev piecewise spaces. Develop their properties for design e.g. existence pseudoaffinity of blossoms. Determine the class of all such spaces. Provide illustrations.,,,
S0169260715001790," This study applied a simulation method to map the temperature distribution based on magnetic resonance imaging of individual patients and investigated the influence of different pelvic tissue types as well as the choice of thermal property parameters on the efficiency of endorectal cooling balloon . MR images of four subjects with different prostate sizes and pelvic tissue compositions including fatty tissue and venous plexus were analyzed . The MR images acquired using endorectal coil provided a realistic geometry of deformed prostate that resembled the anatomy in the presence of ECB . A single slice with the largest two dimensional cross sectional area of the prostate gland was selected for analysis . The rectal wall prostate gland peri rectal fatty tissue peri prostatic fatty tissue peri prostatic venous plexus and urinary bladder were manually segmented . Pennes bioheat thermal model was used to simulate the temperature distribution dynamics by using an in house finite element mesh based solver written in MATLAB . The results showed that prostate size and periprostatic venous plexus were two major factors affecting ECB cooling efficiency . For cases with negligible amount of venous plexus and small prostate the average temperature in the prostate and neurovascular bundles could be cooled down to 25 C within 30min . For cases with abundant venous plexus and large prostate the temperature could not reach 25 C at the end of 3h cooling . Large prostate made the cooling difficult to propagate through . The impact of fatty tissue on cooling effect was small . The filling of bladder with warm urine during the ECB cooling procedure did not affect the temperature in the prostate or NVB . In addition to the 2D simulation in one case a 3D pelvic model was constructed for volumetric simulation . It was found that the 2D slice with the largest cross sectional area of prostate had the most abundant venous plexus and was the most difficult slice to cool thus it may provide a conservative prediction of the cooling effect . This feasibility study demonstrated that the simulation tool could potentially be used for adjusting the setting of ECB for individual patients during hypothermic radical prostatectomy . Further studies using MR thermometry are required to validate the in silico results obtained using simulation . 
",This study investigated the influence of different pelvic tissue types as well as the choice of thermal property parameters on the efficacy of endorectal cooling balloon ECB . Prostate size and periprostatic venous plexus were two major factors affecting ECB cooling efficiency. Large prostate made the cooling difficult to propagate through the prostate. Patients with abundant periprostatic venous plexus required a longer time for neurovascular bundle NVB cooling. This feasibility study demonstrated that the simulation tool could potentially be used for adjusting the setting of ECB for hypothermic radical prostatectomy.,,,
S0167931713002438," Using ab initio calculations we demonstrate that extra electrons in pure amorphous SiO2 can be trapped in deep band gap states . Classical potentials were used to generate amorphous silica models and density functional theory to characterise the geometrical and electronic structures of trapped electrons . Extra electrons can trap spontaneously on pre existing structural precursors in amorphous SiO2 and produce 3.2eV deep states in the band gap . These precursors comprise wide O Si O angles and elongated Si O bonds at the tails of corresponding distributions . The electron trapping in amorphous silica structure results in an opening of the O Si O angle . We estimate the concentration of these electron trapping sites to be 10 19 cm 
",Wide O Si O angles in bulk amorphous silica are shown to trap electrons. Trapped electron levels appear 3.2eV below the bottom of the silica conduction band. Estimated concentration of electron trapping sites is 5 1019.,,,
S0169260714001278," Background and objective Patients who visit emergency department may have symptoms of occult cancers . Methods We studied a random cohort of one million subjects from Taiwan National Health Insurance Research Database between 2000 and 2008 to evaluate the ED utilization of individuals who were subsequently diagnosed with digestive tract cancers . The case group was digestive tract cancer patients and the control group was traumatic fracture patients . We reviewed record of ED visits only from 4 to 15 months before the cancer diagnoses . Results There were 2635 and 6665 in the case and control groups respectively . Patients adjusted odds ratio with 95 confidence interval for the case group were 1.36 for Abdominal ultrasound 2.16 pan endoscopy 1.72 guaiac fecal occult blood test 1.42 plain abdominal X rays 1.20 SGOT 1.27 SGPT 1.66 total bilirubin 2.41 direct bilirubin 1.21 hemoglobin and 3.63 blood transfusion respectively . Blood transfusion in the ED was a significant predictor of the individual subsequently diagnosed with digestive tract cancer . Conclusions The health system could identify high risk patients early by real time review of their ED utilization before the diagnosis of digestive tract cancers . We proposed a follow up methodology for daily screening of patients with high risk of digestive tract cancer by informatics system in the ED . 
",It is the first population based study to identify patients whose ED visits prior to diagnosis of digestive tract cancer. Certain examinations and blood transfusion in ED from 4 to 15 months prior to diagnosis was the significant predictors. We proposed a screening method of potential patients with digestive tract cancer by clinical decision support system in ED. Implementation of informatics system in ED can provide an opportunity for early detection of occult digestive tract cancer.,,,
S0169260715001534," There are various medical image sharing and electronic whiteboard systems available for diagnosis and discussion purposes . However most of these systems ask clients to install special software tools or web plug ins to support whiteboard discussion special medical image format and customized decoding algorithm of data transmission of HRIs . This limits the accessibility of the software running on different devices and operating systems . In this paper we propose a solution based on pure web pages for medical HRIs lossless sharing and e whiteboard discussion and have set up a medical HRI sharing and e whiteboard system which has four layered design HRIs access layer we improved an tile pyramid model named unbalanced ratio pyramid structure to rapidly share lossless HRIs and to adapt to the reading habits of users format conversion layer we designed a format conversion engine on server side to real time convert and cache DICOM tiles which clients requesting with window level parameters to make browsers compatible and keep response efficiency to server client business logic layer we built a XML behavior relationship storage structure to store and share users behavior to keep real time co browsing and discussion between clients web user interface layer AJAX technology and Raphael toolkit were used to combine HTML and JavaScript to build client RIA to meet clients desktop like interaction on any pure webpage . This system can be used to quickly browse lossless HRIs and support discussing and co browsing smoothly on any web browser in a diversified network environment . The proposal methods can provide a way to share HRIs safely and may be used in the field of regional health telemedicine and remote education at a low cost . 
",Designed methods for medical HRIs rapid accessing presentation and discussion on pure web browsers. Improved a tile pyramid to adapt to the reading habits of users for HRIs accessing. Built a format conversion engine FCE to make DICOM browsing compatibility and response efficiency. Designed a XML behavior relationship to store and share users behavior to keep online discussion. Created a HRIs sharing and electronic whiteboard system in a diversified network environment.,,,
S0169260715001765," Breast cancer is one of the most perilous diseases among women . Breast screening is a method of detecting breast cancer at a very early stage which can reduce the mortality rate . Mammography is a standard method for the early diagnosis of breast cancer . In this paper a new algorithm is proposed for breast cancer detection and classification in digital mammography based on Non Subsampled Contourlet Transform and Super Resolution . The presented algorithm includes three main parts including pre processing feature extraction and classification . In the pre processing stage after determining the region of interest by an automatic technique the quality of image is improved using NSCT and SR algorithm . In the feature extraction part several features of the image components are extracted and skewness of each feature is calculated . Finally AdaBoost algorithm is used to classify and determine the probability of benign and malign disease . The obtained results on Mammographic Image Analysis Society database indicate the significant performance and superiority of the proposed method in comparison with the state of the art approaches . According to the obtained results the proposed technique achieves 91.43 and 6.42 as a mean accuracy and FPR respectively . 
",We propose a new algorithm for breast cancer detection and classification. Our system can accurately detect the probability of benign or malign breast lesion. We utilize NSCT transform and super resolution to improve the quality of the images. The results on MIAS database show significant performance of the proposed method. Our system achieves 91.43 and 6.42 as a mean accuracy and FPR respectively.,,,
S0169260715000905," Patients use nurse call systems to signal nurses for medical help . Traditional push button flashing lamp call systems are not integrated with other hospital automation systems . Therefore nurse response time becomes a matter of personal discretion . The improvement obtained by integrating a pager system into the nurse call systems does not increase care efficiency because unnecessary visits are still not eliminated . To obtain an immediate response and a purposeful visit by a nurse regardless of the location of nurse in hospital traditional systems have to be improved by intelligent telephone system integration . The results of the developed Nurse Call System Software the Wireless Phone System Software the Location System Software and the communication protocol are provided together with detailed XML message structures . The benefits of the proposed system are also discussed and the direction of future work is presented . 
",Current nurse call systems fail to transfer a nurse call to the next available nurse if the primary nurse is on the phone with another patient. Current nurse call systems fail to transfer a nurse call to the next available nurse if the primary nurse is currently on another visit. Our contribution Both the nurse and the patient know that if the primary nurse is not available the system will dedicate the next available nurse. Our contribution The patient knows that his her call is guaranteed to be processed by the hospital. Our contribution The patient can reach a nurse equipped with a mobile DECT phone quicker.,,,
S0167865515002263," Better understanding of the anatomical variability of the human cochlear is important for the design and function of Cochlear Implants . Proper non rigid alignment of high resolution cochlear CT data is a challenge for the typical cubic B spline registration model . In this paper we study one way of incorporating skeleton based similarity as an anatomical registration prior . We extract a centerline skeleton of the cochlear spiral and generate corresponding parametric pseudo landmarks between samples . These correspondences are included in the cost function of a typical cubic B spline registration model to provide a more global guidance of the alignment . The resulting registrations are evaluated using different metrics for accuracy and model behavior and compared to the results of a registration without the prior . 
",We create simple parametric centerline descriptions for human CT cochlears. We regularize intensity based image registration with centerline correspondences. We show that a skeleton can act as an useful global anatomical registration prior.,,,
S0169260715002424," The Point of Care version of the interoperability standard ISO IEEE11073 provided a mechanism to control remotely agents through documents X73 10201 and X73 20301 . The newer version of X73 oriented to Personal Health Devices has no mechanisms to do such a thing . The authors are working toward a common proposal with the PHD Working Group in order to adapt the remote control capabilities from X73PoC to X73PHD . However this theoretical adaptation has to be implemented and tested to evaluate whether or not its inclusion entails an acceptable overhead and extra cost . Such proof of concept assessment is the main objective of this paper . For the sake of simplicity a weighing scale with a configurable operation was chosen as use case . First in a previous stage of the research the model was defined . Second the implementation methodology both in terms of hardware and software was defined and executed . Third an evaluation methodology to test the remote control features was defined . Then a thorough comparison between a weighing scale with and without remote control was performed . The results obtained indicate that when implementing remote control in a weighing scale the relative weight of such feature represents an overhead of as much as 53 whereas the number of Implementation Conformance Statements to be satisfied by the manufacturer represent as much as 34 regarding the implementation without remote control . The new feature facilitates remote control of PHDs but at the same time increases overhead and costs and therefore manufacturers need to weigh this trade off . As a conclusion this proof of concept helps in fostering the evolution of the remote control proposal to extend X73PHD and promotes its inclusion as part of the standard as well as it illustrates the methodological steps for its extrapolation to other specializations . 
",The authors are working on a remote command and control extension package for the X73PHD stack. The extension package allows the manager to remotely modify agent s settings. A proof of concept implementation of this extension package applied to a X73 10415 compliant agent is presented. The proof of concept shows the implications in terms of footprint and development time.,,,
S0169260715002394," Introduction Understanding the basic concepts of physiology and biophysics of cardiac cells can be improved by virtual experiments that illustrate the complex excitation contraction coupling process in cardiac cells . The aim of this study is to propose a rat cardiac myocyte simulator with which calcium dynamics in excitation contraction coupling of an isolated cell can be observed . This model has been used in the course Mathematical Modeling and Simulation of Biological Systems . In this paper we present the didactic utility of the simulator MioLab . Methods The simulator enables virtual experiments that can help studying inhibitors and activators in the sarcoplasmic reticulum sodium calcium exchanger thus corroborating a better understanding of the effects of medications which are used to treat arrhythmias on these compartments . The graphical interfaces were developed not only to facilitate the use of the simulator but also to promote a constructive learning on the subject since there are animations and videos for each stage of the simulation . The effectiveness of the simulator was tested by a group of graduate students . Results Some examples of simulations were presented in order to describe the overall structure of the simulator . Part of these virtual experiments became an activity for Biomedical Engineering graduate students who evaluated the simulator based on its didactic quality . As a result students answered a questionnaire on the usability and functionality of the simulator as a teaching tool . All students performed the proposed activities and classified the simulator as an optimal or good learning tool . In their written questions students indicated as negative characteristics some problems with visualizing graphs as positive characteristics they indicated the simulator s didactic function especially tutorials and videos on the topic of this study . Conclusions The results show that the simulator complements the study of the physiology and biophysics of the cardiac cell . 
",Highlight the didactic resources of a simulator MioLab . Tool that complements the study of the physiology and biophysics of the cardiac cell. Learning activities simulating experiments in silico. Inhibition of ATPase Ca2 of the cardiac sarcoplasmic reticulum studies in silico.,,,
S0169260715000954," Background and objective Several abnormal brain regions are known to be linked to depression including amygdala orbitofrontal cortex anterior cingulate cortex and dorsolateral prefrontal cortex etc . The aim of this study is to apply EEG data analysis to investigate with respect to mild depression whether there exists dysregulation in these brain regions . Methods EEG sources were assessed from 9 healthy and 9 mildly depressed subjects who were classified according to the Beck Depression Inventory criteria . t Test was used to calculate the eye movement data and standardized low resolution tomography was used to correlate EEG activity . Results A comparison of eye movement data between the healthy and mild depressed subjects exhibited that mildly depressed subjects spent more time viewing negative emotional faces . Comparison of the EEG from the two groups indicated higher theta activity in BA6 and higher alpha activity in BA38 . Conclusions EEG source location results suggested that temporal pole activity to be dysregulated and eye movement data analysis exhibited mild depressed subjects paid much more attention to negative face expressions which is also in accordance with the results of EEG source location . 
",An experiment measure eye movement and EEG simultaneously. Apply EEG source location to detect abnormal regions of mild depression. Different data to analyses difference between mild depressed subjects and health people.,,,
S0169260714003198," This research focuses on scheduling patients in emergency department laboratories according to the priority of patients treatments determined by the triage factor . The objective is to minimize the total waiting time of patients in the emergency department laboratories with emphasis on patients with severe conditions . The problem is formulated as a flexible open shop scheduling problem and a mixed integer linear programming model is proposed . A genetic algorithm is developed for solving the problem . Then the response surface methodology is applied for tuning the GA parameters . The algorithm is tested on a set of real data from an emergency department . Simulation results show that the proposed algorithm can significantly improve the efficiency of the emergency department by reducing the total waiting time of prioritized patients . 
",A semi online patient scheduling problem in the pathology laboratory is studied. The problem is formulated as a semi online hybrid shop scheduling problem. A genetic algorithm is developed for solving the problem. Several experiments are conducted to evaluate the effectiveness of the approach. The approach reduces waiting time of patients and improves operations efficiency.,,,
S0169260715002205," The design of a novel non contact multimedia controller is proposed in this study . Nowadays multimedia controllers are generally used by patients and nursing assistants in the hospital . Conventional multimedia controllers usually involve in manual operation or other physical movements . However it is more difficult for the disabled patients to operate the conventional multimedia controller by themselves they might totally depend on others . Different from other multimedia controllers the proposed system provides a novel concept of controlling multimedia via visual stimuli without manual operation . The disabled patients can easily operate the proposed multimedia system by focusing on the control icons of a visual stimulus device where a commercial tablet is used as the visual stimulus device . Moreover a wearable and wireless electroencephalogram acquisition device is also designed and implemented to easily monitor the user s EEG signals in daily life . Finally the proposed system has been validated . The experimental result shows that the proposed system can effectively measure and extract the EEG feature related to visual stimuli and its information transfer rate is also good . Therefore the proposed non contact multimedia controller exactly provides a good prototype of novel multimedia controlling scheme . 
",A noncontact multimedia controller is proposed to operate via visual stimuli. A wearable EEG acquisition device is developed to monitor EEG in daily life. The proposed system can effectively reduce long term care burden.,,,
S0167923615001426," By leveraging crowdsourcing Web credibility evaluation systems have become a promising tool to assess the credibility of Web content e.g . Web pages . However existing systems adopt a passive way to collect users credibility ratings which incurs two crucial challenges a considerable fraction of Web content have few or even no ratings so the coverage of the system is low malicious users may submit fake ratings to damage the reliability of the system . In order to realize a highly effective and robust WCES we propose to integrate recommendation functionality into the system . On the one hand by fusing Matrix Factorization and Latent Dirichlet Allocation a personalized Web content recommendation model is proposed to attract users to rate more Web pages i.e . the coverage is increased . On the other hand by analyzing a user s reaction to the recommended Web content we detect imitating attackers which have recently been recognized as a particular threat to WCES to make the system more robust . Moreover an adaptive reputation system is designed to motivate users to more actively interact with the integrated recommendation functionality . We conduct experiments using both real datasets and synthetic data to demonstrate how our proposed recommendation components significantly improve the effectiveness and robustness of existing WCES . 
",We study methods to improve efficiency of crowdsourcing Web credibility evaluation systems WCESs . Recommendation algorithm RA increases coverage in WCES. Joint application of RA and reputation system helps fight down imitation attacks.,,,
S0167947315003047," A common problem in modern genetic research is that of comparing the mean vectors of two populations typically in settings in which the data dimension is larger than the sample size where Hotelling s test can not be applied . Recently a test using random subspaces was proposed in which the data are randomly projected into several lower dimensional subspaces and Hotelling s test is well defined . Superior performance with competing tests was demonstrated when the variables were correlated . Following the research of random subspaces a modified test was proposed that might make more efficient use of covariance structure at high dimension . Hierarchical clustering is performed first such that highly correlated variables are clustered together . Next Hotelling s statistics are computed for every cluster subspace and summed as the new test statistic . High performance was demonstrated via simulations and real data analysis . 
",A two sample test using hierarchical clustering was proposed. Hotelling s statistics are computed in cluster subspaces and summed as the statistic. Highly correlated variables take priority for being processed. A cutoff distance is used to restrain the effect of statistical fluctuations. High performance was demonstrated in simulations and real data analysis.,,,
S0167947313002600," Categorization is often needed for clinical decision making when dealing with diagnostic biomarkers and a binary outcome . Four common methods used to dichotomize a continuous biomarker are compared the minimum value the Youden index the concordance probability and the point closest to corner in the ROC plane . These methods are compared from a theoretical point of view under Normal or Gamma biomarker distributions showing whether or not they lead to the identification of the same true cut point . The performance of the corresponding non parametric estimators is then compared by simulation . Two motivating examples are presented . In all simulation scenarios the point closest to corner in the ROC plane and concordance probability approaches outperformed the other methods . Both these methods showed good performance in the estimation of the optimal cut point of a biomarker . However when methods do not lead to the same optimal cut point scientists should focus on which one is truly what they want to estimate and use it in practice . In addition to improve communicability the Youden index or the concordance probability associated to the estimated cut point could be reported to summarize the associated classification accuracy . The use of the minimum value approach for cut point finding is strongly not recommended because its objective function is computed under the null hypothesis of absence of association between the true disease status and . This is in contrast with the presence of some discrimination potential of that leads to the dichotomization issue . 
",Categorization is needed when dealing with biomarkers and a binary outcome. We compare by simulation four widely used methods for cut point finding. Cut point estimation is better done with the point closest to 0 1 corner method. Scientists should also focus on the meaning of different cut point finding methods. Application examples are discussed through the paper.,,,
S0169260715002436," This paper presents an improved wave based bilateral teleoperation scheme for rehabilitation therapies assisted by robot manipulators . The main feature of this bilateral teleoperator is that both robot manipulators master and slave are controlled by impedance . Thus a pair of motion based adaptive impedance controllers are integrated into a wave based configuration in order to guarantee a stable human robot interaction and to compensate the position drift characteristic of the available schemes of bilateral teleoperation . Moreover the teleoperator stability in the presence of time delays in the communication channel is guaranteed because the wave variable approach is included to encode the force and velocity signals . It should be noted that the proposed structure enables the implementation of several teleoperator schemes from passive therapies without the intervention of a human operator on the master side to fully active therapies where both manipulators interact with humans in a stable manner . The suitable performance of the proposed teleoperator is verified through some results obtained from the simulation of the passive and active constrained modes by considering typical tasks in motor therapy rehabilitation where an improved behavior is observed when compared to implementations of the classical wave based approach . 
",An improved wave based bilateral teleoperator for robotic rehabilitation therapies is proposed. Master and slave manipulators controlled by impedance provide versatility to telerehabiltation systems. A stable human robot interaction with suitable force and path tracking is achieved.,,,
S0169260715002357," Over the past two decades the use of telemedicine as a way to provide medical services has grown as communication technologies advance and patients seek more convenient ways to receive care . Because developments within this field are still rapidly evolving identifying trends within telemedicine literature is an important task to help delineate future directions of telemedicine research . In this study we analyzed 7960 telemedicine related publication records found in the Science Citations Index Expanded database between 1993 and 2012 . Bibliometric analyses revealed that while the total growth in telemedicine literature has been significant in the last twenty years the publication activity per country and over time has been variable . While the United States led the world in the cumulative number of telemedicine publications Norway ranked highest when we ordered countries by publications per capita . We also saw that the growth in the number of publications per year has been inconsistent over the past two decades . Our results identified that neuroscience neurology and nursing as two fields of research in telemedicine that have seen considerable growth in interest in this field and are poised to be the focus of research activity in the near future . 
",The annual number of telemedicine publications in SCI database has grown from 10 in 1993 to 996 in 2012. Neuroscience neurology and nursing are two fields with speedily growth and might be the focus in the near future. Patients seek more convenient ways to receive care by utilizing communication technologies.,,,
S0167865515001269," Constructivist philosophy and Hasok Chang s active scientific realism are used to argue that the idea of truth in cluster analysis depends on the context and the clustering aims . Different characteristics of clusterings are required in different situations . Researchers should be explicit about on what requirements and what idea of true clusters their research is based because clustering becomes scientific not through uniqueness but through transparent and open communication . The idea of natural kinds is a human construct but it highlights the human experience that the reality outside the observer s control seems to make certain distinctions between categories inevitable . Various desirable characteristics of clusterings and various approaches to define a context dependent truth are listed and I discuss what impact these ideas can have on the comparison of clustering methods and the choice of a clustering methods and related decisions in practice . 
",A constructivist view of reality and science is sketched. Context and aim dependent characteristics of clusterings are listed. Formal approaches to define true clusters are presented. Researchers need to communicate their cluster concept transparently. Comparisons should show how different methods are good for different aims.,,,
S0167947314002126," A vital extension to partial least squares path modeling is introduced consistency . While maintaining all the strengths of PLS the consistent version provides two key improvements . Path coefficients parameters of simultaneous equations construct correlations and indicator loadings are estimated consistently . The global goodness of fit of the structural model can also now be assessed which makes PLS suitable for confirmatory research . A Monte Carlo simulation illustrates the new approach and compares it with covariance based structural equation modeling . 
",Consistent PLS estimates path coefficients and indicator loadings consistently. Consistent PLS can estimate parameters of nonrecursive structural equation models. A family of goodness of fit measures makes PLS suitable for confirmatory research. Consistent PLS performs comparably to covariance based structural equation modeling.,,,
S0169260715001558," Background and objectives Dose finding trials using model based methods have the ability to handle the increasingly complex landscape being seen in clinical trials . Issues such as patient heterogeneity in trial populations are important to address in the designing of a trial in addition to the inclusion exclusion criteria . Designs accommodating patient heterogeneity have been described using the continual reassessment method and time to event CRM yet the implementation of these trials in practice have been limited . These methods and other model based methods generally need statisticians to help design and conduct these trials . However the statistical programs which facilitate the use of these methods currently available focus on estimation in the one sample case . Methods A SAS program to accommodate two groups using the TITE CRM and likelihood estimation has been developed . The program consists of macros that assist with the planning and implementation of a trial accounting for patient heterogeneity . Results Description of the program is given as well as examples using the programs . For planning purposes an example will be provided showing how the program can be used to guide sample size estimates for the trial . Conclusions This program provides researchers with a valuable tool for designing dose finding studies to account for the presence of patient heterogeneity and conduct a trial using a hypothetical example . 
",We present a program which accommodates patient heterogeneity in dose finding. The program is applicable to designing and implementing a trial. Examples using the program are given sample size consideration and implementation. Valuable tool for statisticians wanting to address patient heterogeneity.,,,
S0167931715002154," Defect assisted electron transfer processes in metal oxide materials play a key role in a diverse range of effects of relevance to microelectronic applications . However extracting the key parameters governing such processes experimentally is a challenging problem . Here we present a first principles based investigation into electron transfer between oxygen vacancy defects in the high k dielectric material HfO2 . By calculating electron transfer parameters for defects separated by up to 15 we show that there is a crossover from coherent to incoherent electron transfer at about 5 . These results can provide invaluable input into numerical simulations of electron transfer which can be used to model and understand important effects such as trap assisted tunneling in advanced logic and memory devices . 
",First principles calculations of vacancy mediated electron transfer in HfO2. Defect separation dependence of electron transfer rates determined. No significant crystallographic orientation dependence for electron transfer. Electrons found to be delocalized between defects closer than 5 . Key parameters governing electron transfer similar to that in MgO.,,,
S0167947316300408," Numerous facets of scientific research implicitly or explicitly call for the estimation of probability densities . Histograms and kernel density estimates are two commonly used techniques for estimating such information with the KDE generally providing a higher fidelity representation of the probability density function . Both methods require specification of either a bin width or a kernel bandwidth . While techniques exist for choosing the kernel bandwidth optimally and objectively they are computationally intensive since they require repeated calculation of the KDE . A solution for objectively and optimally choosing both the kernel shape and width has recently been developed by Bernacchia and Pigolotti . While this solution theoretically applies to multidimensional KDEs it has not been clear how to practically do so . A method for practically extending the Bernacchia Pigolotti KDE to multidimensions is introduced . This multidimensional extension is combined with a recently developed computational improvement to their method that makes it computationally efficient a 2D KDE on 10 samples only takes 1 s on a modern workstation . This fast and objective KDE method called the fastKDE method retains the excellent statistical convergence properties that have been demonstrated for univariate samples . The fastKDE method exhibits statistical accuracy that is comparable to state of the science KDE methods publicly available in and it produces kernel density estimates several orders of magnitude faster . The fastKDE method does an excellent job of encoding covariance information for bivariate samples . This property allows for direct calculation of conditional PDFs with fastKDE . It is demonstrated how this capability might be leveraged for detecting non trivial relationships between quantities in physical systems such as transitional behavior . 
",A multidimensional fast and robust kernel density estimation is proposed fastKDE. fastKDE has statistical performance comparable to state of the science kernel density estimate packages in R. fastKDE is demonstrably orders of magnitude faster than comparable state of the science density estimate packages in R. A Python based implementation of fastKDE is available at https bitbucket.org lbl cascade fastkde.,,,
S0169260714002922," Insulin pharmacokinetics is not well understood during continuous subcutaneous insulin infusion in type 2 diabetes . We analyzed data collected in 11 subjects with T2D years BMI 30.1kg m2 fasting C peptide 1002.2pmol l fasting plasma glucose 9.6mmol l diabetes duration 8.0 years and HbA1c 8.3 mean who underwent a 24 h study investigating closed loop insulin delivery at the Wellcome Trust Clinical Research Facility Cambridge UK . Subcutaneous delivery of insulin lispro was modulated every 15min according to a model predictive control algorithm . Two complementary insulin assays facilitated discrimination between exogenous and endogenous plasma insulin concentrations measured every 15 60min . Lispro pharmacokinetics was represented by a linear two compartment model whilst parameters were estimated using a Bayesian approach applying a closed form model solution . The time to peak of lispro absorption was 109.6 min and the metabolic clearance rate 1.26 10 2 l kg min . MCR was negatively correlated with fasting C peptide and with fasting plasma insulin concentration . In conclusion compartmental modelling adequately represents lispro kinetics during continuous subcutaneous insulin infusion in T2D . Fasting plasma C peptide or fasting insulin may be predictive of lispro metabolic clearance rate in T2D but further investigations are warranted . 
",A model described lispro PK during closed loop insulin delivery in T2D. The linear two compartment model fitted well the data. Time to peak of lispro and its metabolic clearance rate MCR were estimated. Relationships between PK and metabolic demographic data were examined. Negative correlation found between lispro MCR and fasting insulin C peptide.,,,
S0168169916301399," Corn height measured manually has shown promising results in improving the relationship between active optical sensor readings and crop yield . Manual measurement of corn height is not practical in US commercial corn production so an alternative automatic method must be found in order to capture the benefit of including canopy height into in season yield estimates and from there into in season nitrogen fertilizer applications . One existing alternative to measure canopy height is an acoustic height sensor . A commercial acoustic height sensor was utilized in these experiments at two corn growth stages along with AO sensors . Eight corn N rate sites in North Dakota USA were used to compare the acoustic height sensor as a practical alternative to manual height measurements as an additional parameter to increase the relationship between AO sensor readings and corn yield . Six N treatments 0 45 90 134 179 and 224kgha 1 were applied before planting in a randomized complete block experimental design with four replications . Height measurement using the acoustic sensor provided an improved yield relationship compared to manual height at all locations . The level of improvement of the relationship between AO readings multiplied by acoustic sensor readings and yield was greater at V6 growth stage compared to the V12 growth stage . At V12 corn height measured manually and with the acoustic sensor multiplied by AO readings provided similar improvement to the relationship with yield compared to relating AO readings alone with yield at most locations . The acoustic height sensor may be useful in increasing the usefulness of AO sensor corn yield prediction algorithms for use in on the go in season N application to corn particularly if the sensor height is normalized within site before combining multiple locations . 
",An acoustic height sensor could be used with the INSEY value to improve corn in season N management. The height sensor was especially useful at earlier and later growth stages depending on the site. Normalizing height aided the relationships with active optical sensor INSEY when combining sites.,,,
S0169260714003216," Background Overall survival and progression free survival are key outcome measures for head and neck cancer as they reflect treatment efficacy and have implications for patients and health services . The UK has recently developed a series of national cancer audits which aim to estimate survival and recurrence by relying on institutions manually submitting interval data on patient status a labour intensive method . However nationally data are routinely collected on hospital admissions surgery radiotherapy and chemotherapy . We have developed a technique to automate the interpretation of these routine datasets allowing us to derive patterns of treatment in head and neck cancer patients from routinely acquired data . Methods We identified 122 patients with head and neck cancer and extracted treatment histories from hospital notes to provide a gold standard dataset . We obtained routinely collected local data on inpatient admission and procedures chemotherapy and radiotherapy for these patients and analysed them with a computer algorithm which identified relevant time points and then calculated OS and PFS . We validated these by comparison with the gold standard dataset . The algorithm was then optimised to maximise correct identification of each timepoint and minimise false identification of recurrence events . Results Of the 122 patients 82 had locally advanced disease . OS was 88 at 1 year and 77 at 2 years and PFS was 75 and 66 at 1 and 2 years . 40 patients developed recurrent disease . Our automated method provided an estimated OS of 87 and 77 and PFS of 87 and 78 at 1 and 2 years 98 and 82 of patients showed good agreement between the automated technique and Gold standard dataset of OS and PFS respectively . The automated technique correctly assigned recurrence in 101 out of 122 of the patients 21 of the 40 patients with recurrent disease were correctly identified 19 were too unwell to receive further treatment and were missed . Of the 82 patients who did not develop a recurrence 77 were correctly identified and 2 were incorrectly identified as having recurrent disease when they did not . Conclusions We have demonstrated that our algorithm can be used to automate the interpretation of routine datasets to extract survival information for this sample of patients . It currently underestimates recurrence rates due to many patients not being well enough to be treated for recurrent disease . With some further optimisation this technique could be extended to a national level providing a new approach to measuring outcomes on a larger scale than is currently possible . This could have implications for healthcare provision and policy for a range of different disease types . 
",Routinely collected data can estimate rates of disease progression progression free and overall survival in patients with head neck cancer. Estimates of overall survival are accurate. Estimates of recurrence and progression free survival are less accurate. Patients who do not have treatment for recurrent disease may be missed by this approach.,,,
S0167947316300287," Quantile inference with adjustment for covariates has not been widely investigated on competing risks data . We propose covariate adjusted quantile inferences based on the cause specific proportional hazards regression of the cumulative incidence function . We develop the construction of confidence intervals for quantiles of the cumulative incidence function given a value of covariates and for the difference of quantiles based on the cumulative incidence functions between two treatment groups with common covariates . Simulation studies show that the procedures perform well . We illustrate the proposed methods using early stage breast cancer data . 
",Quantile inferences with adjustment for covariates are proposed. Quantiles are defined based on regression of the cumulative incidence function. The construction of confidence intervals for quantiles is developed. A confidence interval for the difference of two quantiles is derived.,,,
S0167839615001442," In this paper the interpolation of two data points and two tangent directions with spatial cubic rational PH curves is considered . It is shown that interpolants exist for any true spatial data configuration . The equations that determine the interpolants are derived by combining a closed form representation of a ten parametric family of rational PH cubics given in Kozak et al . and the Gram matrix approach . The existence of a solution is proven by using a homotopy analysis and numerical method to compute solutions is proposed . In contrast to polynomial PH cubics for which the range of data admitting the existence of interpolants is limited a switch to rationals provides an interpolation scheme with no restrictions . 
",interpolation scheme with spatial rational PH cubics is presented. The existence of the interpolant is proven for any true spatial data configurations. Homotopy analysis and a Gram matrix approach are used. Numerical method to compute the solutions is proposed.,,,
S0169260714001266," This paper proposes a fast weighted horizontal visibility graph constructing algorithm to identify seizure from EEG signals . The performance of the FWHVA is evaluated by comparing with Fast Fourier Transform and sample entropy method . Two noise robustness graph features based on the FWHVA mean degree and mean strength are investigated using two chaos signals and five groups of EEG signals . Experimental results show that feature extraction using the FWHVA is faster than that of SampEn and FFT . And mean strength feature associated with ictal EEG is significant higher than that of healthy and inter ictal EEGs . In addition an 100 classification accuracy for identifying seizure from healthy shows that the features based on the FWHVA are more promising than the frequency features based on FFT and entropy indices based on SampEn for time series classification . 
",Developing a fast algorithm for constructing a network from a time series in linear time. Discriminating between healthy and seizure EEG signals with 100 accuracy with only two features. Extracted features from a time series is faster and more robust to against noise than those based on FFT.,,,
S0168169916301296," Smart farming is a management style that includes smart monitoring planning and control of agricultural processes . This management style requires the use of a wide variety of software and hardware systems from multiple vendors . Adoption of smart farming is hampered because of a poor interoperability and data exchange between ICT components hindering integration . Software Ecosystems is a recent emerging concept in software engineering that addresses these integration challenges . Currently several Software Ecosystems for farming are emerging . To guide and accelerate these developments this paper provides a reference architecture for Farm Software Ecosystems . This reference architecture should be used to map assess design and implement Farm Software Ecosystems . A key feature of this architecture is a particular configuration approach to connect ICT components developed by multiple vendors in a meaningful feasible and coherent way . The reference architecture is evaluated by verification of the design with the requirements and by mapping two existing Farm Software Ecosystems using the Farm Software Ecosystem Reference Architecture . This mapping showed that the reference architecture provides insight into Farm Software Ecosystems as it can describe similarities and differences . A main conclusion is that the two existing Farm Software Ecosystems can improve configuration of different ICT components . Future research is needed to enhance configuration in Farm Software Ecosystems . 
",We mould the concept Software Ecosystems to the agricultural domain. We propose a reference architecture for Farm Software Ecosystems. Our reference architecture describes an organizational and technical infrastructure. We motivate that our reference architecture can improve farm enterprise integration. Our reference architecture is used to review some existing initiatives.,,,
S0169260714003447," Since falls are a major public health problem in an aging society there is considerable demand for low cost fall detection systems . One of the main reasons for non acceptance of the currently available solutions by seniors is that the fall detectors using only inertial sensors generate too much false alarms . This means that some daily activities are erroneously signaled as fall which in turn leads to frustration of the users . In this paper we present how to design and implement a low cost system for reliable fall detection with very low false alarm ratio . The detection of the fall is done on the basis of accelerometric data and depth maps . A tri axial accelerometer is used to indicate the potential fall as well as to indicate whether the person is in motion . If the measured acceleration is higher than an assumed threshold value the algorithm extracts the person calculates the features and then executes the SVM based classifier to authenticate the fall alarm . It is a 365 7 24 embedded system permitting unobtrusive fall detection as well as preserving privacy of the user . 
",An embedded system for fully automatic fall detection. A method to achieve reliable fall detection with low false alarm ratio. An algorithm with low computational demands for person extraction in depth images. A freely available database for evaluation of fall detection consisting of both accelerometric and depth data.,,,
S0169260714001473," This paper presents a novel method for QRS detection in electrocardiograms . It is based on the S Transform a new time frequency representation . The S Transform provides frequency dependent resolution while maintaining a direct relationship with the Fourier spectrum . We exploit the advantages of the S Transform to isolate the QRS complexes in the time frequency domain . Shannon energy of each obtained local spectrum is then computed in order to localize the R waves in the time domain . Significant performance enhancement is confirmed when the proposed approach is tested with the MIT BIH arrhythmia database . The obtained results show a sensitivity of 99.84 a positive predictivity of 99.91 and an error rate of 0.25 . Furthermore to be more convincing the authors illustrated the detection parameters in the case of certain ECG segments with complicated patterns . 
",This work makes a QRS detection based on S Transform TS . The TS allows us access to the frequency content of the QRS complexes without recourse to any filtering. Shannon energy is computed for each obtained local spectrum. Shannon energy highlights all QRSs even those having a small amplitude.,,,
S0169260715001625," Electrocardiography has been recently proposed as biometric trait for identification purposes . Intra individual variations of ECG might affect identification performance . These variations are mainly due to Heart Rate Variability . In particular HRV causes changes in the QT intervals along the ECG waveforms . This work is aimed at analysing the influence of seven QT interval correction methods on the performance of ECG fiducial based identification systems . In addition we have also considered the influence of training set size classifier classifier ensemble as well as the number of consecutive heartbeats in a majority voting scheme . The ECG signals used in this study were collected from thirty nine subjects within the Physionet open access database . Public domain software was used for fiducial points detection . Results suggested that QT correction is indeed required to improve the performance . However there is no clear choice among the seven explored approaches for QT correction . MultiLayer Perceptron and Support Vector Machine seemed to have better generalization capabilities in terms of classification performance with respect to Decision Tree based classifiers . No such strong influence of the training set size and the number of consecutive heartbeats has been observed on the majority voting scheme . 
",Electrocardiography ECG has been recently proposed as biometric trait for identification purposes. The influence of the QT interval correction method on the performance of ECG based identification systems is analyzed. ECG signals were collected from the Physionet open access database. A public domain software was used for fiducial points detection. Results suggested that QT correction is always required to improve the performance.,,,
S0169260715000267," A normal cardiac activation starts in the sinoatrial node and then spreads throughout the atrial myocardium thus defining the P wave of the electrocardiogram . However when the onset of paroxysmal atrial fibrillation approximates a highly disturbed electrical activity occurs within the atria thus provoking fragmented and eventually longer P waves . Although this altered atrial conduction has been successfully quantified just before PAF onset from the signal averaged P wave spectral analysis its evolution during the hours preceding the arrhythmia has not been assessed yet . This work focuses on quantifying the P wave spectral content variability over the 2h preceding PAF onset with the aim of anticipating as much as possible the arrhythmic episode envision . For that purpose the time course of several metrics estimating absolute energy and ratios of high to low frequency power in different bands between 20 and 200Hz has been computed from the P wave autoregressive spectral estimation . All the analyzed metrics showed an increasing variability trend as PAF onset approximated providing the P wave high frequency energy a diagnostic accuracy around 80 to discern between healthy subjects patients far from PAF and patients less than 1h close to a PAF episode . This discriminant power was similar to that provided by the most classical time domain approach i.e . the P wave duration . Furthermore the linear combination of both metrics improved the diagnostic accuracy up to 88.07 thus constituting a reliable noninvasive harbinger of PAF onset with a reasonable anticipation . The information provided by this methodology could be very useful in clinical practice either to optimize the antiarrhythmic treatment in patients at high risk of PAF onset and to limit drug administration in low risk patients . 
",The P wave spectral content variability over the 2h preceding PAF onset is assessed. The P wave high frequency energy and duration contain complementary information. The onset of PAF is predicted at least with a time in advance of 2h.,,,
S0167947314000899," A mixture of skew factor analyzers is introduced as well as a family of mixture models based thereon . The particular formulation of the skew distribution used arises as a special case of the generalized hyperbolic distribution . Like their Gaussian and distribution analogues mixtures of skew factor analyzers are very well suited for model based clustering of high dimensional data . The alternating expectation conditional maximization algorithm is used for model parameter estimation and the Bayesian information criterion is used for model selection . The models are applied to both real and simulated data giving superior clustering results when compared to a well established family of Gaussian mixture models . 
",A mixture of skew factor analyzers is introduced. A family of parsimonious mixtures of skew factor analyzers is developed. The models are well suited for clustering high dimensional data. Mixture components can be asymmetric and or heavy tailed as required. The models can also be applied for classification or discriminant analysis.,,,
S0169260715000292," The prediction of substantially short survivability in patients is extremely risky . In this study we proposed a probabilistic model using Bayesian network to predict the short survivability of patients with brain metastasis from lung cancer . A nationwide cancer patient database from 1996 to 2010 in Taiwan was used . The cohort consisted of 438 patients with brain metastasis from lung cancer . We utilized synthetic minority over sampling technique to solve the imbalanced property embedded in the problem . The proposed BN was compared with three competitive models namely naive Bayes logistic regression and support vector machine . Statistical analysis showed that performances of BN LR NB and SVM were statistically the same in terms of all indices with low sensitivity when these models were applied on an imbalanced data set . Results also showed that SMOTE can improve the performance of the four models in terms of sensitivity while keeping high accuracy and specificity . Further the proposed BN is more effective as compared with NB LR and SVM from two perspectives the transparency and ability to show the relation of factors affecting brain metastasis from lung cancer it allows decision makers to find the probability despite incomplete evidence and information and the sensitivity of the proposed BN is the highest among all standard machine learning methods . 
",The use of Bayesian network to predict the short survivability of patients. The use of synthetic minority over sampling technique to improve the sensitivity. The proposed model has the highest sensitivity among all benchmark methods.,,,
S0167931713004061," Current progress in tissue engineering is focused on the creation of environments in which cultures of relevant cells can adhere grow and form functional tissue . We propose a method for controlled chemical and topographical cues through surface patterning of self folding hydrogel films . This provides a conversion of 2D patterning techniques into a viable method of manufacturing a 3D scaffold . While similar bilayers have previously been demonstrated here we present a faster and high throughput process for fabricating self folding hydrogel devices incorporating controllable surface nanotopographies by serial hot embossing of sacrificial layers and photolithography . 
",A novel and fast sacrificial layer embossing method for nanopatterning gel films. Method creates nanopatterned environmentally responsive hydrogel films. Double sided micro and nanopatterned gel films are produced in one stage. Films will roll and unroll in response to changes in aqueous pH. Produces sub millimetre injectable and patterned cylindrical tissue scaffolds.,,,
S0169260714002429," Objectives To compare the risk of infection for rheumatoid arthritis patients who took etanercept or adalimumab medication in a nationwide population . Methods RA patients who took etanercept or adalimumab were identified in the Taiwan s National Health Insurance Research Database . The composite outcome of serious infections including hospitalization for infection reception of an antimicrobial injection and tuberculosis were followed for 365 days . A Kaplan Meier survival curve with a log rank test and Cox proportional hazards regression were used to compare risks of infection between the two cohorts of tumor necrosis factor antagonists users . Hazard ratios were obtained and adjusted with propensity scores and clinical factors . Sensitivity analyses and subgroup analyses were also performed . Results In total 1660 incident etanercept users and 484 incident adalimumab users were eligible for the analysis . The unadjusted HR for infection of the etanercept users was significantly higher than that of the adalimumab users 1.09 3.42 p 0.024 . The HRs were 2.04 and 2.02 after adjusting for propensity scores and for propensity scores in addition to clinical factors respectively . The subgroup analyses revealed that HRs for composite infection was significantly higher in patient subgroups of older age female as well as patients who did not have DM COPD and hospitalization history at the baseline . Conclusion In this head to head cohort study involving a nationwide population of patients with RA etanercept users demonstrated a higher risk of infection than adalimumab users . Results of this study suggest the possible existence of an intra class difference in infection risk among TNF antagonists . 
",A head to head cohort study involving a nationwide population has been performed. Etanercept users have higher risk of infection than adalimumab users. Infection free survival is significantly lower in etanercept users. The difference of infection risk is significant in subgroup analysis.,,,
S0167931714004456," Lab on a chip devices are broadly used for research in the life sciences and diagnostics and represent a very fast moving field . LOC devices are designed prototyped and assembled using numerous strategies and materials but some fundamental trends are that these devices typically need to be sealed supplied with liquids reagents and samples and often interconnected with electrical or microelectronic components . In general closing and connecting to the outside world these miniature labs remain a challenge irrespectively of the type of application pursued . Here we review methods for sealing and connecting LOC devices using standard approaches as well as recent state of the art methods . This review provides easy to understand examples and targets the microtechnology engineering community as well as researchers in the life sciences . 
",We review two main challenges of microfluidics technology sealing and interfacing. These are crucial for commercialization and wide spread use of microfluidics. We review well established techniques as well as recent state of the art methods. We discuss both research and commercial activities. We provide up to date examples for each technology discussed.,,,
S0169260714002521," Semen analysis is the first step in the evaluation of an infertile couple . Within this process an accurate and objective morphological analysis becomes more critical as it is based on the correct detection and segmentation of human sperm components . In this paper we present an improved two stage framework for detection and segmentation of human sperm head characteristics that uses three different color spaces . The first stage detects regions of interest that define sperm heads using k means then candidate heads are refined using mathematical morphology . In the second stage we work on each region of interest to segment accurately the sperm head as well as nucleus and acrosome using clustering and histogram statistical analysis techniques . Our proposal is also characterized by being fully automatic where a user intervention is not required . Our experimental evaluation shows that our proposed method outperforms the state of the art . This is supported by the results of different evaluation metrics . In addition we propose a gold standard built with the cooperation of a referent expert in the field aiming to compare methods for detecting and segmenting sperm cells . Our results achieve notable improvement getting above 98 in the sperm head detection process at the expense of having significantly fewer false positives obtained by the state of the art method . Our results also show an accurate head acrosome and nucleus segmentation achieving over 80 overlapping against hand segmented gold standard . Our method achieves higher Dice coefficient lower Hausdorff distance and less dispersion with respect to the results achieved by the state of the art method . 
",We use three different color spaces for detection and segmentation of human sperm head acrosome and nucleus. We propose a gold standard built with the cooperation of a referent expert in the field aiming to create a benchmark set methods for detecting and segmenting sperm cells. We achieve notable improvement in sperm head detection and fewer false positives compared to the state of the art method. Our segmentation approach obtains over 80 overlapping against hand segmented gold standard. Our method achieves higher Dice coefficient lower Hausdorff distance and less dispersion with respect to the results achieved by the state of the art method.,,,
S0167947314003399," In images with low contrast to noise ratio the information gain from the observed pixel values can be insufficient to distinguish foreground objects . A Bayesian approach to this problem is to incorporate prior information about the objects into a statistical model . A method for representing spatial prior information as an external field in a hidden Potts model is introduced . This prior distribution over the latent pixel labels is a mixture of Gaussian fields centred on the positions of the objects at a previous point in time . It is particularly applicable in longitudinal imaging studies where the manual segmentation of one image can be used as a prior for automatic segmentation of subsequent images . The method is demonstrated by application to cone beam computed tomography an imaging modality that exhibits distortions in pixel values due to X ray scatter . The external field prior results in a substantial improvement in segmentation accuracy reducing the mean pixel misclassification rate for an electron density phantom from 87 to 6 . The method is also applied to radiotherapy patient data demonstrating how to derive the external field prior in a clinical context . 
",External field prior improves image segmentation accuracy. Manual segmentation of one image is used as a prior for subsequent images. Applicable to longitudinal imaging such as image guided radiation therapy.,,,
S0169260714003484," In PET CT thoracic imaging respiratory motion reduces image quality . A solution consists in performing respiratory gated PET acquisitions . The aim of this study was to generate clinically realistic Monte Carlo respiratory PET data obtained using the 4D NCAT numerical phantom and the GATE simulation tool to assess the impact of respiratory motion and respiratory motion compensation in PET on lesion detection and volume measurement . To obtain reconstructed images as close as possible to those obtained in clinical conditions a particular attention was paid to apply to the simulated data the same correction and reconstruction processes as those applied to real clinical data . The simulations required 140 000h generating 1.5 To of data . Calibration phantom and patient reconstructed images from the simulated data were visually and quantitatively very similar to those obtained in clinical studies . The lesion detectability was higher when the better trade off between lesion movement limitation and image statistic preservation is considered . We then compared the lesion volumes measured on conventional PET acquisitions versus respiratory gated acquisitions using an automatic segmentation method and a 40 threshold approach . A time consuming initial manual exclusion of noisy structures needed with the 40 threshold was not necessary when the automatic method was used . The lesion detectability along with the accuracy of tumor volume estimates was largely improved with the gated compared to ungated PET images . 
",We create a dataset of clinically realistic Monte Carlo respiratory PET data. We validated the realism of the simulated data. We evaluated the lesion detectability regarding their location volume and contrast. Respiratory gating of 18F FDG PET images improves the accuracy of volume estimates.,,,
S0167947315000559," The asymmetry in the tail dependence between U.S. equity portfolios and the aggregate U.S. market is a well established property . Given the limited number of observations in the tails of a joint distribution standard non parametric measures of tail dependence have poor finite sample properties and generally reject the asymmetry in the tail dependence . A parametric model based on a multivariate noncentral t distribution is developed to measure and test asymmetry in tail dependence . This model allows different levels of tail dependence to be estimated depending on the distribution s parameters and accommodates situations in which the volatilities or the correlations across returns are time varying . For most of the size book to market and momentum portfolios the tail dependence with the market portfolio is significantly higher on the downside than on the upside . 
",The tail dependence between US equity portfolios and the US market is asymmetric. We describe a multivariate t distribution with asymmetry in the lower and upper tail dependence. The lower and upper tail dependence parameters are estimated by maximum likelihood. The estimated tail dependence parameters are consistent with the data provided volatilities and correlations are allowed to vary over time.,,,
S0167947316000165," Excess zeroes are often thought of as a cause of data over dispersion this claim is not entirely accurate . In actuality excess zeroes reduce the mean of a dataset thus inflating the dispersion index . While this results in an increased chance for data over dispersion the implication is not guaranteed . Thus one should consider a flexible distribution that not only can account for excess zeroes but can also address potential over or under dispersion . A zero inflated Conway Maxwell Poisson regression allows for modeling the relationship between explanatory and response variables while capturing the effects due to excess zeroes and dispersion . This work derives the ZICMP model and illustrates its flexibility extrapolates the corresponding likelihood ratio test for the presence of significant data dispersion and highlights various statistical properties and model fit through several examples . 
",Zero inflated Conway Maxwell Poisson models dispersed datasets with excess zeroes. Hypothesis test detects statistically significant dispersion in light of excess zeroes. Data simulations and examples illustrate flexibility in model fit.,,,
S0167865514001263," In mammographic imaging the presence of microcalcifications small deposits of calcium in the breast is a primary indicator of breast cancer . However not all microcalcifications are malignant and their distribution within the breast can be used to indicate whether clusters of microcalcifications are benign or malignant . Computer aided diagnosis systems can be employed to help classify such microcalcification clusters . In this paper a novel method for classifying microcalcification clusters is presented by representing discrete mereotopological relations between the individual microcalcifications over a range of scales in the form of a mereotopological barcode . This barcode based representation is able to model complex relations between multiple regions and the results on mammographic microcalcification data shows the effectiveness of this approach . Classification accuracies of 95 and 80 are achieved on the MIAS and DDSM datasets respectively . These results are comparable to existing state of the art methods . This work also demonstrates that mereotopological barcodes could be used to help trained clinicians in their diagnosis by providing a clinical interpretation of barcodes that represent both benign and malignant cases . 
",A novel multi scale shape descriptor using persistent discrete mereotopology. Introduces the mereotopological barcode that brings together work from discrete mereotopology and computational topology. The method is applied to mammographic microcalcifications with results comparable to state of the art. Describes how mereotopological barcodes could be used in a clinical setting.,,,
S0167865515001531," Several applications aim to identify rare events from very large data sets . Classification algorithms may present great limitations on large data sets and show a performance degradation due to class imbalance . Many solutions have been presented in literature to deal with the problem of huge amount of data or imbalancing separately . In this paper we assessed the performances of a novel method Parallel Selective Sampling able to select data from the majority class to reduce imbalance in large data sets . PSS was combined with the Support Vector Machine classification . PSS SVM showed excellent performances on synthetic data sets much better than SVM . Moreover we showed that on real data sets PSS SVM classifiers had performances slightly better than those of SVM and RUSBoost classifiers with reduced processing times . In fact the proposed strategy was conceived and designed for parallel and distributed computing . In conclusion PSS SVM is a valuable alternative to SVM and RUSBoost for the problem of classification by huge and imbalanced data due to its accurate statistical predictions and low computational complexity . 
",We proposed a new algorithm to preprocess huge and imbalanced data. This algorithm based on distance calculations reduce both size and imbalance. The selective sampling method was conceived for parallel and distributed computing. It was combined with SVM obtaining optimized classification performances. Synthetic and real data sets were used to evaluate the classifiers performances.,,,
S0167931714000203," To restrict pollen tube growth to a single focal plane is an important subject to enable their accurate growth analysis under microscopic observation . In the conventional method to assay pollen tube growth the pollen tubes grow in a disorderly manner on solid medium rendering it impossible to observe their growth in detail . Here we present a new method to assay pollen tube growth using poly dimethylsiloxane microchannel device to isolate individual pollen tubes . The growth of the pollen tube is confined to the microchannel and to the same focal plane allowing accurate microscopic observations . This methodology has the potential for analyses of pollen tube growth in microfluidic environments in response to chemical products and signaling molecules which paves the way for various experiments on plant reproduction . 
",We describe a new pollen tube growth assay using a microchannel device. The microchannel restricts the growth of the pollen tube to a single focal plane. The growth of each pollen tube can be observed under a microscope and quantified. This method might enable exploring the effects of signaling molecules on pollen tube.,,,
S0167923615001967," The prevalence of social media has greatly catalyzed the dissemination and proliferation of online memes . However this information abundance is exceeding the capability of online users to consume it . Ranking memes based on their popularities could promote online advertisement and content distribution . Despite such importance few existing work can solve this problem well . They are either daunted by unpractical assumptions or incapability of characterizing dynamic information . As such in this paper we elaborate a model free scheme to rank online memes in the context of social media . This scheme is capable to characterize the nonlinear interactions of online users which mark the process of meme diffusion . Empirical studies on two large scale real world datasets demonstrate the effectiveness and robustness of the proposed scheme . In addition due to its fine grained modeling of user dynamics this ranking scheme can also be utilized to explain meme popularity through the lens of social influence . 
",We develop a model free scheme for meme ranking task. We have evaluated the proposed scheme on two large scale real world datasets. Our work presents fine grained modeling of dynamic information and can be readily generalized to other domains.,,,
S0169260715001959," In this study we proposed a new adaptive method for fusing multiple emotional modalities to improve the performance of the emotion recognition system . Three channel forehead biosignals along with peripheral physiological measurements were utilized as emotional modalities . Six basic emotions i.e . anger sadness fear disgust happiness and surprise were elicited by displaying preselected video clips for each of the 25 participants in the experiment the physiological signals were collected simultaneously . In our multimodal emotion recognition system recorded signals with the formation of several classification units identified the emotions independently . Then the results were fused using the adaptive weighted linear model to produce the final result . Each classification unit is assigned a weight that is determined dynamically by considering the performance of the units during the testing phase and the training phase results . This dynamic weighting scheme enables the emotion recognition system to adapt itself to each new user . The results showed that the suggested method outperformed conventional fusion of the features and classification units using the majority voting method . In addition a considerable improvement compared to the systems that used the static weighting schemes for fusing classification units was also shown . Using support vector machine and k nearest neighbors classifiers the overall classification accuracies of 84.7 and 80 were obtained in identifying the emotions respectively . In addition applying the forehead or physiological signals in the proposed scheme indicates that designing a reliable emotion recognition system is feasible without the need for additional emotional modalities . 
",A new dynamic fusion method for designing an emotion recognition system is proposed. A weight is assigned to each classifier based on its performance. The performance of the classifiers during the training and testing phases is considered. Static weights in varying contexts such as emotions do not produce acceptable results. Dynamic weighting strategy improves the performance of the system considerably.,,,
S0167947313000571," An important problem in high dimensional data analysis is determining whether sample points are uniformly distributed over some compact support or rather possess some underlying structure . We propose two new graph theoretic tests of uniformity which utilize the minimum spanning tree and a snake . We compare the powers of statistics based on these graphs with other statistics from the literature on an array of non uniform alternatives in a variety of supports . For data in a hypercube we find that test statistics based on the minimum spanning tree have superior power when the data displays regularity . For arbitrarily shaped or unknown supports we use run length statistics of the sequence of segment lengths along the snake s path to test uniformity . The snake is particularly useful because no knowledge or estimation of the support is required to compute the test statistic it can be computed quickly for any dimension and it shows what kinds of non uniformities are present . These properties make the snake unique among multivariate tests of uniformity since others only function on specific and known supports have computational difficulties in high dimension or have inconsistent type I error rates . 
",A test of uniformity determines if data is uniform or possesses underlying structure. Tests of uniformity should be the first step in any pattern or exploratory data analysis. We develop new tests of uniformity and compare with many in the literature. One test outperforms others when regularity minimum spacings exists in the data. Our other test is the only option for testing on arbitrary supports and dimensions.,,,
S0167931716300387,"Optical lithography technique has been applied to fabricate devices from atomically thin sheets exfoliated mechanically from kish graphite bulk MoS2 and WSe2. During the fabrication processes the exfoliated graphene few layer MoS2 and WSe2 sheets have been patterned into specific shapes as required and metal contacts have been deposited on these two dimensional sheets to make field effect devices with different structures. The key to the successful implementation of the technique is the appropriate alignment mark design which can solve the problems of aligning photomasks to the random location orientation and irregular shape exfoliated two dimensional sheets on the substrates. Raman characterization performed on the patterned two dimensional sheets after the fabrication processes shows that little defects have been introduced during fabrication. Field effect has been observed from I V characteristics with the highly doped silicon substrate as the back gate. The extracted field effect hole and electron mobilities of graphene are 1010 cm2 V 1 s 1 and 3550 cm2 V 1 s 1 respectively and the field effect carrier mobilities of MoS2 and WSe2 are 0.06 cm2 V 1 s 1 and 0.03 cm2 V 1 s 1 separately which are comparable with experimental results of other reports. 
",Optical lithography has been applied for the fabrication of 2D electronic devices. The exfoliated graphene few layer MoS2 and WSe2 sheets have been patterned. The key to the successful implementation is the appropriate alignment mark design. The final electronic devices show little defects.,,,
S0167947314002291," Multivariate adaptive regression splines provide a flexible statistical modeling method that employs forward and backward search algorithms to identify the combination of basis functions that best fits the data and simultaneously conduct variable selection . In optimization MARS has been used successfully to estimate the unknown functions in stochastic dynamic programming stochastic programming and a Markov decision process and MARS could be potentially useful in many real world optimization problems where objective functions need to be estimated from data such as in surrogate optimization . Many optimization methods depend on convexity but a non convex MARS approximation is inherently possible because interaction terms are products of univariate terms . In this paper a convex MARS modeling algorithm is described . In order to ensure MARS convexity two major modifications are made coefficients are constrained such that pairs of basis functions are guaranteed to jointly form convex functions and the form of interaction terms is altered to eliminate the inherent non convexity . Finally MARS convexity can be achieved by the fact that the sum of convex functions is convex . Convex MARS is applied to inventory forecasting SDP problems with four and nine dimensions and to an air quality ground level ozone problem . 
",Convex MARS enables a convex approximation without degrading the quality of fit. Convex MARS is appropriate for approximations in convex optimization problems. The threshold version of Convex MARS provides stronger convexity and better accuracy.,,,
S0169260715000553," Recent study shows that tendon sheath system has great potential in the development of surgical robots for endoscopic surgery . It is able to deliver adequate power in a light weight and compact package . And the flexibility and compliance of the tendon sheath system make it capable of adapting to the long and winding path in the flexible endoscope . However the main difficulties in precise control of such system fall on the nonlinearities of the system behavior and absence of necessary sensory feedback at the surgical end effectors . Since accurate position control of the tool is a prerequisite for efficacy safety and intuitive user experience in robotic surgery in this paper we propose a system modeling approach for motion compensation . Based on a bidirectional actuated system using two separate tendon sheaths motion transmission is firstly characterized . Two types of positional errors due to system backlash and environment loading are defined and modeled . Then a model based feedforward compensation method is proposed for open loop control giving the system abilities to adjust according to changes in the transmission route configuration without any information feedback from the distal end . A dedicated experimental platform emulating a bidirectional TSS robotic system for endoscopic surgery is built for testing . Proposed positional errors are identified and verified . The performance of the proposed motion compensation is evaluated by trajectory tracking under different environment loading conditions . And the results demonstrate that accurate position control can be achieved even if the transmission route configuration is updated . 
",We reviewed the current shortage in the development of robotic endoscopic surgery. Motion transmission of a bidirectional tendon driven system was characterized. Two errors uniquely related to the robotic system setup were identified and modeled. Compensation method was proposed for accurate motion control regardless of any variations in the environment loading. Experimental validation was presented by emulating mechanism transmission in a typical robotic endoscopic surgery setup.,,,
S0167839616300115," Recently a construction of spline spaces suitable for representing smooth parametric surfaces of arbitrary topological genus and arbitrary order of continuity has been proposed . These splines called RAGS are a direct generalization of bivariate polynomial splines on planar triangulations . In this paper we discuss how to construct parametric splines associated with the three homogeneous geometries and we also consider a number of related computational issues . We then show how homogeneous splines can be used to obtain RAGS . As examples of RAGS surfaces we consider direct analogs of the Powell Sabin macro elements and also spline surfaces of higher degrees and higher orders of continuity obtained by minimizing an energy functional . 
",Piecewise rational functions called rational geometric splines or RAGS are studied. RAGS are suitable for representing surfaces of arbitrary genus and continuity. A construction of parametric homogeneous splines is proposed. Interpolation and approximation methods for constructing smooth splines are presented. It is shown how homogeneous splines can be used to obtain RAGS.,,,
S0167947314000140," The and distributional family is generated from a relatively simple transformation of the standard normal and can approximate a broad spectrum of distributions . Consequently it is easy to use in simulation studies and has been applied in multiple areas including risk management stock return analysis and missing data imputation studies . A rapidly convergent quantile based least squares estimation method to fit the and distributional family parameters is proposed and then extended to a robust version . The robust version is then used as a more general outlier detection approach . Several properties of the QLS method are derived and comparisons made with competing methods through simulation . Real data examples of microarray and stock index data are used as illustrations . 
",Introduced simple methods to estimate the and distributional parameters. Proved consistency and asymptotic normality. Effective robust version introduced. Robust version used to obtain base distribution for outlier detection. Illustrated use of proposed methods to multiple fields.,,,
S0169260715000589," Background Gastric cancer is among the most common gastrointestinal cancers worldwide . Patients who have undergone surgery for gastric cancer may suffer from malnutrition and potential consequences such as gastrointestinal complications surgical stress and cancer cachexia . A tablet PC based intervention via a mobile application might enhance the early recovery of postgastrectomy patients . Objective The aim of this study was to develop and test a tablet personal computer assisted intervention to hasten the recovery of postgastrectomy cancer patients with respect to nutritional status . Methods This single arm pilot study investigated a tablet PC application developed to serve the functions of nutritional monitoring medical information management drainage follow up and wound care . All services were delivered by medical professionals . Results Twenty consecutive gastrectomy patients at the National Taiwan University Hospital received perioperative care via this application . During the study period we retrospectively collected an additional 20 demographically matched gastrectomy cases as a control group . The App group had a lower body weight loss percentage relative to the control group during a 6 month follow up period . However the patients in the App group had more outpatient clinic visits than did those in the control group . Conclusions This study supported the feasibility of a tablet PC based application for the perioperative care of gastric cancer subjects to promote a lower body weight loss and the collection of comprehensive surgical records . 
",A gap between hospital care and self care at home. App provides a feasible solution for gastrectomy patients to do self recording of postoperative information. With App support the postgastrectomy patients had a lower proportion of body weight loss compared to traditional care.,,,
S0169260715001819," The paper addresses the issue of non invasive real time prediction of hidden inner body temperature variables during therapeutic cooling or heating and proposes a solution that uses computer simulations and machine learning . The proposed approach is applied on a real world problem in the domain of biomedicine prediction of inner knee temperatures during therapeutic cooling after anterior cruciate ligament reconstructive surgery . A validated simulation model of the cryotherapeutic treatment is used to generate a substantial amount of diverse data from different simulation scenarios . We apply machine learning methods on the simulated data to construct a predictive model that provides a prediction for the inner temperature variable based on other system variables whose measurement is more feasible i.e . skin temperatures . First we perform feature ranking using the RReliefF method . Next based on the feature ranking results we investigate the predictive performance and time memory efficiency of several predictive modeling methods linear regression regression trees model trees and ensembles of regression and model trees . Results have shown that using only temperatures from skin sensors as input attributes gives excellent prediction for the temperature in the knee center . Moreover satisfying predictive accuracy is also achieved using short history of temperatures from just two skin sensors as input variables . The model trees perform the best with prediction error in the same range as the accuracy of the simulated data . Furthermore they satisfy the requirements for small memory size and real time response . We successfully validate the best performing model tree with real data from in vivo temperature measurement from a patient undergoing cryotherapy after ACL reconstruction . 
",Non invasive real time prediction of inner body temperature variables during cryotherapy. Validated simulation model of the cryotherapeutic treatment. Machine learning methods on the simulated data to construct a predictive model. Feature ranking with the RReliefF method. Using only skin temperatures as input attributes gives excellent prediction.,,,
S0167839616300085," We present a new method for immersogeometric fluid flow analysis that directly uses the CAD boundary representation of a complex object and immerses it into a locally refined non boundary fitted discretization of the fluid domain . The motivating applications include analyzing the flow over complex geometries such as moving vehicles where the detailed geometric features usually require time consuming labor intensive geometry cleanup or mesh manipulation for generating the surrounding boundary fitted fluid mesh . The proposed method avoids the challenges associated with such procedures . A new method to perform point membership classification of the background mesh quadrature points is also proposed . To faithfully capture the geometry in intersected elements we implement an adaptive quadrature rule based on the recursive splitting of elements . Dirichlet boundary conditions in intersected elements are enforced weakly in the sense of Nitsche s method . To assess the accuracy of the proposed method we perform computations of the benchmark problem of flow over a sphere represented using B rep . Quantities of interest such as drag coefficient are in good agreement with reference values reported in the literature . The results show that the density and distribution of the surface quadrature points are crucial for the weak enforcement of Dirichlet boundary conditions and for obtaining accurate flow solutions . Also with sufficient levels of surface quadrature element refinement the quadrature error near the trim curves becomes insignificant . Finally we demonstrate the effectiveness of our immersogeometric method for high fidelity industrial scale simulations by performing an aerodynamic analysis of an agricultural tractor directly represented using B rep . 
",Immersogeometric analysis that directly uses the B rep CAD model is proposed. A GPU accelerated point membership classification is performed. Distribution of the surface quadrature points is crucial for accuracy. The quadrature error near the trim curves is relatively insignificant. The methodology is found effective on a 3D benchmark and an industrial problem.,,,
S0167947315002595," In large scale genomic analyses dealing with detecting genotype phenotype associations such as genome wide association studies it is desirable to have numerically and statistically robust procedures to test the stochastic independence null hypothesis against certain alternatives . Motivated by a special case in a GWAS a novel test procedure called Correlation Profile Test is developed for testing genomic associations with failure time phenotypes subject to right censoring and competing risks . Performance and operating characteristics of CPT are investigated and compared to existing approaches by a simulation study and on a real dataset . Compared to popular choices of semiparametric and nonparametric methods CPT has three advantages it is numerically more robust because it solely relies on sample moments it is more robust against the violation of the proportional hazards condition and it is more flexible in handling various failure and censoring scenarios . CPT is a general approach to testing the null hypothesis of stochastic independence between a failure event point process and any random variable thus it is widely applicable beyond genomic studies . 
",A novel procedure called Correlation Profile Test CPT is proposed. A novel permutation test the hybrid permutation test is proposed. CPT fits genomics applications better than common survival analysis methods. CPT is general and can have wide applications beyond genomics.,,,
S0167947315000171," Model based clustering associates each component of a finite mixture distribution to a group or cluster . Therefore an underlying implicit assumption is that a one to one correspondence exists between mixture components and clusters . In applications with multivariate continuous data finite mixtures of Gaussian distributions are typically used . Information criteria such as BIC are often employed to select the number of mixture components . However a single Gaussian density may not be sufficient and two or more mixture components could be needed to reasonably approximate the distribution within a homogeneous group of observations . A clustering method based on the identification of high density regions of the underlying density function is introduced . Starting with an estimated Gaussian finite mixture model the corresponding density estimate is used to identify the cluster cores i.e . those data points which form the core of the clusters . Then the remaining observations are allocated to those cluster cores for which the probability of cluster membership is the highest . The method is illustrated using both simulated and real data examples which show how the proposed approach improves the identification of non Gaussian clusters compared to a fully parametric approach . Furthermore it enables the identification of clusters which can not be obtained by merging mixture components and it can be straightforwardly extended to cases of higher dimensionality . 
",Clusters are identified as connected components from high density regions. Gaussian finite mixture models are used for density estimation. Identified clusters are not constrained to have a Gaussian shape. Clusters need not be obtained by combining mixture components. A dimension reduction step is used in cases of higher data dimensionality.,,,
S0167839615001405," We study the dimension of trivariate splines on bipyramid cells that is cells with boundary vertices n of which are coplanar with the interior vertex . We improve the earlier lower bound on the dimension given by J. Shan . Moreover we derive a new upper bound that is equal to the known lower bound in most cases . In the remaining cases our upper bound is close to the known lower bound and we conjecture that the dimension coincides with the upper bound . We use tools from both algebraic geometry and Bernstein B zier analysis . 
",We introduce bipyramid cells in We improve the lower bound on the dimension of splines for bipyramids. We derive a new upper bound that is equal to the known lower bound in most cases. We use tools from algebraic geometry and Bernstein B zier analysis.,,,
S0167947314002333," Estimation of longitudinal models of relationship status between all pairs of individuals in social networks is challenging due to the complex inter dependencies among observations and lengthy computation times . To reduce the computational burden of model estimation a method is developed that subsamples the always null dyads in which no relationships develop throughout the period of observation . The informative sampling process is accounted for by weighting the likelihood contributions of the observations by the inverses of the sampling probabilities . This weighted likelihood estimation method is implemented using Bayesian computation and evaluated in terms of its bias efficiency and speed of computation under various settings . Comparisons are also made to a full information likelihood based procedure that is only feasible to compute when limited follow up observations are available . Calculations are performed on two real social networks of very different sizes . The easily computed weighted likelihood procedure closely approximates the corresponding estimates for the full network even when using low sub sampling fractions . The fast computation times make the weighted likelihood approach practical and able to be applied to networks of any size . 
",Estimation of statistical models for social networks is challenging. Dyads with no relationship null dyads are common in large social networks. Propose to subsample the always null dyads. Develop weighted likelihood Bayesian estimation method. Method enables large social networks to be analyzed feasibly and accurately.,,,
S0167947313004465," In the application of the popular maximum likelihood method to factor analysis the number of factors is commonly determined through a two stage procedure in which stage 1 performs parameter estimation for a set of candidate models and then stage 2 chooses the best according to certain model selection criterion . Usually to obtain satisfactory performance a large set of candidates is used and this procedure suffers a heavy computational burden . To overcome this problem a novel one stage algorithm is proposed in which parameter estimation and model selection are integrated in a single algorithm . This is obtained by maximizing the criterion with respect to model parameters and the number of factors jointly rather than separately . The proposed algorithm is then extended to accommodate incomplete data . Experiments on a number of complete incomplete synthetic and real data reveal that the proposed algorithm is as effective as the existing two stage procedure while being much more computationally efficient particularly for incomplete data . 
",We propose a novel algorithm for learning factor analysis with complete and incomplete data. The algorithm is able to determine the number of factors in an automated manner. The algorithm is as effective as the previous two stage procedure. However the algorithm is much more computationally efficient.,,,
S0169260714002417," Proteins control all biological functions in living species . Protein structure is comprised of four major classes including all class all class and . Each class performs different function according to their nature . Owing to the large exploration of protein sequences in the databanks the identification of protein structure classes is difficult through conventional methods with respect to cost and time . Looking at the importance of protein structure classes it is thus highly desirable to develop a computational model for discriminating protein structure classes with high accuracy . For this purpose we propose a silco method by incorporating Pseudo Average Chemical Shift and Support Vector Machine . Two feature extraction schemes namely Pseudo Amino Acid Composition and Pseudo Average Chemical Shift are used to explore valuable information from protein sequences . The performance of the proposed model is assessed using four benchmark datasets 25PDB 1189 640 and 399 employing jackknife test . The success rates of the proposed model are 84.2 85.0 86.4 and 89.2 respectively on the four datasets . The empirical results reveal that the performance of our proposed model compared to existing models is promising in the literature so far and might be useful for future research . 
",We develop an accurate and high throughput predictor for classification of protein structure classes. It is the combination of Pseudo Average Chemical Shift and SVM. Three datasets were evaluated using jackknife test. Best results are reported so far in the literature.,,,
S0167931715001203," We used two methods namely stamping and printing to transfer arrays of epitaxial gallium phosphide nanowires from their growth substrate to a soft biodegradable layer of polycaprolactone . Using the stamping method resulted in a very inhomogeneous surface topography with a wide distribution of transferred nanowire lengths whereas using the printing method resulted in an homogeneous substrate topography over several mm2 . PC12 cells were cultured on the hybrid nanowire PCL substrates realized using the printing method and exhibited an increased attachment on these substrates compared to the original nanowire semiconductor substrate . Transferring nanowires on PCL substrates is promising for implanting nanowires in vivo with a possible reduced inflammation compared to when hard semi conductor substrates are implanted together with the nanowires . The nanowire PCL hybrid substrates could also be used as biocompatible cell culture substrates . Finally using nanowires on PCL substrates would enable to recycle the expensive GaP substrate and repeatedly grow nanowires on the same substrate . 
",Vertical nanowires were transferred to polycaprolactone substrates. The hybrid nanowire polymer substrate has a homogeneous topography over millimeters. PC12 cells were successfully cultured on the hybrid substrates. The results are promising for bioimplant applications.,,,
S0168169915000022," Optimal design and operation of a planned full scale UASB reactor at a dairy farm are determined using optimization algorithms based on steady state simulations of a dynamic AD process model combined with models of the reactor temperature and heat exchanger temperatures based on energy balances . Available feedstock is 6m3 d dairy manure produced by the herd . Three alternative optimization problems are solved Maximization of produced methane gas flow minimization of reactor volume and maximization of power surplus . Constraints of the optimization problems are an upper limit of the VFA concentration and an upper limit of the feed rate corresponding to a normal animal waste production at the farm . The most proper optimization problem appears to be minimization of the reactor volume assuming that the feed rate is fixed at its upper limit and that the VFA concentration is at its upper limit . The optimal result is a power surplus of 49.8MWh y a hydraulic retention time of 6.1d and a reactor temperature of 35.9 C assuming heat recovery with an heat exchanger and perfect reactor heat transfer insulation . In general the optimal solutions are improved if the ratio of the solids retention time to the hydraulic retention time is increased . 
",Optimal design and operation of a UASB reactor at a dairy farm is determined. Brute force optimization is based on a dynamic AD model and temperature models. Solutions provided are maximum gas flow minimum volume and maximum power surplus. The optimal solutions are improved if the biomass retention time is increased.,,,
S0169260715002333," Background Drug drug interactions have long been an active research area in clinical medicine . In Taiwan however the widespread use of traditional Chinese medicines presents additional complexity to the topic . Therefore it is important to see the interaction between traditional Chinese and western medicine . Objective To create a comprehensive database of multi herb western drug interactions indexed according to the ways in which physicians actually practice and to measure this database s impact on the detection of adverse effects between traditional Chinese medicine compounds and western medicines . Methods First a multi herb western medicine drug interactions database was created by separating each TCM compound into its constituent herbs . Each individual herb was then checked against an existing single herb western drug interactions database . The data source comes from the National Health Insurance research database which spans the years 1998 2011 . This study estimated the interaction prevalence rate and further separated the rates according to patient characteristics distribution by county and hospital accreditation levels . Finally this new database was integrated into a computer order entry module of the electronic medical records system of a regional teaching hospital . The effects it had were measured for two months . Results The most commonly interacting Chinese herbs were Ephedrae Herba and Angelicae Sinensis Radix Angelicae Dahuricae Radix . Ephedrae Herba contains active ingredients similar to in ephedrine . 15 kinds of traditional Chinese medicine compounds contain Ephedrae Herba . Angelicae Sinensis Radix and Angelicae Dahuricae Radix contain ingredients similar to coumarin a blood thinner . 9 kinds of traditional Chinese medicine compounds contained Angelicae Sinensis Radix Angelicae Dahuricae Radix . In the period from 1998 to 2011 the prevalence of herb drug interactions related to Ephedrae Herba was 0.18 . The most commonly prescribed traditional Chinese compounds were MA SHING GAN SHYR TANG followed by SHEAU CHING LONG TANG and DINQ CHUAN TANG . The prevalence of herb drug interactions related to Angelicae Sinensis Radix Angelicae Dahuricae Radix was 4.59 . The most common traditional Chinese compound formula were TSANG EEL SAAN followed by HUOH SHIANG JENQ CHIH SAAN and SHY WUH TANG . Once the multi herb drug interaction database was deployed in a hospital system there were 480 prescriptions that indicated a TCM western drug interaction . Physicians were alerted 24 times during two months . These alerts resulted in a prescription change four times . Conclusion Due to the unique cultural factors that have resulted in widespread acceptance of both western and traditional Chinese medicine Taiwan stands well positioned to report on the prevalence of interactions between western drugs and traditional Chinese medicine and devise ways to reduce their incidence . This study built a multi herb western drug interactions database embedded inside a hospital clinical information system and then examined the effects that drug interaction alerts had on clinician prescribing behaviour . The results demonstrated that western drug traditional Chinese medicine interactions are prevalent and that western trained physicians tend to change. 
",Most commonly interacting Chinese herbs were Ephedrae Herba and Angelicae Sinensis Radix Angelicae Dahuricae Radix. Unique cultural factors that have resulted in widespread acceptance of both western and traditional Chinese medicine. Taiwan stands well positioned to report on the prevalence of interactions between western drugs and traditional Chinese medicine.,,,
S0167931713004991," Structuring or removal of the epoxy based photo sensitive polymer SU 8 by inductively coupled plasma reactive ion etching was investigated as a function of plasma chemistry bias power temperature and pressure . In a pure oxygen plasma surface accumulation of antimony from the photo initiator introduced severe roughness and reduced etch rate significantly . Addition of SF6 to the plasma chemistry reduced the antimony surface concentration with lower roughness and higher etch rate as an outcome . Furthermore the etch anisotropy could be tuned by controlling the bias power . Etch rates up to 800nmmin 1 could be achieved with low roughness and high anisotropy . 
",SU 8 is etched using inductively coupled plasma ICP RIE in O2 with SF6 added. Anisotropic etching at etch rates around 750nmmin 1 is achieved. Etched SU 8 surface roughness is caused by surface antimony acting as a local mask. Sb surface concentration and roughness are reduced significantly by adding SF6.,,,
S0167839615001363," We consider the adaptive refinement of bivariate quartic smooth box spline spaces on the three directional grid G. The polynomial segments of these box splines belong to a certain subspace of the space of quartic polynomials which will be called the space of special quartics . Given a bounded domain and finite sequence of dyadically refined grids we obtain a hierarchical grid by selecting mutually disjoint cells from all levels such that their union covers the entire domain . Using a suitable selection procedure allows to define a basis spanning the hierarchical box spline space . The paper derives a characterization of this space . Under certain mild assumptions on the hierarchical grid the hierarchical spline space is shown to contain all smooth functions whose restrictions to the cells of the hierarchical grid are special quartic polynomials . Thus in this case we can give an affirmative answer to the completeness questions for the hierarchical box spline basis . 
",We identify the subspace of special quartics generated by quartic box splines. We establish the completeness of hierarchical smooth quartic box splines. We characterize the hierarchical space using special quartic polynomials. We obtain a basis for the hierarchical spline space over type I triangulations.,,,
S0169260715002412," Breast thermography still has inherent limitations that prevent it from being fully accepted as a breast screening modality in medicine . The main challenges of breast thermography are to reduce false positive results and to increase the sensitivity of a thermogram . Further it is still difficult to obtain information about tumour parameters such as metabolic heat tumour depth and diameter from a thermogram . However infrared technology and image processing have advanced significantly and recent clinical studies have shown increased sensitivity of thermography in cancer diagnosis . The aim of this paper is to study numerically the possibilities of extracting information about the tumour depth from steady state thermography and transient thermography after cold stress with no need to use any specific inversion technique . Both methods are based on the numerical solution of Pennes bioheat equation for a simple three dimensional breast model . The effectiveness of two approaches used for depth detection from steady state thermography is assessed . The effect of breast density on the steady state thermal contrast has also been studied . The use of a cold stress test and the recording of transient contrasts during rewarming were found to be potentially suitable for tumour depth detection during the rewarming process . Sensitivity to parameters such as cold stress temperature and cooling time is investigated using the numerical model and simulation results reveal two prominent depth related characteristic times which do not strongly depend on the temperature of the cold stress or on the cooling period . 
",Steady state thermal contrast magnitude depends on the tumour diameter and depth as well as on the breast density. The Full Width at Half Maximum calculated at the surface of the breast enlarges as the tumour depth increases but also depends on the tumour diameter. Transient thermal contrast trends present three important characteristics including the response time the transient peak and its corresponding observation time. The observation time is likely a potential time for tumour depth detection as it does not strongly depend on the tumour diameter or on the chilling temperature and its duration.,,,
S0167947315002935," Change point models seek to fit a piecewise regression model with unknown breakpoints to a data set whose parameters are suspected to change through time . However the exponential number of possible solutions to a multiple change point problem requires an efficient algorithm if long time series are to be analyzed . A sequential Bayesian change point algorithm is introduced that provides uncertainty bounds on both the number and location of change points . The algorithm is able to quickly update itself in linear time as each new data point is recorded and uses the exact posterior distribution to infer whether or not a change point has been observed . Simulation studies illustrate how the algorithm performs under various parameter settings including detection speeds and error rates and allow for comparison with several existing multiple change point algorithms . The algorithm is then used to analyze two real data sets including global surface temperature anomalies over the last 130 years . 
",Algorithm quickly updates its inference in linear time with each new observation. Derives uncertainty bounds on the number and location of change points in a data set. Explores potential detection criteria associated with posterior distribution. Simulation studies show high detection rate low false positive rate. Analysis of two real data sets illustrate wide range of potential applications.,,,
S0169260715000917," Computational fluid dynamics modeling of the pulmonary vasculature has the potential to reveal continuum metrics associated with the hemodynamic stress acting on the vascular endothelium . It is widely accepted that the endothelium responds to flow induced stress by releasing vasoactive substances that can dilate and constrict blood vessels locally . The objectives of this study are to examine the extent of patient specificity required to obtain a significant association of CFD output metrics and clinical measures in models of the pulmonary arterial circulation and to evaluate the potential correlation of wall shear stress with established metrics indicative of right ventricular afterload in pulmonary hypertension . Right Heart Catheterization hemodynamic data and contrast enhanced computed tomography imaging were retrospectively acquired for 10 PH patients and processed to simulate blood flow in the pulmonary arteries . While conducting CFD modeling of the reconstructed patient specific vasculatures we experimented with three different outflow boundary conditions to investigate the potential for using computationally derived spatially averaged wall shear stress as a metric of RV afterload . SAWSS was correlated with both pulmonary vascular resistance and arterial compliance but the extent of the correlation was affected by the degree of patient specificity incorporated in the fluid flow boundary conditions . We found that decreasing the distal PVR alters the flow distribution and changes the local velocity profile in the distal vessels thereby increasing the local WSS . Nevertheless implementing generic outflow boundary conditions still resulted in statistically significant SAWSS correlations with respect to both metrics of RV afterload suggesting that the CFD model could be executed without the need for complex outflow boundary conditions that require invasively obtained patient specific data . A preliminary study investigating the relationship between outlet diameter and flow distribution in the pulmonary tree offers a potential computationally inexpensive alternative to pressure based outflow boundary conditions . computational fluid dynamics fluid structure interaction pulmonary hypertension right ventricle coefficient of determination right heart catheterization main pulmonary artery mean pulmonary arterial pressure mmHg systolic pulmonary arterial pressure mmHg diastolic pulmonary arterial pressure mmHg pulse pressure sPAP dPAP mmHg pulmonary vascular resistance dyns cm5 wall shear stress dyn cm2 spatially averaged wall shear stress dyn cm2 cardiac output cm3 s body surface area m2 cardiac index cm3 s m2 heart rate beats min pulmonary capillary wedge pressure mmHg compliance cm3 mmHg vascular luminal area cm2 hydraulic diameter cm 
",We applied computational fluid dynamics modeling to 11 pulmonary vascular trees. Patient specific cardiac output is necessary to obtain accurate wall shear stress. Spatially averaged wall shear stress is highly correlated with pulmonary vascular resistance. Spatially averaged wall shear stress is highly correlated with arterial compliance. Structured tree outflow boundary conditions improve these correlations.,,,
S0169260715000218," Background and objectives Post genomic clinical trials require the participation of multiple institutions and collecting data from several hospitals laboratories and research facilities . This paper presents a standard based solution to provide a uniform access endpoint to patient data involved in current clinical research . Methods The proposed approach exploits well established standards such as HL7 v3 or SPARQL and medical vocabularies such as SNOMED CT LOINC and HGNC . A novel mechanism to exploit semantic normalization among HL7 based data models and biomedical ontologies has been created by using Semantic Web technologies . Results Different types of queries have been used for testing the semantic interoperability solution described in this paper . The execution times obtained in the tests enable the development of end user tools within a framework that requires efficient retrieval of integrated data . Conclusions The proposed approach has been successfully tested by applications within the INTEGRATE and EURECA EU projects . These applications have been deployed and tested for patient screening trial recruitment and retrospective analysis exploiting semantically interoperable access to clinical patient data from heterogeneous data sources . 
",We present a standard based semantic interoperability solution. This solution uses a mechanism that exploits semantic normalization over HL7 model. It has been used for testing in trial recruitment or trial feasibility. Execution times obtained in testing empower its utilization for end user tools.,,,
S0169260715001972," Background and objective Early childhood caries is a potentially severe disease affecting children all over the world . The available findings are mostly based on a logistic regression model but data mining in particular association rule mining could be used to extract more information from the same data set . Methods ECC data was collected in a cross sectional analytical study of the 10 sample of preschool children in the South Ba ka area . Association rules were extracted from the data by association rule mining . Risk factors were extracted from the highly ranked association rules . Results Discovered dominant risk factors include male gender frequent breastfeeding high birth order language and low body weight at birth . Low health awareness of parents was significantly associated to ECC only in male children . Conclusions The discovered risk factors are mostly confirmed by the literature which corroborates the value of the methods . 
",We look for an approach to massive risk factor discovery for early childhood caries. Association rule mining with adequate rule pruning and ranking performs well. Literature supports most risk factors identified in associative analysis. When coupled with other risk factors frequent breastfeeding confirmed as a risk. Parent health awareness significant only for boys.,,,
S0167947313002855," For constructing simultaneous confidence intervals for the ratios of means of several lognormal distributions we propose a new parametric bootstrap method which is different from an inaccurate parametric bootstrap method previously considered in the literature . Our proposed method is conceptually simpler than other proposed methods which are based on the concepts of generalized pivotal quantities and fiducial generalized pivotal quantities . Also our extensive simulation results indicate that our proposed method consistently performs better than other methods its coverage probability is close to the nominal confidence level and the resulting intervals are typically shorter than the intervals produced by other methods . 
",We present parametric bootstrap PB simultaneous confidence intervals. Our PB procedure is different from a previous PB procedure which performs poorly. Using simulation we compare the PB procedure with three other procedures. The coverage probability of the PB method is close to the nominal confidence level. The PB procedure consistently outperforms other procedures.,,,
S0169260715002229," Gait function is traditionally assessed using well lit unobstructed walkways with minimal distractions . In patients with subclinical physiological abnormalities these conditions may not provide enough stress on their ability to adapt to walking . The introduction of challenging walking conditions in gait can induce responses in physiological systems in addition to the locomotor system . There is a need for a device that is capable of monitoring multiple physiological systems in various walking conditions . To address this need an Android based gait monitoring device was developed that enabled the recording of a patient s physiological systems during walking . The gait monitoring device was tested during self regulated overground walking sessions of fifteen healthy subjects that included 6 females and 9 males aged 18 35 years . The gait monitoring device measures the patient s stride interval acceleration electrocardiogram skin conductance and respiratory rate . The data is stored on an Android phone and is analyzed offline through the extraction of features in the time frequency and time frequency domains . The analysis of the data depicted multisystem physiological interactions during overground walking in healthy subjects . These interactions included locomotion electrodermal locomotion respiratory and cardiolocomotion couplings . The current results depicting strong interactions between the locomotion system and the other considered systems warrant further investigation into multisystem interactions during walking particularly in challenging walking conditions with older adults . 
",We developed a smart phone based gait monitor. The proposed device records several physiological signals simultaneously. ECG skin conductance respiration strides and gait acceleration signals were measured. The devices enable us to understand physiological interactions during walking.,,,
S0169260714003897," Metabolic Engineering aims to design microbial cell factories towards the production of valuable compounds . In this endeavor one important task relates to the search for the most suitable heterologous pathway to add to the selected host . Different algorithms have been developed in the past towards this goal following distinct approaches spanning constraint based modeling graph based methods and knowledge based systems based on chemical rules . While some of these methods search for pathways optimizing specific objective functions here the focus will be on methods that address the enumeration of pathways that are able to convert a set of source compounds into desired targets and their posterior evaluation according to different criteria . Two pathway enumeration algorithms based on graph based representations are selected as the most promising ones and are analyzed in more detail the Solution Structure Generation and the Find Path algorithms . Their capabilities and limitations are evaluated when designing novel heterologous pathways by applying these methods on three case studies of synthetic ME related to the production of non native compounds in E. coli and S. cerevisiae 1 butanol curcumin and vanillin . Some targeted improvements are implemented extending both methods to address limitations identified that impair their scalability improving their ability to extract potential pathways over large scale databases . In all case studies the algorithms were able to find already described pathways for the production of the target compounds but also alternative pathways that can represent novel ME solutions after further evaluation . 
",Paper reviews the main hyper graph based algorithms for metabolic pathway enumeration. The FindPath and SSG revealed limitations in metabolic engineering case studies. These algorithms were improved to boost computational efficiency and scalability. Improved versions of the algorithms were able to find previously known pathways from literature. There is still room for improvement in the computational efficiency of the algorithms.,,,
S0167947315002017," We study the property of the Fused Lasso Signal Approximator for estimating a blocky signal sequence with additive noise . We transform the FLSA to an ordinary Lasso problem and find that in general the resulting design matrix does not satisfy the irrepresentable condition that is known as an almost necessary and sufficient condition for exact pattern recovery . We give necessary and sufficient conditions on the expected signal pattern such that the irrepresentable condition holds in the transformed Lasso problem . However these conditions turn out to be very restrictive . We apply the newly developed preconditioning method Puffer Transformation to the transformed Lasso and call the new procedure the preconditioned fused Lasso . We give non asymptotic results for this method showing that as long as the signal to noise ratio is not too small our preconditioned fused Lasso estimator always recovers the correct pattern with high probability . Theoretical results give insight into what controls the ability of recovering the pattern it is the noise level instead of the length of the signal sequence . Simulations further confirm our theorems and visualize the significant improvement of the preconditioned fused Lasso estimator over the vanilla FLSA in exact pattern recovery . 
",We provided necessary and sufficient conditions such that fused Lasso consistently recovers the piecewise constant pattern. We found that in general the fused Lasso is not consistent. We proposed a preconditioned fused Lasso to overcome the non consistent issue. Simulation studies support our findings.,,,
S0169260715000449," Retinal image registration is a necessary step in diagnosis and monitoring of Diabetes Retinopathy which is one of the leading causes of blindness . Long term diabetes affects the retinal blood vessels and capillaries eventually causing blindness . This progressive damage to retina and subsequent blindness can be prevented by periodic retinal screening . The extent of damage caused by DR can be assessed by comparing retinal images captured during periodic retinal screenings . During image acquisition at the time of periodic screenings translation rotation and scale are introduced in the retinal images . Therefore retinal image registration is an essential step in automated system for screening diagnosis treatment and evaluation of DR . This paper presents an algorithm for registration of retinal images using orthogonal moment invariants as features for determining the correspondence between the dominant points in the reference and test retinal images . As orthogonal moments are invariant to TRS moment invariants features around a vessel bifurcation are unaltered due to TRS and can be used to determine the correspondence between reference and test retinal images . The vessel bifurcation points are located in segmented thinned retinal images and labeled in corresponding grayscale retinal images . The correspondence between vessel bifurcations in reference and test retinal image is established based on moment invariants features . Further the TRS in test retinal image with respect to reference retinal image is estimated using similarity transformation . The test retinal image is aligned with reference retinal image using the estimated registration parameters . The accuracy of registration is evaluated in terms of mean error and standard deviation of the labeled vessel bifurcation points in the aligned images . The experimentation is carried out on DRIVE database STARE database VARIA database and database provided by local government hospital in Pune India . The experimental results exhibit effectiveness of the proposed algorithm for registration of retinal images . 
",This paper presents an algorithm for registration of retinal images using orthogonal moment invariants as features. As orthogonal moments are invariant to translation rotation and scale TRS moment invariants features around a vessel bifurcation are unaltered due to TRS and are used to determine the correspondence between reference and test retinal images. The TRS in test retinal image with respect to reference retinal image is estimated using Similarity transformation. The test retinal image is aligned with reference retinal image using the estimated registration parameters. The experimentation is carried out on 4 databases as DRIVE STARE VARIA and database provided by local government hospital.,,,
S0169260714002107," In this paper the model predictive control technology is used for tackling the optimal drug administration problem . The important advantage of MPC compared to other control technologies is that it explicitly takes into account the constraints of the system . In particular for drug treatments of living organisms MPC can guarantee satisfaction of the minimum toxic concentration constraints . A whole body physiologically based pharmacokinetic model serves as the dynamic prediction model of the system after it is formulated as a discrete time state space model . Only plasma measurements are assumed to be measured on line . The rest of the states are estimated in real time by designing an artificial observer . The complete system is able to drive the drug concentration to the desired levels at the organs of interest while satisfying the imposed constraints even in the presence of modelling errors disturbances and noise . A case study on a PBPK model with 7 compartments constraints on 5 tissues and a variable drug concentration set point illustrates the efficiency of the methodology in drug dosing control applications . The proposed methodology is also tested in an uncertain setting and proves successful in presence of modelling errors and inaccurate measurements . 
",The model predictive control methodology was used to design an optimal drug administration scheme for continuous drug administration. A PBPK model served as the predictive model after it is formulated as a state space system. The methodology can impose constraints on all different compartments even if drug concentration measurements are not possible by augmenting the control system with an observer. Through the application to the problem of distributing DMA in mice it was shown that the method is not sensitive to the presence of measurement noise and intra patient variability.,,,
S0169260715001571," Automatic detection of the QRS complexes R peaks in an electrocardiogram signal is the most important step preceding any kind of ECG processing and analysis . The performance of these systems heavily relies on the accuracy of the QRS detector . The objective of present work is to drive a new robust method based on stationary wavelet transform for R peaks detection . The decimation of the coefficients at each level of the transformation algorithm is omitted more samples in the coefficient sequences are available and hence a better outlier detection can be performed . Using the information of local maxima minima and zero crossings of the fourth SWT coefficient detail the proposed algorithm identifies the significant points for detection and delineation of the QRS complexes as well as detection and identification of the QRS individual waves peaks of the pre processed ECG signal . Various experimental results show that the proposed algorithm exhibits reliable QRS detection as well as accurate ECG delineation achieving excellent performance on different databases on the MIT BIH database on the QT Database and on MIT BIH Noise Stress Test Database . Reliability and accuracy are close to the highest among the ones obtained in other studies . Experiments results being satisfactory the SWT may represent a novel QRS detection tool for a robust ECG signal analysis . 
",This work makes a R peaks detection based on stationary wavelet transform SWT . It has the same sampling time of the signal. It inherently leads to more accurate estimation of the QRS fiducial points. It detect R peaks over different types of arrhythmias and types of noise. Only one scale of SWT is applied in order to reduce the computational complexity.,,,
S0169260715002242," Three dimensional reconstruction of lung and vessel tree has great significance to 3D observation and quantitative analysis for lung diseases . This paper presents non sheltered 3D models of lung and vessel tree based on a supervised semi 3D lung tissues segmentation method . A recursive strategy based on geometric active contour is proposed instead of the coarse to fine framework in existing literature to extract lung tissues from the volumetric CT slices . In this model the segmentation of the current slice is supervised by the result of the previous one slice due to the slight changes between adjacent slice of lung tissues . Through this mechanism lung tissues in all the slices are segmented fast and accurately . The serious problems of left and right lungs fusion caused by partial volume effects and segmentation of pleural nodules can be settled meanwhile during the semi 3D process . The proposed scheme is evaluated by fifteen scans from eight healthy participants and seven participants suffering from early stage lung tumors . The results validate the good performance of the proposed method compared with the coarse to fine framework . The segmented datasets are utilized to reconstruct the non sheltered 3D models of lung and vessel tree . 
",This manuscript proposes a novel recursive strategy based on geometric active contour model to extract lung tissues from the volumetric CT slices accurately. The proposed method could settle the challenging task of separating left and right lungs. Non sheltered 3D models of lung and vessels tree are constructed based on the extracted datasets. The proposed method is validated by fifteen scans with good performance.,,,
S0169260715001984," In this paper a new image segmentation method based on Particle Swarm Optimization and outlier rejection combined with level set is proposed . A traditional approach to the segmentation of Magnetic Resonance images is the Fuzzy C Means clustering algorithm . The membership function of this conventional algorithm is sensitive to the outlier and does not integrate the spatial information in the image . The algorithm is very sensitive to noise and in homogeneities in the image moreover it depends on cluster centers initialization . To improve the outlier rejection and to reduce the noise sensitivity of conventional FCM clustering algorithm a novel extended FCM algorithm for image segmentation is presented . In general in the FCM algorithm the initial cluster centers are chosen randomly with the help of PSO algorithm the clusters centers are chosen optimally . Our algorithm takes also into consideration the spatial neighborhood information . These a priori are used in the cost function to be optimized . For MR images the resulting fuzzy clustering is used to set the initial level set contour . The results confirm the effectiveness of the proposed algorithm . 
",New image segmentation method based on PSO and outlier rejection with level set. The use of PSO algorithm allows an optimal choice of cluster centers. IKPCM lead to a fine segmentation using the level set method. IKPCM shows a significant improvement concerning the robustness to noise.,,,
S0169260714003459," Telecare medicine information systems provide a communicating platform for accessing remote medical resources through public networks and help health care workers and medical personnel to rapidly making correct clinical decisions and treatments . An authentication scheme for data exchange in telecare medicine information systems enables legal users in hospitals and medical institutes to establish a secure channel and exchange electronic medical records or electronic health records securely and efficiently . This investigation develops an efficient and secure verified based three party authentication scheme by using extended chaotic maps for data exchange in telecare medicine information systems . The proposed scheme does not require server s public keys and avoids time consuming modular exponential computations and scalar multiplications on elliptic curve used in previous related approaches . Additionally the proposed scheme is proven secure in the random oracle model and realizes the lower bounds of messages and rounds in communications . Compared to related verified based approaches the proposed scheme not only possesses higher security but also has lower computational cost and fewer transmissions . 
",We develop an efficient and secure verifier based three party authentication scheme. Our scheme is based on extended chaotic maps and suitable for data exchange in TMIS. Our scheme proven secure in the random oracle model. Our scheme avoids time consuming modular exponents and scalar multiplications on ECC. Our scheme has lower computational cost fewer transmissions and higher security.,,,
S0169260714002946," In this study we developed an integrated hospital associated urinary tract infection surveillance information system based on existing electronic medical records systems for improving the work efficiency of infection control professionals in a 730 bed tertiary care teaching hospital in Taiwan . The iHAUTISIS can automatically collect data relevant to HAUTI surveillance from the different EMR systems and provides a visualization dashboard that helps ICPs make better surveillance plans and facilitates their surveillance work . In order to measure the system performance we also created a generic model for comparing the ICPs work efficiency when using existing electronic culture based surveillance information system and iHAUTISIS respectively . This model can demonstrate a patient s state and corresponding time spent on surveillance tasks performed by ICPs for the patient in that state . The study results showed that the iHAUTISIS performed better than the eCBSIS in terms of ICPs time cost . It reduced the time by 73.27s when using iHAUTISIS and eCBSIS for each patient on average . With increased adoption of EMR systems the development of the integrated HAI surveillance information systems would be more and more cost effective . Moreover the iHAUTISIS adopted web based technology that enables ICPs to online access patient s surveillance information using laptops or mobile devices . Therefore our system can further facilitate the HAI surveillance and reduce ICPs surveillance workloads . 
",We develop an integrated surveillance system called iHAUTISIS for improving the work efficiency of infection control. The iHAUTISIS can automatically collect surveillance data from EMRs and detect infected cases. Compare to the existing surveillance system the iHAUTISIS can reduce 39 of an ICP s surveillance time for each patient.,,,
S0169260714003186," The main goal of this study was to numerically quantify risk of duodenal stump blowout after Billroth II gastric resection . Our hypothesis was that the geometry of the reconstructed tract after BII resection is one of the key factors that can lead to duodenal dehiscence . We used computational fluid dynamics with finite element simulations of various models of BII reconstructed gastrointestinal tract as well as non perfused ex vivo porcine experimental models . As main geometrical parameters for FE postoperative models we have used duodenal stump length and inclination between gastric remnant and duodenal stump . Virtual gastric resection was performed on each of 3D FE models based on multislice Computer Tomography DICOM . According to our computer simulation the difference between maximal duodenal stump pressures for models with most and least preferable geometry of reconstructed GI tract is about 30 . We compared the resulting postoperative duodenal pressure from computer simulations with duodenal stump dehiscence pressure from the experiment . Pressure at duodenal stump after BII resection obtained by computer simulation is 4 5 times lower than the dehiscence pressure according to our experiment on isolated bowel segment . Our conclusion is that if the surgery is performed technically correct geometry variations of the reconstructed GI tract by themselves are not sufficient to cause duodenal stump blowout . Pressure that develops in the duodenal stump after BII resection using omega loop only in the conjunction with other risk factors can cause duodenal dehiscence . Increased duodenal pressure after BII resection is risk factor . Hence we recommend the routine use of Roux en Y anastomosis as a safer solution in terms of resulting intraluminal pressure . However if the surgeon decides to perform BII reconstruction results obtained with this methodology can be valuable . 
",The main goal of this study was to numerically quantify risk of duodenal stump blowout after Billroth II BII gastric resection. Our hypothesis was that the geometry of the reconstructed tract after BII resection is one of the key factors that can cause increased duodenal pressure and consequently duodenal dehiscence. If surgeon decide to perform BII type of reconstruction findings obtained by this preoperative simulation are very useful in terms of minimizing the risk of duodenal dehiscence.,,,
S0167839615001387," This survey paper describes the role of splines in geometry and topology emphasizing both similarities and differences from the classical treatment of splines . The exposition is non technical and contains many examples with references to more thorough treatments of the subject . The goal of this survey paper is to describe how splines arise in geometry and topology . Geometric splines usually appear under the name GKM theory after Goresky Kottwitz MacPherson who developed them to compute what is called the cohomology ring of a geometric object . Geometers and analysts ask many of the same questions about splines what is their dimension can we identify a basis can we find explicit formulas for the elements of the basis However geometric constraints can change the tone of these questions the splines may satisfy various symmetries or have a basis satisfying certain conditions . And some questions are specific to geometric splines geometers particularly care about the multiplication table with respect to a given basis . 
",Many varieties X have a graph that describes their essential geometric features. The splines  form the equivariant cohomology ring and encode key data about X. We describe geometric tools to analyze splines that differ from classical tools. We give an algebraic and combinatorial generalization called generalized splines.,,,
S0167931713005078," The influence of cathode agitation on the residual stress of electroplated gold has been investigated . Using a custom built plating cell a periodic reciprocating motion was applied to silicon substrates that were electroplated with soft gold . A commercially available gold sulfite solution was used to deposit the 0.6 m thick gold films using a current density of 3.0mA cm2 and a bath temperature of 50 C. By increasing the speed of cathode agitation from 0 to 5cm s the magnitude of the compressive stress decreased from 64 to 9MPa . The results suggest that cathode agitation significantly alters the mass transport within the electrolytic cell and can be used as a method of stress control in gold electroplating . This finding is potentially significant for plating applications in microelectronics and microsystems that require precise stress control . 
",We measure experimentally cathode agitation versus stress in electroplated gold. The stress for gold on silicon substrates is measured using microcantilevers. We demonstrate the feasibility of cathode agitation in gold film stress control. Cathode agitation reduced the compressive stress in the gold films.,,,
S0167931716301770," High aspect ratio GaN based nanostructures are of interest for advanced photonic crystal and core shell devices. Nanostructures grown by a bottom up approach are limited in terms of doping geometry and shape which narrow their potential application areas. In contrast high uniformity and a greater diversity of shape and design can be produced via a top down etching approach. However a detailed understanding of the role of etch process parameters is lacking for creating high aspect ratio nanorods and nanopores. Here we report a systematic analysis on the role of temperature and pressure on the fabrication of nanorod and nanopore arrays in GaN. Our results show a threshold in the etch behaviour at a temperature of 125 C which greatly enhances the verticality of the GaN nanorods whilst the modification of the pressure enables a fine tuning of the nanorod profile. For nanopores we show that the use of higher temperatures at higher pressures enables the fabrication of nanopores with an undercut profile. Such a profile is important for controlling the optical field in photonic crystal based devices. Therefore we expect the ability to create such nanostructures to form the foundation for new advanced LED designs. 
",Impact of the temperature and the pressure on nanorods profile Identification of critical temperature of 125 C for etching vertical nanorods Discussion on the impact of the temperature pressure on the etching mechanisms in nanorod and nanopore arrays Identification and control of the key parameters needed to control the nanostructure profiles for the fabrication of advanced photonic crystal devices.,,,
S0169260715001601," Taiwan is an area where chronic hepatitis is endemic . Liver cancer is so common that it has been ranked first among cancer mortality rates since the early 1980s in Taiwan . Besides liver cirrhosis and chronic liver diseases are the sixth or seventh in the causes of death . Therefore as shown by the active research on hepatitis it is not only a health threat but also a huge medical cost for the government . The estimated total number of hepatitis B carriers in the general population aged more than 20 years old is 3 067 307 . Thus a case record review was conducted from all patients with diagnosis of acute hepatitis admitted to the Emergency Department of a well known teaching oriented hospital in Taipei . The cost of medical resource utilization is defined as the total medical fee . In this study a fuzzy neural network is employed to develop the cost forecasting model . A total of 110 patients met the inclusion criteria . The computational results indicate that the FNN model can provide more accurate forecasts than the support vector regression or artificial neural network . In addition unlike SVR and ANN FNN can also provide fuzzy IF THEN rules for interpretation . 
",FNN is employed to develop the cost forecasting model for acute hepatitis patients in emergency room. The FNN model can provide more accurate forecast than SVR and ANN. Unlike SVR and ANN FNN can also provide fuzzy IF THEN rules for interpretation.,,,
S0169260715000231," Background Chronic hypoxemia has deleterious effects on psychomotor function that can affect daily life . There are no clear results regarding short term therapy with low concentrations of O2 in hypoxemic patients . We seek to demonstrate by measuring the characteristics of drawing these effects on psychomotor function of hypoxemic patients treated with O2 . Methods Eight patients M F age 69.5 yr mean with hypoxemia mmHg performed two drawings of pictures . Tests were performed before and after 30min breathing with O2 . Results Stroke velocity increased after O2 for the house drawing mm s basal 30.9 mm s with O2 mean p 0.025 Wilcoxon test . The drawing time down or fraction time the pen is touching the paper during the drawing phase decreased s basal 17.4 s with O2 p 0.017 Wilcoxon test . Conclusions This study shows that in patients with chronic hypoxemia a short period of oxygen therapy produces changes in psychomotor function that can be measured by means of drawing analysis . 
",First paper devoted to analyze oxygen therapy effects measured by handwritten tasks. This study shows that in patients with chronic hypoxemia a short period of oxygen therapy produces changes in psychomotor function. We show that this changes can be measured by means of drawing analysis.,,,
S0169260715001807," Breast cancer is the most deadly disease affecting women and thus it is natural for women aged 40 49 years to assess their personal risk for developing familial breast cancer . Besides as each individual woman possesses different levels of risk of developing breast cancer depending on their family history genetic predispositions and personal medical history individualized care setting mechanism needs to be identified so that appropriate risk assessment counseling screening and prevention options can be determined by the health care professionals . The presented work aims at developing a soft computing based medical decision support system using Fuzzy Cognitive Map that assists health care professionals in deciding the individualized care setting mechanisms based on the FBC risk level of the given women . The FCM based FBC risk management system uses NHL to learn causal weights from 40 patient records and achieves a 95 diagnostic accuracy . The results obtained from the proposed model are in concurrence with the comprehensive risk evaluation tool based on Tyrer Cuzick model for 38 40 patient cases . Besides the proposed model identifies high risk women by calculating higher accuracy of prediction than the standard Gail and NSAPB models . The testing accuracy of the proposed model using 10 fold cross validation technique outperforms other standard machine learning based inference engines as well as previous FCM based risk prediction methods for BC . 
",Development of a familial breast cancer risk assessment model using Fuzzy Cognitive Map FCM methodology. The proposed methodology concentrates its efforts on personalized individual decision making including the family history and the demographic risk factors to elicit the hidden risk of developing breast cancer. Hebbian based learning capabilities of FCM were used to improve the modeling issue and contribute to risk prediction. The accuracy of the system is compared with benchmark machine learning methods showing its superiority.,,,
S0167947314003570," A mixture of latent trait models with common slope parameters for model based clustering of high dimensional binary data a data type for which few established methods exist is proposed . Recent work on clustering of binary data based on a dimensional Gaussian latent variable is extended by incorporating common factor analyzers . Accordingly this approach facilitates a low dimensional visual representation of the clusters . The model is further extended by the incorporation of random block effects . The dependencies in each block are taken into account through block specific parameters that are considered to be random variables . A variational approximation to the likelihood is exploited to derive a fast algorithm for determining the model parameters . Real and simulated data are used to demonstrate this approach . 
",A method for clustering high dimensional binary data. The model is extended to include random block effects for repeatedly sampled data. A variational EM algorithm is developed for parameter estimation.,,,
S0167839616300139," We propose an automatic method for fast reconstruction of indoor scenes from raw point scans which is a fairly challenging problem due to the restricted accessibility and the cluttered space for indoor environment . We first detect and remove points representing the ground walls and ceiling from the input data and cluster the remaining points into different groups referred to as sub scenes . Our approach abstracts the sub scenes with geometric primitives and accordingly constructs the topology graphs with structural attributes based on the functional parts of objects . To decompose sub scenes into individual indoor objects we devise an anchor guided subgraph matching algorithm which leverages template graphs to partition the graphs into subgraphs which is capable of handling arbitrarily oriented objects within scenes . Subsequently we present a data driven approach to model individual objects which is particularly formulated as a model instance recognition problem . A Randomized Decision Forest is introduced to achieve robust recognition on decomposed indoor objects with raw point data . We further exploit template fitting to generate the geometrically faithful model to the input indoor scene . We visually and quantitatively evaluate the performance of our framework on a variety of synthetic and raw scans which comprehensively demonstrates the efficiency and robustness of our reconstruction method on raw scanned point clouds even in the presence of noise and heavy occlusions . 
",We propose a functional part guided modeling method for cluttered indoor scenes. We design an anchor guided graph matching algorithm for scene decomposition. We devise a data driven approach for object modeling based on RDF.,,,
S0169260715001509," Background and objectives Rule based classification is a typical data mining task that is being used in several medical diagnosis and decision support systems . The rules stored in the rule base have an impact on classification efficiency . Rule sets that are extracted with data mining tools and techniques are optimized using heuristic or meta heuristic approaches in order to improve the quality of the rule base . In this work a meta heuristic approach called Wind driven Swarm Optimization is used . The uniqueness of this work lies in the biological inspiration that underlies the algorithm . Methods WSO uses Jval a new metric to evaluate the efficiency of a rule based classifier . Rules are extracted from decision trees . WSO is used to obtain different permutations and combinations of rules whereby the optimal ruleset that satisfies the requirement of the developer is used for predicting the test data . The performance of various extensions of decision trees namely RIPPER PART FURIA and Decision Tables are analyzed . The efficiency of WSO is also compared with the traditional Particle Swarm Optimization . Results Experiments were carried out with six benchmark medical datasets . The traditional C4.5 algorithm yields 62.89 accuracy with 43 rules for liver disorders dataset where as WSO yields 64.60 with 19 rules . For Heart disease dataset C4.5 is 68.64 accurate with 98 rules where as WSO is 77.8 accurate with 34 rules . The normalized standard deviation for accuracy of PSO and WSO are 0.5921 and 0.5846 respectively . Conclusion WSO provides accurate and concise rulesets . PSO yields results similar to that of WSO but the novelty of WSO lies in its biological motivation and it is customization for rule base optimization . The trade off between the prediction accuracy and the size of the rule base is optimized during the design and development of rule based clinical decision support system . The efficiency of a decision support system relies on the content of the rule base and classification accuracy . 
",Wind driven Swarm Optimization WSO is used for optimizing the rule base of a Clinical Decision Support System. WSO is a bio inspired approach inspired from the flight of birds. WSO uses Jval to evaluate the efficiency of a rule based CDSS. Experiments with six medical datasets show that WSO provides efficient rulesets. The novelty of WSO lies in its biological motivation and customization for rule base optimization.,,,
S0169260714002569," Patients who suffer from chronic renal failure tend to suffer from an associated anemia as well . Therefore it is essential to know the hemoglobin levels in these patients . The aim of this paper is to predict the hemoglobin value using a database of European hemodialysis patients provided by Fresenius Medical Care for improving the treatment of this kind of patients . For the prediction of Hb both analytical measurements and medication dosage of patients suffering from chronic renal failure are used . Two kinds of models were trained global and local models . In the case of local models clustering techniques based on hierarchical approaches and the adaptive resonance theory were used as a first step and then a different predictor was used for each obtained cluster . Different global models have been applied to the dataset such as Linear Models Artificial Neural Networks Support Vector Machines and Regression Trees among others . Also a relevance analysis has been carried out for each predictor model thus finding those features that are most relevant for the given prediction . 
",Different prediction algorithms were used to predict Hb levels in CRF patients. Prediction errors in the validation cohorts of patients were around 0.6g dl. Difficulty to obtain lower errors due to the measuring machine precision 0.2g dl . Relevance analysis of features have been applied for each predictor.,,,
S0167947316300500," We propose Bayesian shrinkage methods for coefficient estimation for high dimensional vector autoregressive models using scale mixtures of multivariate normal distributions for independently sampled additive noises . We also suggest an efficient selection procedure for the shrinkage parameter as a computationally feasible alternative to the traditional MCMC sampling methods for high dimensional data . A shrinkage parameter is selected at the minimum point of a newly proposed score function which is asymptotically equivalent to the mean squared error of the model coefficients . The selected shrinkage parameter is presented in a closed form as a function of sample size level of noise and non normality in data and it can be efficiently estimated by using a suggested variation of cross validation . Consistency of both of the cross validation estimator and proposed shrinkage estimator is proved . The competitiveness of the proposed methods is demonstrated based on comprehensive experimental results using simulated data and high dimensional plant gene expression data in the context of coefficient estimation and structural inference for VAR models . The proposed methods are applicable to high dimensional stationary time series with or without near unit roots . 
",The proposed methods are useful for high dimensional VAR models with small data. A formula for an optimal shrinkage parameter is derived under a Bayes framework. The proposed methods are superior in computation time and performance.,,,
S0167923614002498," The huge amount of textual data on the Web has grown in the last few years rapidly creating unique contents of massive dimension . In a decision making context one of the most relevant tasks is polarity classification of a text source which is usually performed through supervised learning methods . Most of the existing approaches select the best classification model leading to over confident decisions that do not take into account the inherent uncertainty of the natural language . In this paper we pursue the paradigm of ensemble learning to reduce the noise sensitivity related to language ambiguity and therefore to provide a more accurate prediction of polarity . The proposed ensemble method is based on Bayesian Model Averaging where both uncertainty and reliability of each single model are taken into account . We address the classifier selection problem by proposing a greedy approach that evaluates the contribution of each model with respect to the ensemble . Experimental results on gold standard datasets show that the proposed approach outperforms both traditional classification and ensemble methods . 
",A novel ensemble learning methodology is proposed for polarity classification task. A selection strategy is studied to reduce the search space of candidate ensembles. The proposed model has been shown to be effective and efficient in several domains.,,,
S0167923616300197," Deception is an inevitable component of human interaction . Researchers and practitioners are developing information systems to aid in the detection of deceptive communication . Information systems are typically adopted by end users to aid in completing a goal or objective . However end user interactions with deception detection systems are unique because the goals of the system and the user are orthogonal . Prior work investigating systems based deception detection has focused on the identification of reliable deception indicators . This research extends extant work by looking at how users of deception detection systems alter their behavior in response to the presence of guilty knowledge relevant stimuli and system knowledge . An analysis of data collected during two laboratory experiments reveals that guilty knowledge relevant stimuli and system knowledge all lead to increased use of countermeasures . The implications and limitations of this research are discussed and avenues for future research are outlined . 
",We present adversarial systems as a novel growing area of IS research. Knowledge of a deception system s operations increases countermeasure use. Presenting deceivers with relevant stimuli increases countermeasure use. Truth tellers use countermeasures when aware of the system s functionality. An extensive set of novel countermeasures is identified.,,,
S0167947314000929," A methodology based on adaptive likelihood ratios for the detection of emerging disease clusters is presented . The martingale structure of the regular likelihood ratio is preserved by the ALR . The upper limit for the false alarm rate of the proposed method depends only on the quantity of evaluated cluster candidates . Thus Monte Carlo simulations are not required to validate the procedures statistical significance allowing the construction of a fast computational algorithm to detect clusters . The number of evaluated clusters is also significantly reduced through the use of an adaptive approach to prune many unpromising clusters . This further increases the computational speed . Performance is evaluated through simulations to measure the average detection delay and the probability of correct cluster detection . We present applications for thyroid cancer in New Mexico and hanseniasis in children in the Brazilian Amazon . 
",Fast computational algorithm to detect space time clusters. Methodology based on adaptive likelihood ratio. Utilizes an adaptive estimator in the configuration space of spatial clusters. The method has a low delay to detect space time clusters. Allows flexibility in the cluster shape.,,,
S0168169915000575," Detailed and timely information on crop area production and yield is important for the assessment of environmental impacts of agriculture for the monitoring of the land use and management practices and for food security early warning systems . A machine learning approach is proposed to model crop rotations which can predict with good accuracy at the beginning of the agricultural season the crops most likely to be present in a given field using the crop sequence of the previous 3 5years . The approach is able to learn from data and to integrate expert knowledge represented as first order logic rules . Its accuracy is assessed using the French Land Parcel Information System implemented in the frame of the EU s Common Agricultural Policy . This assessment is done using different settings in terms of temporal depth and spatial generalization coverage . The obtained results show that the proposed approach is able to predict the crop type of each field before the beginning of the crop season with an accuracy as high as 60 which is better than the results obtained with current approaches based on remote sensing imagery . 
",A prediction model for crop rotations is proposed. The model uses machine learning techniques applied to historic data. It allows the introduction of expert knowledge without re learning from data. The model is assessed on real data over several years and a large area.,,,
S0169260715000565," Objective Stroke is a prominent life threatening disease in the world . The current study was performed to predict the outcome of stroke using knowledge discovery process methods artificial neural networks and support vector machine models . Materials and methods The records of 297 individuals were acquired from the databases of the department of emergency medicine . Nine predictors the levels of cholesterol and C reactive protein were used for predicting the stroke . Feature selection based on the Cramer s V test was carried out for reducing the predictors . Multilayer perceptron ANN and SVM with radial basis function kernel were used for the prediction based on the selected predictors . Results The accuracy values were 81.82 for ANN and 80.38 for SVM in the training dataset and 85.9 for ANN and 84.62 for SVM in the testing dataset respectively . ANN and SVM models yielded area under curve values of 0.905 and 0.899 in the training dataset and 0.928 and 0.91 in the testing dataset consecutively . Conclusion The findings of the current study pointed out that ANN had more predictive performance when compared with SVM in predicting stroke . The proposed ANN model would be useful when making clinical decisions regarding stroke . 
",We predicted the outcome of stroke using knowledge discovery process KDP methods Artificial Neural Networks ANN and Support Vector Machine SVM models. The performance of the models assessed as accuracy and area under curve AUC for predicting stroke was 85.9 and 0.928 for ANN and 84.62 and 0.91 for SVM respectively. KDP was used for providing better understanding of the stroke data and reducing unnecessary processes to be performed. The proposed ANN model predicted successfully the stroke disease based on the selected predictors.,,,
S0167931713002487," High dielectric gate stacks comprising HfO2 were fabricated on Ge with alumina as the barrier level . This was achieved by thermal annealing in an ultra high vacuum to remove the native oxide followed by deposition of aluminium by molecular beam epitaxy . After in situ oxidation at ambient temperature HfO2 was deposited by atomic layer deposition . The devices underwent physical and electrical characterisation and show low EOT down to 1.3nm low leakage current of less than 10 7 Acm 2 at 1V and CV hysteresis of 10mV . 
",Ge gate MOS devices were investigated due to its applicability to sub 16nm node pMOST technology. GeO2 Al2O3 HfO2 gate stacks were grown by a combined MBE and ALD technique. Devices were characterised by various physical and electrical methods. Devices were found to have low EOT and leakage currents and have small hysteresis.,,,
S0167947315002716," Testing whether two or more independent samples arise from a common distribution is a classic problem in statistics . Several multivariate two sample tests of equality are based on graphs such as the minimum spanning tree nearest neighbor and optimal nonbipartite perfect matching . Here the samples are pooled and the test statistic is the number of edges in the graph that connect points with different sample identities . These tests are typically unbiased and perform well when estimates of underlying probability densities are poor . However these tests have not been thoroughly studied when data is very high dimensional or in the multisample case . We introduce the use of orthogonal perfect matchings for testing equality in distribution . A suite of Monte Carlo simulations on artificial and real data shows that orthogonal perfect matchings and spanning trees typically have higher power than other graphs and are also more effective at discerning when samples have differences in their covariance structure compared to other nonparametric tests such as the energy and triangle tests . 
",We test whether two or more samples have arisen from the same multivariate density. Test uses new graphs as well as the minimum spanning tree or nearest neighbors. Test performs well and is easy to perform for large small datasets. Power of the new tests is competitive or beats other general multivariate tests. Mean and variance of the asymptotically normal null distribution are easy to compute.,,,
S0169260714002910," We develop an autonomous system to detect and evaluate physical therapy exercises using wearable motion sensors . We propose the multi template multi match dynamic time warping algorithm as a natural extension of DTW to detect multiple occurrences of more than one exercise type in the recording of a physical therapy session . While allowing some distortion in time the algorithm provides a quantitative measure of similarity between an exercise execution and previously recorded templates based on DTW distance . It can detect and classify the exercise types and count and evaluate the exercises as correctly incorrectly performed identifying the error type if any . To evaluate the algorithm s performance we record a data set consisting of one reference template and 10 test executions of three execution types of eight exercises performed by five subjects . We thus record a total of 120 and 1200 exercise executions in the reference and test sets respectively . The test sequences also contain idle time intervals . The accuracy of the proposed algorithm is 93.46 for exercise classification only and 88.65 for simultaneous exercise and execution type classification . The algorithm misses 8.58 of the exercise executions and demonstrates a false alarm rate of 4.91 caused by some idle time intervals being incorrectly recognized as exercise executions . To test the robustness of the system to unknown exercises we employ leave one exercise out cross validation . This results in a false alarm rate lower than 1 demonstrating the robustness of the system to unknown movements . The proposed system can be used for assessing the effectiveness of a physical therapy session and for providing feedback to the patient . 
",We use five wearable motion sensor units on the arms or legs. We acquire a physical therapy data set with 1320 exercise executions. We develop the MTMM DTW algorithm for multiple exercise templates. The algorithm can detect classify and evaluate physical therapy exercises. We achieve 93.5 accuracy for exercise classification.,,,
S0168169913002512," Site selection for companies is a complex and unstructured problem that must be analyzed carefully and properly since a localization error could drive to bankrupt . This problem has been discussed widely and effectively using multi attribute methods in a manufacturing context but it has been little studied in agribusiness . The goal of this work is a methodological approach oriented to evaluate optimal locations of new agri food warehouses . Furthermore a literature review is developed analyzing the location problem and the attributes and techniques most widely used applied to agribusiness and a case study is presented in order to exemplify the methodological proposal . The multi attribute technique called Analytic Hierarchy Process has been selected as the basis for the research and it is applied to the real case study analyzed the selection of a site for a new banana distribution warehouse . Six generic criteria have been analyzed accessibility to the area distance cost security of the region local acceptance of the company and its needs . The process includes the assignment of attributes to each one of the generic criteria as well as the assessment of their importance levels . Three different areas of Guadalajara Jalisco and Mexico DF have been evaluated for the case study and the methodological proposal has been utilized to determine the best option . 
",Methodological multi criteria attribute approach for agri food warehouses location. Analysis of criteria and attributes assignment and assessment of importance levels. Development of the agribusiness location model based on Analytic Hierarchy Process. A real case study for a banana distribution warehouse exemplifies the methodology.,,,
S0169260714003885," We present a new SAS macro pshreg that can be used to fit a proportional subdistribution hazards model for survival data subject to competing risks . Our macro first modifies the input data set appropriately and then applies SAS s standard Cox regression procedure PROC PHREG using weights and counting process style of specifying survival times to the modified data set . The modified data set can also be used to estimate cumulative incidence curves for the event of interest . The application of PROC PHREG has several advantages e.g . it directly enables the user to apply the Firth correction which has been proposed as a solution to the problem of undefined maximum likelihood estimates in Cox regression frequently encountered in small sample analyses . Deviation from proportional subdistribution hazards can be detected by both inspecting Schoenfeld type residuals and testing correlation of these residuals with time or by including interactions of covariates with functions of time . We illustrate application of these extended methods for competing risk regression using our macro which is freely available at http cemsiis.meduniwien.ac.at en kb science research software statistical software pshreg by means of analysis of a real chronic kidney disease study . We discuss differences in features and capabilities of pshreg and the recent SAS PROC PHREG implementation of proportional subdistribution hazards modelling . 
",The pshreg SAS macro fits Fine Gray models for competing risks. The macro first modifies a given data set and then uses PROC PHREG for analysis. Many useful features of PROC PHREG can now be applied to a Fine Gray model. Time dependent effects can be accommodated by time by covariate interactions. For small data sets the Firth correction is available.,,,
S0169260715000607," The paper presents a computer based assessment for facioscapulohumeral dystrophy diagnosis through characterisation of the fat and oedema percentages in the muscle region . A novel multi slice method for the muscle region segmentation in the T1 weighted magnetic resonance images is proposed using principles of the live wire technique to find the path representing the muscle region border . For this purpose an exponential cost function is used that incorporates the edge information obtained after applying the edge enhancement algorithm formerly designed for the fingerprint enhancement . The difference between the automatic segmentation and manual segmentation performed by a medical specialists is characterised using the Zijdenbos similarity index indicating a high accuracy of the proposed method . Finally the fat and oedema are quantified from the muscle region in the T1 weighted and T2 STIR magnetic resonance images respectively using the fuzzy c mean clustering approach for 10 FSHD patients . 
",A novel method for lower limb muscle segmentation is proposed. The proposed method is insensitive to the muscle fat infiltration and shape variation. Segmentation is based on the shortest path detection between anatomical structures. The proposed method does not require training set.,,,
S0167947314002382," This paper presents a new efficient and robust smooth threshold generalized estimating equations for generalized linear models with longitudinal data . The proposed method is based on a bounded exponential score function and leverage based weights to achieve robustness against outliers both in the response and the covariate domain . Our motivation for the new variable selection procedure is that it enables us to achieve better robustness and efficiency by introducing an additional tuning parameter which can be automatically selected using the observed data . Moreover its performance is near optimal and superior to some recently developed variable selection methods . Under some regularity conditions the resulting estimator possesses the consistency in variable selection and the oracle property in estimation . Finally simulation studies and a detailed real data analysis are carried out to assess and illustrate the finite sample performance which show that the proposed method works better than other existing methods in particular when many outliers are included . 
",We develop a new efficient and robust variable selection approach for generalized linear models with longitudinal data. The root consistency and asymptotic normality of the proposed estimators are established. An efficient algorithm is proposed to implement the procedures. Simulation studies and a real data example have shown that our proposed estimators are superior to some recently developed variable selection methods.,,,
S0167931715002920," The impact of subjecting a n GaN surface to an in situ argon plasma in an atomic layer deposition tool immediately before deposition of an Al2O3 dielectric film is assessed by frequency dependent evaluation of Al2O3 GaN MOSCAPs . In comparison with a control with no pre treatment the use of a 50W argon plasma for 5min reduced hysteresis from 0.25V to 0.07V frequency dispersion from 0.31V to 0.03V and minimum interface state density as determined by the conductance method from 6.8 1012 cm 2 eV 1 to 5.05 1010 cm 2 eV 1 . 
",Argon in situ plasma treatment before atomic layer deposition of Al2O3 on GaN. Post forming gas annealing process in Al2O3 on GaN. Minimum interface trap density Dit of 5.05 1010 cm 2 eV 1 at 50W argon pre treatment. Approach to GaN based MOS HEMT performance for power electronics applications.,,,
S0167931716301241,"A novel negative tone molecular resist molecule featuring a tert butyloxycarbonyl protected phenol malonate group bonded to a 1 8 Diazabicycloundece 7 ene is presented. The resist shows high resolution capability in electron beam lithography at a range of beam energies. The resist demonstrated a sensitivity of 18.7 C cm2 at 20 kV. Dense features with a line width of 15 nm have been demonstrated at 30 kV whilst a feature size of 12.5 nm was achieved for dense lines at 100 kV. 
",A novel molecular chemically amplified negative tone resist is presented. High resolution features have been patterned at high sensitivity. The performance of the resist at 30 50 and 100kV is evaluated.,,,
S0167947313001047," It is often necessary to test for the presence of seasonal unit roots when working with time series data observed at intervals of less than a year . One of the most widely used methods for doing this is based on regressing the seasonal difference of the series over the transformations of the series by applying specific filters for each seasonal frequency . This provides test statistics with non standard distributions . A generalisation of this method for any periodicity is presented and a response surface regressions approach is used to calculate the values of the statistics whatever the periodicity and sample size of the data . The algorithms are prepared with the Gretl open source econometrics package and two empirical examples are presented . 
",A generalisation of HEGY seasonal unit root tests was presented in Smith Taylor and del Barrio Castro 2009 . We use a response surface regressions approach to calculate values for the HEGY statistics. They can be used for any seasonal periodicity sample size and autoregressive order. A Gretl function package is provided for applying the tests and calculating their values.,,,
S0169260714003861," The Monte Carlo method for photon transport is often used to predict the volumetric heating that an optical source will induce inside a tissue or material . This method relies on constant optical properties specifically the coefficients of scattering and absorption . In reality optical coefficients are typically temperature dependent leading to error in simulation results . The purpose of this study is to develop a method that can incorporate variable properties and accurately simulate systems where the temperature will greatly vary such as in the case of laser thawing of frozen tissues . A numerical simulation was developed that utilizes the Monte Carlo method for photon transport to simulate the thermal response of a system that allows temperature dependent optical and thermal properties . This was done by combining traditional Monte Carlo photon transport with a heat transfer simulation to provide a feedback loop that selects local properties based on current temperatures for each moment in time . Additionally photon steps are segmented to accurately obtain path lengths within a homogenous material . Validation of the simulation was done using comparisons to established Monte Carlo simulations using constant properties and a comparison to the Beer Lambert law for temperature variable properties . The simulation is able to accurately predict the thermal response of a system whose properties can vary with temperature . The difference in results between variable property and constant property methods for the representative system of laser heated silicon can become larger than 100K . This simulation will return more accurate results of optical irradiation absorption in a material which undergoes a large change in temperature . This increased accuracy in simulated results leads to better thermal predictions in living tissues and can provide enhanced planning and improved experimental and procedural outcomes . 
",Couples the Monte Carlo Method of photon transport with heat transfer. Allows for both optical and thermal properties to vary with temperature. Interpolates local optical properties based on local temperatures. Enhances simulation accuracy in systems that undergo large thermal changes.,,,
S0169260714002454," Positron emission tomography with 18fluorodeoxyglucose is increasingly used in neurology . The measurement of cerebral arterial inflow using 18F FDG complements the information provided by standard brain PET imaging . Here injections were performed after the beginning of dynamic acquisitions and the time to arrival of activity in the gantry s field of view was computed . We performed a phantom study using a branched tube and a 18F FDG solution injected at 240mL min . Data processing consisted of reconstruction of the first 3s after t0 vascular signal enhancement and clustering . This method was then applied in four subjects . We measured the volumes of the tubes or vascular trees and calculated the corresponding flows . In the phantom the flow was calculated to be 244.2mL min . In each subject our QA value was compared with that obtained by quantitative cine phase contrast magnetic resonance imaging the mean QA value of 581.4 217.5mL min calculated with 18F FDG PET was consistent with the mean value of 593.3 205.8mL min calculated with quantitative cine phase contrast magnetic resonance imaging . Our 18F FDG PET method constitutes a novel fully automatic means of measuring QA . 
",We proposed the measurement of cerebral arterial inflow using 3 s bolus 18F FDG brain PET acquisition. The arrival of the bolus in the field of view was calculated directly in list mode acquisition. Tubular structure enhancement filter was used to enhance vessels and a clustering step eliminated noise outside them.,,,
S0169260715000929," Pharmacokinetics can be a challenging topic to teach due to the complex relationships inherent between physiological parameters mathematical descriptors and equations and their combined impact on shaping the blood fluid concentration vs. time curves of drugs . A computer program was developed within Microsoft Excel for Windows designed to assist in the instruction of basic pharmacokinetics within an entry to practice pharmacy class environment . The program is composed of a series of spreadsheets linked by Visual Basic for Applications intended to illustrate the relationships between pharmacokinetic and in some cases physiological parameters doses and dose rates and the drug blood fluid concentration vs. time curves . Each module is accompanied by a simulation user s guide prompting the user to change specific independent parameters and then observe the impact of the change on the drug concentration vs. time curve and on other dependent parameters . Slider bars can be selected to readily see the effects of repeated changes on the dependencies . Topics covered include one compartment single dose administration intravenous infusion repeated doses renal and hepatic clearance nonlinear elimination two compartment model plasma protein binding and the relationship between pharmacokinetics and drug effect . The program has been used in various forms in the classroom over a number of years with positive ratings generally being received from students for its use in the classroom . 
",A pharmacokinetics simulation teaching program is described. It is written in VBA in Excel for Windows. The program covers a wide variety of basic pharmacokinetic content. Students have responded favorably to its use.,,,
S0169260715002473," Background and objectives The broad adoption of clinical decision support systems within clinical practice has been hampered mainly by the difficulty in expressing domain knowledge and patient data in a unified formalism . This paper presents a semantic based approach to the unified representation of healthcare domain knowledge and patient data for practical clinical decision making applications . Methods A four phase knowledge engineering cycle is implemented to develop a semantic healthcare knowledge base based on an HL7 reference information model including an ontology to model domain knowledge and patient data and an expression repository to encode clinical decision making rules and queries . A semantic clinical decision support system is designed to provide patient specific healthcare recommendations based on the knowledge base and patient data . Results The proposed solution is evaluated in the case study of type 2 diabetes mellitus inpatient management . The knowledge base is successfully instantiated with relevant domain knowledge and testing patient data . Ontology level evaluation confirms model validity . Application level evaluation of diagnostic accuracy reaches a sensitivity of 97.5 a specificity of 100 and a precision of 98 an acceptance rate of 97.3 is given by domain experts for the recommended care plan orders . Conclusions The proposed solution has been successfully validated in the case study as providing clinical decision support at a high accuracy and acceptance rate . The evaluation results demonstrate the technical feasibility and application prospect of our approach . 
",We develop a semantic healthcare knowledge base to support the practical use of CDSS. We propose a unified representation of healthcare domain knowledge and patient data based on HL7 RIM and ontology. We encode semantic rules and queries for data driven and knowledge based inference. We design a semantic CDSS to enable data interoperability and knowledge sharing for patient specific clinical decision support.,,,
S0169260715002230," Background and objective Sperm morphology analysis is an important factor in the diagnosis of human male infertility . This study presents an automatic algorithm for sperm morphology analysis using images of human sperm cells . Methods The SMA method was used to detect and analyze different parts of the human sperm . First of all SMA removes the image noises and enhances the contrast of the image to a great extent . Then it recognizes the different parts of sperm and analyzes the size and shape of each part . Finally the algorithm classifies each sperm as normal or abnormal . Malformations in the head midpiece and tail of a sperm can be detected by the SMA method . In contrast to other similar methods the SMA method can work with low resolution and non stained images . Furthermore an image collection created for the SMA has also been described in this study . This benchmark consists of 1457 sperm images from 235 patients and is known as human sperm morphology analysis dataset . Results The proposed algorithm was tested on HSMA DS . The experimental results show the high ability of SMA to detect morphological deformities from sperm images . In this study the SMA algorithm produced above 90 accuracy in sperm abnormality detection task . Another advantage of the proposed method is its low computation time as such the expert can quickly decide to choose the analyzed sperm or select another one . Conclusions Automatic and fast analysis of human sperm morphology can be useful during intracytoplasmic sperm injection for helping embryologists to select the best sperm in real time . 
",Automatically selection of normal sperms. Working on unstained sperms. Ability to work on low resolution images. High accuracy and real time processing time. Preparing a freely available dataset.,,,
S0169260715002023," This paper proposes an integrated modelling approach for location planning of radiotherapy treatment services based on cancer incidence and road network based accessibility . Previous research efforts have established travel distance time barriers as a key factor affecting access to cancer treatment services as well as epidemiological studies have shown that cancer incidence rates vary with population demography . Our study is built on the evidence that the travel distances to treatment centres and demographic profiles of the accessible regions greatly influence the uptake of cancer radiotherapy services . An integrated service planning approach that combines spatially explicit cancer incidence projections and the placement of new RT services based on road network based accessibility measures have never been attempted . This research presents a novel approach for the location planning of RT services and demonstrates its viability by modelling cancer incidence rates for different age sex groups in New South Wales Australia based on observed cancer incidence trends and estimations of the road network based access to current NSW treatment centres . Using three indices we show how the best location for a new RT centre may be chosen when there are multiple competing locations . 
",Proposes a methodology for location planning of radiotherapy treatment services. The methodology is based on cancer incidence and road network based accessibility measures. Applied to the current radiotherapy treatment centres in New South Wales state of Australia. Current and future regional radiotherapy demand are estimated.,,,
S0169260715000760," In recent years several computational methods have been developed to predict RNA binding sites in protein . Most of these methods do not consider interacting partners of a protein so they predict the same RNA binding sites for a given protein sequence even if the protein binds to different RNAs . Unlike the problem of predicting RNA binding sites in protein the problem of predicting protein binding sites in RNA has received little attention mainly because it is much more difficult and shows a lower accuracy on average . In our previous study we developed a method that predicts protein binding nucleotides from an RNA sequence . In an effort to improve the prediction accuracy and usefulness of the previous method we developed a new method that uses both RNA and protein sequence data . In this study we identified effective features of RNA and protein molecules and developed a new support vector machine model to predict protein binding nucleotides from RNA and protein sequence data . The new model that used both protein and RNA sequence data achieved a sensitivity of 86.5 a specificity of 86.2 a positive predictive value of 72.6 a negative predictive value of 93.8 and Matthews correlation coefficient of 0.69 in a 10 fold cross validation it achieved a sensitivity of 58.8 a specificity of 87.4 a PPV of 65.1 a NPV of 84.2 and MCC of 0.48 in independent testing . For comparative purpose we built another prediction model that used RNA sequence data alone and ran it on the same dataset . In a 10 fold cross validation it achieved a sensitivity of 85.7 a specificity of 80.5 a PPV of 67.7 a NPV of 92.2 and MCC of 0.63 in independent testing it achieved a sensitivity of 67.7 a specificity of 78.8 a PPV of 57.6 a NPV of 85.2 and MCC of 0.45 . In both cross validations and independent testing the new model that used both RNA and protein sequences showed a better performance than the model that used RNA sequence data alone in most performance measures . To the best of our knowledge this is the first sequence based prediction of protein binding nucleotides in RNA which considers the binding partner of RNA . The new model will provide valuable information for designing biochemical experiments to find putative protein binding sites in RNA with unknown structure . 
",A method to predict protein binding sites in RNA from sequence data alone is proposed. Protein binding sites in RNA are predicted with consideration of protein in addition to RNA. This is the first sequence based prediction of protein binding sites in RNA at the nucleotide level. The prediction method achieved an accuracy of 79.2 and MCC of 0.48 in independent testing.,,,
S0167839615001193," We propose a general scheme for detecting critical locations of piecewise polynomial multivariate equation systems . Our approach generalizes previously known methods for locating tangency events or self intersections in contexts such as surface surface intersection problems and the problem of tracing implicit plane curves . Given the algebraic constraints of the original problem we formulate additional constraints seeking locations where the differential matrix of the original problem has a non maximal rank . This makes the method independent of a specific geometric application as well as of dimensionality . Within the framework of subdivision based solvers test results are demonstrated for non linear systems with three and four unknowns . 
",A method for detecting critical points of nonlinear algebraic systems. Detecting isolated critical points. Point cloud approximation for critical varieties of dimension greater than zero.,,,
S0169260715001108," Telemedicine is the medical practice of information exchanged from one location to another through electronic communications to improve the delivery of health care services . This research article describes a telemedicine framework with knowledge engineering using taxonomic reasoning of ontology modeling and semantic similarity . In addition to being a precious support in the procedure of medical decision making this framework can be used to strengthen significant collaborations and traceability that are important for the development of official deployment of telemedicine applications . Adequate mechanisms for information management with traceability of the reasoning process are also essential in the fields of epidemiology and public health . In this paper we enrich the case based reasoning process by taking into account former evidence based knowledge . We use the regular four steps approach and implement an additional step establish diagnosis retrieve treatment apply evidence adaptation retain . Each step is performed using tools from knowledge engineering and information processing . The case representation is done by the taxonomy component of a medical ontology model . The proposed approach is illustrated with an example from the oncology domain . Medical ontology allows a good and efficient modeling of the patient and his treatment . We are pointing up the role of evidences and specialist s opinions in effectiveness and safety of care . 
",Telemedicine framework with structured knowledge to support medical professionals. Enrichment of the case based reasoning process with evidences and expert opinions. Knowledge engineering using reasoning of ontology modeling and semantic similarity. Flexible collaborative model to provide a participative and extensible environment. Application in oncology to improve the reliability and safety of medical services.,,,
S0169260715000450," Gene expression data analysis is based on the assumption that co expressed genes imply co regulated genes . This assumption is being reformulated because the co expression of a group of genes may be the result of an independent activation with respect to the same experimental condition and not due to the same regulatory regime . For this reason traditional techniques are recently being improved with the use of prior biological knowledge from open access repositories together with gene expression data . Biclustering is an unsupervised machine learning technique that searches patterns in gene expression data matrices . A scatter search based biclustering algorithm that integrates biological information is proposed in this paper . In addition to the gene expression data matrix the input of the algorithm is only a direct annotation file that relates each gene to a set of terms from a biological repository where genes are annotated . Two different biological measures FracGO and SimNTO are proposed to integrate this information by means of its addition to be optimized fitness function in the scatter search scheme . The measure FracGO is based on the biological enrichment and SimNTO is based on the overlapping among GO annotations of pairs of genes . Experimental results evaluate the proposed algorithm for two datasets and show the algorithm performs better when biological knowledge is integrated . Moreover the analysis and comparison between the two different biological measures is presented and it is concluded that the differences depend on both the data source and how the annotation file has been built in the case GO is used . It is also shown that the proposed algorithm obtains a greater number of enriched biclusters than other classical biclustering algorithms typically used as benchmark and an analysis of the overlapping among biclusters reveals that the biclusters obtained present a low overlapping . The proposed methodology is a general purpose algorithm which allows the integration of biological information from several sources and can be extended to other biclustering algorithms based on the optimization of a merit function . 
",Biclustering of gene expression data. Integration of biological knowledge. Scatter search.,,,
S0169260715001753," In this paper we propose a robust semi autonomous algorithm for 3D vessel segmentation and tracking based on an active contour model and a Kalman filter . For each computed tomography angiography slice we use the active contour model to segment the vessel boundary and the Kalman filter to track position and shape variations of the vessel boundary between slices . For successful segmentation via active contour we select an adequate number of initial points from the contour of the first slice . The points are set manually by user input for the first slice . For the remaining slices the initial contour position is estimated autonomously based on segmentation results of the previous slice . To obtain refined segmentation results an adaptive control spacing algorithm is introduced into the active contour model . Moreover a block search based initial contour estimation procedure is proposed to ensure that the initial contour of each slice can be near the vessel boundary . Experiments were performed on synthetic and real chest CTA images . Compared with the well known Chan Vese model the proposed algorithm exhibited better performance in segmentation and tracking . In particular receiver operating characteristic analysis on the synthetic and real CTA images demonstrated the time efficiency and tracking robustness of the proposed model . In terms of computational time redundancy processing time can be effectively reduced by approximately 20 . 
",We propose 3D vessel segmentation and tracking algorithm based on an active contour model and a Kalman filter. To obtain refined segmentation results we model an adaptive control spacing algorithm for the active contour model. To obtain refined tracking results we model an initial contour estimation algorithm for the Kalman filter. ROC analyses demonstrated the time efficiency and tracking robustness of the proposed model. Processing time can be effectively reduced by approximately 20 .,,,
S0169260715001613," Background Proteomics the study of proteomes has been increasingly utilized in a wide variety of biological problems . The Two Dimensional Gel Electrophoresis technique is a powerful proteomics technique aiming at separation of the complex protein mixtures . Spot detection and segmentation are fundamental components of 2D gel image analysis but remain arduous and difficult tasks . Several software packages and academic approaches are available for 2D gel image spot detection and segmentation . Each one has its respective advantages and disadvantages and achieves a different level of success in dealing with the challenges of 2D gel image analysis . A common characteristic of the available methods is their dependency on user intervention in order to achieve optimal results a process that can lead to subjective and non reproducible results . In this work the authors propose a novel spot detection and segmentation methodology for 2D gel images . Methods This work introduces a novel spot detection and spot segmentation methodology that is based on a multi thresholding scheme applied on overlapping regions of the image a custom grow cut algorithm a region growing scheme and morphological operators . The performance of the proposed methodology is evaluated on real as well as synthetic 2D gel images using well established statistical measures including precision sensitivity and their weighted measure F measure as well as volumetric overlap volumetric error and volumetric overlap error . Results Experimental results show that the proposed methodology outperforms state of the art software packages and methods proposed in the literature and results in more plausible spot boundaries and more accurate segmentation . The proposed method achieved the highest F measure for spot detection and the lowest volumetric overlap error for the segmentation process . Conclusions Evaluation against state of the art 2D gel image analysis software packages and techniques proposed in the literature including Melanie 7 Delta2D PDQuest and Scimo demonstrates that the proposed approach outperforms the other methods evaluated in this work and constitutes an advantageous and reliable solution for 2D gel image analysis . 
",A novel approach for 2D gel image spot detection and segmentation. The segmentation process includes a grow cut approach with a custom update rule that takes into consideration the inherent characteristics of 2D gel images. Real and synthetic 2D gel images containing a total of more than 20 000 protein spots are used for qualitative and quantitative evaluation. Better detection and segmentation performance than state of the art methods.,,,
S0167839616000030," In this paper we define Tchebycheffian spline spaces over planar T meshes and we address the problem of determining their dimension . We extend to the Tchebycheffian spline context the homological approach previously used to characterize polynomial spline spaces over T meshes and we exploit this characterization in the study of the dimension . In particular we give combinatorial lower and upper bounds for the dimension and we show that these bounds coincide if the dimensions of the underlying extended Tchebycheff section spaces are large enough with respect to the smoothness under some mild conditions on the T mesh . Finally we provide simple examples of Tchebycheffian spline spaces over T meshes with unstable dimension which means that their dimension depends on the exact geometry of the T mesh . These results are extensions of those known in the literature for polynomial spline spaces over T meshes . 
",We define Tchebycheffian spline spaces over planar T meshes and we address the problem of determining their dimension. We give combinatorial lower and upper bounds for the dimension. We show that these bounds coincide under certain conditions on the T mesh and or the Tchebycheffian spline space. We provide simple examples of Tchebycheffian spline spaces over T meshes with unstable dimension. These results are extensions of known results in the literature for polynomial spline spaces over T meshes.,,,
S0167865515002275,"Although studied for decades effective face recognition remains difficult to accomplish on account of occlusions and pose and illumination variations. Pose variance is a particular challenge in face recognition. Effective local descriptors have been proposed for frontal face recognition. When these descriptors are directly applied to cross pose face recognition the performance significantly decreases. To improve the descriptor performance for cross pose face recognition we propose a face recognition algorithm based on multiple virtual views and alignment error. First warps between poses are learned using the Lucas Kanade algorithm. Based on these warps multiple virtual profile views are generated from a single frontal face which enables non frontal faces to be matched using the scale invariant feature transform SIFT algorithm. Furthermore warps indicate the correspondence between patches of two faces. A two phase alignment error is proposed to obtain accurate warps which contain pose alignment and individual alignment. Correlations between patches are considered to calculate the alignment error of two faces. Finally a hybrid similarity between two faces is calculated it combines the number of matched keypoints from SIFT and the alignment error. Experimental results show that our proposed method achieves better recognition accuracy than existing algorithms even when the pose difference angle was greater than 30 . 
",Cross pose face recognition with only a single frontal gallery face is a challenging task. Multiple virtual views are generated from a single frontal gallery face in advance. Alignment error of two faces is calculated after a two phase pose and individual alignment. Correlation between patches is considered by overlapping and covariance. Cross pose face recognition accuracy is significantly improved than compared methods.,,,
S0167931715301155," We present an improved nanofabrication method of high aspect ratio tungsten structures for use in high efficiency nanofocusing hard X ray zone plates. A ZEP 7000 electron beam resist layer used for patterning is cured by a second much larger electron dose after development. The curing step improves pattern transfer fidelity into a chromium hard mask by reactive ion etching using Cl2 O2 chemistry. The pattern can then be transferred into an underlying tungsten layer by another reactive ion etching step using SF6 O2. A 630 nm thick tungsten zone plate with smallest line width of 30 nm was fabricated using this method and characterized. At 8.2 keV photon energy the device showed an efficiency of 2.2 with a focal spot size at the diffraction limit measured at Diamond Light Source I 13 1 beamline. 
",Tungsten structures with 30nm line width 630nm height 21 1 aspect ratio E beam resist curing provides increased pattern transfer fidelity. Diffraction limited zone plate optics performing at 2.2 efficiency at 8.2keV,,,
S0167839615001399," This expository paper exhibits the power and versatility of the Bernstein B zier form of a polynomial and the role that it has played in the analysis of multivariate spline spaces . Several particular applications are discussed in some detail . The purpose of the paper is to provide the reader with a working facility with the Bernstein B zier form . 
",This is a survey of the Bernstein B zier form of a multivariate polynomial.,,,
S0167923614002012," In today s dynamic media landscape products are reviewed by consumers in social media and reported by journalists in traditional media . This paper will focus on the relationship among the two types of earned media and product sales . Previous studies have focused on either traditional or social earned media but rarely both . We will aim to bridge that gap using the following points of analysis the New York Times Best Seller List as traditional media Amazon user reviews as social media and book purchases through Amazon as product sales . We find that both traditional and social earned media influence sales sales have a reciprocal effect on social earned media and traditional and social earned media influence each other . Communication through multiple media is known to produce the synergy effect in which one media activity enhances the effect of another . Our results suggest a new benefit unique to the use of multiple earned media . We call this the multiplier effect which occurs when one earned media activity increases the level of another by becoming a sounding board that amplifies positive messages as well as a bridge that allows messages to propagate freely in an interactive media system . Therefore multiple earned media produce combined sales effects greater than those resulting from the sum of their parts . This analysis supports Amazon s decision to use multiple earned media to benefit from an ecosystem where product sales and earned media both influence and are influenced by one another . The paper will address the implications for marketing communication and media industry . 
",Both social and traditional earned media influence product sales. Product sales have a reciprocal effect on social earned media. Social and traditional earned media influence each other. Multiplier effect makes combined earned media effects larger than sum of the parts. Social and traditional media have a foe and friend relationship in a digital ecosystem.,,,
S0169260715002187," Immunization saves millions of lives against vaccine preventable diseases . Yet 24 million children born every year do not receive proper immunization during their first year . UNICEF and WHO have emphasized the need to strengthen the immunization surveillance and monitoring in developing countries to reduce childhood deaths . In this regard we present a software application called Jeev to track the vaccination coverage of children in rural communities . Jeev synergistically combines the power of smartphones and the ubiquity of cellular infrastructure QR codes and national identification cards . We present the design of Jeev and highlight its unique features along with a detailed evaluation of its performance and power consumption using the National Immunization Survey datasets . We are in discussion with a non profit organization in Haiti to pilot test Jeev in order to study its effectiveness and identify socio cultural issues that may arise in a large scale deployment . 
",Jeev is a novel smartphone application for tracking the vaccination coverage of children. Jeev synergistically combines smartphones QR codes and national id cards. Jeev is scalable and power efficient.,,,
S0169260714002557," The domain of cancer treatment is a promising field for the implementation and evaluation of a protocol based clinical decision support system because of the algorithmic nature of treatment recommendations . However many factors can limit such systems potential to support the decision of clinicians technical challenges related to the interoperability with existing electronic patient records and clinical challenges related to the inherent complexity of the decisions often collectively taken by panels of different specialists . In this paper we evaluate the performances of an Asbru based decision support system implementing treatment protocols for breast cancer which accesses data from an oncological electronic patient record . Focusing on the decision on the adjuvant pharmaceutical treatment for patients affected by early invasive breast cancer we evaluate the matching of the system s recommendations with those issued by the multidisciplinary panel held weekly in a hospital . 
",We design and built a decision support system for the treatment of breast cancer based on Asbru. The DSS is integrated with the electronic patient record in use in a Medical Oncology Unit. We evaluated the DSS in the multidisciplinary meeting for the decision of the adjuvant treatment. The results show an agreement of nearly 90 between the meeting decisions and the DSS recommendations. We discuss the reasons for non agreement and additional insights into the design of effective DSS.,,,
S0169260715002278," Increased heterogeneity of the lung disturbs pulmonary gas exchange . During bronchoconstriction inflammation of lung parenchyma or acute respiratory distress syndrome inhomogeneous lung ventilation can become bimodal and increase the risk of ventilator induced lung injury during mechanical ventilation . A simple index sensitive to ventilation heterogeneity would be very useful in clinical practice . In the case of bimodal ventilation the index can be defined as the ratio between the longer and shorter time constant characterising regions of contrary mechanical properties . These time constants can be derived from the Otis model fitted to input impedance measured using forced oscillations . In this paper we systematically investigated properties of the aforementioned approach . The research included both numerical simulations and real experiments with a dual lung simulator . Firstly a computational model mimicking the physical simulator was derived and then used as a forward model to generate synthetic flow and pressure signals . These data were used to calculate the input impedance and then the Otis inverse model was fitted to Z in by means of the Levenberg Marquardt algorithm . Finally the obtained estimates of model parameters were used to compute H. The analysis of the above procedure was performed in the frame of Monte Carlo simulations . For each selected value of H forward simulations with randomly chosen lung parameters were repeated 1000 times . Resulting signals were superimposed by additive Gaussian noise . The estimated values of H properly indicated the increasing level of simulated inhomogeneity however with underestimation and variation increasing with H. The main factor responsible for the growing estimation bias was the fixed starting vector required by the LM algorithm . Introduction of a correction formula perfectly reduced this systematic error . The experimental results with the dual lung simulator confirmed potential of the proposed procedure to properly deduce the lung heterogeneity level . We conclude that the heterogeneity index H can be used to assess bimodal ventilation imbalances in cases when this phenomenon dominates lung properties however future analyses including the impact of lung tissue viscoelasticity and distributed airway or tissue inhomogeneity on H estimates as well as studies in the time domain are advisable . 
",We analyse the method for bimodal lung ventilation heterogeneity identification. Respiratory impedance measurement by forced oscillations in bimodal model was proposed. In case of the Otis model the index H was defined as the ratio of two time constants. Mean values of heterogeneity were computed both for simulated and experimental data. The knowledge of bimodality may be helpful when using mechanical ventilation.,,,
S0167947315002066," Financial data are often thick tailed and exhibit skewness . The versatile Generalized Tukey Lambda distribution is able to capture varying degrees of skewness in thin or thick tailed data . Such versatility makes the GTL distribution potentially useful in the area of financial risk measurement . Moreover for GTL distributed random variables the familiar risk measures of Value at Risk and Expected Shortfall may be expressed in simple analytical forms . It turns out that both analytically and through Monte Carlo simulations GTL s VaR and ES differ significantly from other flexible distributions . The asymptotic properties of the maximum likelihood estimator of the GTL parameters are also examined . In order to study risk in financial data the GTL distribution is inserted into a GARCH model . This GTL GARCH model is estimated with data on daily returns of GE stock demonstrating that for certain data sets GTL may capture risk measurements better than other distributions . Online supplementary materials consist of appendices with proofs and additional Monte Carlo results data used in this study an R script for fitting GTL densities by maximum likelihood and an R script for estimation of the GTL GARCH model . 
",We derive Value at Risk and expected shortfall for GTL distributed random variables. GTL s VaR and ES statistics differ significantly from other flexible distributions. We derive asymptotic properties of the ML estimator of the GTL parameters. We insert GTL and other flexible distributions in a GARCH model. A GTL GARCH model may fit real data better than GARCH with other distributions.,,,
S0169260715002254," Objective To survey researchers efforts in response to the new and disruptive technology of smartphone medical apps mapping the research landscape form the literature into a coherent taxonomy and finding out basic characteristics of this emerging field represented on motivation of using smartphone apps in medicine and healthcare open challenges that hinder the utility and the recommendations to improve the acceptance and use of medical apps in the literature . Methods We performed a focused search for every article on smartphone medical or health related app in four major databases MEDLINE Web of Science ScienceDirect and IEEE Xplore . Those databases are deemed broad enough to cover both medical and technical literature . Results The final set included 133 articles . Most articles are reviews and surveys that refer to actual apps or the literature to describe medical apps for a specific specialty disease or purpose or to provide a general overview of the technology . Another group carried various studies from evaluation of apps to exploration of desired features when developing them . Few researchers presented actual attempts to develop medical apps or shared their experiences in doing so . The smallest portion proposed general frameworks addressing the production or operation of apps . Discussion Since 2010 researchers followed the trend of medical apps in several ways though leaving areas or aspect for further attention . Regardless of their category articles focus on the challenges that hinder the full utility of medical apps and do recommend mitigations to them . Conclusions Research on smartphone medical apps is active and various . We hope that this survey contribute to the understanding of the available options and gaps for other researchers to join this line of research . 
",Mapping the research landscape of smartphone medical apps into a coherent taxonomy. Figure out the motivation of using smartphone apps in medicine healthcare. Highlight the open challenges that hinder the utility of medical apps. Recommendations Lists to improve the acceptance of medical apps in the literature.,,,
S0167839615001351," We derive explicit formulas for the bases of conic sections and planar rational cubic curves . Using the bases for planar rational cubic curves we find explicit formulas for their implicit equations and double points . We also extend the explicit formula for the bases of conic sections to bases for rational curves of degree n in n dimensions . 
",We provide explicit formulas for the bases of conic sections and planar rational cubic curves. We derive explicit formulas for the implicit equations and double points of planar rational cubic curves from the bases of planar rational cubic curves. We also give explicit formulas for the bases of rational curves of degree n in n dimensions.,,,
S0169260715002643," Bone drilling is a common procedure in many types of surgeries including orthopedic neurological and otologic surgeries . Several technologies and control algorithms have been developed to help the surgeon automatically stop the drill before it goes through the boundary of the tissue being drilled . However most of them rely on thrust force and cutting torque to detect bone layer transitions which has many drawbacks that affect the reliability of the process . This paper describes in detail a bone drilling algorithm based only on the position control of the drill bit that overcomes such problems and presents additional advantages . The implication of each component of the algorithm in the drilling procedure is analyzed and the efficacy of the algorithm is experimentally validated with two types of bones . 
",Description of an admittance algorithm for bone drilling procedures. Layer transitions properly detected before bone breakthrough. Robust performance without collecting experimental data or retuning the control parameters. Only a position sensor is needed.,,,
S0169260714003848," Background and objectives Document annotation is a key task in the development of Text Mining methods and applications . High quality annotated corpora are invaluable but their preparation requires a considerable amount of resources and time . Although the existing annotation tools offer good user interaction interfaces to domain experts project management and quality control abilities are still limited . Therefore the current work introduces Marky a new Web based document annotation tool equipped to manage multi user and iterative projects and to evaluate annotation quality throughout the project life cycle . Methods At the core Marky is a Web application based on the open source CakePHP framework . User interface relies on HTML5 and CSS3 technologies . Rangy library assists in browser independent implementation of common DOM range and selection tasks and Ajax and JQuery technologies are used to enhance user system interaction . Results Marky grants solid management of inter and intra annotator work . Most notably its annotation tracking system supports systematic and on demand agreement analysis and annotation amendment . Each annotator may work over documents as usual but all the annotations made are saved by the tracking system and may be further compared . So the project administrator is able to evaluate annotation consistency among annotators and across rounds of annotation while annotators are able to reject or amend subsets of annotations made in previous rounds . As a side effect the tracking system minimises resource and time consumption . Conclusions Marky is a novel environment for managing multi user and iterative document annotation projects . Compared to other tools Marky offers a similar visually intuitive annotation experience while providing unique means to minimise annotation effort and enforce annotation quality and therefore corpus consistency . Marky is freely available for non commercial use at http sing.ei.uvigo.es marky . 
",The ultimate goal of Text Mining is to learn how to recognise and contextualise information of interest. Depending on the application area the creation of such semantically annotated corpora is a resource and time consuming activity. Annotation consistency needs to be actively monitored during the annotation process in order to guarantee the quality of the generated corpus. Marky is a novel environment for managing multi user and iterative document annotation projects. Marky implements the main steps of the annotation project life cycle with particular emphasis on annotation quality assessment.,,,
S0169260714001461," In this paper the gHRV software tool is presented . It is a simple free and portable tool developed in python for analysing heart rate variability . It includes a graphical user interface and it can import files in multiple formats analyse time intervals in the signal test statistical significance and export the results . This paper also contains as an example of use a clinical analysis performed with the gHRV tool namely to determine whether the heart rate variability indexes change across different stages of sleep . Results from tests completed by researchers who have tried gHRV are also explained in general the application was positively valued and results reflect a high level of satisfaction . gHRV is in continuous development and new versions will include suggestions made by testers . 
",gHRV is an open source tool for HRV analysis. gHRV is implemented in python and binaries are available for Linux Windows and Apple OS X. It imports and exports files in several formats and creates high quality plots that can be exported to file.,,,
S0169260715000619," The Gene Ontology is a structured repository of concepts that are associated to one or more gene products . The process of association is referred to as annotation . The relevance and the specificity of both GO terms and annotations are evaluated by a measure defined as information content . The analysis of annotated data is thus an important challenge for bioinformatics . There exist different approaches of analysis . From those the use of association rules may provide useful knowledge and it has been used in some applications e.g . improving the quality of annotations . Nevertheless classical association rules algorithms do not take into account the source of annotation nor the importance yielding to the generation of candidate rules with low IC . This paper presents GO WAR a methodology for extracting weighted association rules . GO WAR can extract association rules with a high level of IC without loss of support and confidence from a dataset of annotated data . A case study on using of GO WAR on publicly available GO annotation datasets is used to demonstrate that our method outperforms current state of the art approaches . 
",GO WAR is a tool for learning cross ontology weighted association rules. GO WAR is currently the only tool available for this problem. GO WAR is more flexible than previous approaches. A case study demonstrates the effectiveness of GO WAR approach.,,,
S0167947315000699," Methods are introduced for the analysis of large sets of sleep study data using a 5 state 20 transition type structure defined by the American Academy of Sleep Medicine . Application of these methods to the hypnograms of 5598 subjects from the Sleep Heart Health Study provide the first analysis of sleep hypnogram data of such size and complexity in a community cohort with a range of sleep disordered breathing severity introduce a novel approach to compare 5 state to 3 state sleep structures to assess information loss from combining sleep state categories extend current approaches of multivariate survival data analysis to clustered recurrent event discrete state discrete time processes and provide scalable solutions for data analyses required by the case study . The analysis provides detailed new insights into the association between sleep disordered breathing and sleep architecture . The example data and both R and SAS code are included in online supplementary materials . 
",We explore associations of sleep disordered breathing and sleep structure. 5 state hypnograms are systematically compared to 3 state. We analyze a community cohort of 5598 subjects 2.7 million rows total . We reduce analysis time from 8 hours to 30 s.,,,
S0169260715002345," Breast ultrasound image segmentation is a challenging task due to the speckle noise poor quality of the ultrasound images and size and location of the breast lesions . In this paper we propose a new BUS image segmentation algorithm based on neutrosophic similarity score and level set algorithm . At first the input BUS image is transferred to the NS domain via three membership subsets T I and F and then a similarity score NSS is defined and employed to measure the belonging degree to the true tumor region . Finally the level set method is used to segment the tumor from the background tissue region in the NSS image . Experiments have been conducted on a variety of clinical BUS images . Several measurements are used to evaluate and compare the proposed method s performance . The experimental results demonstrate that the proposed method is able to segment the BUS images effectively and accurately . 
",This study proposes a novel method to segment the tumor in the breast ultrasound images. The proposed approach combined the neutrosophic similarity score with level set algorithm for image segmentation. Neutrosophic similarity score is employed to deal with uncertain information on image segmentation.,,,
S0167839615001429," We show that a weighted least squares approximation of B zier coefficients with factored Hahn weights provides the best constrained polynomial degree reduction with respect to the Jacobi norm . This result affords generalizations to many previous findings in the field of polynomial degree reduction . A solution method to the constrained multi degree reduction with respect to the Jacobi norm is presented . 
",We study the problem of constrained degree reduction with respect to Jacobi norms. Our results generalize several previous findings on polynomial degree reduction. We explore the space of Jacobi parameters on the reduced polynomial approximation.,,,
S0169260714003538," Non invasive treatment of neurodegenerative diseases is particularly challenging in Western countries where the population age is increasing . In this work magnetic propagation in human head is modelled by Finite Difference Time Domain method taking into account specific characteristics of Transcranial Magnetic Stimulation in neurodegenerative diseases . It uses a realistic high resolution three dimensional human head mesh . The numerical method is applied to the analysis of magnetic radiation distribution in the brain using two realistic magnetic source models a circular coil and a figure 8 coil commonly employed in TMS . The complete model was applied to the study of magnetic stimulation in Alzheimer and Parkinson Diseases . The results show the electrical field distribution when magnetic stimulation is supplied to those brain areas of specific interest for each particular disease . Thereby the current approach entails a high potential for the establishment of the current underdeveloped TMS dosimetry in its emerging application to AD and PD . 
",Magnetic propagation in Transcranial Magnetic Stimulation TMS for neurodegenerative diseases is modelled by FDTD. A computing tool for the current underdeveloped TMS dosimetry in its emerging clinical application to AD and PD is presented. A realistic high resolution three dimensional human head mesh is used. Two realistic magnetic source models are employed a circular and a figure 8 coil. Results show the electrical field distribution when magnetic stimulation is supplied to those brain areas of specific interest for each particular disease.,,,
S0167931715300022,"The properties of electroless films produced from a bath designed for horizontal plating the preferred technology for high production volumes in printed circuit board metallization are reported. Film thickness substrate type and electrolyte temperature were varied. Formation of a continuous layer of copper film is correlated with a change in the visual and spectroscopic appearance. Grain orientation is random in thin films and a 110 texture develops with increasing thickness. The plating solution contains Cu and Ni ions. Nickel co deposits in copper films in the form of Ni hydroxide and its concentration decreases from about 6 in the vicinity of the substrate to about 1 at the film surface. Film stress and strain were measured by substrate curvature and X ray diffraction respectively. Both stress and strain decrease as the film thickness increases. Stress remains tensile throughout during deposition and during relaxation promoting film adhesion by preventing blisters. After deposition stress relaxes first towards compressive and then towards tensile. The stress the stress relaxation and the Ni concentration are high at the base of the film. We attribute this to the higher volume fraction of grain boundaries smaller grain size in this region. 
",A production level electroless copper bath designed for horizontal plating was characterized. In situ stress evolution of the Cu Ni films was studied with XRD and via substrate curvature. Stress of the Cu films depends on substrate choice metal or ABS polymer . Stress and its relaxation are inversely proportional to film thickness. The Ni content of the Cu Ni films is highest at the film substrate interface.,,,
S0167839616300188," This paper proposes a fast algorithm for computing the real roots of univariate polynomials given in the Bernstein basis . Traditionally the polynomial is subdivided until a root can be isolated . In contrast herein we aim to find a root only to subdivide the polynomial at the root . This subdivision based algorithm exploits the property that the B zier curves interpolate the end points of their control polygons . Upon subdivision at the root both resulting curves contain the root at one of their end points and hence contain a vanishing coefficient that is factored out . The algorithm then recurses on the new sub curves now of lower degree yielding a computational efficiency . In addition the proposed algorithm has the ability to efficiently count the multiplicities of the roots . Comparison of running times against the state of the art on thousands of polynomials shows an improvement of about an order of magnitude . The problem of numerically finding zeros of univariate polynomials is ubiquitous in computer aided design and engineering . Many geometric problems can be cast into that of finding zeros of polynomials for instance computing intersections of curves and surfaces contact analysis of shapes kinematic analysis etc . There have been many approaches to the problem of computing zeros of univariate polynomials in the past such as based on Newton s method using Descartes rule of signs based on subdivision to name a few . In this work we use the Bernstein representation for polynomials . Bernstein polynomials have several useful properties such as the variation diminishing property the convex hull property and numerical stability with respect to perturbation of coefficients which makes such a representation especially amenable for numerical applications . One of the earliest methods to exploit the variation diminishing property of B zier curves in order to isolate the roots of polynomials was given by Lane and Riesenfeld . In 1990 Sederberg and Nishita proposed the technique of B zier clipping for identifying regions of the domain which contain roots . This was done by intersecting the convex hull of the control polygon with the zero axis . In 2007 Barto and J ttler improved the technique of B zier clipping using degree reduction to generate a strip bounded by two quadratic polynomials which encloses the graph of the input polynomial . This strip when intersected with the zero axis gives the new interval potentially containing the roots . This approach that is also known as quadratic clipping was shown to have quadratic convergence by Schulz . In 2009 Liu et al . improved quadratic clipping by using cubic polynomials yielding faster rates of convergence . In 2007 M rken and Reimers utilized the close relationship between the spline and its control polygon for computing zeros of polynomials . They use the zeros of the control polygon as an initial guess for tracing the zeros of the polynomial . The control polygon is iteratively refined until the roots are found . 
",A fast algorithm for computing roots of univariate scalar B ziers is proposed. A speed up of about an order of magnitude is gained compared to previous methods. The proposed algorithm has the ability to count multiplicities of roots.,,,
S0169260714003253," Background and objective Vascularity evaluation on breast dynamic contrast enhanced magnetic resonance imaging has a potential diagnostic value but it represents a time consuming procedure affected by intra and inter observer variability . This study tests the application of a recently published method to reproducibly quantify breast vascularity and evaluates if the vascular volume of cancer bearing breast calculated from automatic vascular maps may correlate with pathologic tumor response after neoadjuvant chemotherapy . Methods Twenty four patients with unilateral locally advanced breast cancer underwent DCE MRI before and after NAC 8 responders and 16 non responders . A validated algorithm based on multiscale 3D Hessian matrix analysis provided AVMs and allowed the calculation of vessel volume before the initiation and after the last NAC cycle for each breast . For cancer bearing breast the difference in vascular volume before and after NAC was compared in responders and non responders using the Wilcoxon two sample test . A radiologist evaluated the vascularity on the subtracted images before and after treatment assigning a vascular score for each breast according to the number of vessels with length 30mm and maximal transverse diameter 2mm . The same evaluation was repeated with the support of the simultaneous visualization of the AVMs . The two evaluations were compared in terms of mean number of vessels and mean vascular score per breast in responders and non responders by use of Wilcoxon two sample test . For all the analysis the statistical significance level was set at 0.05 . Results For breasts harboring the cancer evidence of a difference in vascular volume before and after NAC for responders and non responders was found . A significant difference was also found in the number of vessels and vascular score before or after NAC according to the evaluation supported by the AVMs . Conclusions The encouraging although preliminary results of this study suggest the use of AVMs as new biomarker to evaluate the pathologic response after NAC but also support their application in other breast DCE MRI vessel analysis that are waiting for a reliable quantification method . Solid tumors rely on blood vessels to obtain necessary nutrient and oxygen and new vessels also offer path way for tumor expansion . Therefore the assessment of tumor vascularity may provide useful information to characterize underlying malignancy and monitor vascular abnormalities . In the past decade microvessel density represented the most commonly used prognostic indicator for tumor vascularization in a wide range of cancers although its quantification remained unreliable because of the lack of sufficient morphological and functional information of blood vessels and it could be only assessed after the surgical resection of the primary tumor . A better alternative to study tumor vasculature may be derived from non invasive imaging techniques such as contrast enhanced computed tomography magnetic resonance imaging and dynamic contrast enhanced magnetic resonance imaging positron emission tomography and more recently B mode and Doppler ultrasound . With varied types of cancer there is no general agreement as to which. 
",A validated algorithm quantifies breast vascularity through vascular maps AVM . Breast DCE MR exams are analyzed before and after neoadjuvant chemotherapy NAC . Responders and non responders show difference in vascularity before and after NAC. AVMs may represent a reliable method to evaluate pathologic response after NAC.,,,
S0169260715001789," Background Our lives are connected by a thousand invisible threads and along these sympathetic fibers our actions run as causes and return to us as results . It is Herman Melville s famous quote describing connections among human lives . To paraphrase the Melville s quote diseases are connected by many functional threads and along these sympathetic fibers diseases run as causes and return as results . The Melville s quote explains the reason for researching disease disease similarity and disease network . Measuring similarities between diseases and constructing disease network can play an important role in disease function research and in disease treatment . To estimate disease disease similarities we proposed a novel literature based method . Methods and results The proposed method extracted disease gene relations and disease drug relations from literature and used the frequencies of occurrence of the relations as features to calculate similarities among diseases . We also constructed disease network with top ranking disease pairs from our method . The proposed method discovered a larger number of answer disease pairs than other comparable methods and showed the lowest p value . Conclusions We presume that our method showed good results because of using literature data using all possible gene symbols and drug names for features of a disease and determining feature values of diseases with the frequencies of co occurrence of two entities . The disease disease similarities from the proposed method can be used in computational biology researches which use similarities among diseases . 
",We proposed a novel method which calculates disease disease similarity. The proposed method can be used in constructing of disease network. The proposed method discovered the largest number of answer disease pairs and showed lowest p value when compared with other comparable methods. The proposed method can provide an insight of relationship between diseases.,,,
S0168169916301260," In this study we assess the interchangeability and statistical agreement of two prevalent instruments from the non invasive sniffer method and compare their precision . Furthermore we develop and validate an effective algorithm for aligning time series data from multiple instruments to remove the effects of variable and fixed time shifts from the instrument comparison . The CH4 and CO2 gas concentrations for both instruments were found to differ for population means and intra cow variation and for inter cow variation . The CH4 and CO2 gas concentrations from both instruments can be used interchangeably to increase statistical power for example in genetic evaluations provided sources of disagreement are corrected through calibration and standardisation . Additionally averaging readings of cows over a longer period of time is an effective noise reduction technique which provides phenotypes with considerable inter cow variation . 
",Developed an algorithm for detecting and correcting time shifts between time series. Compared instruments simultaneously to identify and correct sources of disagreement. Demonstrated the effect of averaging readings over a longer period of time to reduce error.,,,
S0167947314001753," Limited information statistics have been recommended as the goodness of fit measures in sparse contingency tables but the values of these test statistics are computationally difficult to obtain . A Bayesian model diagnostic tool Relative Entropy Posterior Predictive Model Checking is proposed to assess the global fit for latent trait models in this paper . This approach utilizes the relative entropy to resolve possible problems in the original PPMC procedure based on the posterior predictive value . Compared with the typical conservatism of PPP value the RE value measures the discrepancy effectively . Simulated and real data sets with different item numbers degree of sparseness sample sizes and factor dimensions are studied to investigate the performance of the proposed method . The estimates of univariate information and difficulty parameters are found to be robust with dual characteristics which produce practical implications for educational testing . Compared with parametric bootstrapping RE PPMC is much more capable of evaluating the model adequacy . 
",A novel Relative Entropy Posterior Predictive Model Checking method is proposed. RE PPMC with limited information checks the model fitness effectively. Compared with the original PPMC with PPP value RE value is better. Compared with parametric bootstrapping RE PPMC is more feasible. The univariate information is robust with dual characteristics.,,,
S0167947313001576," The authors consider a dynamic probit model where the coefficients follow a first order Markov process . An exact Gibbs sampler for Bayesian analysis is presented for the model using the data augmentation approach and the forward filtering backward sampling algorithm for dynamic linear models . The authors discuss how our approach can be used for dynamic probit models as well as its generalizations including Markov regressions and models with Student link functions . An approach is presented to compare static and dynamic probit models as well as for Markov order selection in these classes of dynamic models . The developed approach is implemented to some actual data . 
",A dynamic probit model with a first order Markov process was developed. Gibbs sampler using data augmentation approach and forward filtering backward sampling algorithm were presented. The discussion was extended to propose models to generalized models including Student link function. Model fit was compared between static and dynamic probit models.,,,
S0167947315003187," Hierarchical centering has been described as a reparameterization method applicable to random effects models . It has been shown to improve mixing of models in the context of Markov chain Monte Carlo methods . A hierarchical centering approach is proposed for reversible jump MCMC chains which builds upon the hierarchical centering methods for MCMC chains and uses them to reparameterize models in an RJMCMC algorithm . Although these methods may be applicable to models with other error distributions the case is described for a log linear Poisson model where the expected value includes fixed effect covariates and a random effect for which normality is assumed with a zero mean and unknown standard deviation . For the proposed RJMCMC algorithm including hierarchical centering the models are reparameterized by modeling the mean of the random effect coefficients as a function of the intercept of the model and one or more of the available fixed effect covariates depending on the model . The method is appropriate when fixed effect covariates are constant within random effect groups . This has an effect on the dynamics of the RJMCMC algorithm and improves model mixing . The methods are applied to a case study of point transects of indigo buntings where without hierarchical centering the RJMCMC algorithm had poor mixing and the estimated posterior distribution depended on the starting model . With hierarchical centering on the other hand the chain moved freely over model and parameter space . These results are confirmed with a simulation study . Hence the proposed methods should be considered as a regular strategy for implementing models with random effects in RJMCMC algorithms they facilitate convergence of these algorithms and help avoid false inference on model parameters . 
",We consider a hierarchical centering approach for reversible jump MCMC algorithms. We describe the case for a log linear Poisson model with mixed effects. The zero mean of the random effect is replaced with part of the linear predictor. We apply the methods to point transect data of indigo buntings and simulated data. Our methods improve model mixing and inference on parameters.,,,
S0167839616300280," The theory of the isoptic curves is widely studied in the Euclidean plane . The analogous question was investigated by the authors in the hyperbolic and elliptic planes but in the higher dimensional spaces there are only few results in this topic . In Csima and Szirmai we gave a natural extension of the notion of the isoptic curves to the n dimensional Euclidean space which is called isoptic hypersurface . Now we develope an algorithm to determine the isoptic surface of a 3 dimensional polyhedron We will determine the isoptic surfaces for Platonic solids and for some semi regular Archimedean polytopes and visualize them with Wolfram Mathematica . 
",Isoptic curves are generalized to isoptic surfaces. Implicit equations of isoptic surfaces for polytopes are given algorithmically. Spherical geometry is useful to determine solid angles.,,,
S0167839616300413," Hierarchical generating systems that are derived from Zwart Powell elements can be used to generate quadratic splines on adaptively refined criss cross triangulations . We propose two extensions of these hierarchical generating systems firstly decoupling the hierarchical ZP elements and secondly enriching the system by including auxiliary functions . These extensions allow us to generate the entire hierarchical spline space which consists of all piecewise quadratic smooth functions on an adaptively refined criss cross triangulation if the triangulation fulfills certain technical assumptions . Special attention is dedicated to the characterization of the linear dependencies that are present in the resulting enriched decoupled hierarchical generating system . 
",Hierarchical Zwart Powell elements are studied. Sufficient conditions for algebraic completeness are given. Construction uses decoupling and partial chessboard functions. Characterization of linear dependencies is provided.,,,
S0169260714003927," Volume is one of the most important features for the characterization of a tumour on a macroscopic scale . It is often used to assess the effectiveness of care treatments thus making its correct evaluation a crucial issue for patient care . Similarly volume is a key feature on a microscopic scale . Multicellular cancer spheroids are 3D tumour models widely employed in pre clinical studies to test the effects of drugs and radiotherapy treatments . Very few methods have been proposed to estimate the tumour volume arising from a 2D projection of multicellular spheroids and even fewer have been designed to provide a 3D reconstruction of the tumour shape . In this work we propose Reconstruction and Visualization from a Single Projection an automatic method conceived to reconstruct the 3D surface and estimate the volume of single cancer multicellular spheroids or even of spheroid cultures . As the input parameter ReViSP requires only one 2D projection which could be a widefield microscope image . We assessed the effectiveness of our method by comparing it with other approaches . To this purpose we used a new strategy that allowed us to achieve accurate volume measurements based on the analysis of home made 3D objects built by mimicking the spheroid morphology . The results confirmed the effectiveness of our method for both 3D reconstruction and volume assessment . ReViSP software is distributed as an open source tool . 
",We present a new image based approach to compute multicellular spheroid volume. The reconstruction method is validated by using home made 3D objects. A new strategy to compare different volume estimation methods is discussed. The results obtained by ReViSP overcome those of the other methods tested. ReViSP source code and standalone executable version are freely available.,,,
S0167947314000279," In non parametric regression analysis the advantage of frames with respect to classical orthonormal bases is that they can furnish an efficient representation of a more broad class of functions . For example fast oscillating functions as audio speech sonar radar EEG and stock market are much more well represented by a frame with similar oscillating characteristic than by a classical orthonormal basis . In this respect a new frame based shrinkage estimator is derived as the Empirical Regularized version of the optimal Shrinkage estimator generalized to the frame operator . An analytic expression of it is furnished leading to an efficient implementation . Results on standard and real test functions are shown . 
",Totally automatic procedure for denoising fast oscillating functions. The procedure is valid for any frame operator. The procedure is a generalization of the empirical Wiener filter.,,,
S0169260715000243," Mathematical models that predict the complex dynamic behaviour of cellular networks are fundamental in systems biology and provide an important basis for biomedical and biotechnological applications . However obtaining reliable predictions from large scale dynamic models is commonly a challenging task due to lack of identifiability . The present work addresses this challenge by presenting a methodology for obtaining high confidence predictions from dynamic models using time series data . First to preserve the complex behaviour of the network while reducing the number of estimated parameters model parameters are combined in sets of meta parameters which are obtained from correlations between biochemical reaction rates and between concentrations of the chemical species . Next an ensemble of models with different parameterizations is constructed and calibrated . Finally the ensemble is used for assessing the reliability of model predictions by defining a measure of convergence of model outputs that is used as an indicator of confidence . We report results of computational tests carried out on a metabolic model of Chinese Hamster Ovary cells which are used for recombinant protein production . Using noisy simulated data we find that the aggregated ensemble predictions are on average more accurate than the predictions of individual ensemble models . Furthermore ensemble predictions with high consensus are statistically more accurate than ensemble predictions with large variance . The procedure provides quantitative estimates of the confidence in model predictions and enables the analysis of sufficiently complex networks as required for practical applications . 
",We propose a method for quantifying the quality of predictions in dynamic models. The approach helps in deciding which additional measurements are more useful. Meta parameters are used for accelerating parameter estimation and reducing the risk of over fitting. An ensemble of models is built and consensus among their predictions is used as an indication of reliability. The method is demonstrated on a metabolic model of Chinese Hamster Ovary cells CHO .,,,
S0167931715002129," Solid on liquid deposition techniques are of great interest to the MEMS and NEMS community because of potential applications in biomedical engineering on chip liquid trapping tunable micro lenses and replacements of gate oxides . However depositing solids on liquid with subsequent hermetic sealing is difficult because liquids tend to have a lower density than solids . Furthermore current systems seen in nature lack thermal mechanical or chemical stability . Therefore it is not surprising that liquids are not ubiquitous as functional layers in MEMS and NEMS . However SOLID techniques have the potential to be harnessed and controlled for such systems because the gravitational force is negligible compared to surface tension and therefore the solid molecular precursors that typically condense on a liquid surface will not sediment into the fluid . In this review we summarize recent research into SOLID where nucleation and subsequent cross linking of solid precursors results in thin film growth on a liquid substrate . We describe a large variety of thin film deposition techniques such as thermal evaporation sputtering plasma enhanced chemical vapor deposition used to coat liquid substrates . Surprisingly all attempts at deposition to date have been successful and a stable solid layer on a liquid can always be detected . However all layers grown by non equilibrium deposition processes showed a strong presence of wrinkles presumably due to residual stress . In fact the only example where no stress was observed is the deposition of parylene layers . Using all the experimental data analyzed to date we have been able to propose a simple model that predicts that the surface property of liquids at molecular level is influenced by cohesion forces between the liquid molecules . Finally we conclude that the condensation of precursors from the gas phase is rather the rule and not the exception for SOLID techniques . 
",We studied the condensation of matter on liquid surfaces from the vapor phase. First solid on liquid deposition based devices are presented. A thin polymer film can chemically react with the liquid at condensation. Encapsulation of a liquid with parylene exhibits perfect optical surfaces. We present a large variety of solid on liquid systems.,,,
S0169260714001680," This paper presents a method for fast computation of Hessian based enhancement filters whose conditions for identifying particular structures in medical images are associated only with the signs of Hessian eigenvalues . The computational costs of Hessian based enhancement filters come mainly from the computation of Hessian eigenvalues corresponding to image elements to obtain filter responses because computing eigenvalues of a matrix requires substantial computational effort . High computational cost has become a challenge in the application of Hessian based enhancement filters . Using a property of the characteristic polynomial coefficients of a matrix and the well known Routh Hurwitz criterion in control engineering it is shown that under certain conditions the response of a Hessian based enhancement filter to an image element can be obtained without having to compute Hessian eigenvalues . The computational cost can thus be reduced . Experimental results on several medical images show that the method proposed in this paper can reduce significantly the number of computations of Hessian eigenvalues and the processing times of images . The percentage reductions of the number of computations of Hessian eigenvalues for enhancing blob and tubular like structures in two dimensional images are approximately 90 and 65 respectively . For enhancing blob tubular and plane like structures in three dimensional images the reductions are approximately 97 75 and 12 respectively . For the processing times the percentage reductions for enhancing blob and tubular like structures in two dimensional images are approximately 31 and 7.5 respectively . The reductions for enhancing blob tubular and plane like structures in three dimensional images are approximately 68 55 and 3 respectively . 
",A method for fast computation of Hessian based enhancement filters is proposed. The method relies on avoiding computation of Hessian eigenvalues. The percentage reductions in processing time can be over 65 .,,,
S0167839615001028," In this paper we investigate the problem of interpolating a B spline curve network in order to create a surface satisfying such a constraint and defined by blending functions spanning the space of bivariate quadratic splines on criss cross triangulations . We prove the existence and uniqueness of the surface providing a constructive algorithm for its generation . We also present numerical and graphical results and comparisons with other methods . 
",Interpolation of a B spline curve network by a surface based on quadratic B splines on criss cross triangulations. Proof of the existence and uniqueness of the surface and constructive algorithm for its generation. Numerical and graphical results and comparisons with other spline methods.,,,
S0167947314000577," The latent class model provides an important platform for jointly modeling mixed mode data i.e . discrete and continuous data with various parametric distributions . Multiple mixed mode variables are used to cluster subjects into latent classes . While the mixed mode latent class analysis is a powerful tool for statisticians few studies are focused on assessing the contribution of mixed mode variables in discriminating latent classes . Novel measures are derived for assessing both absolute and relative impacts of mixed mode variables in latent class analysis . Specifically the expected posterior gradient and the Kolmogorov variation of the posterior distribution as well as related properties are studied . Numerical results are presented to illustrate the measures . 
",Two measures are proposed for assessing continuous and discrete variables in LCA. Both measures are either in closed form or straightforward to compute. Both measures perform reasonably well compared to existing measures such LRT and st Both absolute and relative interpretations of one measure are possible.,,,
S0169260716301833," Background and objective Feature extraction of electroencephalogram plays a vital role in brain computer interfaces . In recent years common spatial pattern has been proven to be an effective feature extraction method . However the traditional CSP has disadvantages of requiring a lot of input channels and the lack of frequency information . In order to remedy the defects of CSP wavelet packet decomposition and CSP are combined to extract effective features . But WPD CSP method considers less about extracting specific features that are fitted for the specific subject . So a subject based feature extraction method using fisher WPD CSP is proposed in this paper . Methods The idea of proposed method is to adapt fisher WPD CSP to each subject separately . It mainly includes the following six steps original EEG signals from all channels are decomposed into a series of sub bands using WPD average power values of obtained sub bands are computed the specified sub bands with larger values of fisher distance according to average power are selected for that particular subject each selected sub band is reconstructed to be regarded as a new EEG channel all new EEG channels are used as input of the CSP and a six dimensional feature vector is obtained by the CSP . The subject based feature extraction model is so formed the probabilistic neural network is used as the classifier and the classification accuracy is obtained . Results Data from six subjects are processed by the subject based fisher WPD CSP the non subject based fisher WPD CSP and WPD CSP respectively . Compared with non subject based fisher WPD CSP and WPD CSP the results show that the proposed method yields better performance and the classification accuracy from subject based fisher WPD CSP is increased by 6 12 and 14 respectively . Conclusions The proposed subject based fisher WPD CSP method can not only remedy disadvantages of CSP by WPD but also discriminate helpless sub bands for each subject and make remaining fewer sub bands keep better separability by fisher distance which leads to a higher classification accuracy than WPD CSP method . 
",A subject based feature extraction method is discussed. The feature extraction method combines wavelet packet decomposition common spatial patterns and fisher distance. The selection of sub bands based on fisher distance tells apart those useless sub bands and remain best sub bands for each subject. The proposed method can extract suitable features for specific subject and achieve a higher classification accuracy than non subject based method.,,,
S0262885613000905,"Answering to the growing demand of machine vision applications for the latest generation of electronic devices endowed with camera platforms several moving object detection strategies have been proposed in recent years. Among them spatio temporal based non parametric methods have recently drawn the attention of many researchers. These methods by combining a background model and a foreground model achieve high quality detections in sequences recorded with non completely static cameras and in scenarios containing complex backgrounds. However since they have very high memory and computational associated costs they apply some simplifications in the background modeling process therefore decreasing the quality of the modeling. Here we propose a novel background modeling that is applicable to any spatio temporal non parametric moving object detection strategy. Through an efficient and robust method to dynamically estimate the bandwidth of the kernels used in the modeling both the usability and the quality of previous approaches are improved. Furthermore by adding a novel mechanism to selectively update the background model the number of misdetections is significantly reduced achieving an additional quality improvement. Empirical studies on a wide variety of video sequences demonstrate that the proposed background modeling significantly improves the quality of previous strategies while maintaining the computational requirements of the detection process. 
",Real time dynamic bandwidth estimation that reduces the amount of false detections. Selective update mechanism that significantly reduces the number of misdetections. The quality provided by previous background modeling strategies is improved. The computational cost of the simplest modeling strategy is barely increased. The proposed methods can be used by any spatio temporal non parametric strategy.,,,
S0262885614000754,"We present a novel approach for the estimation of a person s overall body orientation 3D shape and texture from overlapping cameras. A distinguishing aspect of our approach is the use of spherical harmonics for 3D shape and texture representation it offers a compact low dimensional representation which elegantly copes with rotation estimation. The estimation process alternates between the estimation of texture orientation and shape. Texture is estimated by sampling image intensities with the predicted 3D shape i.e. torso and head and the predicted orientation from the last time step. Orientation i.e. rotation around torso major axis is estimated by minimizing the difference between a learned texture model in a canonical orientation and the current texture estimate. The newly estimated orientation allows to update the 3D shape estimate taking into account the new 3D shape measurement obtained by volume carving. We investigate various components of our approach in experiments on synthetic and real world data. We show that our proposed method has lower orientation estimation error than other methods that use fixed 3D shape models for data involving persons. 
",Estimation of a person s orientation 3D shape and texture from overlapping cameras Spherical harmonics are used as low dimensional 3D shape and texture representation. Spherical harmonics properties allow to cope with orientation estimation elegantly. Outperformance of orientation estimation methods that use a fixed 3D shape model,,,
S0169260715002680," Despite the widespread use of cardiotocography in foetal monitoring the evaluation of foetal status suffers from a considerable inter and intra observer variability . In order to overcome the main limitations of visual cardiotocographic assessment computerised methods to analyse cardiotocographic recordings have been recently developed . In this study a new software for automated analysis of foetal heart rate is presented . It allows an automatic procedure for measuring the most relevant parameters derivable from cardiotocographic traces . Simulated and real cardiotocographic traces were analysed to test software reliability . In artificial traces we simulated a set number of events to be recognised . In the case of real signals instead results of the computerised analysis were compared with the visual assessment performed by 18 expert clinicians and three performance indexes were computed to gain information about performances of the proposed software . The software showed preliminary performance we judged satisfactory in that the results matched completely the requirements as proved by tests on artificial signals in which all simulated events were detected from the software . Performance indexes computed in comparison with obstetricians evaluations are on the contrary not so satisfactory in fact they led to obtain the following values of the statistical parameters sensitivity equal to 93 positive predictive value equal to 82 and accuracy equal to 77 . Very probably this arises from the high variability of trace annotation carried out by clinicians . 
",A software for automatic CTG analysis which could be a useful clinical tool is shown. The software provides important classical and nonlinear parameters. Simulated and real CTG traces were analysed to test software reliability. The software showed good performance in fact all simulated events were detected. On real CTG sensitivity equal to 93 positive predictive value 82 accuracy 77 .,,,
S0169260716000080," Registration of mammograms plays an important role in breast cancer computer aided diagnosis systems . Radiologists usually compare mammogram images in order to detect abnormalities . The comparison of mammograms requires a registration between them . A temporal mammogram registration method is proposed in this paper . It is based on the curvilinear coordinates which are utilized to cope both with global and local deformations in the breast area . Temporal mammogram pairs are used to validate the proposed method . After registration the similarity between the mammograms is maximized and the distance between manually defined landmarks is decreased . In addition a thorough comparison with the state of the art mammogram registration methods is performed to show its effectiveness . 
",The comparison of mammograms requires a registration between them. A mammogram registration method based on the curvilinear coordinates is proposed. Temporal mammogram pairs are used to validate the proposed method.,,,
S0262885613001819," In this paper we present an unsupervised distance learning approach for improving the effectiveness of image retrieval tasks . We propose a Reciprocal kNN Graph algorithm that considers the relationships among ranked lists in the context of a k reciprocal neighborhood . The similarity is propagated among neighbors considering the geometry of the dataset manifold . The proposed method can be used both for re ranking and rank aggregation tasks . Unlike traditional diffusion process methods which require matrix multiplication operations our algorithm takes only a subset of ranked lists as input presenting linear complexity in terms of computational and storage requirements . We conducted a large evaluation protocol involving shape color and texture descriptors various datasets and comparisons with other post processing approaches . The re ranking and rank aggregation algorithms yield better results in terms of effectiveness performance than various state of the art algorithms recently proposed in the literature achieving bull s eye and MAP scores of 100 on the well known MPEG 7 shape dataset . 
",Presentation of an unsupervised manifold learning algorithm using Reciprocal kNN Graphs Presentation of the Reciprocal kNN Graph ReRanking for improving the effectiveness of CBIR systems Description of how Reciprocal kNN Graph algorithm can be used for rank aggregation tasks Discussion about the computational complexity and the convergence of proposed algorithm Experimental evaluation considering different datasets descriptors and baselines,,,
S0198971516300102," Characterizing urban landscapes is important given the present and future projections of global population that favor urban growth. The definition of urban on a thematic map has proven to be problematic since urban areas are heterogeneous in terms of land use and land cover. Further certain urban classes are inherently imprecise due to the difficulty in integrating various social and environmental inputs into a precise definition. Social components often include demographic patterns transportation building type and density while ecological components include soils elevation hydrology climate vegetation and tree cover. In this paper we adopt a coupled human and natural system CHANS integrated scientific framework for characterizing urban landscapes. We implement the framework by adopting a fuzzy sets concept of urban characterization since fuzzy sets relate to classes of object with imprecise boundaries in which membership is a matter of degree. For dynamic mapping applications user defined classification schemes involving rules combining different social and ecological inputs can lead to a degree of quantification in class labeling varying from highly urban to least urban . A socio economic perspective of urban may include threshold values for population and road network density while a more ecological perspective of urban may utilize the ratio of natural versus built area and percent forest cover. Threshold values are defined to derive the fuzzy rules of membership in each case and various combinations of rules offer a greater flexibility to characterize the many facets of the urban landscape. We illustrate the flexibility and utility of this fuzzy inference approach called the Fuzzy Urban Index for the Boston Metro region with five inputs and eighteen rules. The resulting classification map shows levels of fuzzy membership ranging from highly urban to least urban or rural in the Boston study region. We validate our approach using two experts assessing accuracy of the resulting fuzzy urban map. We discuss how our approach can be applied in other urban contexts with newly emerging descriptors of urban sustainability urban ecology and urban metabolism. 
",Fuzzy Sets to Characterize Urban Landcover. Fuzzy Urban Index consisting of combinations of rules. Flexible user specified fuzzy inference and mapping platform.,,,
S0262885613001455," Topological Active Nets are promising parametric deformable models that integrate features of region based and boundary based segmentation techniques . Problems associated with the complexity of the model however have limited their utility . This paper introduces an extension of the model defining a new behavior for changing its topology as well as a novel external force definition and a new local search optimization procedure . In particular we propose a new automatic pre processing phase a new external energy term based on the Extended Vector Field Convolution node movement constraints to avoid crossing links and different procedures to perform link cuts and hole detection . Moreover the new local search procedure also incorporates heuristics to correct the position of eventually misplaced nodes . The proposal has been tested on 18 synthetic images which present different segmentation difficulties along with 3 real medical images . Its performance has been compared with that of the original Topological Active Net optimization approach along with both state of the art parametric and geometric active contours two snakes and two level sets . Our new method outperforms all the others for the given image sets in terms of segmentation accuracy measured by using four standard segmentation metrics . 
",ETANs overcome the limitations of TANs while keeping their promising features. ETANs combine the best capabilities of two DMs TANs and EVFC snakes. ETANs employ novel mechanisms to tackle topological changes as link cuts and holes. ETANs employ node movement constraints to avoid crossing links. ETANs employ a new local search procedure and a new automatic pre processing phase.,,,
S0262885614000766," Most current tracking approaches utilize only one type of feature to represent the target and learn the appearance model of the target just by using the current frame or a few recent ones . The limited representation of one single type of feature might not represent the target well . What s more the appearance model learning from the current frame or a few recent ones is intolerant of abrupt appearance changes in short time intervals . These two factors might cause the track s failure . To overcome these two limitations in this paper we apply the Augmented Kernel Matrix classification to combine two complementary features pixel intensity and LBP features to enrich the target s representation . Meanwhile we employ the AKM clustering to group the tracking results into a few aspects . And then the representative patches are selected and added into the training set to learn the appearance model . This makes the appearance model cover more aspects of the target appearance and more robust to abrupt appearance changes . Experiments compared with several state of the art methods on challenging sequences demonstrate the effectiveness and robustness of the proposed algorithm . 
",Augmented Kernel Matrix AKM is applied to combine complementary features. AKM clustering is utilized to group the tracking results into a few aspects. Representative patches are selected to learn the appearance model. Adding representative patches our tracker more robust to abrupt appearance changes.,,,
S0262885614000195," This paper presents an on line adaptive metric to estimate the similarity between the target representation model and new image received at every time instant . The similarity measure also known as observation likelihood plays a crucial role in the accuracy and robustness of visual tracking . In this work an L2 norm is adaptively weighted at every matching step to calculate the similarity between the target model and image descriptors . A histogram based classifier is learned on line to categorize the matching errors into three classes namely i image noise ii significant appearance changes and iii outliers . A robust weight is assigned to each matching error based on the class label . Therefore the proposed similarity measure is able to reject outliers and adapt to the target model by discriminating the appearance changes from the undesired outliers . The experimental results show the superiority of the proposed method with respect to accuracy and robustness in the presence of severe and long term occlusion and image noise in comparison with commonly used robust regressors . 
",We present an adaptive metric for measuring the similarity of a target for the purpose of visual tracking. This metric assigns a robust weight to each matching error based on the error type. A histogram based classifier is learned on line to determine the error type. The proposed robust metric dynamically adapts to the actual appearance changes by tuning its parameters.,,,
S0262885614000468,"Changes in eyebrow configuration in conjunction with other facial expressions and head gestures are used to signal essential grammatical information in signed languages. This paper proposes an automatic recognition system for non manual grammatical markers in American Sign Language ASL based on a multi scale spatio temporal analysis of head pose and facial expressions. The analysis takes account of gestural components of these markers such as raised or lowered eyebrows and different types of periodic head movements. To advance the state of the art in non manual grammatical marker recognition we propose a novel multi scale learning approach that exploits spatio temporally low level and high level facial features. Low level features are based on information about facial geometry and appearance as well as head pose and are obtained through accurate 3D deformable model based face tracking. High level features are based on the identification of gestural events of varying duration that constitute the components of linguistic non manual markers. Specifically we recognize events such as raised and lowered eyebrows head nods and head shakes. We also partition these events into temporal phases. We separate the anticipatory transitional movement the onset from the linguistically significant portion of the event and we further separate the core of the event from the transitional movement that occurs as the articulators return to the neutral position towards the end of the event the offset . This partitioning is essential for the temporally accurate localization of the grammatical markers which could not be achieved at this level of precision with previous computer vision methods. In addition we analyze and use the motion patterns of these non manual events. Those patterns together with the information about the type of event and its temporal phases are defined as the high level features. Using this multi scale spatio temporal combination of low and high level features we employ learning methods for accurate recognition of non manual grammatical markers in ASL sentences. 
",Eyebrow gestures and periodic head movements convey linguistic information. We propose multi scale approach for recognizing non manual grammatical marker. We obtain high level features from spatio temporal analysis of non manual events. We use two stage CRFs to recognize events and partition them into phases. Experiments demonstrate superior improvements in ASL.,,,
S0169260715003326," This article was motivated by the doctors demand to make a technical support in pathologies of gastrointestinal tract research which would be based on machine vision tools . Proposed solution should be less expensive alternative to already existing RF methods . The objective of whole experiment was to evaluate the amount of animal motion dependent on degree of pathology . In the theoretical part of the article several methods of animal trajectory tracking are presented two differential methods based on background subtraction the thresholding methods based on global and local threshold and the last method used for animal tracking was the color matching with a chosen template containing a searched spectrum of colors . The methods were tested offline on five video samples . Each sample contained situation with moving guinea pig locked in a cage under various lighting conditions . 
",Five methods used for animal trajectory tracking which were proposed with the usage of visual system. All the algorithms are proposed for poor quality images non homogenous lightning presence of grid etc. . All methods are compared and their effectiveness is evaluated. Proposed methods are compared with standard used methods for animal tracking as GPS tracking or RFID tracking. They bring several advantages such as user friendly and simple interface appropriate for medical non technical specialists too.,,,
S0262885614000791," Human age gender and ethnicity are valuable demographic characteristics . They are also important soft biometric traits useful for human identification or verification . We present a framework that can estimate the three traits jointly . The joint estimation framework could deal with the mutual influence of age gender and ethnicity implicitly . Under this joint estimation framework we explore different methods for simultaneous estimation of age gender and ethnicity . The canonical correlation analysis based methods and partial least squares models are explored under our joint estimation framework . Both the linear and nonlinear methods are investigated to measure the performance . We also validate some extensions of these methods such as the least squares formulations of the CCA methods . We found some consistent ranking of these methods under our joint estimation framework . More importantly we found that the CCA based methods can derive an extremely low dimensionality in estimating age gender and ethnicity . An analysis of this property is given based on the rank theory . The experiments are conducted on a very large database containing more than 55 000 face images . 
",A framework for joint estimation of age gender and ethnicity in a single step A novel finding on feature dimensionality in estimating age gender and ethnicity A rank theory based analysis of dimensionality problem in using CCA based methods A ranking of CCA and PLS based methods under our joint estimation framework Investigation of LS formulations of the CCA based methods for our problem.,,,
S0262885613001777," Identical twins pose a great challenge to face recognition due to high similarities in their appearances . Motivated by the psychological findings that facial motion contains identity signatures and the observation that twins may look alike but behave differently we develop a talking profile to use the identity signatures in the facial motion to distinguish between identical twins . The talking profile for a subject is defined as a collection of multiple types of usual face motions from the video . Given two talking profiles we compute the similarities of the same type of face motion in both profiles and then perform the classification based on those similarities . To compute the similarity of each type of face motion we give higher weights to more abnormal motions which are assumed to carry more identity signature information . Our approach named Exceptional Motion Reporting Model is unrelated with appearance and can handle realistic facial motion in human subjects with no restrictions of speed of motion or video frame rate . We first conduct our experiments on a video database containing 39 pairs of twins . The experimental results demonstrate that identical twins can be distinguished better by the talking profiles over the traditional appearance based approach . Moreover we collected a non twin YouTube dataset with 99 subjects . The results on this dataset verified that the talking profile can be the potential biometric . We further conducted an experiment to test the robustness of talking profile to the time . Videos from 10 subjects which span across years or even decades in their lives are collected . The results indicated the robustness of talking profile to the aging process . 
",We prove that twins look alike but they behave differently. A talking profile consisting of usual facial motions is proposed as a biometric. Our model counts more on abnormal action and gains the superior performance. Our experiments show that the talking profile is robust to some aging effect.,,,
S0169260715300298," This work presents a systematic review of techniques for the 3D automatic detection of pulmonary nodules in computerized tomography images . Its main goals are to analyze the latest technology being used for the development of computational diagnostic tools to assist in the acquisition storage and mainly processing and analysis of the biomedical data . Also this work identifies the progress made so far evaluates the challenges to be overcome and provides an analysis of future prospects . As far as the authors know this is the first time that a review is devoted exclusively to automated 3D techniques for the detection of pulmonary nodules from lung CT images which makes this work of noteworthy value . The research covered the published works in the Web of Science PubMed Science Direct and IEEEXplore up to December 2014 . Each work found that referred to automated 3D segmentation of the lungs was individually analyzed to identify its objective methodology and results . Based on the analysis of the selected works several studies were seen to be useful for the construction of medical diagnostic aid tools . However there are certain aspects that still require attention such as increasing algorithm sensitivity reducing the number of false positives improving and optimizing the algorithm detection of different kinds of nodules with different sizes and shapes and finally the ability to integrate with the Electronic Medical Record Systems and Picture Archiving and Communication Systems . Based on this analysis we can say that further research is needed to develop current techniques and that new algorithms are needed to overcome the identified drawbacks . 
",A review about 3D automatic detection of pulmonary nodules in CT images is presented. Tasks tools public image databases and strategies are introduced. The integration with related data systems is taking into account. The techniques found are discussed and possible advances are identified. This review is interesting both for researchers and health professionals.,,,
S0169260715300535," Recently there has been a growing interest in the field of metabolomics materialized by a remarkable growth in experimental techniques available data and related biological applications . Indeed techniques as nuclear magnetic resonance gas or liquid chromatography mass spectrometry infrared and UV visible spectroscopies have provided extensive datasets that can help in tasks as biological and biomedical discovery biotechnology and drug development . However as it happens with other omics data the analysis of metabolomics datasets provides multiple challenges both in terms of methodologies and in the development of appropriate computational tools . Indeed from the available software tools none addresses the multiplicity of existing techniques and data analysis tasks . In this work we make available a novel R package named specmine which provides a set of methods for metabolomics data analysis including data loading in different formats pre processing metabolite identification univariate and multivariate data analysis machine learning and feature selection . Importantly the implemented methods provide adequate support for the analysis of data from diverse experimental techniques integrating a large set of functions from several R packages in a powerful yet simple to use environment . The package already available in CRAN is accompanied by a web site where users can deposit datasets scripts and analysis reports to be shared with the community promoting the efficient sharing of metabolomics data analysis pipelines . 
",The paper firstly reviews a number of tools for metabolomics data analysis identifying some of their limitations. The work proposes a novel R package that provides a set of methods for metabolomics and spectral data analysis. The provided functions include data loading pre processing metabolite identification univariate multivariate data analysis machine learning and feature selection. The implemented methods provide support for the analysis of data from experimental techniques including NMR GC and LC MS UV vis and IR. The paper provides a number of case studies illustrating the use of the package s functions covering different types of data and analyses types.,,,
S0169260716301614," Background and objective Lung sound auscultation is one of the most commonly used methods to evaluate respiratory diseases . However the effectiveness of this method depends on the physician s training . If the physician does not have the proper training he she will be unable to distinguish between normal and abnormal sounds generated by the human body . Thus the aim of this study was to implement a pattern recognition system to classify lung sounds . Methods We used a dataset composed of five types of lung sounds normal coarse crackle fine crackle monophonic and polyphonic wheezes . We used higher order statistics to extract features Genetic Algorithms and Fisher s Discriminant Ratio to reduce dimensionality and k Nearest Neighbors and Naive Bayes classifiers to recognize the lung sound events in a tree based system . We used the cross validation procedure to analyze the classifiers performance and the Tukey s Honestly Significant Difference criterion to compare the results . Results Our results showed that the Genetic Algorithms outperformed the Fisher s Discriminant Ratio for feature selection . Moreover each lung class had a different signature pattern according to their cumulants showing that HOS is a promising feature extraction tool for lung sounds . Besides the proposed divide and conquer approach can accurately classify different types of lung sounds . The classification accuracy obtained by the best tree based classifier was 98.1 for classification accuracy on training and 94.6 for validation data . Conclusions The proposed approach achieved good results even using only one feature extraction tool . Additionally the implementation of the proposed classifier in an embedded system is feasible . 
",A pattern recognition system to classify five lung sounds is proposed. The system is based on HOS and on a divide and conquer approach. The proposed approach uses Genetic Algorithms to dimensionality reduction. K Nearest Neighbor and Naive Bayes classifiers are used to recognize the signals. The system achieved a high classification accuracy and can be implemented in an embedded system.,,,
S0169260716000031," Background and objective This article presents a multimodal analysis of startle type responses using a variety of physiological facial and speech features . These multimodal components of the startle type response reflect complex brain body reactions to a sudden and intense stimulus . Additionally the proposed multimodal evaluation of reflexive and emotional reactions associated with the startle eliciting stimuli and underlying neural networks and pathways could be applied in diagnostics of different psychiatric and neurological diseases . Different startle type stimuli can be compared in the strength of their elicitation of startle responses i.e . their potential to activate stress related neural pathways underlying biomarkers and corresponding behavioral reactions . Methods An innovative method for measuring startle type responses using multimodal stimuli and multimodal feature analysis has been introduced . Individual s multimodal reflexive and emotional expressions during startle type elicitation have been assessed by corresponding physiological speech and facial features on ten female students of psychology . Different startle eliciting stimuli like noise and airblast probes as well as a variety of visual and auditory stimuli of different valence and arousal levels based on International Affective Picture System images and or sounds from International Affective Digitized Sounds database have been designed and tested . Combined together into more complex startle type stimuli such composite stimuli can potentiate the evoked response of underlying neural networks and corresponding neurotransmitters and neuromodulators as well this is referred to as increased power of response elicitation . The intensity and magnitude of multimodal responses to selected startle type stimuli have been analyzed using effect sizes and medians of dominant multimodal features i.e . skin conductance eye blink head movement speech fundamental frequency and energy . The significance of the observed effects and comparisons between paradigms were evaluated using one tailed t tests and ANOVA methods respectively . Skin conductance response habituation was analyzed using ANOVA and post hoc multiple comparison tests with the Dunn id k correction . Results The results revealed specific physiological facial and vocal reflexive and emotional responses on selected five stimuli paradigms which included acoustic startle probes airblasts IAPS images IADS sounds and image sound airblast composite stimuli . Overall composite and airblast paradigms resulted in the largest responses across all analyzed features followed by sound and acoustic startle paradigms while paradigm using images consistently elicited the smallest responses . In this context power of response elicitation of the selected stimuli paradigms can be described according to the aggregated magnitude of the participants multimodal responses . We also observed a habituation effect only in skin conductance response to acoustic startle airblast and sound paradigms . Conclusions This study developed a system for paradigm design and stimuli generation as well as real time multimodal signal processing and feature calculation . Experimental paradigms for monitoring individual responses to stressful startle type stimuli were designed in order to compare the response elicitation power across various stimuli . The developed system applied paradigms and obtained results might be useful in further research for evaluation of. 
",Startle type responses were analyzed by multimodal physiological speech and facial features. Acoustic startle probes airblasts images sounds and composite stimuli were used. Dominant responses occurred in skin conductance eye blink head movement speech fundamental frequency and energy. Strongest responses were obtained for composite stimuli what illustrates an approach to enhance the power of elicitation.,,,
