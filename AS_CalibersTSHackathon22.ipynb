{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "521ddccc",
      "metadata": {
        "id": "521ddccc"
      },
      "outputs": [],
      "source": [
        "import pandas as  pd \n",
        "df=pd.read_csv(\"/train (1).csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d7e1fe6f",
      "metadata": {
        "id": "d7e1fe6f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "outputId": "5d786eb9-bac2-4da8-f069-eee05dd0933b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "               FileName                                           Abstract  \\\n",
              "0     S0262885614000857   This paper introduces four classes of rotatio...   \n",
              "1     S0169260715300419   In this paper a MATLAB based graphical user i...   \n",
              "2     S0169260715003260   Background and objective A markerless low cos...   \n",
              "3     S0262885613001443   This paper presents an improved multiple inst...   \n",
              "4     S0262885614000511   Text based image retrieval may perform poorly...   \n",
              "...                 ...                                                ...   \n",
              "1995  S0262885613001777   Identical twins pose a great challenge to fac...   \n",
              "1996  S0169260715300298   This work presents a systematic review of tec...   \n",
              "1997  S0169260715300535   Recently there has been a growing interest in...   \n",
              "1998  S0169260716301614   Background and objective Lung sound auscultat...   \n",
              "1999  S0169260716000031   Background and objective This article present...   \n",
              "\n",
              "                                                    RHS  \n",
              "0     We generalize polar harmonic transforms for pa...  \n",
              "1     Design of a MATLAB based GUI tool for general ...  \n",
              "2     A markerless low cost system for the estimatio...  \n",
              "3     We adopt Distribution Field DF layer as featur...  \n",
              "4     A system that constructs multi instance bags f...  \n",
              "...                                                 ...  \n",
              "1995  We prove that twins look alike but they behave...  \n",
              "1996  A review about 3D automatic detection of pulmo...  \n",
              "1997  The paper firstly reviews a number of tools fo...  \n",
              "1998  A pattern recognition system to classify five ...  \n",
              "1999  Startle type responses were analyzed by multim...  \n",
              "\n",
              "[2000 rows x 3 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3d13aee8-6237-4c18-9bf2-fa13872bd93c\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>FileName</th>\n",
              "      <th>Abstract</th>\n",
              "      <th>RHS</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>S0262885614000857</td>\n",
              "      <td>This paper introduces four classes of rotatio...</td>\n",
              "      <td>We generalize polar harmonic transforms for pa...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>S0169260715300419</td>\n",
              "      <td>In this paper a MATLAB based graphical user i...</td>\n",
              "      <td>Design of a MATLAB based GUI tool for general ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>S0169260715003260</td>\n",
              "      <td>Background and objective A markerless low cos...</td>\n",
              "      <td>A markerless low cost system for the estimatio...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>S0262885613001443</td>\n",
              "      <td>This paper presents an improved multiple inst...</td>\n",
              "      <td>We adopt Distribution Field DF layer as featur...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>S0262885614000511</td>\n",
              "      <td>Text based image retrieval may perform poorly...</td>\n",
              "      <td>A system that constructs multi instance bags f...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1995</th>\n",
              "      <td>S0262885613001777</td>\n",
              "      <td>Identical twins pose a great challenge to fac...</td>\n",
              "      <td>We prove that twins look alike but they behave...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1996</th>\n",
              "      <td>S0169260715300298</td>\n",
              "      <td>This work presents a systematic review of tec...</td>\n",
              "      <td>A review about 3D automatic detection of pulmo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1997</th>\n",
              "      <td>S0169260715300535</td>\n",
              "      <td>Recently there has been a growing interest in...</td>\n",
              "      <td>The paper firstly reviews a number of tools fo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1998</th>\n",
              "      <td>S0169260716301614</td>\n",
              "      <td>Background and objective Lung sound auscultat...</td>\n",
              "      <td>A pattern recognition system to classify five ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1999</th>\n",
              "      <td>S0169260716000031</td>\n",
              "      <td>Background and objective This article present...</td>\n",
              "      <td>Startle type responses were analyzed by multim...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2000 rows × 3 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3d13aee8-6237-4c18-9bf2-fa13872bd93c')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-3d13aee8-6237-4c18-9bf2-fa13872bd93c button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-3d13aee8-6237-4c18-9bf2-fa13872bd93c');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "55113d18",
      "metadata": {
        "id": "55113d18",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc55551b-8fca-4004-bd5e-a270be78d07c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 2000 entries, 0 to 1999\n",
            "Data columns (total 3 columns):\n",
            " #   Column    Non-Null Count  Dtype \n",
            "---  ------    --------------  ----- \n",
            " 0   FileName  2000 non-null   object\n",
            " 1   Abstract  2000 non-null   object\n",
            " 2   RHS       2000 non-null   object\n",
            "dtypes: object(3)\n",
            "memory usage: 47.0+ KB\n"
          ]
        }
      ],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "27128688",
      "metadata": {
        "id": "27128688",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "288fb3d6-5f18-46e3-ce77-9f8f1c5b005a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FileName    0\n",
              "Abstract    0\n",
              "RHS         0\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "df.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3935d9cc",
      "metadata": {
        "id": "3935d9cc"
      },
      "outputs": [],
      "source": [
        "df_new=df.iloc[:,0:2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e8f6189",
      "metadata": {
        "id": "9e8f6189",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "outputId": "0cffeebe-5e28-4d6c-9e02-477011589c1a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "               FileName                                           Abstract\n",
              "0     S0262885614000857   This paper introduces four classes of rotatio...\n",
              "1     S0169260715300419   In this paper a MATLAB based graphical user i...\n",
              "2     S0169260715003260   Background and objective A markerless low cos...\n",
              "3     S0262885613001443   This paper presents an improved multiple inst...\n",
              "4     S0262885614000511   Text based image retrieval may perform poorly...\n",
              "...                 ...                                                ...\n",
              "1995  S0262885613001777   Identical twins pose a great challenge to fac...\n",
              "1996  S0169260715300298   This work presents a systematic review of tec...\n",
              "1997  S0169260715300535   Recently there has been a growing interest in...\n",
              "1998  S0169260716301614   Background and objective Lung sound auscultat...\n",
              "1999  S0169260716000031   Background and objective This article present...\n",
              "\n",
              "[2000 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8cbe70c9-f8b3-435e-8e83-bf987b43fddd\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>FileName</th>\n",
              "      <th>Abstract</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>S0262885614000857</td>\n",
              "      <td>This paper introduces four classes of rotatio...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>S0169260715300419</td>\n",
              "      <td>In this paper a MATLAB based graphical user i...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>S0169260715003260</td>\n",
              "      <td>Background and objective A markerless low cos...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>S0262885613001443</td>\n",
              "      <td>This paper presents an improved multiple inst...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>S0262885614000511</td>\n",
              "      <td>Text based image retrieval may perform poorly...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1995</th>\n",
              "      <td>S0262885613001777</td>\n",
              "      <td>Identical twins pose a great challenge to fac...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1996</th>\n",
              "      <td>S0169260715300298</td>\n",
              "      <td>This work presents a systematic review of tec...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1997</th>\n",
              "      <td>S0169260715300535</td>\n",
              "      <td>Recently there has been a growing interest in...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1998</th>\n",
              "      <td>S0169260716301614</td>\n",
              "      <td>Background and objective Lung sound auscultat...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1999</th>\n",
              "      <td>S0169260716000031</td>\n",
              "      <td>Background and objective This article present...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2000 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8cbe70c9-f8b3-435e-8e83-bf987b43fddd')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-8cbe70c9-f8b3-435e-8e83-bf987b43fddd button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-8cbe70c9-f8b3-435e-8e83-bf987b43fddd');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "df_new"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rhs=[]\n",
        "for i in range(2000):\n",
        "   b=[]\n",
        "   b.append(df.at[i,'RHS'])\n",
        "   rhs.append(b)\n",
        "   b=[]\n",
        "\n"
      ],
      "metadata": {
        "id": "ppS1j_O18WpR"
      },
      "id": "ppS1j_O18WpR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rhs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pTKkFxqu861D",
        "outputId": "8119e502-bfcb-4f29-b88e-889ed3a1971e"
      },
      "id": "pTKkFxqu861D",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['We generalize polar harmonic transforms for pattern description recognition. The generalization maintains beneficial properties of existing transforms. The completeness of the corresponding basis sets is proven. The numerical stability of the computation is discussed. The new generic transforms have superior performance to comparison methods.'],\n",
              " ['Design of a MATLAB based GUI tool for general biomedical signal processing and analysis. EEG and ECG signals can be processed and analyzed by the designed tool. The easy to use and easy to learn intuitive tool provides non linear chaos analysis. The tool provides entropy multivariate analysis with PCA and ICA pattern classification. The tool can also be utilized for training in biomedical engineering education.'],\n",
              " ['A markerless low cost system for the estimation of some spatial temporal parameters of human gait. A system usable on not instrumented treadmill. A system characterized by a very high level of automation. A system can be used to analyze up to one hundred steps. A system with precision comparable with the gold standard.'],\n",
              " ['We adopt Distribution Field DF layer as feature instead of traditional Haar like one to robustly model the target. We derive an online weighted geometric mean MIL classifier to select the most discriminative layers. Our tracker is more robust while needing fewer features than the traditional Haar like one and the original DFs one. The experiments show higher performance of our tracker than five state of the art ones.'],\n",
              " ['A system that constructs multi instance bags from text based retrieval order. Ensemble of MI classifiers is learned using these multi instance bags. We report image re ranking performance on multiple datasets. Our system receives on par or better results than the state of the art.'],\n",
              " ['ncappc performs i NCA and ii simulation based posterior predictive checks using NCA metrics. ncappc package is highly flexible and comprehensive. Can perform both individual and population level diagnostics. Produces output summarizing the diagnostic results including the model specific outliers. The output is easy to interpret and to use in evaluation of a population PK model.'],\n",
              " ['Nonlinear dynamic analysis shows sympathetic and vagal activities fluctuations. Hemodynamic analysis did not show any statistical differences between subjects. Blood pressure variability showed the highest differences between CON and SYN. In patients systolic blood pressure variability increased immediately after tilt up. Healthy women show a stable sympatho vagal balance during orthostatic phase.'],\n",
              " ['Integration of spatial planning and flood risk management has three dimensions territorial policy and institutional. Developing infrastructures sharing both data and models and systems is a promising way to support integration. This paper conceptualises the Spatially Integrated Policy Infrastructure SIPI which focuses on policy integration.'],\n",
              " ['Hghlights A novel local stereo matching algorithm with linear complexity is proposed. The overall algorithm generates the state of the art results. Two level local adaptation is introduced to guide the adaptive guided filtering. The novel post processing method handles both occlusions and textureless regions. A parallel algorithm is proposed to speed up the algorithm on GPU.'],\n",
              " ['We review experiments comparing humans and machines in NIST face recognition tests. We introduce the cross modal performance analysis CMPA framework. We apply the CMPA framework to the human machine experiments in the NIST tests. We propose a challenge problem to develop algorithms with human level performance.'],\n",
              " ['New method to calculate alpha angle for DDH classification from ultrasound 2DUS . New modality independent rounding index M to quantify acetabular rounding. Proposed indices tested on 114 scans and gave 95.7 AUC for normal vs dysplastic. Indices are modality independent and extendable to modalities like CT and MRI.'],\n",
              " ['Need for novel algorithms in diagnosis and treatment of cardiac arrhythmias. Combined development using MATLAB and C VTK library. Allows for rapid implementation of new algorithms. Interactive visualization of cardiac anatomy measured data and analysis results. Promotes interdisciplinary research and discussion.'],\n",
              " ['We propose a face recognition model based on albums in online social networks. Albums tend to be composed of many pictures of a small group of people. Limiting the set of unique labels and using album level features improve recognition. We present two systems to learn how best to use these social features. We validate our model on two datasets independently downloaded from Facebook.'],\n",
              " ['mPCA in ASMs can be used in medical and dental image analysis. Results provided by mPCA in initial studies appear to be sensible. Between and within subject variations are modelled correctly using mPCA. mPCA has more flexibility control and accuracy than standard PCA. mPCA is the correct method of combining sets of landmark points from different experts.'],\n",
              " ['Bayesian method provides resolution enhanced segmentation of human head. Segmentation classes incllude muscle bone fat air and skin. Tests were performed on 3D MR and CT images as well as registered MR CT images. The most successful results were obtained by the information fusion of MR and CT. Free parameters of the algorithm can be adjusted in a more systematic way.'],\n",
              " ['We study oxygen saturation by vascular occlusion test. NIRS camera captured 2D images of hand during ischaemia and in perfusion. We applied edge detection and segmentation to NIRS images. With the applied methods we can visualize the oxygenation changes. The method allows to quantitate and visualize oxygenation at the same time.'],\n",
              " ['Multifaceted aspects in time and time oriented concepts. Comparison of clinical data models in handling time. Ontologies of representation and reasoning about time in the clinical domain. Constructing the timelines for the medical histories of patients. Temporal concept coreference resolution problem.'],\n",
              " ['In emergency and crisis situations many communication channels can be unserviceable because of damage to equipment or loss of power. Thus data transmission over wireless communication to achieve uninterrupted network services is a major obstacle. The proposed middleware can sense its operating environment and optimize the spectrum usage to integrate the existing heterogeneous wireless communication systems. The proposed middleware was ported into an embedded system which is compatible with the actual network environment without the need for changing the original system architecture.'],\n",
              " ['Fundus image analysis provides great potentials for automatic cataract diagnosis. Ensemble learning is exploited for combining multiple models to improve the performance. Three independent feature sets i.e. wavelet sketch and texture based features are utilized. Two base learning models i.e. Support Vector Machine and Back Propagation Neural Network are investigated in the ensemble approach. The experiments demonstrate that our approach outperforms single model significantly.'],\n",
              " ['Innovative representation learning framework for breast cancer lesion classification. A hybrid CNN method to learn image based features in a supervised way. New breast cancer benchmarking dataset to support computer aided diagnosis methods.'],\n",
              " ['A navigation system was developed for pelvic tumor resection and reconstruction. A novel method for tracking the 3D printed patient specific implant was proposed. Both the phantom and clinical case study demonstrated the system feasibility. The high and stable accuracy regarding TRE demonstrated the system repeatability.'],\n",
              " ['Integerisation of weights obtained from iterative proportional fitting. Evaluation of five methods for integerisation. New method presented based on truncation replication and sampling TRS . The new method outperforms previously published integerisation strategies. Easily reproducible results using publicly available R code and data.'],\n",
              " ['NPF framework serves to catalyze the computer aided analysis of mammograms. It provides the radiologists with improved mammograms for precision in diagnosis. An extension of NPF for sharpening and edge enhancement of mammograms is presented. Integration of LIP model provides enhancement response in coherence to HVS. The obtained results are evaluated using CII PSNR CEM as image quality metrics.'],\n",
              " ['Developed a liver cancer prediction model for type II diabetic patients. Found the risk factors of liver cancer for type II diabetic patients. Built a web based application for liver cancer prediction for diabetic patient. Provided methods and model facilitating clinicians to remind those diabetic patients prevent liver cancer as early as possible.'],\n",
              " ['To formulate and to solve a multi objective optimal control problem applied to treatment of tumors. To minimize the cancerous cells concentration and the drug concentration. The results demonstrated that an optimal protocol can be obtained considering both objectives.'],\n",
              " ['A publicly available sleep dataset called ISRUC Sleep is introduced. It contains data of healthy and patient subjects with different characteristics. Two recording sessions per subject are included in subgroup II of the dataset. ISRUC Sleep were evaluated using the ASSC method SSM4S. Effects of sleep disorders diseases and medications are presented.'],\n",
              " ['Learn a discriminative dictionary with low rank regularization Fisher discriminant function is applied to the coding coefficients. IPM and ALM algorithms are adopted to solve our objective function.'],\n",
              " ['The BCCT.core is a computer program used for the esthetic evaluation of breast cancer conservative treatment. The BCCT.core software has been used by centres worldwide for clinical use and research. Several publications have confirmed its easiness of use and reproduciblity.'],\n",
              " ['We tackle the 3D model based tracking using the entire image and a textured 3D model. We propose a nonlinear optimization of the problem based on mutual information. Mutual information deals with the different modalities of real and virtual images. Our proposed method withdraws feature detection and matching issues. Results show the success of the approach and its precision.'],\n",
              " ['A novel remote quantitative Fugl Meyer assessment FMA framework was proposed for stroke patients. Seven training exercises were designed to represent the upper limb related 33 items in FMA scale. Ensemble machine learning and RRelief algorithm were applied to establish the quantitative assessment model. The proposed framework has been implemented in both clinical and home settings.'],\n",
              " ['robust scale estimation without a breakdown point insensitive to inlier noise distribution efficient and practical application to many computer vision problems'],\n",
              " ['Comparative analysis of systems with different feature sets. HOS features extraction for psoriasis images. Largest set of mathematical features ever computed for psoriasis images. Understanding the reliability analysis of the CADx system. Accurate system with 100 accuracy and 100 sensitivity and specificity.'],\n",
              " ['A novel approach to study Urban Airborne Disease spreading. Integrate as much information as possible in a simulation environment. GIS enabled city modeling and travel routing calculation. Synthesizing the population in a city and modeling people s daily behavior in the presence of epidemics.'],\n",
              " ['Computer aided cephalometric systems tend not to be practical and intuitive. A new haptic enabled landmarking approach for 3D cephalometry is proposed. Several experimental tests were conducted to evaluate the proposed approach. Haptic technologies facilities the landmark selection process in 3D cephalometry. The haptic user interface allows the user to feel and touch the virtual patient s skull.'],\n",
              " ['A DLT like algorithm for Euclidean upgrading from segment lengths is proposed. The constraint equation is parameterized in a simple way. The plane at infinity is directly extracted from the intermediate results. This algorithm has higher accuracy than the existing linear algorithms. Weighting and nonlinear refinement further improve the accuracy.'],\n",
              " ['We propose two strategies for face recognition through multiple features. Our methods integrate multi feature fusion and dictionary learning. The fusion process and dictionary learning are learned simultaneously. Extensive experiments validate the merits of our methods.'],\n",
              " ['Introducing a stochastic model for simulating eye movements Implementing the proposed model Implementing an algorithm for predicting saccades and fixations'],\n",
              " ['We constructed a new region based pressure force RBPF function. We proposed a novel level set based model for inhomogeneous image segmentation. We combine the local and global intensity information adaptively. The local image information can significantly increase image contrast. We apply a fast and simple level set method to implement the curve evolution.'],\n",
              " ['R package dfcomb available for phase I combination clinical trials according to previous methodological paper. Implement a single prospective clinical trial or simulation studies of phase I combination trials in oncology. Features and illustration of the functions. Promote the use of innovative adaptive design for early phase combination trials.'],\n",
              " ['Coronary risk stratification. Machine learning to link two arteries. 56 grayscale features. Coronary tissue characterization. Classification accuracy 94.95 and AUC 0.95.'],\n",
              " ['Content creation is a very time demanding task. RadEd is a web based framework with a smart editor to create image based exercises. RadEd provides teachers functionalities to prepare more realistic cases. RadEd allows learners to make more specific diagnosis.'],\n",
              " ['The clinical requirement for the early diagnosis of malignant skin lesions from images is introduced and justified. An up to date review about the proposed techniques for the image segmentation of pigmented skin lesions is presented. Additionally the tasks related to image acquisition and pre processing are also taken into account. The techniques are introduced classified and some examples of their results are illustrated and discussed. This review is of interest both for researchers and for health professionals.'],\n",
              " ['We propose a workflow centric network architecture for teleradiology applications. We optimize inspection assignments to radiologists for medical reporting process. Subspecialty response time and workload parameters are improved for assignments. Solution gives better results for workflow efficiency compared to manual assignment. Architecture was tested in 52 sites and 3.35 million inspections were processed.'],\n",
              " ['We propose an ensemble dictionary learning EDL framework for saliency detection. The saliency detection within this framework is treated as a novelty detection problem. A novel dictionary atom reduction is proposed for boosting the distinctness of salient region. A good probabilistic interpretation is with the proposed EDL model.'],\n",
              " ['We model the common 3D shape descriptor from Kinect like depth image. We evaluate the 3D shape descriptor in object recognition. We introduce 3D shape descriptor performance frameworks. Shape distribution and local spin image outperformed other 3D shape descriptors.'],\n",
              " ['We enrich face detection model by hierarchical structure and part subtype. We propose to explore the face body co occurrence to improve face detection. We achieve state of the art performance on FDDB AFW and a self annotated dataset.'],\n",
              " ['A framework including data selection task assignment and annotation combination stages for a confidence based benchmark dataset for retinal image processing is proposed. A novel task assignment is used to remove data and reader biases. The annotation of readers is combined based on their accuracy and performance. The framework is used to build a confidence based benchmark dataset for cyst segmentation. The generated benchmark can be used to reliably evaluate cyst segmentation methods.'],\n",
              " ['Data fusion based method for activity recognition using multiple views Straightforward architecture to incorporate new cameras or new features Performance generally increases when there are more cameras and features. Comparable performance with that produced by reconstruction Detailed experiments to answer different system considerations'],\n",
              " ['We developed a function written the R software performing Q TWiST analyses. Three health states are defined toxicity relapse and TWiST. Inputs are checked and descriptive comparative and graphical results are provided. Several arguments take the opportunity to add more options to the analyze.'],\n",
              " ['A novel use of the ORD feature to both globally and locally characterize hands A dataset for hand gesture classification and fingertip localization is proposed. We reduce the search space of fingertip locations conditioned to a hand gesture. A new cost function to solve the graph matching problem of fingertip localization'],\n",
              " ['Acceleration of iterative Compton camera reconstruction using graphics processing unit. Exactly matched pair of conic projector and backprojector. Significantly reduced reconstruction time without loss of accuracy. Improvement of reconstruction accuracy in advanced iterative methods.'],\n",
              " ['2D and 3D face recognition systems are vulnerable to mask spoofing. Countermeasures are proposed based on reflectance texture and depth analysis. Reflectance analysis provides best performance to detect 3D mask attacks. Fusion of countermeasures increases the mask spoofing detection performance. Classification accuracy of 99 almost perfect is reached to detect mask attacks.'],\n",
              " ['We study monochromatic cues in the modeling of bottom up attention. We propose a novel monochromatic cue for ROI detection. Experimental results demonstrate the effectiveness of our method.'],\n",
              " ['Goal new approach to enable global registration of large collections of point sets. We consider an optimization on a manifold for global registration of multiple scans. We evidence computational and convergence issues in the original approach. We propose computationally effective correspondence update and other improvements. Results better accuracy compared to state of the art good computational performance.'],\n",
              " ['An epidemiologic system analysis of cardiovascular risk is presented through a Bayesian network model. The Bayesian network model can serve as a generic tool for application oriented activities explanation prediction monitoring and prevention. Due to cardiovascular disease is multifactorial the application of this kind of model is of special interest both from theoretical and practical point of view. The induced Bayesian network was used to make inferences taking into account three reasoning patterns causal reasoning evidential reasoning and intercausal reasoning.'],\n",
              " ['An alternative adaptive geographical masking method referred to as Adaptive Areal Elimination AAE is proposed. K anonymized areas can be safely disclosed as well without increasing the risk of re identification. For an actual K anonymity random perturbation of AAE preserves the spatial characteristics better the other masks.'],\n",
              " ['A method for the study of enzyme kinetics in the presence of two inhibitors. Integrated Michaelis Menten equations were developed to carry out the analysis. This methodology is useful to discriminate the inhibition mechanisms. This method can be automated and applied in real time. An Excel template has been developed available in Appendix .'],\n",
              " ['We review the quantitative and qualitative behaviour of 22 infectious diseases caused by viruses. The information is compared and visualized by means of Multidimensional Scaling technique. The results are consistent with clinical practice. The proposed mathematical tools tackle a large number of virus and information.'],\n",
              " ['Surveys the feature description methods and the learning algorithms employed. Also surveys the ECG signal preprocessing and the heartbeat segmentation techniques. Description of databases used for methods evaluation indicated by the AAMI standard. Discussion of limitations and drawbacks of the methods in the literature. Concluding remarks and future challenges are also pointed out.'],\n",
              " ['Symbolic entropy approximate entropy and fuzzy entropy are used to measure the irregularity of knee vibroarthrographic signals. Mean standard deviation and root mean squared value parameters of the signal envelope amplitude are computed to characterize the signal fluctuations. Gender dependent statistics of entropy and envelope amplitude parameters are discussed.'],\n",
              " ['A theoretical model of urban networks is developed linking connectivity to city sizes. The theoretical model predicts inter city links scale linearly while same city connections exhibit super linear scaling. The data for the U.S. New England and South Korea support the theoretical model s predictions. Agent based simulations are conducted to identify the conditions that preserve the diversity of norms in scale free networks.'],\n",
              " ['A genetic programming GP approach is proposed to design a combined evaluation measure for image segmentation algorithms. The proposed approach improved the performance of image segmentation evaluation. A new fitness evaluation scheme is proposed for GP. A new evaluation measure based on intra region similarity and inter region disparity is proposed in this paper.'],\n",
              " ['Spare feature selection method based on l 2 1 2 matix norm is proposed. Graph Laplacian based semi supervised learning is exploited. A effective algorithm for optimizing the objective function is introduced. The convergence of the algorithm is proven. Experiments demonstrate that the method is suitable for web image annotation.'],\n",
              " ['Similar coarse hair probability maps should correspond to similar segmentations. Data driven Isomorphic Manifold Inference is proposed to exploit the shape priors. We integrate the inferred shape color and texture into a unified framework. Besides hair segmentation we also validate our IMI on horse class segmentation. Experiments on hair and horse databases show impressive performances.'],\n",
              " ['Target problem of pain classification and localization using weakly labeled pain videos Algorithm combines multiple instance learning with multiple segment representation. Rigorous experiments show that our algorithm achieves state of the art performance. Empirically evaluate the contributions of different components of our algorithm Visualize discriminative facial patches as learned by our algorithm'],\n",
              " ['Multipath Interference MpI is a non systematic error in Time of Flight ToF imaging. MpI causes severe distortions in ToF measurements. We propose a ray tracing model that renders realistic ToF images including MpI. We use nonlinear optimization to find the scene that best renders the input image. We present results with real and synthetic datasets. Error is reduced in all cases.'],\n",
              " ['We constructed a model describing event related potential ERP by trend model. A 400 particle filter produced the best mean square error in the ERP estimation. The filter reduced an amount of average by 42.8 compared with simple averaging. The filter could estimate P300 robustly by application to EEG in P300 speller. Real time processing is realized in any computer with an appropriate particle number.'],\n",
              " ['Interactive online mapping tools are reviewed with workflows for site production. The mapping case studies illustrate best practice alongside the methods used. All the case study sites received tens of thousands of visitors. Exploratory interactive mapping functionality is increasingly extensive. Analytical functionality is more limited but there are signs of innovation.'],\n",
              " ['This is a novel work to solve camera array auto focusing and occluded object imaging simultaneously. A unified framework is proposed to achieve seamless interaction between focusing and imaging. An optimization algorithm is presented to dynamically estimate the focus plane. A visibility analysis is proposed to remove the occluder and improve the imaging quality.'],\n",
              " ['A simple linear method to self calibrate stationary zooming cameras Neither special camera motion nor scene knowledge required Two zoom images of a stationary camera provide two parallel principle planes. The plane at infinity can be located from two or more such zooming cameras. Simulation laboratory and real scene experiments with 3D measurements validate our method.'],\n",
              " ['We propose an interactive method combining semi supervised learning SSL and active learning AL for segmenting Crohns disease affected regions in MRI. A novel query strategy for AL has been proposed that makes use of context information to identify query samples. Compared to fully supervised methods we obtain high segmentation accuracy with fewer samples and lesser computation time. Our method has the potential to be used in scenarios which pose difficulties in obtaining large numbers of accurately labeled data.'],\n",
              " ['We assessed different medical data mining approaches to predict ischemic stroke. Grid search were used for improving classification performance of the models. The accuracy and AUC values were higher than 0.8947 and 0.8953 respectively. SVM and SGB models yielded remarkable performance for classifying ischemic stroke.'],\n",
              " ['A comprehensive evaluation of STIP based features on depth based action recognition Two schemes to refine STIP features for a deeper understanding of their behaviors A fusion approach is developed which outperforms many state of the art methods.'],\n",
              " ['A new quality assessment method for AS OCT images using wavelet based LBP features. It is a first work so far there is no objective assessment of AS OCT image quality. The proposed algorithm does not require any additional information from the AS OCT device. This work aimed at collecting high quality AS OCT images for Glaucoma diagnosis. Our proposed quality index score is similar to the quality assessment of Glaucoma experts.'],\n",
              " ['A semi automatic method was proposed to assess the cardiac LV diastolic function. A non rigid registration was applied to track the endocardial boundary points. LV volume and filling rates are computed over the entire cardiac cycle. The peaks of early and late ventricular filling waves were detected automatically. Experimental evaluations were performed over CMR and echo datasets from 47 subjects.'],\n",
              " ['We propose an information theoretical measure for biometric systems. We present several useful properties and interpretations of the measure. We prove that the measure can be approximated by the relative entropy. We discuss how to evaluate the measure and show experimental evaluations.'],\n",
              " ['We proposed a novel computational approach for modeling the normal elderly subjects s brain age by connectivity analyses of networks of the brain. Principal component analysis PCA is applied to reduce the redundancy in network topological parameters. BP artificial neural network BPANN is improved by hybrid genetic algorithm GA and Levenberg Marquardt LM algorithm to model the relation among principal components PCs and brain age. The method has shown good performance for old cohort with limited samples.'],\n",
              " ['Permutation entropy PE is a fast method to evaluate the irregularity of signals. PE does not consider the average of amplitude values and equal amplitude values. We propose amplitude aware PE AAPE . We evaluate the AAPE method for signal segmentation and spike detection applications.'],\n",
              " ['Novel automatic cardiac T2 relaxation time estimation algorithm from MR images. Novel automatic ROI segmentation algorithm in cardiac MR images. Segmentation results from our technique are very close to the experts opinions. Cardiac T2 values from our technique are very close to that calculated by experts. Our method does not need manual processes in ROI segmentation and T2 calculation.'],\n",
              " ['The wealth of knowledge laying in biomedical publications is invaluable to make new scientific discoveries. Domain specific search engines aim to improve retrieval performance and to enhance user search experience. The BIOMedical Search Engine Framework aims to speed up the construction of biomedical domain specific search engines. The BIOMedical Search Engine Framework enables the retrieval and annotation of biomedical documents and the customised visualisation of indexed concepts and documents. The BIOMedical Search Engine Framework supported the construction of the Smart Drug Search SDS in assistance of antimicrobial research.'],\n",
              " ['We analysed neutral stress modified and rhymed speech in Parkinson s disease. We proposed quantitative prosodic analysis of poem recitation task. We showed rhythmical demands improve identification of hypokinetic dysarthria. We introduced a concept of permutation test in dysarthric speech analysis.'],\n",
              " ['Novel matching strategies for histogram based descriptors are presented. Global dominant orientation is used by exploiting the image context. A new 3D extensible framework to evaluate feature descriptors is introduced. 2D 3D comparisons with state of the art rotational invariant descriptors are reported. Results show the effectiveness of the proposed matching approaches.'],\n",
              " ['Regular texture patterns exhibiting translational symmetry were modelled. Model comparison was implemented using Bayes Information Criterion. Texel lattice geometry was estimated from e.g. images from a textile archive. Accuracy was better than other methods with which it was compared.'],\n",
              " ['A novel method to detect eye blink artifacts from a single channel frontal EEG signal was proposed. Overall accuracy of detecting epochs contaminated by eye blink artifacts was markedly increased. An online experiment showed that our method is useful for headband type wearable EEG applications. A MATLAB package of our library and sample data is open for free download.'],\n",
              " ['A phonetic modeling approach for unsupervised sequentiality of dynamic static SUs. Model based sign segmentation and subunit modeling with HMMs. Construction of a sign to subunits data driven lexicon. Comparison of different signers pronunciations and unseen signer pronunciation compensation. Evaluation on data from three corpora two sign languages and unseen signers.'],\n",
              " ['A 3D model is developedto study how force induced substrate controls cell differentiation and or proliferation. Cells are cultured within 3D Neo Hookeanhyperelastic hydrogels with encapsulated magnetic nanoparticles. Internal forces can play an outstanding role in remotelycontrolling the lineage specification of MSCs and cell proliferation. A new perspective to delineate the role of force induced substrates in controlling the cell fate.'],\n",
              " ['Novel particle filtering based visual ego motion estimation algorithm robust to abrupt camera motion. Multi layered importance sampling via particle swarm optimization PSO Reformulation of the conventional vector space PSO algorithm by geometric special Euclidean group SE 3 Efficient convergence of PSO and real time visual ego motion estimation performance'],\n",
              " ['In this work glaucoma identification is done using wavelet features of optic disk. Wavelet features are extracted from segmented and blood vessels removed optic disk. Several machine learning algorithms are used for prominent feature selection. Genetic algorithm is used to reduce the dimensionality of feature vector. Accuracy of glaucoma identification achieved in this work is 94.7 .'],\n",
              " ['Multi Kernel Appearance Model is a new facial point detector. Multi Kernel SVM combines multi resolution features. A SVM cascade combines increasingly complex kernels to reduce the computational time. A shape model fitting introduces constraints between SVM detections.'],\n",
              " ['Methods for automatic detection of abnormal vital signs occurring during anesthesia are poorly documented in the literature. Existing methods are not reproducible between databases. We have developed a data model and an algorithm that allow automatic detection of abnormal values of vital parameters occurring during anesthesia. Predefined thresholds and various parameters such as time between measurements and time spent outside predefined thresholds provide adaptability to various clinical situations e.g. hypotension occurring after start of anesthesia. The relation between occurrence of abnormal values of vital parameters and mortality and length of stay may then be studied on a large and automated scale.'],\n",
              " ['A real time OBS was proposed for the real time removal of ballistocardiogram artifacts. The real time OBS performed better than rtAAS in the real time removal of BCG artifacts. A real time analysis during a simultaneous EEG fMRI acquisition is essential to achieving neurofeedback.'],\n",
              " ['Human centric boundary extraction criteria and new boundary model. A novel boundary visualization method though a what material you pick is what boundary you see approach. Point to material distance measure. A complete application.'],\n",
              " ['Integrative approaches for the study of biological systems have gained popularity.. There is lack of integrative simulation techniques to assess systems biology methods. InterSIM bridges this gap by simulating complex interrelated genomic data. InterSIM allows researchers to evaluate and test new integrative methods. The software tool InterSIM is implemented in R and is freely available from CRAN.'],\n",
              " ['The texture information is leveraged to complement color in image matting. The proposed method combines color and texture information in matting process. The proposed combined method presents the best among current matting methods. The proposed method has first ranks with respect to MSE and Gradient error. Several experiments are carried out to reveal potential power of texture in matting.'],\n",
              " ['We propose ANAlyte an automated tool for HEp 2 image analysis. ANAlyte is a valid solution to the problem of ANA test automatization. We integrate modules for the full characterization of HEp 2 slides. We propose a new technique for HEp 2 intensity characterization. Our tool is validated on two different public benchmarks of HEp 2 images.'],\n",
              " ['A new 3D spatial field for visualizing constricted tubular structures is proposed. Constrictions are quantified by new measures using the localized structure analysis. A new transfer function maps the degree of constriction to color and opacity. Our method provides intuitive adjustment of the visual appearance of constrictions. Our method can be applied to various constrictions with relevant parameters.'],\n",
              " ['A statistical method of forecasting housing stock variability from average density A unique method for studying the variability of dwellings by type and age bands The 3D tile typologies include both dwelling characteristics and the plot sizes. The tiles are useful for estimating the potential for sustainable technologies. The outputs have been validated at against housing survey and footprint data.'],\n",
              " ['A novel measurement system can be used to measure ankle ROM and perception in multiple planes under weight bearing condition. The programmable graphic user interface and data management for this developed system with excellent validity was established. The proprioception measures JPS DE JPS AE and JPS VE for differentiating the ankle perception deficits were demonstrated.'],\n",
              " ['Face and iris authentication on mobile Fusion strategy driven by response reliability Spoofing detection for mobile face recognition Best sample selection for higher accuracy Optimization for Android'],\n",
              " ['We have developed a freely available software package for semi automated tracking of muscle fascicles in B mode ultrasound image sequences. Includes features to track multiple fascicles in multiple regions of the image and to correct for measurement drift with time. The software is available for Windows and MacOS as a standalone program or source code.'],\n",
              " ['Two new codebook based methods for writer identification are proposed. The introduced code extraction methods are very efficient. Optimal parameter values for English and Farsi handwritings are determined. The method is compared with the existing methods comprehensively. The proposed method outperforms existing methods for Farsi and English languages.'],\n",
              " ['A powerful texture descriptor is developed for texture classification. The descriptor is built via fractal analysis on the local binary patterns. The descriptor enjoys both high discriminative power and robustness. The descriptor is compact and computationally efficient. The descriptor demonstrated excellent performance on four datasets.'],\n",
              " ['The symmetric weight scheme allows reducing the computation time by a factor 2. Our implementation of NL Means reduces computation time by a factor of 510. Self adaptive method is proposed to allow the filter to be more efficient.'],\n",
              " ['We model filtered image responses with the Weibull distribution. We represent images as points on the 2D Weibull manifold. We solve image processing problems as optmisation on the manifold. Application to automatic image focusing'],\n",
              " ['We propose an online method for tracking dense crowds capable of handling anomalies An alternative to existing methods which require modeling of crowd flows Introduce the notion of prominent individuals for tracking dense crowds Employ a new model Neighborhood Motion Concurrence to predict individual s position Comparison with existing methods using ground truth for eight sequences'],\n",
              " ['Introduced a set of experiments to judge the biological plausibility of visual saliency models. Introduced a novel method to evaluate saliency map comparison metrics using a database of human fixation maps. Employed the introduced method to identify the best saliency map comparison metric. Examined nine well known models of visual saliency using the best metric to identify the best visual saliency models.'],\n",
              " ['We propose a novel method for vehicle appearance modeling and matching. A key novelty is automatic collection of good observations for matching. Our method is robust in real world scenarios. Our method outperforms more complex object matching methods. Our method is data and computationally efficient deployable on smart cameras.'],\n",
              " ['We analyze the load distribution through the wrist joint with several arthrodesis treatments. We used the rigid body spring model method on a 3D model of the wrist joint. We found the load distributions on each carpal articular surface of radius for each treatment. Obtained results allowed comparing the different treatments and their efficacy under static conditions.'],\n",
              " ['Present the use of computational spatial optimisation framework to enable planners to identify optimal spatial plans in the presence of competing sustainability objectives Application over case study provides a set of best trade off spatial plans Many optimised spatial plans outperform local authorities development plan Rich set of diagnostic information provides an evidence basis to assist planners to achieve more sustainable patterns of development'],\n",
              " ['The peristaltic flow of a copper oxide water fluid investigate the effects of heat generation and magnetic field. The mathematical formulation is presented the resulting equations are solved exactly. The obtained expressions for pressure gradient pressure rise temperature velocity profile are described through graphs for various pertinent parameters. The streamlines are drawn for some physical quantities to discuss the trapping phenomenon.'],\n",
              " ['To develop an integrated linear nonlinear feature selection technique to identify the determinants of sustained HAART. To generate a hybrid model based on rough set classifiers to verify its performance. To create a relevant decision rule set of an LEM2 algorithm for specialist physicians as a diagnosis reference. To effectively offer the study findings and results from the given data set to relevant medical institutions and patients.'],\n",
              " ['A new realistic facial expression recognition FER database Compared feature and performance differences between lab based and realistic data Examined factors that affect FER performance regarding data feature and training'],\n",
              " ['A feature selection framework is proposed to achieve high performance model free gait recognition. The feature selection mechanism relies on the Random Forest algorithm. Regions selected are more robust to covariates while reducing the computational cost. Panoramic gait recognition is achieved under covariate conditions.'],\n",
              " ['Easy data collection in any 3D feature rich scene with any calibrated camera Ground truth acquired automatically by high resolution structure from motion Rigorous 2 view geometry evaluation with many reduced resolution test samples More elaborate rigid geometry tests are trivial to devise given 3D ground truth.'],\n",
              " ['A tracking algorithm which exploits both appearance and geometric information Two complementary features are adopted in building the appearance dictionary. A shape context approach to capture the stable geometric patterns of keypoints The proposed tracking algorithm performs well in challenging conditions.'],\n",
              " ['A novel method for joint tracking and fine object segmentation in videos Efficient integration of EM based tracking and Random Walker based image segmentation No strong constraints are imposed on the target appearance or the camera motion. Experimental evaluation against a large collection of state of the art methods Explicit and efficient fine object segmentation facilitates drift free tracking.'],\n",
              " ['iSS Hyb mRMR model is proposed for identification of splicing sites. Trinucleotide and tetranucleotide composition are used as feature extraction schemes. Hybrid space is formed by using TNC and TetraNC spaces. Various classification algorithms are analyzed. mRMR is utilized to reduce feature space.'],\n",
              " ['We present the first 3D dynamic spontaneous facial expression database. Meta data include FACS coding head pose data and 2D 3D landmarks. We present an effective emotion elicitation protocol using eight tasks. The database is analyzed by self report observers rating and AU annotation. The database is validated through expression AU recognition and pain analysis.'],\n",
              " ['This study proposed and examined a new approach employing the light sharing PET detector configuration with thick light guide and GAPD array having large area microcells for high effective quantum efficiency. The number of counted photons was considerably increased and the LCE of 40 could be obtained with the detector configuration. The flood histogram showed good separation and wide margins between the spots of the scintillators. Moreover a considerable improvement in the energy linearity performance was observed and severe saturation effects could be clearly avoided. This study demonstrated that GAPDs with large area microcells which have not been actively studied as PET photosensors because of their non linearity properties could be utilized for PET applications and even high photon flux regime.'],\n",
              " ['We propose a new depth enhancement method for RGB D sensors. It extends the fast marching method to incorporate color and depth information. It outperforms state of the art local methods in terms of visual and metric qualities. It achieves visually comparable results to time consuming global methods. It provides better inputs to the applications based on RGB D sensors.'],\n",
              " ['We simulated five motor units for normal neurogenic and myopathic cases. We investigated the effect of recording sites on electromyography signals. Myopathic cases demonstrated the most prolonged phase duration near the tendon. Highest number of peaks are observed near the tendon in myopathic conditions. A new feature referred as number of peaks spike duration was defined.'],\n",
              " ['We propose a novel framework for a world s first LF based 3D telemedicine system. We develop an algorithm to convert the LF into 3D models with high levels of details. We design a cross platform solution for different systems and platforms. We develop a demo platform with multidiscipline 3D telemedicine applications.'],\n",
              " ['Wear behaviors of the total hip replacement THR with various abduction angles are investigated. The current finite element approach can improve the computational efforts without significant loss of data precision. The THR with larger abduction angles may produce deeper depth of wear but the volume of wear presents an opposite tendency.'],\n",
              " ['We developed a system that estimates in real time the articulated pose of the hand. Our approach is discriminative with low computational load and fast error recovery. Our system is robust to occlusions implicitly extracting information from them. The system is thoroughly evaluated with quantitative and qualitative experiments.'],\n",
              " ['We propose an algorithm for road traffic congestion estimation from video scenes. We compare between macroscopic and microscopic parameters in terms of accuracy. The method proposed is accurate and it is computationally inexpensive. It does not require segmentation or tracking of vehicles. It is robust towards illumination changes.'],\n",
              " ['The performance of focus measure depends on the optics and imaging conditions. The concept of reliability in focus measure is introduced. A method for computing the reliability of shape from focus is presented. The proposed reliability integrates efficiently to shape from focus. The proposed method is experimentally effective.'],\n",
              " ['The system uses complex numerical simulations in an online e learning environment. Complex calculations are performed on the Web server as a CGI application. The user enters simulation parameters on the browser which are sent to the server. The simulation result is re transferred as a graphical representation. Two examples were realized glucose insulin homeostasis and basic neurophysiology.'],\n",
              " ['We divide a video sequence on overlapping subsequences. On each subsequence we perform over segmentation handling efficiently asynchronous trajectories. The number of moving objects is automatically estimated. The segmentation results are aggregated into a final segmentation. Our method is tested on the Berkeley motion segmentation benchmark.'],\n",
              " ['The pulsatile MHD third grade blood flow through arteries is analytically studied. The influence of externally imposed periodic body acceleration is considered. Analytical formulas for velocity profile wall shear stress and flow rate are given.'],\n",
              " ['Appearance based gaze estimation with head motion is decomposed into subproblems. Subproblems are solved by compensating for two types of estimation biases. The compensation method only requires a 5 second video for training. Eye images for different head poses are aligned via rectification and optimization. We achieve a gaze estimation accuracy of 3 with free head motion.'],\n",
              " ['Novel multi sector framework for integrated local scale land use modeling. We apply deductive theory based and inductive data driven methods to describe land use change. We incorporate hedonic pricing net present value and statistics based methods. Utility based approaches outperform common statistics based approaches in our Dutch case study. Output from other research methods can be incorporated to describe utility of land for other uses.'],\n",
              " ['Novel algorithm for large scale human pose estimation problems. Uses multiple Gaussian processes in a mixture of expert framework. Allows the accurate regression of Gaussian processes to be scaled to large data. Algorithm gives state of the art performance on 3 pose estimation data sets.'],\n",
              " ['We present a method for perspective recovery of text in natural scenes. It relies on the characters geometry to estimate a rectifying homography. The proposed method is efficient and fast. Comparative results show improved recognition accuracy against the state of the art.'],\n",
              " ['This paper proposes a novel person face image representation. The representation avoids the use of body segmentation or image normalization. The representation relies on the combination of BIF and Covariance descriptor. The representation can handle background and illumination variations. The matching rate at rank 1 on VIPeR is 31.11 and the accuracy on LFW is 84.48 .'],\n",
              " ['We propose a more accurate similarity measurement for object retrieval. Our method improves two features of the BoW model. Spatial expansion can incorporate more latent visual words into a query. Visual word re weighting can increase weights of reliable visual words. The combination of them can improve both precision and recall.'],\n",
              " ['Coronary artery disease risk assessment in intravascular ultrasound. A link between carotid and coronary grayscale plaque morphology. Principal component analysis PCA for dominant feature selection. Classification accuracy of 98.43 and reliability index of 97.32 .'],\n",
              " ['Robust learning and fusing of orientation appearance models. Combination of image gradient orientations with the directions of surface normals. Use of a robust kernel based on the Euler representation of angles. Performing 2D plus 3D rigid object tracking achieving robust performance.'],\n",
              " ['A method for embedding patient s information into medical image is proposed. Two coding methods have been utilized to embed the EPR and improve imperceptibility. Cost optimization function is contributed to enhance the quality of the stego image. The proposed system is robust against textural feature steganalysis.'],\n",
              " ['Study of gender recognition from neutral expressive and occluded faces Comparison of global local approaches grey level PCA LBP features and three classifiers Three statistical tests over two performance measures are employed to support the conclusions. Local models surpass global ones with different types of training and test faces. Global and local models perform equally with the same type of training and test faces.'],\n",
              " ['Data driven model is presented for the response of glucose to food intake. Model describes blood glucose dynamics in people with and without diabetes. First study introducing a continuous data driven nonlinear stochastic model. Model s parameters belong to different ranges for diabetes and controls. Variational Bayesian learning approach was employed for model identification.'],\n",
              " ['Propose an online algorithm t GRASTA for the transformed robust PCA problem. Use a union of subspaces to approximate the nonlinear subspace learning process. Demonstrate the fully online mode of t GRASTA with videos with camera jitter.'],\n",
              " ['A 2 D cross correlation based scheme is introduced for classification of THz signals. This work establishes a general way for assessing performance of other THz datasets. The proposed approach yields better performance than the existing method. The results confirm the superiority in classification accuracy of the MLR and KNN. It advances the wider proliferation of automated THz signals across new applications.'],\n",
              " ['We present the problem of person re identification Re ID and associated challenges. A methodology based taxonomy and survey of current approaches is presented. We identify open and closed set re identification scenarios. Public datasets and current evaluation techniques for Re ID are discussed. Unaddressed issues like open set Re ID and long period Re ID are also discussed.'],\n",
              " ['We use rotation and reflection symmetries for structural shape description. The posterior shape similarity enhances the shape matching performance. Test results on public dataset show increased retrieval accuracy. Any other previous or future shape representation can be combined with our method.'],\n",
              " ['We address a novel boosting algorithm by taking advantage of Universum data. A greedy stagewise functional gradient procedure is taken to derive the method. Explicit weighting schemes for labeled and Universum samples are provided. Practical conditions to verify effectiveness of Universum learning are described. This algorithm obtains superior performances over AdaBoost with Universum data.'],\n",
              " ['Our aim was to evaluate the use of fractional order FrOr modeling in asthma. We compared FrOr models with traditional parameters and an integer order model FrOr parameters best fit the data showing good associations with spirometry. They also showed a high accuracy in the detection of the mild obstruction in asthma FrOr models provide meaningful information in asthmatic patients.'],\n",
              " ['A wireless blood pressure measuring device used together with a smart mobile device was developed. The smart mobile device was used as an indicator and a control device. The cuff communicating with this device through Bluetooth was designed to measure blood pressure on the arm. Digital filter was used on the cuff instead of classical analog signal processing and filtering circuit. The test results showed that higher accuracy could be achieved with the device developed.'],\n",
              " ['The HEGM method for classifying hand postures with a hit rate of 97.08 on average over uniform and complex backgrounds. This method allows computing only features corresponding to highly discriminative nodes thus decreasing computing time. A semi automatic technique to annotate bunch graphs is described which is efficient and leads to faster graph creation.'],\n",
              " ['A new color texture feature is constructed. Multiphase successive active contour model MSACM is proposed. MSACM can be discretely optimized by multilayer graph. The combined energy of local and global is iteratively optimized. Performances are assessed through the qualitative and quantitative comparisons.'],\n",
              " ['A new fingerprint feature representation. The proposed fingerprint fuzzy vault scheme has three layers of security. The hardened fuzzy vault scheme produces different templates. The hardened fuzzy vault scheme can resolve the problem of cross matching attacks. The results show better user authentication performance with FRR 6.31 at FAR 0 .'],\n",
              " ['Discover certain regularities at the primitive concept level. Improve the efficiency of the discovery process and express the user s preference. Develop high quality attribute oriented and rule based models. Find the causes for solitary pulmonary nodule and results of the long term treatment.'],\n",
              " ['Introduce the concept of nonverbal communication computing and it use cases Review motion analysis techniques used for nonverbal communication computing Discuss future directions of this area'],\n",
              " ['We robustly fit deformable 3D face models on facial images. We estimate the 3D position 3D orientation shape and actions of faces. We quickly and robustly initialize user specific face tracking approaches. Our approach outperforms others under challenging illumination conditions. Our approach runs in real time in smartphones and tablets.'],\n",
              " ['We perform automatic modeling of building facades from single view stereo. Goal is to provide semantic landmarks for mobile platform localization. Use of novel appearance model with stereo features and plane fitting to disparity. Facade segmentation by graphical model hierarchical Markov Random Field. Good performance in segmenting and modeling major facades in the scene.'],\n",
              " ['A stylized Pigovian intervention into Schelling s classic segregation model. A bottom up computational approach to modelling interactions between private preferences and public institutions. A technical attempt to address the relations between race space and planning.'],\n",
              " ['We demonstrate the unification classification and validation of online POI data. The classified POI data is used to disaggregate land use at a high spatial resolution. The disaggregated land use data is useful for LUTE models and for economic analyses.'],\n",
              " ['A novel matched filter approach with the Gumbel PDF as its kernel is proposed. Pre processing includes PCA based gray scale conversion and contrast enhancement. Post processing includes the entropy based optimal thresholding and length filtering. On the basis of exhaustive experiment select the appropriate value of parameters.'],\n",
              " ['We present a method for the recognition of complex actions in a unified way. We encode simple action HMMs within the stochastic grammar that models complex actions. As input our method receives a sequence of crossings of tracks through a set of zones. We evaluate our method in a threat recognition setting.'],\n",
              " ['We propose an orientation estimation method based on edgels and M estimation. Edgels are extracted with a grid mask that can compromise between speed and accuracy. Any camera model can be used and errors are calculated on the original image space. The estimation starts with a random search and ends with a continuous optimization. The method uses quaternions and all derivative calculations use closed formulas.'],\n",
              " ['We developed a novel virtual reality therapy VRT program for online gaming addiction OGA . We compared the treatment effect of VRT to cognitive behavior therapy for OGA. VRT reduced the severity of OGA showing effects similar to cognitive behavior therapy. VRT improved the functional connectivity of the cortico limbic circuit.'],\n",
              " ['A semi supervised hubness aware classifier is proposed. The classifier is evaluated on publicly available real gene expression data. We made the implementation of hubness aware machine learning techniques available in the PyHubs software package.'],\n",
              " ['A hierarchical structure for accurate video to video matching and event recognition Incorporating contextual information to the bag of video words framework Coding spatio temporal compositions of video volumes by a probabilistic framework'],\n",
              " ['We investigated the relation between nonverbal behavior and severity of depression. When symptoms were severe participants showed less AU 12 and AU 15 and more AU 14. When symptoms were severe participants head motion was reduced in size and speed. The pattern of findings was highly consistent for automated and manual measurements. The findings support the hypothesis of nonverbal social withdrawal in depression.'],\n",
              " ['The transparency of the watermark is applicable to the various host images. The browsers can recognize the watermark and the host image is not seriously obscured. The new scheme can remove the embedded watermark from the watermarked image. The PSNR values of the various restored images are around 56dB after the visible watermark has been removed.'],\n",
              " ['This article explicates and evaluates the features of the ANOVA F test and the studentized range test for determining the equivalence of multiple standardized effects. The primary emphasis is to reveal the underlying properties of the two methods with regard to power behavior and sample size requirement across a variety of design configurations. To enhance the practical usefulness complete sets of SAS and R computer algorithms for calculating the critical values p values power levels and sample sizes are also developed.'],\n",
              " ['A theoretical discussion on the capabilities of differing types of landscape presentation methods Proposed and implemented landscape visualisation on demand and on site from existing virtual 3D landscape models Use of smartphones and standard web browsing technologies to access the landscape visualisations.'],\n",
              " ['links social media usage with different urban land uses and locations. provides geo temporal Twitter profiles of behaviour and social attitudes. infers demographic and socio economic characteristics of Twitter users. profiles Twitter messages with respect to inferred age and gender.'],\n",
              " ['We propose a contextual word model for keyword spotting from handwritten Chinese documents. The contextual word model combines character classifier geometric and linguistic contexts. Promising results were obtained on a large handwriting database CASIA HWDB. The geometric and linguistic contexts improve the spotting performance significantly.'],\n",
              " ['We propose a new MCs detection method using contourlet transform and non linking simplified PCNN in mammograms. We first introduce the non linking simplified PCNN to detect MCs. We first come up with the evaluate indicators MCC PS CEI which take the samples proportion into account. The results tested on two open and common databases including the MIAS and the database from JSMIT. This method is verified on the mammograms from the People s Hospital of Gansu Province to show our method can be used in clinical application.'],\n",
              " ['Usage of Ensemble Empirical Mode Decomposition EEMD on object skeletonization Modeling of object contour with a 2D EEMD like procedure The produced IMFs provide a very good and novel workspace for image skeletonization. The proposed method is fully automated and unsupervised.'],\n",
              " ['A computational approach is developed for BMI prediction in face images for the first time. Our work can validate the psychology study results on a large scale database. Our computational approach can be useful for smart health.'],\n",
              " ['A new hierarchical classification scheme by sparse approximation is proposed. Leverage large scale structured data for the accurate hierarchical classification. Distance function taking into account the hierarchical structure is introduced. Defined two images to be similar if they shared a similar path in the hierarchy Achieved better performances than flat 1 vs N classification methods'],\n",
              " ['Automated segmentation of X ray images for skeletal bone age assessment. Hidden Markov models for bone development modelling. A software tool for supporting clinicians in the X Ray investigation according to the TW2 clinical method.'],\n",
              " ['This paper focuses on segmenting the motion from dense optical flow fields. Two unsupervised clustering methods are presented and a model selection is proposed. A comparison between the proposed techniques with the K means and EM is made. Experiments are conducted in a surveillance scenario with an autonomous mobile. The proposed techniques are superior in terms of robustness and computational demands'],\n",
              " ['Present a model based approach to investigate the morphology of the third ventricle. Assess the regional deformations in relation to the atrophy of surrounding structures. Use a symmetric template model with the midplane definition for unbiased analysis. Achieve a robust surface modeling using a progressive surface deformation. Validate the method on a healthy aging sample with different clinical variables.'],\n",
              " ['We introduce a two step algorithm for eliminating rib shadows in chest radiographic images. The algorithm delineates the ribs using a novel hybrid self template approach. It suppresses the delineated ribs using an unsupervised regression model. The rib delineations can improve the performance of CAD systems.'],\n",
              " ['New England Journal of Medicine NEJM is a very prestigious medical journal. NEJM Facebook page currently has more than 1.25 million of users. Medical quizzes are one of the methods to test the knowledge of future physicians. Our approach allows extracting medical quizzes published in NEJM Facebook page. This is the first study done about the content of medical quizzes in social networks.'],\n",
              " ['A topic model with three different parameter settings is fit to a large collection of clinical reports. The interpretability of discovered topics is evaluated by clinicians and laypersons. Clinicians are significantly more capable of interpreting topics than laypersons. Topics hold potential for applications in automatic summarization.'],\n",
              " ['Aim investigate the effect of different rheological models of blood within the LV. Model the MRI images are used to reconstruct the time resolved geometry of LV. Finding the non Newtonian assumption is significant on the LV flow dynamics.'],\n",
              " ['We propose an alternative system for classifying the risk in paediatric congenital heart surgery. Four methods are tested a perceptron multilayer self organising maps a radial basis function neural network and decision trees. We obtain an accuracy of 99.87 using pre and post surgical data and 83 using just pre surgical data .'],\n",
              " ['The main objective of this study was to investigate the impact of implementing a computer based order entry system without clinical decision support on the number of radiographs ordered for patients seen in the emergency department. Our results show a decrease in the number of radiographs ordered after computer based order entry system implementation despite an increase in the number of emergency department admissions. Our study also shows that the time interval between emergency department admission and medical imaging was not affected by this new workflow.'],\n",
              " ['We present a new control approach to adapt trackers to scene condition variations. Tracking context is defined as six features describing scene condition. Best tracker parameters are learned offline for tracking contexts. Trackers are then controlled by tuning online their parameters. Experimental results are compared with several recent state of the art trackers.'],\n",
              " ['Glomerulus diameter and Bowman s capsule thickness in renal microscopic images indicate various diseases. Detection of the renal corpuscle and related objects detection is a key step in histopathological evaluation of renal microscopic images. This work proposed the Analysis Particles algorithm based on median filter for morphological image processing to detect the renal corpuscle objects. Afterwards the glomerulus diameter and Bowman s capsule thickness are measured. The proposed system was tested with a dataset of 21 rats renal corpuscle images acquired using light microscope. The experimental results proven that the proposed solution can detect the renal corpuscle and its objects efficiently with robustness to the deformations of the glomeruli even in the case of glomerular hypertrophy that leads to split the bowman space. The results also reported significant difference between the controlled and affected groups in terms of glomerulus diameter 97.40 19.02 m and 177.03 54.48 m respectively .'],\n",
              " ['We have created a software PopED lite in order to increase the use of optimal design in preclinical drug discovery. PopED lite is designed to be simple fast and intuitive so that it is highly useful in practice and complements existing tools. Key functionality of PopED lite is demonstrated by three case studies from real drug discovery projects.'],\n",
              " ['We select key frames from a video and detect candidate blocks only in key frames. We prepare 3D spatiotemporal volumes by combining the candidate blocks. We introduce a new weighting scheme for generating a more reasonable BoF. The random forest classifier is built during the training phase by using the BoF.'],\n",
              " ['We examine bi modal face speaker authentication in challenging mobile environment. We release new protocols and data with significant mismatch conditions MOBIO . We study bi modal and multi algorithm fusion using generative modelling techniques. Multi algorithm and multi modal fusion provides a consistent performance improvement. The proposed bi modal system significantly outperforms the state of the art.'],\n",
              " ['Heartbeat classification is substantial for diagnosing heart failure. Machine learning methods classify normal and congestive heart failure CHF . The random forest method gives 100 classification accuracy in detecting CHF.'],\n",
              " ['Proposed system separates text regions from images under unconstrained environment. Generalized clustering utilizes properties of scene text to detect text boundaries. Multiple image segmentations provide various interpretations on text regions. Two step CRF approach models properties and relationship of text in graph structure. Character proposals are generated and integrated to find proper character regions.'],\n",
              " ['We proposed RQA ApEn SampEn and wavelet based energy as feature extraction techniques. We have proposed feature selection algorithm to extract the most significant and relevant features. We have used LS SVM and random forest as classifiers. Performance among feature selection algorithms are compared.'],\n",
              " ['Modeling recognition of multi agent activities American football plays . Activities modeled as graphs inexact graph matching used for comparison. Single agent activity represented as motion patterns modeled as graph nodes. Spatio temporal relationships between single agent behaviors modeled as graph edges. We present our own dataset of football plays called UCF Football .'],\n",
              " ['A novel approach for visualization of temporal patterns focused on the association of cancers with other diseases. A dynamic animation of cancer disease association across different age groups and gender. Identifying comorbidity relationships and providing more information for medical researchers.'],\n",
              " ['A GUI was created to calculate the multi phase BED distributions. The GUI is effective at determining the BED distributions and the dose statistics of the ROIs for multi phase treatment plans. Using this GUI has shown the inaccuracies of using the approximate BED calculation method for multi phase cases. This GUI can be used to optimize multi phase treatment plans.'],\n",
              " ['A powerful statistical image reconstruction algorithm for CT is proposed. Data obtained from earlier scans are used to construct a probabilistic atlas with Laplacian mixture model. Prior information obtained from a probabilistic atlas is modeled for the CT image reconstruction. We consider low dose CT imaging setups using proposed method and alternative approaches. The proposed method outperforms other alternative methods in terms of image quality.'],\n",
              " ['The software tool presented allows the deployment of different benchmarking tests for M2M protocols. The most relevant M2M protocols were evaluated considering different specific performance metrics. Benchmark results allowed to select the most suitable M2M protocol a clinical case of use respiratory rehabilitation.'],\n",
              " ['A calculation method of transfer function TF was proposed by a TLM model of human artery tree. The effects of artery stenosis on the TF were simulated and discussed by a series of simulation. A novel method of artery stenosis diagnosis was proposed and validated by TF and SVM. The accuracies of the method for moderate and serious stenosis were 87 and 99 respectively. The proposed method is a theoretically feasible method for diagnosis of artery stenosis.'],\n",
              " ['We use inverse probability of treatment weighting a propensity score based technique for covariate adjustment of the cumulative incidence functions in competing risk analysis. This method requires no assumption about the form of the cumulative incidence functions and the interpretation of the adjusted cumulative incidence functions is intuitively appealing. We developed a SAS macro to make the method readily usable.'],\n",
              " ['A method based on using large correlation windows with adaptive support weights Three new weighting constraints from image gradient color statistics and occlusion Contributes to suppress the effect of cluttered background in the windows Elevates the quality of estimations especially on object boundaries'],\n",
              " ['Collected thermal profile face database using a middle wave infrared 3 5 microns camera . Developed a fully automated thermal ear recognition system for real time human identification works in day or night . Local Ternary Pattern yields Rank 1 80.68 and 68.18 using manually and automatically segmented ears respectively. Score Level fusion of the Local Ternary Pattern LTP and Local Binary Pattern LBP enhanced the performance by 5 .'],\n",
              " ['Semantic search using ontology overcomes the limitation of the current keyword based search. The ranking method considering semantic relationships improves the search accuracy. The ranking method is based on the weighting measure of semantic relationships. Pruning based on the weight for the semantic relationship reduces the search space. Top k Answering based on the keyword index improves the search efficiency.'],\n",
              " ['We propose a Link Bridged Topic model for cross domain document classification. LBT utilizes an auxiliary link network to discover the co citation relationship. LBT combines the content information and link structures into a graphical model. LBT outperforms both multi view learning and single view transfer baselines.'],\n",
              " ['We propose a background modeling method to deal with non stationary conditions. The object based updating strategy allows to work with SFO and RFO. Non Gaussian pixel dynamics are modeled using a Correntropy cost function. Analyzing the regions movement direction avoids tracking background objects. Object based updating strategies improve the performance for indoor scenarios.'],\n",
              " ['Each video is divided in sub sequences. Motion models are extracted for each video sub sequence. The model error distribution and the model ranking for each trajectory are used to correlate different trajectories. Occlusions cause segmentation leakages when sub sequences are merged. To avoid this effect we model segmentation as a graph coloring problem.'],\n",
              " ['A phrasal concept query suggestion method for literature searches is proposed. Key phrasal concepts are suggested with related concepts. Evaluation was performed using two test collections. User experiments verify preferences to use phrasal concept queries. Phrasal concepts can lead significant improvements over state of the art baselines.'],\n",
              " ['A discriminative orthonormal dictionary learning method is proposed for low rank representation with fast computation. Two kinds of discriminative information is used for learning the dictionary. Experiments on face recognition from both image and video demonstrate the effectiveness of the proposed method.'],\n",
              " ['We derive six generative models for related entity finding. We conduct both analytical and empirical studies to compare these six models. We propose a novel entity relation based feedback method to improve performance.'],\n",
              " ['A technique that captures information of objects with longer duration. A feature selection like approach that delivers better performance than several trajectory variants. Removal of a large number of trajectories related to background noise. We apply our technique on action datasets HMDB51 UCF50 and UCF101 containing largest number of classes till date.'],\n",
              " ['We model a lip biometric approach based on shape information. This system is working with static lip on three public datasets. We develop a kernel based Hidden Markov Model DHMMK . The use of DHMMK obtains discriminative information. The use of DHMMK on lip gets a robust approach for identification.'],\n",
              " ['Proposed a new transfer learning method for cross dataset action recognition. A new dual many to one encoder method for feature extraction across action datasets. Achieved over 10 increase in recognition accuracy over recent work.'],\n",
              " ['Feature extraction for the processing of supervised entailment classification. Comparison of various ensemble methods to single learning approaches. Consideration of a novel heterogeneous homogeneous ensemble combination for the problem area.'],\n",
              " ['We develop a model to examine the adoption of location based recommendation agents. The theoretical model is developed based on the technology acceptance model. Perceived usefulness control and institutional assurance are driving factors. Perceived effort special treatment social benefit and accuracy are antecedents. Perceived usefulness is a significant mediator.'],\n",
              " ['Text mining system for extraction of information related to crime from Arabic texts. Local grammar used to extract information and build dictionaries automatically. Visualisation of clustering enhances ability to analyse crime information in corpora.'],\n",
              " ['Stabilization of panoramic and stabilization of single camera videos are separate problems. Panoramic videos suffer from global and inter camera vibrations. Blend masks are useful for dealing with inter camera vibrations. Our survey suggests that viewers prefer this scheme over prior works.'],\n",
              " ['Named Entity Recognition is addressed by constraining inference in CRF. An two phases integer linear programming approach is proposed. Complex relationships among labels are automatically extracted from data. Extracted relationships are introduced as soft constraints in the ILP formulation. The proposed method significantly outperforms the state of the art approach.'],\n",
              " ['We propose a new implementation of the batch IDR QR method. Our new implementation is theoretically equivalent to the original one but is more efficient. Based on our new implementation of batch IDR QR we propose the chunk IDR method. Chunk IDR is capable of processing multiple data instances at a time. Chunk IDR can accurately update the discriminant vectors when new data items are added dynamically.'],\n",
              " ['The adoption of Web accessibility by banks can be motivated by several factors. Among these we test firm size operational factors and CSR commitment. We test our hypotheses using PLS methodology and a sample of European banks. Our results indicate a significant influence of CSR commitment on WA adoption. The other two factors do not seem to exert a significant influence.'],\n",
              " ['Disaster management knowledge is dispersed and requires a centralised and better access. We present metamodelling a way of unifying DM knowledge. We develop our DM Metamodel DMM. We validate theoretically and illustrate how it would apply in representing DM knowledge used in two recent disasters.'],\n",
              " ['We investigate how face detection affects face alignment. We improve the CSR model by multi view multi scale and multi component strategies. We obtain impressive results on the IBUG and 300 W challenge datasets.'],\n",
              " ['CST was refined by formalizing pruning and organizing relations. Refinements improved the annotation agreement in CSTNews corpus. A parser was built based on the refined version of CST. The results obtained by the parser outperformed previous CST based parsers.'],\n",
              " ['A directed structural model is proposed for feature correspondence. It provides more discriminating ability than commonly used undirected model. The feature correspondence is casted as a directed structural matching problem. The convex concave relaxation procedure CCRP is generalized to solve the problem.'],\n",
              " ['A user study of the book finding tools on aNobii was conducted. A set of performance measures were tested on these exploratory tools. Browsing friends bookshelves was more conducive to novelty and serendipity. Users preference structure shown to impact on the performance of the tools.'],\n",
              " ['The proposed tracker combines the flexibility of interest points and robustness of sparse representation. Proposed a robust matching criteria for reliable tracking via L1 minimization The proposed tracker is computationally efficient and provides real time performance. The tracker is benchmarked with many publicly available complex video sequences. Performance is compared with many recent state of the art trackers using 50 benchmark video dataset.'],\n",
              " ['A 3D model based algorithm for face hallucination at any pose and illumination A method for including non local effects e.g. blur in 3D analysis by synthesis The algorithm combines low spatial frequency information with details of a 3D model. Transfer of high spatial frequency details for hallucination on the level of pores Occlusion handling and seamless texture reconstruction'],\n",
              " ['We apply collective matrix factorization to cross domain action recognition. We build a novel graph Laplacian regularization term. The framework jointly learns semantic representations and a linear classifier.'],\n",
              " ['A user based diabetes subject directory was discovered in this study. Terms and their relationships in each of the 12 categories were visually displayed and analyzed. The relationships among the 12 categories were analyzed in a visual context. Descriptive statistic data on diabetes in the Q A were revealed.'],\n",
              " ['Propose an example based face hallucination framework. Proposed ensemble learning for face hallucination. Reconstruct both global and local facial features. Individual ambiguity of system low resolution inputs is considered in this framework.'],\n",
              " ['Summarization using Wikipedia and graph based ranking. Theoretical analysis for various sentence concept models. Real time incremental summarization. Performance analysis using the ROUGE metric and user evaluations. Personalized and query focused summarization multi document summarization.'],\n",
              " ['Presents multi stage stochastic programming models the complex stochastic nature of operating room scheduling in overcrowded Chinese big city hospitals. Develops mathematical results and techniques to solve the formulated models to almost optimality. Attains substantial and significant schedule improvements for single operating room instances derived from real life data.'],\n",
              " ['Scheduling with delivery coordination. Identical machines and assignable delivery times. Minimize the maximum delivery completion time. A 3 2 approximation algorithm and a PTAS.'],\n",
              " ['Graph Edit Distance GED is the most used error tolerant graph matching method. Bipartite graph matching algorithm BP is the most used algorithm to solve GED. 2 new versions of BP published that reduce the runtime but restrictions are imposed. We study how much extend these restrictions limits their applicability. Empirical validation shows new versions reduce runtime but keep recognition ratio.'],\n",
              " ['We present a state of the art multi view face detector based on Cascade Deformable Part Models CDPM . We propose to combine data mining and bootstrapping to learn CDPM models from weakly labelled data. We report extensive validation of our models in the FDDB AFLW HDDB and COFW databases. We show the suitability of our models for face alignment initialization and face detection under partial occlusions.'],\n",
              " ['A 3 axis gyro mounted to a video camera can frequently predict the main component of optical flow. The gyro predicted flow can be used to regularize feature tracking to help track ambiguous features through bad imagery. Gyro regularization does not require all features to belong to a rigid scene. Gyro regularization adds very little computational cost to feature tracking. The common practice of using gyros to initialize trackers offers no advantage over careful optical only initialization.'],\n",
              " ['We propose BFiVe a new supervised algorithm for single shot person re identification. The descriptors are a set of compressed local Fisher vectors extracted from a coarse to fine image subdivision. In the training step each region gives rise to a learnt weak ranking function. The ranking function of the image gallery is obtained by a boosted selection of a weak learner subset. The matching rate at rank 1 on VIPeR is 38.9 on 3DPes 41.7 on PRID 2011 19.6 and on i LIDS 119 48.1 .'],\n",
              " ['We propose a modeling approach and a visual modeling notation for social collaboration processes. We consider collaboration processes as the evolution of a network of documents and people. The visual modeling notation is a fusion of statecharts and graph query languages. The approach is supported by a formal definition to enable automatic reasoning and verification. We present sensitive use cases to demonstrate expressiveness and usefulness of our approach.'],\n",
              " ['A new skewed stereo ToF camera for detecting and imaging translucent objects under minimal prior of environment Translucent region detection method Translucent region recovery method Extensive analysis and evaluations'],\n",
              " ['Background removal technique based on adaptive median filtering and thresholding A Local Co occurrence Map with local contrast can distinguish between document text and document stains and background. Low complexity approach with fast and accurate binarization results'],\n",
              " ['We empirically study coding and pooling under a large range of vocabulary sizes. We provide detailed application guidelines to use BoW in practical applications. Combined with average pooling most coding methods are comparable. The best performance of max pooling is better than the one of average pooling. A saturant point exists in maximum pooling.'],\n",
              " ['A new rotation and scale invariant line based color aware descriptor is introduced. The descriptor captures both local texture color and global inter line spatial information. Experiments show that proposed descriptor is robust to rotation scale and illumination. The descriptor is compared to the well known descriptors. The proposed descriptor is more accurate on matching line rich objects such as faces.'],\n",
              " ['On board accelerometers make feasible perspective correction in digital photography. Perspective correction is obtained with no human intervention. Perspective correction can be applied also to zoom lenses. Accelerometer calibration is necessary to get high accuracy. Accelerometer pose has to be estimated carefully to get high accuracy.'],\n",
              " ['We investigate the models used in Web traffic generators. We examine deterministic chaos as an alternative explanation of self similarity. Deterministic components are found to be significant in Web session data. Deterministic and stochastic predictive models of the data are equally accurate.'],\n",
              " ['The problem of facial landmark detection in unconstrained conditions is tackled. A new way of constructing per landmark response maps is proposed. The method is based on the selection at test time of globally similar images. We use local Gaussian process regression for inference.'],\n",
              " ['We perform a comprehensive study on keyword bidding in sponsored search. We propose a competitiveness and relevance based bid keyword suggestion method. We model bid keyword suggestion as a mixed integer optimization problem. Experiments demonstrate the effectiveness of our proposed method.'],\n",
              " ['No training or camera calibration is needed and achieve real time processing. Extract descriptors from targets with rotations and scale change. Descriptors of partially visible people can also be extracted. No high resolution cameras are needed.'],\n",
              " ['A novel bisecting k means clustering based privacy preserving CF scheme is proposed. A two level preprocessing scheme is suggested to enhance scalability and accuracy. Effects of scalability and sparseness challenges are alleviated considerably. Accuracy of the solution is significantly better than knn based CF and PPCF methods.'],\n",
              " ['A generalized reprojection error is proposed to fuse stereo and silhouette cues. The method composes of convex optimization and mesh based surface refinement. Insensitive to initialization and scalable for high resolution reconstruction. Good performance for data with smooth texture and sparsely sampled viewpoints.'],\n",
              " ['We model an action as sequence of outputs of linear time invariant LTI systems. We represent the outputs of LTI systems by means of Hankelets. We adopt an HMM to model the transitions from one LTI system to another. We formulate an inference and supervised learning formulation for our model. We also present a deep analysis of the parameter settings for our action representation.'],\n",
              " ['Motion Artifacts MoArt are a non systematic error in Time of Flight ToF imaging. MoArt emerge when motion appears during the integration time. MoArt cause distorted ToF measurements depth and amplitude . We propose a single frame correction of ToF measurements with motion flow estimation. We present quantitative and qualitative results in challenging scenarios.'],\n",
              " ['The paper deals with interfaces that prompt users to download or not a product. We compare 3 different designs of such software download interfaces. Question Answer Q A interfaces result the most effective in minimizing users incoherent behaviors. Differences in reputation rankings significantly influence users. Results suggest guidelines to design the best interface depending on the context.'],\n",
              " ['The impact of preprocessing on text classification in terms of various aspects is extensively examined. Experiments are conducted on two different domains and in two different languages. Choosing appropriate preprocessing tasks may improve classification accuracy significantly.'],\n",
              " ['We propose a new local part model for action recognition. A feature sampling strategy with high feature density is used. We explore and prove the benefits of using accurate optical flow algorithm for action recognition. High performance and fast action recognition are achieved.'],\n",
              " ['An end to end metadata driven intelligent system for automatic photobook creation A flexible intermediate story representation for creating multiple product types Automatic generation of representations for event based and theme based groupings A new pagination algorithm for mapping selected images onto photobook pages Metadata aided UI enabling users to view groupings and find alternative images'],\n",
              " ['We address the detection of chromatic moving shadows. Gradient and colour information is exploited for separating shadow regions. Temporal gradients and spatial angle and brightness distortions are used to detect shadows. Shadows and objects are tracked using motion filters. Mutual information and data association between objects and shadows are used to recover misdetected shadows.'],\n",
              " ['We present a cloud based platform for data stream processing with workflows. The ClowdFlows platform enables processing of multiple concurrent data streams. We implement an active learning scenario for sentiment analysis on data streams. Machine learning methods are shown to be suitable for sentiment analysis. Active learning improves the accuracy of sentiment classification.'],\n",
              " ['Real time multi view landmark detector independent on initialization Detector composed of a mixture of tree based Deformable Part Models Landmark detector learned by the Structured Output Support Vector Machines Coarse to fine strategy to speed up inference based on dynamic programming'],\n",
              " ['The weighted volume local binary pattern descriptor is formed to represent dynamic texture. The weights are learned with a regularized collaborative representation model. The representation residual is used for dynamic texture classification.'],\n",
              " ['An algorithm has been developed for automatic detection of vanishing points. The behavior of gradient based edge detectors has been analyzed. The results have been analyzed according to the number of vanishing points and image resolution.'],\n",
              " ['We develop a novel algorithm to inspect the defects on touch panel. Based on periodic patterns of touch panel an adaptive model is learned online to extract defects. The experimental results indicate that our proposed method achieves accurate detection with efficient computation. The users pay very little effort for the testing of different panel products.'],\n",
              " ['Propose new automated tri spectral visible MWIR and LWIR FR approach Design experiments to quantitatively measure benefits of global vs. local matchers Evaluation of global vs. local based matchers when fused at the score level Achieve rank 1 identification rate of at least 99.43 per spectrum of operation'],\n",
              " ['A new compression method of the double array is proposed. The BASE array is removed from the double array. The retrieval and construction algorithms are proposed. The space usage of our method is more compact than that of the double array. The retrieval speed of our method is almost the same as the double array.'],\n",
              " ['Accurate scale is estimated with laser data and features extracted from camera. Many more loop closures are detected by sensor than that of camera only methods. Scale and loop closure geometric constraints are enforced on Constrained Bundle Adjustment which produces a robust pose estimate. Experiments show that the proposed method is practicable and more accurate than vision only methods.'],\n",
              " ['For non rigid registration a hierarchical piece wise affine transform is examined. We propose a metric to quantify the ability of being correctly registered. We examine ROC analysis on the proposed metric and Moran s spatial correlation. Proposed metric reflects the ability of being registered better than Moran s. Registration error is reduced by using the proposed metric as a sub image indicator.'],\n",
              " ['A new strategy of overlapped non uniform patch is proposed. 2D DWT and integral projection technique are used to obtain the patch strategy. The overlapped patches are elected to keep the patches robust. The proposed face recognition method processes good performance.'],\n",
              " ['We derive an upper bound for the variance of RE sampling. We show that variance of RN sampling grows with data size for scale free networks. Thereby RE sampling excels when data size is large. We support the analytical results with simulation studies and 18 real networks.'],\n",
              " ['We propose new heuristics for object centered and personalized tag recommendation. We also propose new learning to rank L2R based strategies for the same tasks. They exploit tag co occurrences textual features relevance metrics and user history. Our solutions greatly outperform state of the art methods on real datasets. Tag personalization produces better descriptions of the objects.'],\n",
              " ['We propose a tracking method to solve both the problem of partial occlusions and non rigid deformations in real time. The target object is modeled through an elastic structure of local patches for robust performance. Hierarchical diffusion method is proposed to obtain an acceptable solution in real time. Extensive evaluation shows that the proposed method outperforms state of the art.'],\n",
              " ['A stereo matching approach motivated by the particle filter framework in robot localization. Highly accurate GCPs acquired by the computation of multiple cost efficient disparity maps. A Markov chain model has been introduced in the process to reduce the computational complexity of particle filtering. Application of RANSAC algorithm along with a histogram technique to refine any outliers.'],\n",
              " ['The 3D object is split into 18 portions according to its principal axes. Each portion is represented by a size function. A well suited similarity measure is used to compare between the 3D objects.'],\n",
              " ['The ideas of sub category and part based are used to learn a robust object model. A novel object representation is proposed based on the topic model. An iterative learning process is presented under the semi supervision way.'],\n",
              " ['A new gait feature is extracted from a raw gait video directly. The proposed gait feature extraction is performed in the spatio temporal domain. The attribute based learning is used to enhance the SVM based gait classification. The proposed method has outstanding performances under changes of clothing types. The proposed method has outstanding performances under changes of carry conditions.'],\n",
              " ['We propose a new model free approach for gait recognition. The recognition is achieved independently of the trajectory. Our method is based on 3D morphological analysis of gait sequences. Our approach is able to identify people walking on curved paths.'],\n",
              " ['A combination of coarse alignment and refinement is employed in the method. Input images are coarsely aligned by fitting and normalizing the matched MSERs. A phase congruency and point set registration based refinement step is used. The method accurately and efficiently aligns images with affine distortions. The method is robust to illumination changes.'],\n",
              " ['We propose a formalized evaluation framework for topic specific influence theories specialized in Twitter. We introduce a novel evaluation metric that assesses the aggregate sentiment of a group of user. We introduce a two dimensional taxonomy for classifying influence theories. We use the framework to evaluate five existing theories from literature and assess their performance.'],\n",
              " ['Several state of the art stock prediction models are constructed. Different metrics for evaluating prediction models are discussed. Explanatory techniques are applied to gain insights into the model s decisions.'],\n",
              " ['A new recommendation approach based on the Relevance Modelling RM of the problem is proposed. The neighbour selection problem is improved by Posterior Probabilistic Clustering PPC . Background information such as item popularity is successfully integrated by using RM models. Performance of the recommendation improves when more clusters are considered in the PPC technique. Combination of both contributions leads to an even better performance than their separate application.'],\n",
              " ['We extend the TF ISF method to use local context. We extend the TF ISF method to promote retrieval of long sentences. Context and promoting retrieval of long sentences both improves sentence retrieval. We also combine using context and promoting retrieval of long sentences. It is useful to use at the same time context and promoting retrieval of long sentences.'],\n",
              " ['We design a scale invariant shape descriptor for shape matching and object detection. A graph based segment matching algorithm is introduced. Accurate boundary of object can be detected in natural image. Compute the similarity between descriptors properly in clutter background'],\n",
              " ['Analysis of solution degeneracy in general multicamera cluster SLAM. Decomposition of system determines conditions for degeneracy. Dense sub matrix reveals geometric structure for futher analysis. Identification of degenerate motions independent of feature constellation. Experiments demonstrate that adding features and cameras decreases degenerate set.'],\n",
              " ['A piecewise planar reconstruction pipeline is proposed. Line cuts are detected based on signal symmetry analysis. SymStereo and PEARL are combined for carrying the 3D reconstruction and plane fitting simultaneously. Experiments show significant improvements over state of the art methods.'],\n",
              " ['Fully automatic annotation of real world tennis video State of the art tennis ball tracking algorithm An integration of computer vision machine listening and machine learning The only system that can annotate tennis game at such a detailed level'],\n",
              " ['Consider the spatial distribution information in feature pooling Handle the misalignment problems in image classification and action recognition Improve the discrimination of feature pooling in visual recognition tasks Conduct extensive experiments on visual recognition tasks'],\n",
              " ['Compared performance of texture SIFT Gabor LBP geometry and their fusions Investigated correlation of arousal and valence dimensions to six basic emotions Novel insights into categorized emotion recognition using dimensional axes'],\n",
              " ['We testify the significance of text structure during identification categorization task. We examine scanpaths of textual genres using temporal and distance based measures. We show how the form features of a genre aid in text interpretation and use. Format and layout plays an important role in human text categorization. We are looking for evidence of skimming and scanning.'],\n",
              " ['Different segmentation representations SRs cause little difference in performance. Different SRs result in quite different outputs. Incorporation of different SRs is beneficial to NER task. We proposed a new feature generation method that uses multiple SRs. The proposed method improves the performance and the stability of NER.'],\n",
              " ['We developed a method called DST KLPP which is effective in classification of surface defects of different metals. DST KLPP was tested with samples of three typical metals including slabs hot rolled steels and aluminum sheets. DST KLPP provides higher classification rates than other methods including Wavelet Curvelet and Contourlet transform. DST KLPP can recognize defects from complex backgrounds efficiently. DST KLPP can recognize tiny defects from low contrast images availably.'],\n",
              " ['Citation analytic methods as approaches to knowledge organization. Semantic relations between citing and cited documents. The distinction between intellectual and social organizations of knowledge. Citation based maps as social organizations. Positivistic versus pragmatic approaches in bibliometrics.'],\n",
              " ['We show how to achieve state of the art facial landmark localization by CNN. The system s performance is improved by deeper network. We show our system s performance is close to human.'],\n",
              " ['We study the retrieval effectiveness stability tradeoff in query model estimation. This tradeoff is investigated through a novel angle i.e. bias variance tradeoff. We formulate the performance bias variance and estimation bias variance. We investigate various query estimation methods using bias variance analysis. Experiments have been conducted to verify hypotheses on bias variance analysis.'],\n",
              " ['We deal with the challenging problem of high level event detection in video. We build event detectors based solely on textual descriptions of the event classes. We also learn event detectors from very few positive and related training samples. We present results and comparisons on a large scale TRECVID MED video dataset.'],\n",
              " ['Personalization in SNS increases its switching cost and satisfaction. Switching cost and satisfaction affect user continuance of SNSs. SNS users cannot easily switch to other SNS because of time and efforts invested. Social exchange theory provides a useful framework and proves its feasibility. A new insight on research concerning participatory networks such as SNSs.'],\n",
              " ['Scene layout estimation using both trajectory data and image features No assumptions are considered about the structure of the scene. Efficient estimation of the scene structure in the presence of scene clutter We show that using line segments we obtain better surface normal. Our data and segmentation results will be publicly available for comparison.'],\n",
              " ['A novel and accurate method to refine low dimensional human shape using RGB D data is proposed. Uses of multiple modalities do not carry any features from the shape provider. Combines low and high level observations jointly in multi layer graph structure Extensive experiments showed that it outperforms compared suitable algorithms. Also an existing method is extended by fusing more generic cues for this purpose.'],\n",
              " ['To observe features of Malay Tweets three distinct corpus based analyses are done. A rule based architecture is developed based on results of the analyses. The architecture consists of seven distinct modules in a pipeline structure. Experimental results indicate high accuracy in term of BLEU score. The architecture outperforms SMT like normalization approach.'],\n",
              " ['The use of Wikipedia as a knowledge source for question answering system. Wikipedia article content structure infobox category and definition are used. Each knowledge source has its unique strength for certain question types. Answer merging strategy for multiple answer matching modules.'],\n",
              " ['A novel feature named OViF for violence detection. OViF makes full use of the orientation information of optical flows compared to ViF. We use AdaBoost as a feature selector and Linear SVM as a classifier. Experiments show that ViF OViF obtain the state of the art violence detection performance when using AdaBoost with SVM.'],\n",
              " ['XML documents contain temporal information and evolve over time. Our work explores change detection versioning and querying support in XML documents. It describes the storage structures to efficiently store and retrieve XML documents. It explains the change management features in some commercial tools. It highlights several open research issues for multi versioned XML documents.'],\n",
              " ['A collaborative ranking model involving users with different domain expertise levels. EM based learning method for document allocation. Simulation based evaluation with effective results at session and user roles levels.'],\n",
              " ['We use an MDP formulation for optimal adaptation of tracking algorithms. We optimize the tracker control parameters using prioritized Q learning. The proposed prioritized Q learning approach is based on sensitivity analysis. The performance of our method is superior to other approaches. The proposed method can balance tracking accuracy and speed.'],\n",
              " ['The label information is enforced in the generation of visual and annotation terms. The zero mean Laplace distribution is added to a topic generative process. The sparse image representation is helpful to learn a training model. A series of experiments on four data sets demonstrate the performance of DSTM.'],\n",
              " ['An exhaustive review of ranking approaches for semantic search on Web. We identified three stages of ranking in semantic search process. We identified criteria for the comparison of semantic search approaches. We examined some open issues relevant to efficient semantic search.'],\n",
              " ['We explore state of the art supervised machine learning methods for sentiment analysis of Czech social media. We provide a large human annotated Czech social media corpus. We explore different pre processing techniques and employ various features and classifiers. We experiment with five different feature selection algorithms. Results are also reported on other widely popular domains such as movie and product reviews.'],\n",
              " ['An improvement of the classical energy function graph cut is proposed. Integration of spatial priori and gradient information improves segmentation result. A new coarse to fine flower segmentation method is presented and implemented.'],\n",
              " ['Multi view Laplacian sparse feature selection MLSFS algorithm is proposed. Multi view learning is utilized to exploit the complementation of different views features. A effective iterative algorithm is introduced to optimize the objective function. The convergence of the algorithm is proven. Experiments demonstrate MLSFS has good performance of feature selection.'],\n",
              " ['We present PubSearch a new system for academic search and retrieval. PubSearch is based on a cascade hierarchy of three heuristics. The heuristics include Term Frequency citation distribution and topics inter relations. We compare PubSearch performance against ACM Portal on 58 user queries. The system outperforms ACM Portal in terms of ERR NDCG LEX metric by large margin.'],\n",
              " ['Illumination change detection is carried out without assuming a specific color transformation. The method is designed to improve existing standard foreground detection algorithms. Backup procedures are designed to recover from detection failures. The method is applicable to a wide range of varying illumination conditions.'],\n",
              " ['A study of a conditional risk based on both variable and constant losses. A geometrical interpretation of the decision function of a classifier. An analysis of performance of different Na ve Bayes classifiers compared to SVM.'],\n",
              " ['We have presented a method for facial expression invariant face recognition. We propose a novel method for real world pose invariant face recognition. Proposed method use a single image in gallery with any facial expressions. We generate a sparse dictionary matrix for each people based on triplet pose angles. Convincing results were acquired to handle pose on the FERET CMU PIE and LFW databases.'],\n",
              " ['A characterization of metadata quality. A discussion on the role of metadata quality in data infrastructures. A survey of metadata quality frameworks to assess metadata quality. An array of solutions to mitigate the effects of metadata quality issues. Directions for future works related to metadata quality in data infrastructures.'],\n",
              " ['Uses temporal mining technique event recognition in dynamic scenes Temporal association rules are then generated from frequent patterns. These association rules help model the sequence cycle. Spatio temporal anomalies are identified and detected in a hierarchical manner.'],\n",
              " ['We investigate multi modal 2D 3D facial gender and ethnicity classification. We propose a novel local descriptor LCP to capture both 2D and 3D facial clues. LCP is more discriminative and more robust to noise than LBP like features. Combing both modalities improves results in gender and ethnicity classification. We achieve competitive performance compared to state of the art in such tasks.'],\n",
              " ['Multimedia objects retrieved by their textual context. Structural factors help to identify relevant textual parts. Multimedia elements are useful to retrieve multimedia fragments. Specific multimedia retrieval system outperforms classical XML retrieval systems.'],\n",
              " ['We introduce the 3DSPMK for object and scene recognition in depth images. Our model repeatedly subdivides a cube inscribed in the point cloud. Then a weighted sum of histogram of visual word occurrences is computed. Results on publicly available benchmarks have been reported.'],\n",
              " ['Novel multi scale frequency analysis is proposed for intensity feature analysis. Multi scale analysis is introduced to extract color and orientation features. Spectral information content is proposed for spectral feature analysis. Propose a novel weight multi scale feature fusion method Synthesize frequency and space domain features in proposed algorithm frame.'],\n",
              " ['We infer a very accurate 3 D face model for a freely moving user from a single depth camera. Using unwrapped cylindrical 2 D images enables us to use simple 2 D image processing algorithms to process the 3 D information. We use a combination of spatial smoothing and temporal integration for noise removal. A robust rejection method produces reliable results in the presence of facial expression changes and partial occlusions. Our system runs online and in real time.'],\n",
              " ['We combine active and semi supervised learning for cross lingual sentiment classification. Density analysis of unlabeled data is used in active learning. We test our proposed model on three different languages. This model reduce manual labelling efforts in cross lingual sentiment classification. Results show that incorporating density analysis can speed up learning process.'],\n",
              " ['We empirically study how Wikipedians summarize entity descriptions in practice. We compare entity descriptions in DBpedia with their Wikipedia abstracts. We analyze the length of a summary and the priorities of property values. We analyze the priorities of diversity of and correlation between properties. Implications for automatic entity summarization are drawn from the findings.'],\n",
              " ['A novel unrestricted contactless palmprint image acquisition device is developed. A novel model based ROI extraction method robust to the small errors is proposed. A contactless palmprint DB is constituted by collecting 1752 images from 145 subjects. The images in DB have pose rotation position variations and cluttered backgrounds. The results achieved by the proposed system are encouraging 0.277 EER and 99.488 RR.'],\n",
              " ['We review the prevailing discourse around the concept of uncertainty. We describe a communication based theory of uncertainty management. We explore how this theory can inform the design of information retrieval systems.'],\n",
              " ['Preprocessing of the extracted tracklets before extracting the persistent track An intermediate step on multi object multi camera tracking problem Variational data association model for estimating true tracklets Extracting the least possible true tracklets from erroneous tracklets'],\n",
              " ['A joint discriminative and generative vocabulary tree model A non parametric image patch layout model and matching method Evaluation in vein authentication and recognition tasks with good performance Comprehensive in depth discussion of the technique'],\n",
              " ['Using facility location analysis as a well known branch of Operation Research to solve an applied problem in information retrieval. Proposing a unified framework to solve three types of problems for expert group formation. Testing our framework on a real test collection. Proof of optimality of the solution.'],\n",
              " ['We examine information source use during KM strategic decision making. We revised De Alwis et al. s typology of information source preferences. Organizational context is highly determinative of information source use.'],\n",
              " ['Polygonal approximations using relaxed approximations allowing the vertices of an approximation to lie outside the contour. Adaptive estimation of contour neighborhoods where the vertices of an approximation are located. A general approach to reduce error measure of any polygonal approximations through vertex relocation.'],\n",
              " ['Facial landmark detection using a cascade of regressors New regression model based on L21 norm Multiple initialisations are used to improve robustness. The method is evaluated on multiple datasets and on the 300W challenge.'],\n",
              " ['A study of the use of topic models for video retrieval is presented. A new topic model to deal with incremental retrieval scenarios is proposed. Comparison of four topic models using two different retrieval functions The results highlight the performance differences among the topic models.'],\n",
              " ['A statistical adaptive metric learning SAML is proposed to classify action features. SAML explores multiple statistic combinations for feature sets in different scales. Discriminative statistic subspace is learned by a unified metric learning framework. High competitive performances are achieved by SAML on five benchmark databases.'],\n",
              " ['The prime pyramid expands the training dataset with less redundancy. Sparse representation provides a compact model for textural images in each class. Multi manifold analysis improves discriminative power while mitigates overfitting. Reasonably high classification accuracy is achieved with very few training images.'],\n",
              " ['Assignment of projective models becomes a problem of probabilistic inference through clustering. A Markov network formulation that models data points in terms of projective relationships in two views is introduced. An algorithm that fits multiple varieties to data points is specified using MCMC based inference. Use of a global energy measure to capture the quality of convergence. Comparative results indicate less susceptibility to parameter tuning and increased accuracy of convergence.'],\n",
              " ['Distributions of GMRF local parameter estimates as improved texture descriptors. Spatially localized parameter estimation using local linear regression. Approaches to overcome inconsistencies in localized parameter estimation. Proposed descriptors capture both spatial dependencies and distributions of texture.'],\n",
              " ['We consider query verbosity and query length two distinct query properties. We propose a methodology to automatically detect and process verbose queries. The methodology uses concept expansion and syntactic features for query modification. Experiments were carried out on a test collection built on TREC2004 Robust collection. Retrieval effectiveness can be significantly improved by considering query verbosity.'],\n",
              " ['BIG OH binary quantization of gradient orientation based descriptors is proposed. Quantized SIFT descriptors reduce memory by 88 compared to classical SIFT. BIG OH has performance comparable to SIFT and GLOH. BIG OH has better performance than BRISK CARD BRIEF and other descriptors. BIG OH is effective for large scale applications such as copy detection.'],\n",
              " ['Proposed an online sparse feature selection method for modeling tracking target from its neighboring background. Introduced a correlation based feature updating strategy to accommodate significant appearance change of the target Achieved more stable and accurate tracking results compared to several state of the art methods Real time processing speed'],\n",
              " ['Knowledge industries KI are the largest segment of the Austrian information sector. University libraries can serve as basis for estimating the need of info specialists in KI. The ratio information specialists to knowledge workers is 1 130 in KI in Austria.'],\n",
              " ['A novel multi document summarization system that employs the tag cluster on Flickr. The FoDoSu detects meaningful words by exploiting tag cluster for summarizing multi documents. We demonstrate the superiority of FoDoSu through experiments on TAC2008 and TAC2009.'],\n",
              " ['Proposed a low false alarm rate pedestrian detection method Extracted foregrounds with the employment of depth sensor Designed a pre evaluation stage to save computation time and reduce false alarm rate Built a pedestrian database with 673 depth images collected from 11 different scenes'],\n",
              " ['TFK a kernel framework between arbitrary length sequences. Some complex activities are defined by the order of sub actions. The new kernel framework improves results in complex activities recognition. Combination of several levels of granularity in temporal divisions reduces clutter.'],\n",
              " ['Fast and efficient superpixel algorithm based on the Eikonal equation Dynamic color based potential function Parameter free refinement of the superpixels Equivalent or better performances than the state of the art Flexibility of the approach'],\n",
              " ['Two types of radial Legendre moments for image analysis are proposed. The proposed moments are transformed Legendre moments. These proposed moments can be thought as orthogonalized complex moments. A framework of deriving RST invariance of the transformed moments is investigated. The proposed invariants are robust to additive white noise.'],\n",
              " ['We propose a method for estimating the human upper body orientation. Our algorithm uses partial least squares based gradient and texture feature models. Integration with an UKF based movement prediction increases the performance. Comparison with the state of the art shows the benefit of our algorithm. Experiment results using image video and camera are provided.'],\n",
              " ['The representation error is modelled as a Laplacian distribution. We derive our new LAD Lasso model based on a Bayesian MAP estimate. LAD Lasso model is robust to outliers. The number of optimisation variable in the new model reduces greatly. We use ADMM algorithm to solve the new optimisation problem.'],\n",
              " ['The novel concept of non recurring De Bruijn NRDB sequence The algorithms to generate NRDB sequences for given parameters The optimal subsequence search algorithm using branch and bound Discussion on similarities and differences of active stereo and structured light'],\n",
              " ['An initial step for optical flow estimation in poorly textured images is proposed. The simple yet effective step preserves motion boundaries where other methods fail. The proposed algorithm reduces computation time meaningfully. Mathematical analysis is employed to explain the advantages provided. Quantitative measures have been introduced to assess the performance.'],\n",
              " ['We introduce a parameter free calibration model for a TOF camera IRD map . This model partially compensates certain aberrations from the pinhole model. The IRD is computed by 1 or 2 depth images of a flat surface. Introducing featureless calibration procedures for TOF sensors blank planes . Satisfying calibration for low resolution sensors.'],\n",
              " ['A set theoretical formal model for digital archives is presented. The 5S model is extended for digital archives and concrete applications are given. Address EAD issues in the Web and with compound digital objects. Use cases based on OAI PMH OAI ORE and annotation frameworks for digital archives. Properties and mappings between the tree and NESTOR are formally proved.'],\n",
              " ['We propose a novel deep learning model for action recognition. The deep learning model exploits multilinear algebra. The method derives spatio temporal features.'],\n",
              " ['We propose a novel qualitative method for analysing gene networks based on formal verification technique. Behaviours and properties of networks are described in temporal logic formulae. By checking satisfiability of the formula we can analyse properties of the network. To improve the efficiency of analysis we developed the modular and approximate method.'],\n",
              " ['We propose a novel and accurate ARO classification method. We propose a hierarchical FSM consisting of pixel region and event layers. State transition is done by the pre trained SVM using 7 different input features. The proposed ARO method shows higher classification and low false alarm. The proposed ARO method can be applied to many practical applications.'],\n",
              " ['This paper proposed a compensation method for eliminating the effects of temperature variation in the long duration application of image vision. A model of the relationship between the camera parameters and temperature variations is established with the system identification method. Experiments are carried on. The analyses and experiments demonstrate the feasibility and efficiency of the proposed method.'],\n",
              " ['Framework of ten compliance monitoring functionalities CMF in business processes is defined. Related techniques and enabling technologies are discussed. Systematic literature review and presentation of real world business constraints for compliance are extracted from five case studies. Systematic comparison with compliance patterns and classification of existing approaches using the framework is presented. CMF framework based on two realistic data sets using three different compliance monitoring tools is applied.'],\n",
              " ['We introduce mean univariate GARCH VaR actual portfolio optimization in accordance with Basel Capital Accords. We develop optimization software that combines NSGA II algorithm and R statistical software. Computational results show that our approach provides better mean univariate GARCH VaR trade offs of actual portfolios in comparison to benchmarks estimated by analytical methods. Empirical tests cover both low and high volatility samples.'],\n",
              " ['We propose novel method for mining high quality translation from comparable corpus. We introduce Term Association Network TAN for mining Translation knowledge. We propose a new method for term translation validity using cross outlier detection. Results show that proposed methods significantly outperforms dictionary based method. Our methods are specially effective in translating OOV terms by expanding query words.'],\n",
              " ['Expertise seeking involves selecting people for consultation about info needs. People e.g. work group colleagues are among the most frequent sources. Seekers balance quality and accessibility in their selection of sources. Source selection is affected by task related seeker related and contextual factors. Multiple barriers complicate degrade or prevent expertise seeking.'],\n",
              " ['We develop a robust stereo matching algorithm using a random walk with restart. The proposed method considers occlusion and depth discontinuities. The method achieves high accuracy with low computational costs in the reconstruction. Our method works well on the varying illumination and exposure test.'],\n",
              " ['We present a review of the state of the art in eye movement biometrics. We explain the general steps for the creation of a database of eye movement recordings. We describe basic eye movement features and methodologies with application in biometrics. We present extended analysis and results for the BioEye 2015 competition.'],\n",
              " ['Tackle the empty cavity issue by properly selecting reference points. Reveal codeword ambiguity by imbalance assignment and enhance the reliable codeword. Incorporate GMP into VLAD.'],\n",
              " ['New multi lateral filter to efficiently increase the spatial resolution of low resolution and noisy depth maps in real time. ToF camera coupled with a 2 D camera of higher resolution to which the low resolution depth map will upsampled. We account for the inaccuracy of depth edges position due to the low resolution ToF depth maps. Unwanted artefacts such as texture copying and edge blurring are almost entirely eliminated. The proposed filter is convolution based and achives a real time performance by data quantization and downsampling. The proposed filter has been effectively and efficiently implemented for dynamic scenes in real time applications. The proposed filter can be easily adapted for alternative depth sensing systems than ToF cameras.'],\n",
              " ['Formalization of object re identification problem in a distributed environment Re identification treated as an open world problem Novelty detection and forgetting included in the scheme A set of performance measures geared towards open world distributed surveillance Experiments on a many camera 36 surveillance dataset and publicly available source code'],\n",
              " ['A two phase strategy for combining separate cost volumes is described. A mean shift segmentation driven approach to handle disparity outliers is utilized. Low textured area plane fitting is fostered by using disparity histogram analysis. Our method ranks first among published methods in the Middlebury evaluation system.'],\n",
              " ['A novel technique for automatic lip reading is proposed. A weighted finite state transducer cascade is used incorporating a confusion model. Performance was slightly better than a standard HMM system. The issue of suitable units for automatic lip reading was also studied. It was found that visemes are sub optimal because of reduced contextual modelling.'],\n",
              " ['This paper proposes a globally rotation invariant multi scale co occurrence of LBPs MCLBP . The proposed MCLBP can effectively capture the correlation between the LBPs in different scales. Three globally rotation invariant encoding methods are introduced for MCLBP. The proposed MCLBP performs very well on texture material and medical cell classification.'],\n",
              " ['A novel idea has been presented for invariant texture classification. The idea uses a combination of wavelet analysis and spatial filter bank method. The method has been tested on a variety of texture databases.'],\n",
              " ['A semi supervised graph approach is proposed to infer the hash codes. We simultaneously learn visual features and hash functions. Extensive experiments are conducted to exploit the performance of our method.'],\n",
              " ['We digitized political texts from the 1980s and 1990s. We used these data to learn a classifier that can label more recent political texts. Change of themes over the years affects recall of the learned classifier. But precision is comparable to the precision obtained by a human expert labeller. For political themes a high level of detail seems to be preferred by domain experts.'],\n",
              " ['Development of an algorithm for the estimation of the focus of expansion Algorithm based on the cross ratio property applied to interest point trajectories The algorithm gives accurate focus of expansion localization even for low textured scenes.'],\n",
              " ['A detailed review of the recent advances in the area of visual speech decoding. Visual features tackling speaker dependency head poses and temporal information. Dynamic audio visual speech information fusion. Recent techniques of facial landmark localization. Summary of audio visual speech databases and ASR performance on them.'],\n",
              " ['We propose a two level annotation evaluation protocol for text detection algorithms. Algorithms with different granularity outputs are equitably compared. All matching strategies between the ground truth and the detections are handled. Quantity and quality scores are given to describe a detector s behavior. The protocol can manage any irregular text representation.'],\n",
              " ['Developing a multi label classification framework with a convex optimization process for activity detection. Histogram correction for activity representation in each class to localize activities in a weakly supervised setting. Proposing a new formulation for matrix completion to deal with classification localization in video. Developing an activity recognition system in a totally weakly supervised multi label setting. Developing a non negative matrix completion framework based on Alternating Direction Method ADM .'],\n",
              " ['The paper presents a new coding scheme for photo collections of social events. The approach is intended for server side storage of image databases. Heterogeneous users could contribute to the dataset. The algorithm implements a predictive coder exploiting the visual similarity. The coding efficiency significantly improves with respect to existing solutions.'],\n",
              " ['A motion boundary based sampling strategy is proposed for dense trajectory. A set of 3D co occurrence descriptors is developed to describe cuboids. Two decomposition strategies are presented to further improve performance. We achieve state of the art results on several human action datasets.'],\n",
              " ['China Korea and LC have mixed headings in Kanji and other Chinese characters. Few organizations display correspondences between Kanji and their yomi. Romanized descriptions are different between organizations. Some names in hiragana might escape a search.'],\n",
              " ['We made usability stress tests on mobile biometrics successfully. We optimized resources through automation. Stress is not a major drawback for handwritten signature recognition. The use of colours as a feedback of the recognition benefits usability and performance.'],\n",
              " ['Enabling flexibility and change in process choreographies. Proposing change propagation algorithms for process choreographies. Handling transitive change propagation across several partners. Ensuring consistency and compatibility when propagating changes. First comprehensive proof of concept implementation of change propagation framework for process choreographies.'],\n",
              " ['A regional bounding spherical descriptor is used for 3D face and emotion analysis. Region segmentation is based on shape index and spherical band on 3D face. The regional descriptors are obtained by projecting regional bounding spheres. A regional and global regression is proposed for the weighted regional descriptor.'],\n",
              " ['Developed SSMW system in SWIR band can acquire a series of images in short duration. Proposed an automated quality score fusion scheme for classification of MW images. Proposed an automated method for classification of frontal vs non frontal face images. Proposed algorithms are beneficial when designing face recognition systems.'],\n",
              " ['We present an evaluation framework for the study of cross lingual link discovery. All aspects of this evaluation framework are detailed. The effectiveness of various cross lingual link discovery systems is discussed. Top approaches for realisation of English to Chinese link discovery are compared.'],\n",
              " ['A framework is proposed to restructure an initial video analysis system dynamically. The initial structure of the system must be hierarchical containing some units. The framework imposes a learning based dynamic feature selection method on each unit. The system learns how to direct attention to the informative units to achieve a goal. The framework is tested for goal and card event detection in broadcast soccer videos.'],\n",
              " ['A discriminating feature extraction DFE method for two class problems is proposed. The DFE method is applied to derive the discriminatory Haar features DHFs for eye detection. An efficient support vector machine eSVM is proposed to improve the efficiency of the SVM. An accurate and efficient eye detection method is presented using the DHFs and the eSVM.'],\n",
              " ['We examine the role of visual attention and image semantics in understanding image memorability. We propose an attention driven spatial pooling strategy for image memorability. Considering image features from the salient parts of images improves the results of the previous models. We also investigate different semantic properties of images. Combining attention driven pooling with semantic features yields state of the art results.'],\n",
              " ['We tackle the problem of microblog search differently by data fusion. We propose to integrate temporal characteristics into data fusion. We propose a novel and effective probabilistic data fusion model for microblog search.'],\n",
              " ['We propose methods to compare non deterministic IR systems. We show pitfalls in using standard significance tests to compare such systems. We verify the applicability of proposed methods using simulations and a case study. We show how to compare a non deterministic IR system for equivalent effectiveness.'],\n",
              " ['We propose a sentiment aware social media recommendation framework. An ensemble learning based method is proposed to classify sentiments from affective texts. We conduct comprehensive experiments to verify the effectiveness of the proposed methods.'],\n",
              " ['A tool assisting authority control in large bibliographic collections. An efficient data structure accelerates the identification of name variants. The space search is pruned using a parser of complex temporal expressions. The algorithm and the parser s grammar are distributed as open source resources.'],\n",
              " ['We propose a novel temporal re ranking algorithm. We devise and provide new datasets for time sensitive evaluation purposes. We conduct comparative experiments including algorithms with a temporal focus . We investigate the effectiveness of GRank by running a crowdsourcing experiment. We build a prototype system that can be tested by the research community.'],\n",
              " ['We collected ground truth data on induced musical emotion for 400 musical excerpts. We designed an online game with a purpose to attract a big number of participants. We analyzed inter rater agreement on emotional terms from GEMS model. We found that mood gender and liking or disliking the music influence induced emotion. We suggested improvements to GEMS scale.'],\n",
              " ['We automatically compile a dataset of 2012 US presidential election tweets. We annotate the tweets for sentiment emotion style and purpose. We show that the tweets convey negative emotions twice as often as positive. We describe two automatic systems that predict emotion and purpose in tweets.'],\n",
              " ['Broad topics on Twitter are highly dynamic. Boolean filtering retrieve high precision but limited number of tweets. A proposed adaptive filtering achieved 84 gain in recall with slight drop in prec. Proposed method showed robustness over time across domains and query formulations. Our method is currently adopted in a live service that follows news from Twitter.'],\n",
              " ['We design a Knowledge Infusion KI process for providing systems with background knowledge. We design a KI based recommendation algorithm for providing serendipitous recommendations. An in vitro evaluation shows the effectiveness of the proposed approach. We collected implicit emotional feedback on serendipitous recommendations. Results show that serendipity is moderately correlated with surprise and happiness.'],\n",
              " ['We model the supply chain that deals with products having high demand uncertainty. We characterize the optimal order strategies with bidirectional options provision. We examine the feedback effects that come from bidirectional options provision. We develop the bidirectional option contracts to coordinate the supply chain.'],\n",
              " ['Detection of negative deceptive opinion spam. Improved PU learning approach. Compares the performance of the proposed approach and the original PU learning method. The role of opinions polarity in the detection of deception. Reports experimental results on a set of negative deceptive opinions.'],\n",
              " ['Scalable algorithm based on bipartite networks to perform transduction. Unlabeled data effectively employed to improve classification performance. Better performance than algorithms based on vector space model or networks. Rigorous evaluation to show the drawbacks of the existing transductive algorithms. Trade off analysis between inductive supervised and transductive classification.'],\n",
              " ['To study a general model considering consumer uncertain post purchase valuation. To explore inventory control problem under consignment contract with consumer returns. To propose optimal inventory control strategy characterizing vendor s return policy.'],\n",
              " ['The paper deals with the Robust PCStT addressing interval uncertainty on its data. Different mathematical programming formulations are proposed. Branch and cut Algorithms are designed and extensively tested. Recent theoretical results are adapted for the Robust PCStT and some of it variants.'],\n",
              " ['We propose a new way of measuring document filtering system performance over time. Performance is calculated per batch and a trend line is fitted to the results. Systems are compared by their performance at the end of the evaluation period. Important insights emerge by re evaluating TREC KBA CCR runs of 2012 and 2013.'],\n",
              " ['4D framework for citation distribution analysis are proposed. Easily computable indices X Y and XY are proposed. The index X represents the breadth of citation distribution. The index Y represents the depth of citation distribution. The index XY synthetically represents indices X and Y.'],\n",
              " ['Risk sensitive evaluation of approaches for handling diacritics in Turkish information retrieval. Application of diacritics restoration to Turkish information retrieval. Investigation of the diacritics sensitivity of stemming algorithms.'],\n",
              " ['A new approach using ICD codes to substitute the manual relevance judgements. Improving IR effectiveness by using the proposed ICD based query expansion. A novel method to automatically map clinical queries into ICD codes.'],\n",
              " ['Reinforcement learning formulation for complex question answering. Abstract summaries used for small amount of supervision using reward scores. User interaction component incorporated to guide candidate sentence selection. Experiments reveal that systems trained with user interaction perform better. The reinforcement system is able to learn automatically and effectively.'],\n",
              " ['We describe a nested column generation algorithm for staff rostering. We show how generic programming can be used within a column generation implementation. Our resulting system is faster and more flexible than traditional systems. We can obtain good solutions to practical staff rostering problems.'],\n",
              " ['We propose a methodology to design a participatory budget processes. This methodology is based on a multicriteria decision making model. We have identified alternatives constraints criteria and a value function model. We illustrate the use of the methodology with an example. With this approach we hope to facilitate the design of participatory budget processes.'],\n",
              " ['IntoNews is an Intonow app for matching web news with news broadcasted on TV. We present a framework that models the task as 2 separate sub tasks Find retrieve. We design and publicly provide an evaluation testbed for the IntoNews problem. Quality evaluation has to trade off 2 competing metrics coverage and precision.'],\n",
              " ['Environmental sustainability through the reprocessing of end of life products. Traditional perspective and social responsibility perspective analysis. Critical aspects for sustainability in supply chain design are analysis. Transportation efficiency maximization through closed loop transportation system.'],\n",
              " ['We face the real world problem of having a limited set of pairwise constraints. Using pairwise constraints connected components CC are generated. The points local neighborhoods of the same CC are dynamically adapted. Constraints propagation to CC neighborhoods to increase the clustering accuracy. Scalability is ensured by following a landmark strategy.'],\n",
              " ['We develop a grey analytical network process based model for green supplier development programs selection. The model is applied to a leading manufacturer in the China s pivot irrigation equipment industry. We conclude that suppliers performance evaluation and involvement propensity should be simultaneously considered.'],\n",
              " ['We propose a novel hybrid method to capture group relation of sentences. We cluster sentences with a KL divergence based on word topic distribution. We proposed a vertex reinforcement random walk process in a hypergraph model. The process simultaneously consider the query similarity the centrality and the diversity of sentences. We implement our framework and verify improvement over appropriate baselines.'],\n",
              " ['Hightlights Query suggestion QS problem is reduced to comparison of queries problem. A modular and practical framework is suggested for development of QS algorithms. Breadth First Search graph traversal method is better for query log traversing. Combining QS algorithms improves the performance. Suggested new QS algorithms achieved 66 90 performance increase.'],\n",
              " ['In the legal domain this method for statute prediction is a new research topic. We predict relevant statutes for the problem described by everyday vocabulary. The gap between lay terms and legal terms was remedied without using a synopsis. Employing the Normalized Google Distance SVM and Apriori algorithms into TPP. The result shows the performance of TPP is accurately and effectively.'],\n",
              " ['We proposed a specialized method that works well in assessing short summaries. It integrates the semantic relations between words and their syntactic composition. We experimentally examine the influence of the combination of semantic and syntactic information on short summary assessment. Experiments have displayed that it is to be preferred over the existing techniques. We have developed the method as an intelligent tool to grade students summaries.'],\n",
              " ['We study different strategies to classify sentiment from tweets using supervised learning with hybrid features. We experiment with English and Spanish data and compare against benchmark competitions. We employ machine translated data from other languages for training. We show that the use of multilingual data improves the sentiment classification accuracy.'],\n",
              " ['The presented study performs two different tests in intra and inter subject context. A set of 180 features is implemented to be selected based on clustering performance. Our algorithm searches for the best feature extraction parameter. A new clustering metric based on the construction of the confusion matrix is proposed. A novel gesture recognition system based on data from a single 3 dimensional accelerometer.'],\n",
              " ['We introduce two novel LM based models to relax the exact matching assumption in IR. The class based model clusters words to provide a coarse grained word representation. The trigger model captures pairs of trigger and target words to find word relationships. Different types of word co occurrence and triggering are studied within the models. We further studied the combination of both models to achieve the best result.'],\n",
              " ['Information embedding and retrieval in digital images. Digital Watermarking. Image processing.'],\n",
              " ['We propose linear models for the supply chain of a speciality oils company. We consider operation and sales decisions in decoupled and integrated approaches. The integrated outperforms the decoupled planning for the company. The sellers may get worse premiums in the integrated solution. Company and sellers are better off by reallocating the integrated contribution.'],\n",
              " ['A new framework for answering definitional and factoid questions. Producing new features by combining effective features with arithmetic operators. Genetic Programming GP algorithm has been employed for feature learning. Three discriminant based methods have been used for learning features weights.'],\n",
              " ['Set based access to XML data. Improved XPath primitive operations. Up to eight orders of magnitude speed up.'],\n",
              " ['We present a framework SenticRank to incorporate sentiment for personalized search. Content based and collaborative sentiment ranking methods are proposed. We compare the proposed sentiment based search with baselines experimentally. We study the influence of sentiment corpora by using some sentiment dictionaries. Sentiment based information can significantly improve performance in folksonomy.'],\n",
              " ['Reviews OR analysis of the effects of changes to the rules and laws of sports. Also reviews OR analysis of proposed future changes. Cites 67 references involving 21 different sports.'],\n",
              " ['The three basic tools for deepening the capacity of mobile wireless networks are described. Historical growth in spectrum quantity geographic reuse and throughput efficiency is reported. The capability of 4G LTE and LTE Advanced to further improve network capacity is projected. These projected capacity improvements are compared with forecasts of future U.S. mobile demand. U.S. networks will require an additional 560MHz of spectrum by 2022 to meet forecasted demand.'],\n",
              " ['We analyse the named entity recognition and disambiguation performance on tweets. Multiple state of the art systems are included. Commercial and academic systems suffer the same range of problems. Lack of context is a major problem demanding new custom NER NEL approaches. A named entity linking corpus is released with the paper.'],\n",
              " ['We analyse the ramp up process and identify the key performance metrics. We formalise the ramp up process as a state transition model. We provide a framework for measuring the overall ramp up performance based on the identified metrics. The performance measure has been shown to improve ramp up efficiency and reduce time.'],\n",
              " ['We study the lot sizing problem with time dependent batch sizes. We prove that the problem is NP hard if one of the cost parameters is time varying. The problem is polynomial for stationary cost parameters and null holding cost.'],\n",
              " ['The use of hashtags such as sarcasm reduces the further use of linguistic markers of sarcasm in tweets. Hashtags such as sarcasm appear to be the extralinguistic equivalent of non verbal expressions in live interaction. Sarcastic hashtags are 90 appropriate. Sarcastic tweets without hashtags are hard to distinguish from non sarcastic hyperbolic tweets. In French tweets the hashtag sarcasme conveys a polarity switch less frequently than in Dutch.'],\n",
              " ['We define a new measure of relevance of a node in the graph to a keyword query. We propose an extended answer structure for a top k query over graph databases. We propose an inverted list index and search algorithm to find top k answer trees. We enhanced the basic method for more efficient and scalable processing the query. Experiments show that the proposed method can find effective top k answers efficiently.'],\n",
              " ['Proposal of graph and count weightage based ranking algorithms. Addition of parameters number of runs and wickets for graph weightage. Study of the affect of damping factor on different PageRank based ranking methods.'],\n",
              " ['We argue that the h index has a deficiency in comparing journals for ranking purpose. We propose the global h index Gh index and its extensions to fix this problem. We apply the new indexes to produce an automatic journal classification without arbitrary cut off points. The new ranking results are compatible with other peer reviewed based ranking results.'],\n",
              " ['Three complexity levels of information problems are defined related to psychology domain. 40 students in psychology and in other domains performed information problems with an online encyclopedia. Students in psychology performed better than the others especially for complex problems. Students in psychologies used more relevant strategies than the others. These expertise related differences are stronger for the complex problems.'],\n",
              " ['Proposed an analytical model for the two and three echelon reverse supply chain. Defined the PC recycling supply chain as foundation for generating the analytical model. Analyzed the model behavior and profit implications by the applicability of a numerical example.'],\n",
              " ['We study the effects of misspelled queries on the performance of CLIR systems. Word based approaches as both indexing and translation units are highly sensitive to the presence of misspellings. The use of correction mechanisms can significantly reduce their negative effects. Classical techniques are suitable for shorter queries while context based corrections are suitable for longer queries. Our approach based on character n grams as both indexing and translation units shows remarkable strength.'],\n",
              " ['We examine RecSys studies in China quantitatively empirically and longitudinally. Research on RecSys is relatively mature and well developed overall. Twelve theme clusters and six larger branches are identified. Some undeveloped or immature research themes continue to persist. Emerging themes with great potential for development are also identified.'],\n",
              " ['We propose a novel way to compare learning to rank methods. We perform a meta analysis on a large set of papers that report ranking accuracy on a benchmark dataset. LRUF FSMRank FenchelRank SmoothRank and ListNet are the most accurate with increasing certainty.'],\n",
              " ['We considered the fact that multi phones exist in a traveling vehicle. We used a clustering method to estimate freeway traffic measures from mobile phone location data. Three factors influence estimation accuracy of traffic measures. Further development of location technologies will improve estimation accuracy.'],\n",
              " ['New approach for transforming polynomial MINLP into MILP. Approximating upper bounding and relaxation lower bounding formulations are given. Bilinear term is basic building block where one variable is discretized. Base 2 to base 10 numeric representation systems can be employed. Problem size increases logarithmically with number of discrete points optimality gap decreases linearly.'],\n",
              " ['External search and internal search cohere searching sessions. The referral query from external search can be predictive of internal searching. There are common patterns that one can use to classify internal searchers.'],\n",
              " ['We propose a semantic question answering system MEANS for the medical domain. We introduce a novel query relaxation approach for question answering. MEANS integrates NLP methods allowing a deep analysis of questions and documents. MEANS uses semantic Web technologies and standards for data sharing and integration. Our experiments show promising results in terms of MRR and precision.'],\n",
              " ['Study the equilibrium strategic behavior and social optimization in the M M 1 retrial queues. Two different levels of information provided to arriving customers have been investigated extensively. Nash equilibrium and the equilibrium social benefit for all customers are derived. The effect of the information levels is illustrated by extensive numerical examples. It provides insight into the optimal design and control of the retrial queueing systems.'],\n",
              " ['We model special day effects to predict hourly electricity demand in Korea. The regression model with hourly special day effect and the double SARMA error is suggested. The hourly special day effects are more important than the daily ones. The suggested model is effective during both the special day prediction and the non special day prediction.'],\n",
              " ['We propose a framework for evaluating exploratory search using implicit features. User search action sequences are also used to find behavior patterns. Show effectiveness with above 70 prediction accuracy for user search performance. Provide search process based recommendation to assist underperforming users. Demonstrate recommendation effectiveness both qualitatively and quantitatively.'],\n",
              " ['We analyze the timing decision of a company which can invest in a CO2 saving project. Doing this we use a real options game theoretic model. Total emission of the supply chain increases with carbon price uncertainty. Furthermore inefficiency increases with the length of the supply chain. Cooperation and vertical integration improve ecologic as well as economic efficiency.'],\n",
              " ['Study of the impact of the implicit aspects of knowledge graphs for cross language plagiarism detection. We present a new weighting scheme for relations between concepts based on distributed representations of concepts. We obtain state of the art performance compared to several state of the art models.'],\n",
              " ['We propose two novel aspect rating prediction approaches i.e. Quad tuple prediction and Expectation prediction. We analyze and investigate the performance of the proposed aspect rating prediction methods in contrast with Local Prediction and Global Prediction. We experimentally inspect the influence of aspect rating variance for different rating prediction approaches.'],\n",
              " ['We propose a new unsupervised method that uses word sense discrimination in IR. The re ranking method is based on spectral clustering. The effectiveness over queries with ambiguous terms is proved on TREC corpora. Our interest regards improving the precision after 5 10 and 30 retrieved documents. The method improves results for queries with poor baseline performance.'],\n",
              " ['A novel personalized semantic movie summarization approach is proposed. User s subjective preferences are exploited for personalized movie summarization. An extensive user study demonstrates the advantages of the proposed system. A subjective study shows the users diverse behavior on the system usability. Proof of concept prototype of the proposed movie summarization system is provided.'],\n",
              " ['This article provides an inventory model for a serial supply chain with multiple suppliers and time varying demand. The model minimizes the total cost of purchasing production inventory setup and holding and transportation. Considering lead times necessary feasibility conditions regarding the demand at the last stage are established. Actual transportation costs are modeled by exact piecewise linear functions. Our results show that inventory setup and holding costs can affect supplier selection and order lot sizing decisions.'],\n",
              " ['Based on the theories of comprehensive decision making a multiattribute ranking method is presented. The proposed method is superior to other methods through contrast experiments. The theories of comprehensive decision making are utilized to detect important nodes in social networks. This method can be used in different types and different scales of networks.'],\n",
              " ['We designed the first model for effectively carrying out opinion mining on YouTube comments. We propose kernel methods applied to a robust shallow syntactic structure which improves accuracy for both languages. Our approach greatly outperforms other basic models on cross domain settings. We created a YouTube corpus in Italian and English and made it available for the research community. Comments must be classified in subcategories to make opinion mining effective on YouTube.'],\n",
              " ['The paper is an inter disciplinary research of applying OR in projects management. Address a sustainability issue of handling continuous IS project demands in contemporary IT departments. Features mixed integer programming model to optimize the selection and assignment of projects on a multi period multi project basis. It innovatively considers the losses due to unselecting or delaying projects in each time of selection and assignment. It implements a computerized solution that automatically enables the sustainability of continuous and cumulative selections and assignments.'],\n",
              " ['We show that the Malmquist Luenberger ML index may yield inconsistent results. The ML index can signal declining productivity when productivity is increasing. We discuss this issue within the standard decomposition of the ML index. We use a simple example and give cautionary advice to practitioners. We propose a solution for solving the inconsistency through a new postulate.'],\n",
              " ['Westeros Sentinel a visual analytics dashboard for Game of Thrones. Extraction of affective and factual knowledge from news and social media coverage. Emotional categories from semantic knowledge bases. Automated annotation services for contextualized information spaces. Interactive visualizations to explore context features.'],\n",
              " ['The concept of coupling and major coupling relationships. Coupling layers and forms appearing in complex data and applications. Modeling couplings measuring couplings and the curse of couplings. A new theoretical framework for the next generation recommender systems. Case studies of learning couplings in data mining and recommendation.'],\n",
              " ['We propose a measure for cloud computing using existing datasets. Our empirical findings map out an agenda for future research. Cloud studies should be at the industry level. Researchers need to figure out what firms actually do with cloud computing. Studying cloud usage across the value chain helps understanding the mechanisms.'],\n",
              " ['We identify classify and analyze 15 multi attribute vehicle routing problems. We analyze in detail 64 of the most efficient heuristics for these problems. We identify winning strategies for designing effective heuristics for MAVRP s.'],\n",
              " ['We propose a novel parallel Algorithm for inferring topic hierarchies using HLDA. We use loosely coupled parallel tasks that do not require frequent synchronization. The parallel Algorithm is well suited to be run on distributed computing systems. The proposed Algorithm achieves a predictive accuracy on par with that of HLDA. The parallel Algorithm exhibits a near linear speed up and scales well.'],\n",
              " ['MRDQA a model based approach for supporting the Data Quality task on KDD. Evaluation of quality requirements of weakly structured data via model checking. A fine grained quality analysis of the cleansing procedures effectiveness. Automatic identification of error patterns and interactive visualisation. Experiments done on a real scenario making data publicly available.'],\n",
              " ['Emotions evoked by artworks are captured by emotion analysis of social tags. An OWL 2 ontology based on Plutchik s circumplex model of emotions is developed. Ontologies linked data and affective lexicons are combined in a novel framework. Emotion driven organization and access to online art collections is enabled. The proposal is applied to a real dataset of tagged multimedia artworks.'],\n",
              " ['We carry out an empirical analysis to determine characteristics of social media channels. User generated content is noisy and contains mistakes emoticons etc. We evaluate text preprocessing algorithms regarding user generated content. Discussion of improvements to opinion mining process.'],\n",
              " ['New vehicle routing model with operational constraints. Routes satisfy a blend of plain delivery and pick up and delivery requests. Feasible pallet packing patterns must be identified for the transported goods. Local search framework equipped with appropriate memory components. Results are reported on new benchmark instances derived from well known routing problems.'],\n",
              " ['The paper is one of the few papers to explore the role of emotion in eWOM. It includes a practical study of a business using eWOM and the data is analyzed. It provides recommendations on how to use emotion in eWOM to encourage participation and improve sales.'],\n",
              " ['A new precedence based crossover operator has been developed based on extensive research on related work. The operator can be applied to a variety of permutation problems. The procedure strikes a balance between diversification and intensification. The computational experiments show the effectiveness of the procedure. The operator is versatile the procedure is robust and the results are very promising.'],\n",
              " ['Linguistically motivated query expansion framework. Classification of query constituents within four concept types descriptive relational structural and concepts of interest. Algorithm capitalizing on syntactical dependencies to capture relationships between adjacent and non adjacent query concepts. Weighting scheme emphasizing the importance of query constituents based on their linguistic role.'],\n",
              " ['We extend citation based summarization techniques to use both cited and citing documents. Our method combines citation text with other elements of cited and citing documents such as the full text and catchphrases. We investigate using citations to extract information from the target document. We apply these techniques on the domain of legal judgements a new area for citations based summarization. On a legal summarization corpus our methods outperform competitive baselines.'],\n",
              " ['Urban legends are viral deceptive texts in between credible and incredible. To be credible they mimic news articles while being incredible like a fairy tale. High level features who where when of news emotional readable of fairy tales. Quantitative analysis and machine learning experiments for recognizing urban legends.'],\n",
              " ['Augment frequency based aspect extraction with web based similarity measure. Improve the efficiency of aspect extraction methods that use PMI IR measures. Demonstrate the generality of the proposed method across various products.'],\n",
              " ['Weighted p center model is proposed for locating urgent relief distribution centers. Uncertain relief demands and delivery times are represented by fixed intervals. The objective is to optimize worst case performances. Property of worst case scenarios is identified and applied in the algorithm. The proposed model is applied to a real world case based on a massive earthquake.'],\n",
              " ['We applied the hidden Markov model for stock bond and commodity markets. HMM identified two extreme regimes and two transition regimes. We employed the stochastic programming under regime switching framework. Optimized portfolio performed well in crash periods by reducing the risky assets.'],\n",
              " ['We examined the robustness of the User Engagement Scale UES . Three studies were conducted in Canada and the United Kingdom. The UES sub scales were reliable across three samples of online news browser. A four factor structure rather than six may be more appropriate. The UES differentiated between online news sources and experimental conditions.'],\n",
              " ['We explore state of the art supervised machine learning methods for sentiment analysis of Czech social media. We provide a large human annotated Czech social media corpus. We explore different pre processing techniques and employ various features and classifiers. We experiment with five different feature selection algorithms. Results are also reported on other widely popular domains such as movie and product reviews.'],\n",
              " ['We used the character n gram method to predict topic changes in search engine queries. We obtained more successful estimations than previous studies and made remarkable contributions. We compared the character n gram method with the Levenshtein edit distance method. We analyzed ASPELL Google and Bing search engines as pre processed spelling correction methods. We conclude that Google could be used as a pre processed spelling correction method.'],\n",
              " ['This paper discusses the individual choice of numerical scale in AHP. We present an algorithm to obtain the linguistic individual characteristics in AHP. We set individual numerical scales to optimally match the individual characteristics.'],\n",
              " ['A graph of terms can be effectively used for query expansion. Such a graph is extracted from documents thanks to a LDA based methodology. Proposed method achieves good performances on standard datasets.'],\n",
              " ['This investigation deals with the problem of language identification of noisy texts. Two statistical approaches are proposed High Frequency Approach and Nearest Prototype Approach. The proposed methods are evaluated on forum datasets containing 32 different languages. An experimental comparison is made with LIGA NTC Google translate and Microsoft Word. Results show that the proposed approaches are interesting in language identification of forum texts.'],\n",
              " ['A systematic overview of multilingual probabilistic topic modeling MuPTM . A tutorial on methodology modeling training output inference and evaluation of MuPTM. Language independent and language pair independent data representations. A model independent framework and applications in various cross lingual tasks. A complete MuPTM based framework for cross lingual semantic similarity.'],\n",
              " ['Elucidation of children s info seeking behaviors using virtual retrieval interfaces. The use of SEM to explore the relationship between fun and uncertainty. Children prefer a virtual retrieval interface when at play. Text sound channels are less definite but easier to use than icon channels. Children at play are willing to endure a high degree of uncertainty control.'],\n",
              " ['We take into account emotions for author profiling. We model the six basic emotions with a novel graph based approach EmoGraph . We achieve comparable results to the best performing systems of the PAN Lab of CLEF 2013.'],\n",
              " ['We review the literature on parallel machine scheduling with additional resources. We structure the review around five categories. We provide both the strengths and weaknesses of the studies together with open areas. We propose several extensions on some of the mathematical models given in the literature. We conclude with several remarks based on our computational studies and the review.'],\n",
              " ['We review representative optimization problems arising in biodiversity protection. They concern e.g. nature reserves landscape fragmentation and genetic diversity. Some of these problems come from the literature and others are new. For each problem we present one or several mathematical programming formulations. Some computational results are reported.'],\n",
              " ['We use only web document collection instead of query logs and external resources. Our simple patterns are based on noun phrases and alternative partial queries. We maintain a balance between popularity and diversity of subtopics. Our method covered various search intentions of a query by its few subtopics. Our results were steadily improved by extracting more relevant and various subtopics.'],\n",
              " ['We propose a semantic sentiment representation of words called SentiCircle. SentiCircle captures the contextual semantic of words from their co occurrences. SentiCircle updates the sentiment of words based on their contextual semantics. SentiCircle can be used to perform entity and tweet level level sentiment analysis.'],\n",
              " ['Statistical approach for estimating the focus time of text documents. Classification framework for categorizing documents into temporal and atemporal. Bi Temporal Document Representation using document focus time and creation time.'],\n",
              " ['We introduce a new information retrieval task given a topic try to find knowledgeable groups that have expertise on the topic. Five probabilistic language models are proposed to tackle the challenge of automatically finding groups of experts in heterogeneous document collections. For evaluation purpose a data set is created based on a publicly downloadable corpus used in the TREC Enterprise 2005 and 2006 tracks and three types of ground truth are defined. We provide a detailed analysis of the performance of the proposed group finding models.'],\n",
              " ['New unsupervised stemming algorithm is introduced in this article. The algorithm exploits lexical as well as semantic information of words. Performance of stemming is measured on several languages Czech Slovak Polish Hungarian Spanish and English . We outperform competing stemmers in inflection removal test information retrieval task and language modeling task.'],\n",
              " ['We consider three quadratic optimization problems applied in portfolio theory. Conditions are derived under which the solutions of these three optimization procedures are lying on the efficient frontier. We show that the three quadratic optimization problems are mathematically equivalent but not from a stochastic point of view. We derive the probabilities that the estimated solutions of the Markowitz problem and the quadratic utility problem are mean variance efficient.'],\n",
              " ['We find that reputation polarity of a post is different from sentiment. We model reputation polarity using feature classes from communication theory. We introduce new features based on the replies to a post. We propose different ways to operationalise the RepLab 2012 and 2013 tasks.'],\n",
              " ['We apply knapsack analysis to the problem of screening for nuclear materials. The analysis considers various levels of intelligence based pre screening . We illustrate the effectiveness of screening methods using speculative values. The analysis models a range of threats when screening for nuclear material.'],\n",
              " ['We establish a formal link between expected utility and spectral risk measures. We give sufficient conditions for existence and uniqueness of such a link. We provide a spectral version of the Pratt Arrow risk aversion measure.'],\n",
              " ['The explosion of online user generated content UGC and the development of big data analysis provide a new opportunity and challenge to understand and respond to public opinions in the G2C e government context. We proposed an approach based on the latent Dirichlet allocation LDA and designed a practical system to provide users with satisfying searching results and the longitudinal changing curves of related topics. Municipal administrators could better understand citizens online comments based on the proposed semantic search approach and could improve their decision making process by considering public opinions.'],\n",
              " ['The formal problem of planning a tourist visit as a fully automatic two step process. An unsupervised method for mining common patterns of movements of tourists. A comprehensive evaluation of TripBuilder.'],\n",
              " ['We introduce an iterative approach to solve the unoriented two stage DEA model using the classical radial objective. Our approach determines optimal levels for each input and output by simultaneously reducing inputs and increasing outputs while the intermediate products oscillate and converge. Unlike the slacks based approach our model does not require arbitrary weights on inputs and outputs. We demonstrate our model by applying it to Major League Baseball teams during the 2009 season. The model has one input two intermediate products and one output.'],\n",
              " ['Genetic Algorithm is an effective scheme in determining data fusion weights. Tuning Genetic Algorithm increases time efficiency. Weight learning from only top ranked documents is useful. Redundant runs can be removed based on correlation between scores.'],\n",
              " ['Proposing a language modeling method to extract translations from comparable corpora. Comparing two similarity functions for deriving bilingual word correlations. Improving translation quality by integrating co occurrence relations into word models. Comparing different estimations of translation probabilities from word correlations. Showing the significant impact of probability estimation methods on CLIR performance.'],\n",
              " ['We model a closed loop supply chain in which the manufacturer can remanufacture used products. The interaction with the supplier significantly impacts the performance of remanufacturing. The remanufacturing opportunity can form a lose lose situation making the players profits shrunk. The manufacturer may be worse off with a lower remanufacturing cost or a larger return rate. The remanufacturing may be detrimental to the environment.'],\n",
              " ['Examine the influence factors of the Internet on Gens Y and Z. Empirical evidence was collected via an online survey in Australia and Portugal. This study composed new positive and negative factors of the Internet usage. Set of recommendations are raised to reduce the negativities of the Internet usage.'],\n",
              " ['We consider the optimal ship navigation problem in the presence of obstacles. We explicitly account for safety distance and realistic ship turn radius constraints. We present a graph theoretic solution on an appropriately weighted directed graph. Our methodology is illustrated on a merchant ship ice navigation example.'],\n",
              " ['We develop a model where two players simultaneously choose their facilities sites. One of the players has preferential rights over the other. Nash equilibria are calculated by an algorithmic approach using linear programming. Including some level of asymmetry between players can benefit the franchiser.'],\n",
              " ['We show a set containment characterization with data uncertainty. We investigate surrogate strong duality theorem for robust quasiconvex programming with its constraint qualification. We investigate surrogate min max duality theorem for robust quasiconvex programming with its constraint qualification. We obtain a surrogate duality theorems for semi definite optimization problems in the face of data uncertainty.'],\n",
              " ['Introduce a probabilistic graphical model extracting topics with numerical guidance. Enhance the regression performance with unified PGM of text and numbers. Tightly links the analysis on numeric and text data over time.'],\n",
              " ['We introduce the robustness measure and its properties. We propose new robust formulations that allow investors to control the factor exposure of portfolios. The optimal portfolio of our model is not only robust but also has a desired factor exposure.'],\n",
              " ['We model three different agencies involved in disaster management through OR. We coordinate agencies responsible for different phases of disaster management. Coordination indicates an average improvement of 7.5 24 in the death toll. Through multi agent optimization connections between the agencies are observed. Efficiency of relief operations is dependent on transportation network reliability.'],\n",
              " ['Temporal information searching behaviour and strategies were investigated. Searching patterns were identified for past recency and future search tasks. Implications for the development of temporal IR systems are discussed.'],\n",
              " ['Digital transition had resulted in changes in researcher behaviour. It is now easier for scientists to discover and disseminate research. The way scientists exercise trust has not changed. Metrics are less important than experience and personal recommendation.'],\n",
              " ['We developed a model for the joint optimization of process control and maintenance. Information from the control charts is used to facilitate maintenance decisions. Process control and maintenance procedures are inter dependent. Potential cost savings can be obtained from joint SPC and maintenance policies.'],\n",
              " ['Development of a search task difficulty reason scheme. Relationship between task difficulty and task type common and different reasons across task types. Relationship between task difficulty and user knowledge common and different reasons among different knowledge groups. Implications for general and personalized information retrieval system design.'],\n",
              " ['We analyze which social software features influence the usefulness of BI reports. We test theory based hypotheses in an experiment with knowledge workers. Users of BI reports are mostly influenced by the report s argument quality. Personal characteristics also influence the users perception of report usefulness.'],\n",
              " ['We propose a learning to rank based query auto completion model L2R QAC that exploits contributions from so called homologous queries for a QAC candidate in which two kinds of homologous queries are taken into account. We propose semantic features for QAC using the semantic relatedness of terms inside a query candidate and of pairs of terms from a candidate and from queries previously submitted in the same session. We analyze the effectiveness of our L2R QAC model with newly added features and find that it significantly outperforms state of the art QAC models either based on learning to rank or on popularity.'],\n",
              " ['The rise of social media has fueled interest in sentiment classification. POS RS is proposed for sentiment analysis based on part of speech analysis. Ten public datasets were investigated to verify the effectiveness of POS RS. Experimental results reveal POS RS can be used as a viable method.'],\n",
              " ['We develop an incremental algorithm to solve clusterwise linear regression problems. The algorithm gradually computes clusters and linear regression functions. Two special procedures to construct initial solutions are proposed. The algorithm finds global or near global minimizers of the overall fit function.'],\n",
              " ['A new model for software reliability is proposed. Proposed model enables us to infer performance of the debugging process. Bayesian inference is developed. An approach is introduced to assess dimension of the Markov process.'],\n",
              " ['Proposes automatic semantics and image retrieval system for hierarchical databases. System uses 1 3 search space to retrieve semantics and finally similar images. 77 semantic retrieval accuracy on ImageNet. Uses 4 space to retrieve images. System reports precision of 0.78 @ 20 and 0.67 @ 100 images on categorized WANG. The study explores adequacy of visual signatures set used to represent a semantic.'],\n",
              " ['We study 10 techniques to compress users cursor interactions on the Web. A systematic evaluation of both lossy and lossless algorithms is performed. We found that different compression techniques excel depending on the objective. LZW and piecewise linear interpolation balance well accuracy and efficiency.'],\n",
              " ['We formally described the answerer ranking problem in CQA service. A novel model with tensor decomposition is proposed to solve the problem. The parameters of the above model are learned by maximizing the multi class AUC. Introducing the asker dimension improves the answerer ranking performance. Our approach outperforms the previous methods on two real world CQA datasets.'],\n",
              " ['We have developed a new approach that effectively retrieves event based images. We have proposed a rigorous technique to extract spatial features from image tags. We have developed a method for summarizing spatial distributions of single tags. We have developed new techniques for spatial relatedness between two tag terms. Our spatio temporal IR method improves the retrieval performance significantly.'],\n",
              " ['A mutual influence and exclusive citations based method for author ranking is proposed. It considers effect of publications citations and publications as first author for authors and co authors. It also considers exclusivity in citations received by an author.'],\n",
              " ['Novel group hybrid method combining collaborative and content based recommendation. Proposed method improves the quality of recommended items ordering. Proposed method increases the recommendation precision for very Top N results. Applicable for single user as well as group recommendation.'],\n",
              " ['Existing literature mainly focuses on fluid models . We formulate discrete versions of basic and extended forward reserve problems. We discuss their computational complexity and present suitable solution procedures. We show the basic allocation problem to be solvable in polynomial time. We propose and computationally examine a very effective repair heuristic .'],\n",
              " ['The precedence constrained single machine scheduling problem is considered. An efficient exact algorithm is proposed based on our previous studies. The SSDP Successive Sublimation Dynamic Programming method is employed. Instances of the total weighted tardiness problem with up to 100 jobs are solved.'],\n",
              " ['A formal representation of Wikipedia concepts is presented. A framework for feature based similarity is proposed. Some novel feature based approaches to semantic similarity measures are presented. Results show that several proposed methods have good human correlation.'],\n",
              " ['A novel solution heuristic for the General Lotsizing and Scheduling Problem for Parallel production Lines is presented. The idea is to iteratively decompose the parallel line problem into a series of single line problems. The new heuristic improves already existing approaches. Large practical applications can now be solved.'],\n",
              " ['We formalize the problem of retrieving daily deals in the context of Web search. We effectively combine keyword based retrieval with automated classification. Our solution outperforms state of the art query expansion and prior ad ranking work.'],\n",
              " ['This paper proposes a test for whether data are over represented in a subset of a DEA estimated production possibility set. The binomial test the number of observations inside a zone to a discrete probability weighted relative volume of that zone. A Monte Carlo simulation illustrates the performance of the proposed test.'],\n",
              " ['The summarization of changes addresses a new challenge the automatic summarization of changes in dynamic text collections. Four different approaches are proposed for the summarization of changes. A system based on Latent Dirichlet Allocation model is used to find the hidden topic structures of changes. The approach based on LDA model outperforms all the other approaches. The differences in ROUGE scores for LDA based approach is statistically significant at 99 over baseline.'],\n",
              " ['The impact of multilevel broadband policies on network development in Canada. Analyzes variations in the levels and patterns of network performance growth. Federal regulations limited the scope for policy decentralization. Higher rate of progress in provinces that promoted access to essential facilities.'],\n",
              " ['This article studies the effect of the Average Sample Number ASN on the performance of the SPRT chart. A design algorithm is proposed to explore the optimal ASN of the SPRT chart for monitoring the fraction nonconforming p. The optimal SPRT chart significantly outperforms the optimal np chart optimal CUSUM chart and basic SPRT chart.'],\n",
              " ['A new multi view clustering algorithm is proposed. The proposed MVNC algorithm uses spectral partitioning and local refinement. MVNC is compared to state of the art algorithms using three real world datasets. MVNC significantly outperforms the other algorithms. MVNC is parameter free unlike existing multi view clustering algorithms.'],\n",
              " ['Presents alternative procedure for separating subtour elimination constraints. Procedure is based on computing the strong components of the support graph. Procedure has better worst case time complexity than standard way of separating SECs. Moreover procedure is easier to implement. Computational experiments verify practical usefulness of the procedure.'],\n",
              " ['We propose a bi level fuzzy weighting to discriminate view and feature simultaneously. We develop a hybrid optimization based on particle swarm optimization. We propose a representative based method to determine hyper parameters. Our method is compared with six clustering algorithm on three real world datasets.'],\n",
              " ['We present a new query processing method for text search. We extend the BMW CS algorithm to now preserve the top k results proposing BMW CSP. We show through experiments that the method is competitive when compared to baselines.'],\n",
              " ['We propose a probabilistic model for matching clusters in different domains without correspondence information. The proposed method can handle data with more than two domains and the number of objects in each domain can be different. We extend the proposed method for a semi supervised setting. We demonstrate that the proposed method achieve better matching performance than existing methods using synthetic and real world data sets.'],\n",
              " ['A new fast optimal split for the team orienteering problem. An effective particle swarm algorithm PSOiA . PSOiA is robust and outperforms the state of the art algorithms in the literature.'],\n",
              " ['Semi Markov decision problem formulation. Benefits of proactive over different reactive transshipment rules. Networks with intermediate opportunities of demand pooling. The difference between reactive transshipment rules is negligible.'],\n",
              " ['We present a systematic framework for discovering topic time from Web news. We propose a new approach to determine the referential time for implicit temporal expressions. We devise a relation model between news topics and topic time for topic time extraction. We build a prototype system and conduct comparative experiments on real datasets.'],\n",
              " ['An interactive approach for a mixed integer multi criteria optimization is introduced. The DM makes pairwise comparisons of identified Pareto optimal points and gives reference points. Assuming a quasi concave value function pairwise comparisons are used to contract the cone of admissible objective vectors. Numerical simulation tests indicate reasonably fast convergence. Convergence is guaranteed for the pure integer case.'],\n",
              " ['A lexicon based domain adaptation method is proposed. Several domain polar lexicons were compiled following a corpus based approach. The new resources are assessed over a Spanish corpus. The promising results encourage us to follow improving this domain adaptation method.'],\n",
              " ['A new branch and bound is proposed for the simple assembly line balancing problem. A novel flow based logical test and a strengthened dominance rule are put forward. The results from the proposed algorithm outperform those provided by the previous procedures found in the literature.'],\n",
              " ['First paper to focus on discrete r q and s T inventory systems. Some well known properties of the continuous systems no longer hold. Show exactly where joint convexity breaks down and establish other useful properties. Revise comparative properties of the continuous model for the discrete model.'],\n",
              " ['We model how to implement a rubric in latent semantic analysis. The proposed method change abstract dimensions into meaningful dimensions. The method allows to detect easily written contents. Inbuilt rubric method has been used to give feedback to 924 university students.'],\n",
              " ['We provide an integrative review of the existing literature of trade credit in the interface of operations and finance. We analyze in detail four literature areas trade credit motives order quantity decisions credit term decisions and settlement period decisions. The main findings of the literature review is that trade credit increases the economic order quantity and could serve as a buyer supplier coordination mechanism. We derive a detailed agenda for future research around two core themes opportunities arising from inside operations management and opportunities arising from outside operations management.'],\n",
              " ['To capture the sentiment of messages several expressive forms are investigated. Expressive signals enrich the feature space of baseline and ensemble classifiers. Only adjectives play a fundamental role as expressive signal. Pragmatic particles and expressive lengthening could lead to the de finition of erratic polarity classifiers.'],\n",
              " ['A dynamic model of the firm under multiple correlated uncertainties. Completely generalize the previous static results. The impact of one risk on the aversion to another.'],\n",
              " ['We study dynamic pricing and advertising policies in a sales model for a monopolist firm. Both autonomous and advertising dependent word of mouth are considered. Optimal limit cycles alternating building and leveraging of the price advantage are found. Multiple equilibria separated by Skiba curves are numerically ascertained.'],\n",
              " ['Positive and negative externalities ought to be modelled to obtain unbiased measures of productivity. We develop an additive style data envelopment analysis model that accounts for both externalities. We model limited increases in input utilisation in order to further reduce negative externalities. The directional economic environmental distance DEED function is a unified approach. DEED draws from a unique reference technology enabling reasonable productivity measure comparisons.'],\n",
              " ['We develop and apply a methodology to characterise hand held detection systems. Methodology which incorporates field trials and workshops allows scrutiny. Robust data presentation technique was developed to facilitate common understanding. Methodology and presentation satisfies the needs of multiple stakeholder groups. Can be applied to future detectors allowing direct comparisons to be made.'],\n",
              " ['We analyze a time fenced planning system with penalized expediting and canceling. We formulate deterministic and stochastic dynamic programs and develop bounds. Allowing both expediting and canceling lowered costs 28 versus 11 for expediting only.'],\n",
              " ['We consider the RCPSP with general temporal constraints and break calendars. A new and powerful time planning method is proposed. Three binary linear model formulations are presented. A priority rule method and three different versions of a scatter search procedure are developed. We provide a test set and perform an extensive computational study.'],\n",
              " ['Coelli et al. s 2007 environmental efficiency measure does not reward pollution control. Neglecting efforts to control pollutants may lead to biased environmental efficiency scores. A new environmental efficiency measure that rewards pollution control efforts is proposed. A numerical example using Data Envelopment Analysis is provided.'],\n",
              " ['We introduce a problem generalizing previous VRPs with 2 D loading constraints. Memorization strategies are employed for keeping the CPU times in acceptable levels. Extensive computational results on well known benchmark instances are reported. The proposed model promotes high loading space utilization throughout the routes. Item rearrangements along vehicle routes bring significant routing cost savings.'],\n",
              " ['This paper raises the problem of measuring partial derivatives of transformation functions in DEA models. The derivatives of the DEA frontier imply a displacement on the frontier and a shift of the frontier. We reconcile the theoretical partial derivatives and the empirical DEA version of these derivatives. We show the invariance of the marginal rate of substitution to the partial derivatives definition. We show the invariance of the returns to scale to the partial derivatives definition.'],\n",
              " ['Managing major societal risks involves the need to understand public risk responses. The social amplification of risk framework has been our main theoretical approach. We explore how to model endogenised risk observation behaviour and communication. Agent simulation shows characteristic outcomes like peaks and drift in risk beliefs. The model indicates the key areas where further empirical research is needed.'],\n",
              " ['We provide a competitive difference analysis of the online one way trading problem with limited information on prices. We formulate the problem recursively and obtain closed form solutions via backward induction. Our competitive difference analysis reveals for the first time all possible worst case scenarios. Simulations show that our method exhibits robustness against information inaccuracy. Our method can outperform trading policies by competitive ratio analysis in simulations.'],\n",
              " ['The idea of machine learning is introduced to the QMC area. A new path generation method called the auto realignment method is proposed. The k means clustering algorithm is used to select representative normal vectors. Experiments demonstrate the efficiency and robustness of the proposed method.'],\n",
              " ['A heuristic is proposed for solving large scale multi stage stochastic mixed integer programs. The algorithm significantly accelerates the scenario cluster decomposition approach. An accelerated sub gradient algorithm is used to solve scenario cluster sub models. The algorithm is specialized to lumber supply chain tactical planning under uncertainty.'],\n",
              " ['The effectiveness of demand based pricing under social interactions in moderating demand fluctuations is studied. Results show that the demand dynamics can exhibit chaos when social interactions are at work. Social interactions and customers forward looking behavior may offset the intended effect of a demand based pricing. Operational policies must be examined in the appropriate context e.g. under the influence of social interactions.'],\n",
              " ['We present a new model for stacking problems with payload constraints. Due to uncertain item weights two robust optimization approaches are developed. Problems are solved using exact decomposition and heuristic algorithms. Using a characterization of worst case scenarios a compact model is presented.'],\n",
              " ['We analyze the licensing of a cost reducing innovation in a Cournot duopoly. We focus on two part tariff policies for both drastic and non drastic innovations. A drastic innovation is licensed not licensed if cost is super additive sub additive . A non drastic innovation is licensed if technology s average gain exceeds marginal gain. Royalties are positive fees could be zero.'],\n",
              " ['Algorithm to minimize investment cost for tree shaped gas distribution networks. Minimum cost obtained by converting the original tree into a single equivalent arc. Optimal continuous pipe diameters are converted to discrete approximate diameters. The algorithms are suitable for designing real gas distribution networks.'],\n",
              " ['This study provides a state of the art of OR use in agriculture under uncertainty. It offers an overview of the commonly used OR approaches for handling uncertainty. Main review findings are pointed out. Future research directions are derived and suggested.'],\n",
              " ['The exertion of parking reservation policies in vehicle sharing systems is studied. Partial parking reservations are introduced and analyzed. Mathematical programming based lower bounds on the quality of service are devised. The complete parking reservation policy is shown to be both simple and effective. Parking overbooking policies are demonstrated not to be worthwhile.'],\n",
              " ['We present a cyclic production scheme for multiple products with stochastic demand. We impose bounds for the cycle length in order to achieve a regular schedule. We consider sequence dependent setup times service levels and storage capacity. The computational study based on real world data compares six strategies that control the cycle length.'],\n",
              " ['We present a stochastic programming model for supplier selection. We incorporate uncertainty in the demand and in the supplier operation. We propose an enhanced Benders decomposition algorithm for this problem. Sourcing actions vary between using an integrated and a decoupled approach. Solving a single master problem in the Benders decomposition is more efficient.'],\n",
              " ['Present different reformulations for the quadratic traveling salesman problem. Develop new bound based on reformulation linearization technique. Propose a cycle based model and solve it by column generation approach. Develop a stabilized column generation.'],\n",
              " ['A meta approach is proposed for collaborative multi criteria supplier selection and order allocation. The model incorporates both heterogeneous objective data and subjective judgments. The model considers synergistic effects within a hierarchical decision structure. The suppliers strategic performance indicators are monitored in the proposed model. The applicability of the model is demonstrated in commodity trading.'],\n",
              " ['We develop a generalized asymmetric linguistic term set GALTS with six properties. The value at risk fitting method is designed to obtain the risk appetite parameters. The transitivity improvement approach of the GALTS is provided. A new QDM process involving risk appetites of the decision maker is proposed.'],\n",
              " ['We present a model of flexible loads and renewable supply in the smart grid. Supply and demand are matched locally via an incentive compatible online mechanism. Demand flexibility reduces payments and increases allocation probability. Increasing demand flexibility will increase suppliers profits. The cost of establishing incentive compatibility is decreasing in flexibility.'],\n",
              " ['We review the recent advances in global optimization for Mixed Integer Nonlinear Programming MINLP. We review the recent advances in global optimization for Constrained Derivative Free optimization CDFO. We present theoretical contributions software implementations and applications for both MINLP and CDFO. We discuss possible interactions between the two areas of MINLP and CDFO. We present a complete test suite for MINLP and CDFO algorithms.'],\n",
              " ['First systematic research on variations of online source credibility perceptions. Experimental study on online re contextualization effects. Stratified sample for education levels of 700 secondary school students. Provocative campaigning takes the risk of boomerang effects. Media and argumentation type affects affective self perceptions of weight.'],\n",
              " ['We introduce a novel task that of inferring social relationships from everyday conversations. We collected a corpus of natural telephone conversations unlike any other publicly available corpora. We show that 30 words of the beginning of a conversation is sufficient to infer the relationship accurately. We show that classifiers are useful in estimating the social engagement using conversations spanning 3 months.'],\n",
              " ['New models of speech rhythm and auditory knowledge are proposed. Use both duration and intensity metrics to classify native and non native accents. Perform accent classification by logistic regression LR and with baseline systems. LR based approach provides the best classification of native non native Arabic speech. Combination of auditoryc indicative features and rhythm metrics provides the best classification.'],\n",
              " ['We study the possibility to employ Machine Translation MT systems and supervised methods for multilingual sentiment analysis. Experiments are done for English German Spanish and French. We use three MT systems Google Bing and Moses different supervised learning algorithms and various types of features. We show how meta classifiers can be employed to mitigate the noise introduced by translation. Our extensive evaluations show that MT systems can be used for multilingual sentiment analysis.'],\n",
              " ['Approach for AF based ASR in framework of probabilistic lexical modeling is proposed. Most approaches in literature use a knowledge based deterministic phoneme to AF map. Approach incorporates a probabilistic phoneme to AF map learned through acoustic data. Analysis has shown that the approach allows different AFs to evolve asynchronously. Approach has potential to reduce word error rates by incorporating AFs in an ASR system.'],\n",
              " ['This work presents a comprehensive study on the use of deep neural networks for automatic language identification. It includes a detailed performance analysis for different data selection strategies and DNN architectures. Proposed systems are tested on the NIST Language Recognition Evaluation 2009 against an state of the art i vector baseline. It also presents a novel approach that combines DNN and i vector systems by using bottleneck features. The combination of i vector and bottleneck systems outperforms our baseline system by 45 in EER and Cavg on 3s and 10s.'],\n",
              " ['We explain in detail the different steps in computing a language model based on a recurrent neural network. We survey the applications and findings based on the current literature. We survey the methods for reducing computational complexity.'],\n",
              " ['High level features for cyberpedophilia detection are proposed. The fixated discourse model is suggested. Experiments on distinguishing between pedophiles and non pedophiles chats are performed. Feature analysis is presented.'],\n",
              " ['Feature analysis for spoken term detection STD on English meeting domain and Spanish read speech data in a discriminative confidence estimation framework. Feature analysis is based on groups that are defined according to their information sources lattice based features duration based features lexical features Levenshtein distance based features position and prosodic features pitch and energy . Feature analysis employs two well known and established models linear regression a generative approach and logistic linear regression a discriminative approach . Individual and incremental analyses are presented for both models. Results demonstrate significant improvement with the 3 5 most informative features compared with using the single best feature for STD confidence estimation. The best feature set comprises features from different groups lattice based and lexical features are among the most informative groups in general and duration and energy are more informative for read speech data.'],\n",
              " ['We present a method to customize machine translation systems when in domain data is not available. For that we perform an online learning automatic post editing from ready to use generic machine translation systems. The results show that the method is very effective on rule based machine translation systems. On statistical machine translation systems the method performs well if no in domain data was used in the training. Finally if there is not enough repetition our method has limited use.'],\n",
              " ['Paraphrastic language models proposed. Statistical paraphrase learning from standard texts. Improved LM context coverage and generalization performance. Combination with word and phrase level neural network LMs. Significant error rate reductions of 5 9 relative.'],\n",
              " ['We present a system for subjectivity and sentiment analysis SSA for Arabic social media data. Individual settings are required per genre and task. Using either lemmas or lexemes improves SSA results. Using a POS tagset leads improved results as do standard features. Processing dialects does not improve when it is know which sentences are in dialect. Genre specific features tend to be helpful for sentiment analysis but not for subjectivity.'],\n",
              " ['We present a sequential algorithm for detecting laughters and fillers in speech. The algorithm performs stepwise probability prediction context inclusion masking. We test several architectures for each of the above steps. Our models are more sensitive to change in feature carrying higher predictive power.'],\n",
              " ['Preprocessed elderly voice signals were tested with an android smart phone. Speech recognition accuracy increased to 1.5 by increasing the speech rate. Speech recognition accuracy increased to 4.2 by eliminating intersyllabic pauses. Speech recognition accuracy increased to 6 by boosting formant frequency bands. After all the preprocessing 12 increase in the recognition accuracy was achieved.'],\n",
              " ['Experimental demonstration of 3 1 internal resonance in an easy to reproduce structure with transparent underlying physics. Experimental demonstration of isolated region in frequency response. Normal forms backbone analysis of the free structure used to explain the rich dynamics seen. Reasonable match with continuation analysis in AUTO the revealed structure of bifurcations sheds further light on the response of the forced and damped system.'],\n",
              " ['Users of tutorial systems are affected by auditory modulations in system feedback. Users learn faster with naturally spoken than with synthesized computer feedback. Users learning rates benefit from system feedback with motivational prosody.'],\n",
              " ['Online discussion forums have benefits at individual and society level. They are positively linked to well being for stigmatised group members. Online discussion forum use is linked to offline civic engagement in related areas. Identification with other forum users mediates the above relationships. Online discussion forums are of greater applied importance than has been realized.'],\n",
              " ['Positive emotions are more prevalent than negative emotions while browsing Facebook. Users are happier when a positive post comes from a strong tie rather than a weak tie. Similarly users experience more benign envy when a post comes from a strong tie. The experience of malicious envy is independent of tie strength.'],\n",
              " ['The proposed approach leverages social Q A collections to improve automatic complex QA system. There is no need to manually collect training Q A pairs that are necessary for supervised machine learning approaches. Extensive comparison experiments are conducted i.e. LexRank question specific and translation based approaches are compared. Experiments on the extension of NTCIR 2008 test questions indicate that the proposed approach is more effective.'],\n",
              " ['The social casino games and gambling industries and products are converging. 521 adults who played social casino games in the previous 12 months completed an online survey. A minority 19.4 of social casino game players gambled as a result of these games. Gambling was typically motivated by a desire to win money which is not possible in games. Social casino games may increase gambling for some players including those vulnerable to gambling problems.'],\n",
              " ['We propose novel sentence level features to capture atypical variation. Our sentence level features are effective for intelligibility classification. We propose a post classification posterior smoothing scheme. Our smoothing scheme improves classification accuracy of our systems. We test feature level and subsystem fusions for the final intelligibility decision.'],\n",
              " ['Normalization of abbreviations in noisy informal text. Collection filtering and annotation of Twitter status messages. Comparison of statistical and machine translation approaches. Effects of language model order on accuracy. Combination of methods to achieve best results.'],\n",
              " ['A method for merging an arbitrary number of nouns mixing their meanings. Successful model using vector representation scantily explored for concept retrieval. Experiments with 3 semantic space models WN a thesaurus and a topic based model. Good performance with an automatically obtained resource comparable to a manual one. Evaluation by qualified reviewers and comparison with a traditional dictionary.'],\n",
              " ['ISO IEC 15504 philosophy helps to formalize effective practices to manage living labs. The PRM covers the absence of a formalized approach to guide living lab management. The reference model contributes to facilitating benchmarking among living labs. The maturity levels provide a feasible path to create and evolve a living lab.'],\n",
              " ['The DIMAG framework generates platform independent connected mobile applications. Existing standards were used and refined in the design and implementation of DIMAG. Client server applications are specified in a declarative way. Server side of applications is generated for the Java EE platform. It dynamically generates client applications for Android Java ME and Windows Phone.'],\n",
              " ['We have developed an application for specifying and simulating queuing network models. We reuse and extend a de facto standard metamodel for queuing network models PMIF. A modular and easily extensible architecture has been achieved using MDE techniques. The behavior for queuing network models is easily extensible with our approach.'],\n",
              " ['We illustrate that designing cross vendor reusable modules in automation systems is hard. Some theoretical foundations on how to design reusable modules are given. Five dependency problems in IEC 61131 3 projects are illustrated. Three rules are proposed in order to avoid these dependencies.'],\n",
              " ['Data interchange format for food consumption data Definition of the key concepts used with food consumption data A generic data model for food consumption data Linking food consumption data with food composition data'],\n",
              " ['Choroid is statistically separated from sclera using structural similarity index. Smoothness in choroid delineation is achieved using tensor voting. Automated choroidal volume estimation is attempted for the first time. Choroidal thickness and volume estimates are given vis vis observer repeatability. Quotients are defined to fairly compare among methods tested on disparate datasets.'],\n",
              " ['By taking advantages of deformable image registration we can successfully convert a single a low quality cone beam CT into a high quality multi phase cone beam CT images.'],\n",
              " ['We propose the voxel visibility model for transfer function in volume rendering. We propose an optimization algorithm for automatic transfer function design. The voxel visibility model is a feature and voxel level model. The voxel visibility model can be efficiently used by users. The voxel visibility model provides an importance based strategy.'],\n",
              " ['We developed a novel method for automated 3D 2D registration of microCT and histology. We used the model of dental implant biopsies and found high agreement among the registered specimens. We directly compared the bone to implant contact BIC using the registered samples. We found significant positive association for the BIC measurements. Metal artifacts increased automated BIC measurements systematically.'],\n",
              " ['We propose an automatic detection method to localize prostate cancer in MRI. We localize prostate cancer in peripheral zone PZ as well as in central gland CG and transition zone TZ . The random forest is employed to effectively integrate features from multi source images. Experimental results show that our method can accurately localize the cancerous sections. The results of our proposed method are better than that of other four conventional methods.'],\n",
              " ['A new approach for microaneurysm detection is tested in two public image databases. Normalization of grayscale content is an essential preprocessing stage. Coarse segmentation for candidate selection is based on morphologic processing. Principal component analysis and Radon transform are used during feature extraction. The number of features to extract is very small with only two features.'],\n",
              " ['We propose automated segmentation of HEp 2 cells in immunofluorescence imaging. We apply the same pipeline to images with different fluorescent pattern and intensity. Our segmentation approach is based on adaptive marker controlled watershed. We assess the accuracy of our approach on a public dataset. We compare our performance with significant works from literature.'],\n",
              " ['We have investigated a framework for monitoring 4D dose distributions. A portal dose image based 2D 3D registration has been proposed. Dose errors can be calculated to ensure the quality of the radiation therapy.'],\n",
              " ['We proposes a QoS AQM mechanism M GREEN for multi QoS classes. It extends the concept of Random in RED to Global Random . It extends the concept of Early Detection in RED to Early Estimation . It extends the linear concept of RED to an exponential one.'],\n",
              " ['Traditional methods for reconstructing complicated multi furcation are limited. Turing points are yield on borders if three branches intersect with each other. Turning points having anisotropic opposite relationship exactly make a triangle. Any border segment can pair one and only opposite segment. Topology of refined surface can be re maintained by checking original vertices.'],\n",
              " ['A Software Engineering Maturity Model was produced for Spanish software industry. The model is based on standards ISO IEC 12207 ISO IEC 15504 and ISO IEC 17021. It allows a certification of the organizational maturity for software enterprises. Helping to improve the software development quality in all types of enterprises. Currently there are 38 development enterprises certified by AENOR in this model.'],\n",
              " ['The approach segments a polyp by propagating an initial manual delineation. Polyp segmentation is performed by a combination of motion and appearance features. A defocus strategy estimates the camera distance and the actual polyp size. Four experts and the method did not show significant differences in endoscopy. The approach is robust to different types of noise and can be used in real scenarios.'],\n",
              " ['We present a mass spring model for simulating mitral valve repair. The patient specific model geometry is based on pre surgical 3D TEE images. We use a rest length estimation algorithm for modeling of the tendinous cords. A comparison with post surgical 3D TEE images proves the feasibility of our approach. Our simulation is faster than comparable state of the art finite element simulations.'],\n",
              " ['We designed and verified computerized support of acute stroke diagnosis. Novelty is application of integrated variational approach to detect ischemic edema. New content oriented image restoration and descriptors of asymmetric hypodensity distribution were proposed. Clarified visualization of direct symptoms of pathology was completed with automated recognition of ischemic stroke. Achieved recognition accuracy was 1 for test set of over 500 CT scans.'],\n",
              " ['We propose a novel method to detect distinctive landmarks for effective correspondence matching. The method can accurately detect corresponding landmarks in the case of large anatomical variations. The method can automatically identify informative features and search correspondence in the entire image domain. The method can provide good initialization to registration especially for images with large deformation difference.'],\n",
              " ['This paper proposes a framework for real time unsupervised segmentation of human motions and automatic symbolization of the motions. The segmentation is based on prediction uncertainty and symbolization is based on competitive learning of human motion. Their integration was verified on the human motion datasets.'],\n",
              " ['Gaussian Processes incorporating SAM at core for hyperspectral data classification. Systematic comparison of machine learning methods under various conditions. Data acquired from different materials sensors and different illumination. Classification using such independent training and test data sets. The presented method provides a Bayesian framework as basis for data fusion.'],\n",
              " ['We present some technical issues for testing distributed frameworks with timing constraints. We model an architecture taking into account the delay of messages exchanged between the components of the testing distributed applications. We propose a Multi Agent based system to capture the complex monitoring tasks of distributed tester behaviors'],\n",
              " ['We make a further research on privacy preserving location sharing on mobile online social networks. BMobishare is proposed a security improved mechanism which employs dummy query and the Bloom Filter to protect privacy. A performance evaluation that attests to the practicality of our solution.'],\n",
              " ['This proposed methodology can be used to improve the SIM plan. With the proposed SIM plan software development process is improved. The related software architecture is examined in different models. A real case with the working scenario to verify this proposed SIM plan.'],\n",
              " ['We present a new robot pose prediction method for static stability estimation. The method approximates the terrain by least squares planes to reduce the runtime. A stochastic version accounts for noise in the robot state and the terrain model. We systematically compared it with a physics simulation in many distinct scenarios. The new method is significantly faster and competitive in realistic situations.'],\n",
              " ['An architecture to integrate Controller Area Network CAN in intranets is proposed. Key design requirements were flexibility and scalability. The proposed architecture suits both inexpensive and very complex applications. A prototype implementation has been tested to assess feasibility and performance.'],\n",
              " ['kNN algorithm performance. Collaborative filtering hardware similarity measure. Low cost recommender systems hardware circuits.'],\n",
              " ['Shows a mashup platform that lets end users make their own web applications Proposes a new scale based on a set of objective factors Proposes a reference architecture built taking into account the factors of success This architecture achieves more success among end users that current platforms It is useful to know which decisions are relevant for achieving end user satisfaction.'],\n",
              " ['The paper studies several natural language processing NLP techniques to extract predictors from uncoded data in electronic medical records EMRs . Some techniques are well known while other have been developed specifically for this research. The approaches have been applied to a large dataset we have access to covering 90 000 patients in general practices. We focus on predictive modelling of colorectal cancer which is a challenging disease to study as it is a common type of cancer while the symptoms are very a specific for the disease. The results show that some of the NLP techniques studied can complement the coded EMR data and hence result in improved predictive models.'],\n",
              " ['This paper is concerned with a case based reasoning CBR system for radiotherapy treatment planning for brain cancer patients which has been developed in collaboration with medical physicists at the Nottingham University Hospitals NHS Trust City Hospital Campus UK. The developed CBR system generates the parameters of a treatment plan by capturing the subjective and intuitive knowledge of medical physicists. In this research we focus on the adaptation stage of the CBR system in which the solution treatment plan of the retrieved case is adapted to meet the needs of the new case patient by considering differences between the retrieved and new cases. We investigate approaches to adaptation that do not require much domain knowledge referred to as knowledge light adaptation. Results obtained by three developed adaptation approaches including adaptation based on Neural Networks and na ve Bayes classifier and adaptation guided retrieval are presented.'],\n",
              " ['The hidden and exposed effects are assessed on Wireless Mesh Sensor Networks WMSNs . Performance evaluation reveals a clear negative impact on WMSNs. Different scientific proposals are discussed in order to mitigate these problems. Some of these proposals are stressed and compared according to WMSN premises. Best solutions of our comparative study are shown to the audience.'],\n",
              " ['A unique approach to measure size of the customized ERP package has been proposed. Case study research method has been used to validate the proposed approach. A positive correlation was observed between Package Points and the efforts of these projects. The study presents theoretical and practical implications for better understanding of ERP package size measurement process.'],\n",
              " ['The first study which uses public transportation for collecting data from SM Our study only needs the IEEE 802.11p communication protocol capability on WAMR. Unlike other VANET studies our study uses both I2V V2I communications.'],\n",
              " ['A novel wrapper method searches forward in a hierarchical feature subset space. Prior domain knowledge is incorporated into the selection procedure. Promising results are obtained on two cancer patient datasets.'],\n",
              " ['We present a classification model for the automatic quality grading of clinical evidence. We propose NLP based approaches for extraction of informative features from text. We present a supervised learning approach using SVM classifiers for evidence grading. We show that the performance of our approach is comparable to human performance. Our quality grading approach can significantly reduce practitioners time needs.'],\n",
              " ['EE MC is a new unsupervised methodology for predicting protein complexes from weighted PPI graphs. It is by design able to overcome intrinsic limitations of existing methodologies. It outperformed existing methodologies increasing the separation metric by 10 20 . 72.58 of the predicted protein complexes in human are enriched for at least one GO function term.'],\n",
              " ['A dataset of 14 480 critically ill patients within the ICU was collected. Machine learning models are constructed to predict patient length of stay and mortality. Prediction accuracy was improved by using a two by two classification grid. Probabilistic model outputs can improve interpretation by physicians. Moving data window shows potential for real time ICU patient analysis.'],\n",
              " ['We implement a cloud based platform to obtain and manage context data The platform is designed for people with no programming skills in order to develop context aware mobile apps The reasoning process of the platform has been optimized using the conditions as facts rule pattern The context data gathering process has been optimized using an algorithm that detects the activity of the user'],\n",
              " ['A method for developing Bayesian networks based on both data and knowledge. The method targets complex questionnaire and interviewing medical data. The objective is to provide real world decision support and risk management. The method is based on two original and validated applications in forensic psychiatry. Both applications provide improvements in predictive accuracy and decision support.'],\n",
              " ['We present the experimental results of a focused Web crawler that combines link based and content based approaches to predict the topical focus of an unvisited page. We present a custom method using Dewey decimal classification system to best classify the subject of an unvisited page into standard human knowledge categories. To prioritize an unvisited URL we use a dynamic flexible and updating hierarchical data structure called T Graph which helps find the shortest path to get to on topic pages on the Web. For the background review the experimental results from several crawlers are presented. We compare our results against other significant focused Web crawlers.'],\n",
              " ['A EEMDAN GRNN algorithm is proposed for diarrhoea outpatient visits prediction. Predictor based on decomposition and ensemble principle superior to single predictor. GRNN is a good candidate as predictor for diarrhoea outpatient visits prediction. EEMDAN is superior to EMD and Wavelet for diarrhoea outpatient visits time series.'],\n",
              " ['We present efficient algorithms for mapping FEA data between meshes. An in core spatial index is created on the source mesh. Nearest neighbour searches of nodes and elements in the index are very fast. An experimental evaluation of the mapping techniques using the index is conducted. The algorithms have been implemented in the finite element data exchange system FEDES.'],\n",
              " ['A new hybrid mixed stress finite element model is presented. Independent approximation of stress strain and displacement fields. Static and dynamic physically non linear analysis with isotropic damage models. Use of effective p refinement procedures. Legendre polynomials are used to define the approximation bases.'],\n",
              " ['A new variant of PSO abbreviated as MLPSO STP is proposed. A novel learning strategy is used to enhance the global search ability. Space transformation perturbation is used to obtain better solutions. MLPSO STP outperforms its peers in terms of searching accuracy and reliability. MLPSO STP is used to optimize the operating conditions of ethylene cracking furnace.'],\n",
              " ['A RS approach for structural reliability analysis using evidence theory is proposed. A design of experiments technique is proposed to guarantee the RS precision. The results of the numerical examples show pretty good efficiency and precision.'],\n",
              " ['We propose several improvements for the windowing algorithm. We evaluated model performance interpretability and stability. Our methodology focuses on the interpretability of the model. Our approach shows differences in terms of interpretability without harming performance. Our approach may yield better classification models.'],\n",
              " ['Novel multivariate IB model is proposed for unsupervised video categorization. Effective solution is designed to integrate multiple features simultaneously. Information theoretic optimization is constructed to alleviate the semantic gap.'],\n",
              " ['We develop a region growing algorithm based on parallel computation architecture. The old algorithm needs to be redesigned to fit the requirement of GPU architecture. Our algorithm can speed up the mesh reconstruction over 10 times. The proposed algorithm also have the same quality compared to original algorithm.'],\n",
              " ['We have proposed CAST a new context aware stereotypical trust model. We have considered a comprehensive set of seven context aware stereotypes. We have applied a deep learning architecture to keep trust stereotyping robust. We have confirmed the effectiveness of CAST using a rich set of experiments.'],\n",
              " ['Nonlinear localization can deal with larger increments than the classical NKS methods. Mixed DDM can use larger increments and pass limit points with respect to NKS methods. Important reduction on the number of global iterations. The amount of exchanged data between processors is reduced. Sensitivity study for the Robin operator.'],\n",
              " ['Proposing an interval forecasting method for agricultural commodity futures prices. Extending the linear and nonlinear modeling framework for ITS forecasting. VECM and MSVR are integrated abbreviated as VECM MSVR . The experimental analysis is based on one step ahead and multi step ahead forecasts. VECM MSVR is a promising method for interval forecasting in future markets.'],\n",
              " ['We reduce UK means to K means. We experimentally show that K means performs much faster than existing pruning algorithms. We propose Approximate UK means to heuristically identify boundary objects and re assign them to better clusters. We propose three models for the representation of cluster representative. To our knowledge this is the first time to introduce uncertain model of cluster representative.'],\n",
              " ['Give conditions for neighborhoods in a covering form a reduction of the covering. Give conditions for covering lower and upper approximations are dual to each other. Axiomatize lower and upper rough approximations based on partial orders.'],\n",
              " ['We propose a new robust multi objective maintenance planning method of deteriorating bridges against model uncertainties. Simulated system uncertainties are incorporated into the GA based multi objective optimization framework. The preference based objective space reduction method is utilized to enhance searching efficiency. Numerical example of a typical prestressed concrete girder bridge is provided. Proposed method produces well balanced maintenance strategy both in terms of bridge performance and maintenance cost.'],\n",
              " ['We present a parallel technique for generating two dimensional triangular meshes. Generated meshes show the same quality as those generated with serial approaches. Our parallel technique presents a fairly good speed up. Our load estimation is very effective and can be used for efficient load balancing.'],\n",
              " ['A fast method for computing necessarily optimal solutions is developed. Four kinds of elicitation criterions for one time elicitation are explored. Idea of expected elicitation times is used in one elicitation criterion. A system is built to implement the proposed elicitation criterions.'],\n",
              " ['We provide a maturity model for interoperability of ultra large scale systems. We provide a framework for interoperability of ultra large scale systems. Increasing interoperability of component systems in ultra large scale systems.'],\n",
              " ['We mainly explore the low rank image recovery problem. A bilinear low rank image coding framework is proposed. For recovery TLRR preserves both column and row information of given data. Out of sample extension of TLRR is presented for handling outside data. We propose two local and global low rank subspace learning methods for feature learning.'],\n",
              " ['A novel bi population EDA for NIFPSP. Suitable adjusting mechanisms for adjusting probability models. Insertion based local search procedure to enhance exploitation. Better results obtained than those by the existing meta heuristics.'],\n",
              " ['We proposed a novel noise reduction method for document images. Semi supervised learning is applied to classify noise from character components. The proposed method is suitable for Non Latin based scripts i.e. Thai document image. We proposed an enhance labeling method of semi supervised cluster and label approach. The performance of proposed methods are significantly better than comparison methods.'],\n",
              " ['The primal maximum margin problem of OCSVM is equivalent to a nearest point problem. A generalized Gilbert GG algorithm is proposed to solve the nearest point problem. An improved MIES is developed for the Gaussian kernel parameter selection. The GG algorithm is computationally more efficient than the SMO algorithm.'],\n",
              " ['We proposed a general method of knowledge reduction in formal fuzzy contexts. We gave some judging methods of attribute characteristics in fomal fuzzy contexts. We constucted the discernibility functions to calculating the attribute reducts.'],\n",
              " ['We apply a two stage DEA model to evaluate cost efficiency and revenue efficiency. The additive efficiency decomposition DEA approach is utilized. We examine whether intellectual capital affects performance through Tobit models. We find that intellectual capital has a positive impact on CPA firms performance.'],\n",
              " ['WRF is used to discuss characteristic of non linear speedup parallel jobs. We give an analysis under different loads for non linear speedup parallel jobs. An adaptive scheduling method is used to schedule resources for parallel jobs. Simulations based on WRF are used to test our method.'],\n",
              " ['We propose a robust evidential reasoning approach for multiple attribute decision making. The possible set of best alternatives is identified using unknown attribute weights. The robustness of the alternatives in the set is measured from two perspectives. A robust rank order of the alternatives in the set is generated by their robustness. Intervals of utilities and relevant constraints are handled in the proposed approach.'],\n",
              " ['Temperature distribution in medium fire compartment is investigated experimentally. Numerical simulation of the travelling fire test is completed by using FDS. Measurements from the travelling fire test are used to validate the simulation. Travelling fire is compared to compartment fire with uniform temperature conditions. A potential impact of temperature heterogeneity on a structure is highlighted.'],\n",
              " ['A VR fire training simulator with smoke hazard assessment capacity is proposed. Realistic and accurate smoke environment is created for virtual training. Integrated smoke hazard in the path for evacuation or rescue is accurately assessed. A subway station and a primary school are investigated for virtual fire training. The simulator helps trainee identify the safest path and minimize smoke hazards.'],\n",
              " ['An improved sequential approximation optimization algorithm is developed. An estimate of width of Gaussian kernel functions for surrogate models is proposed. An adaptive sampling strategy is developed based on the improved surrogate model. A framework that integrates CAD CAE and SAO tools is developed.'],\n",
              " ['A case study was simulated to provide a realistic depiction of an operating system. Investigated factors were number of pump number of cashier and IATs. 2 level full factorial experiments were used for further analysis of simulation. The performance of a service industry queuing system has been analyzed. The influential parameters on queue length and sales rate have been obtained.'],\n",
              " ['An advanced hybrid method for computational acoustics is proposed. Acoustic propagation is simulated via the Perturbed Euler Equations. Acoustic generation and propagation stages are weakly coupled through an interface. The method is optimized via innovative features forcing derivation interpolation . The method is validated and illustrated via noise problems of increasing complexity.'],\n",
              " ['An image analysis method for crack observation in a concrete pier is proposed. This method manifests concrete cracks before there became visible to the naked eyes. We present the procedures flowchart and software implementation of this method.'],\n",
              " ['Proposed approach adopts MOGAs to minimize software project cost and duration. Solutions representing resource allocations and task schedules are evolved. Objective functions consider productivity of developers and task interdependence. The performance and scalability of four MOGAs were compared using several datasets. MOCell NSGA II and SPEA2 outperform PAES in the majority of project instances.'],\n",
              " ['Recognition of physiological signal patters involving non stationarity. The non stationary signal patterns are partitioned into variable size stationary segments. Deterministic features are recognized using Kalman filter. Indeterministic features are estimated using fractal dimensions. A revolutionary approach is proposed to determine the best feature sets.'],\n",
              " ['A modified version of CBO denoted by MCBO is utilized to optimize the cost of bridge superstructures. The problem consists of 17 variables and 101 implicit constraints based on AASHTO standard. Optimization is performed for bridges with different span lengths deck widths and with various unit costs of concrete. A comparison among the PSO CBO and MCBO algorithms is conducted.'],\n",
              " ['A new multi agent algorithm inspired by a collision between two objects in one dimension is presented. An enhanced colliding bodies optimization which uses memory to save some best solutions is developed. A mechanism is utilized to escape from local optima. Performance of the proposed algorithm is compared to those of standard CBO and some optimization techniques.'],\n",
              " ['An efficient approach to generating MA in parallel by multiple CPUs is proposed. An adaptive division method is proposed to maximize the parallelism degree. Inter dilation is proposed to ensure the correctness of final MA.'],\n",
              " ['A GUI for commercial micro and nanomanipulators was developed. Multiple control options include point and click movement and mouse following. Tip detection using image processing corrects for the manipulator s error. The GUI provides faster and more accurate control even for inexperienced users.'],\n",
              " ['The algorithms proposed allow automatically obtaining models from the graphs representing a conical perspective. The system proposed does not need to know the faces defined in the model. The algorithms proposed allow expanding the reconstruction model through some oblique edges in the model. The methods have been implemented and simulated obtaining a success rate of 100 .'],\n",
              " ['The hybrid FA PPCA system is presented. Two approaches termed AN and AC are proposed to speed up the i vector estimation during testing in a speaker recognition. Significant speed ups are obtained for each proposed approach. The scalability of the hybrid system is demonstrated using two suitable features MFCC and MFS . Fusion of systems that use AC type approximation perform similar to that of the corresponding Hybrid system baseline.'],\n",
              " ['An efficient approach to generating hierarchical multi resolution MA of a CAD model is proposed. The affected region and the re voxelization region are proposed to reuse voxels when the MA level is upgraded. An adaptive double queue distance dilation based algorithm is proposed to refine MA automatically. The MA quality metric is proposed to evaluate the MA.'],\n",
              " ['Procedural generation of high fidelity 3D road models from 2D road GIS data. A set of carefully selected civil engineering rules for road design. A novel approach that generates road geometry from centerline information. Parameterization of network data into elements which compose sophisticated geometry. The algorithm produces seamless road geometry quickly and efficiently.'],\n",
              " ['A novel parallel approach of numerical implementation of PIES named GPU accelerated PIES is proposed. Processing power of GPU is used to accelerate computations. The accuracy of the solutions obtained using GPU accelerated PIES was examined. Computational time of GPU accelerated PIES was examined. The effectiveness of the serial version of PIES program and GPU accelerated PIES was compared.'],\n",
              " ['Diffuse interface method for heat and mass transfer simulation of cellular solids. Method for realistic modelling of complex microscale structures of cellular solids. Novel tensorial mobility approach is employed for diffuse interface heat transfer. Pore scale level CFD simulation using diffuse interface lattice Boltzmann method. Validation on complex engineering type geometries show excellent agreement.'],\n",
              " ['A new C code is implemented for generating and analyzing granular materials. We set up a numerical method based on the DEM for generating alumina Al materials. A statistical analysis is performed for ensuring the randomness of granular packings. Thermal conductivities are estimated using FFT based scheme of Eyre and Milton. Effects of anisotropy are investigated and related to the thermal conductivity.'],\n",
              " ['We show that the standard CPGA is rotationally variant. We then construct a rotationally invariant CPGA. We ensure diversity using a modified mutation scheme. We also ensure diversity by adding a self scaling random vector.'],\n",
              " ['We estimate an acoustic source orientation using only two microphones. A simple physical model that explains ITD and ILD variations is proposed. We propose a new directivity model that includes backwards emitted energy. Multiframe estimation is more robust than frame estimation in adverse conditions. In multiframe estimation there is a tradeoff between COR and estimation delay.'],\n",
              " ['We present a method for estimating a time scale local Hurst exponent on time series. The method has proven to be sensitive to sudden behavior changes on time series. Lower scales evaluate short range correlations. Larger scales evaluate long range correlations. The analysis evaluates pattern changes regardless the amplitude and scale.'],\n",
              " ['A framework for illustrative visualization of fluid simulation datasets is presented. New algorithms are developed for feature identification and matching in field data. Novel implementations are described for multiple illustrative visualization effects.'],\n",
              " ['We model a multi spur pollution and take into account the impairments affecting the spurs pulsation shift phase noise . We derive all closed form formulae optimal step Signal to Interference Ratio of the LMS based spurs cancellation scheme. We add an original adaptive step overlay to improve the transitional mode while keeping the same asymptotic performance.'],\n",
              " ['New solutions for coupled circles and coupled ellipses fittings in explicit forms. The non iterative solutions have computation advantage. The iterative solutions are self initialized and achieve optimum performance. They have higher noise tolerance levels before the thresholding effects happen. They can be reduced back to the fittings of a single circle and a single ellipse.'],\n",
              " ['This paper quantifies the effect of fuzzy clustering in the design process of a typical RBF network. It is analytically shown that the fuzzy clustering acts to minimize an upper bound of the network s square error. The PSO algorithm is used to minimize the upper bound and to provide an estimation of the network s parameters. Finally the widths and connection weights are further tuned using a steepest descent approach.'],\n",
              " ['Single channel source separation using nonnegative matrix factorization NMF . Using MMSE under GMM for regularizing the NMF gains online estimation of uncertainty. Less sensitive to the regularization parameter. Efficient update rules for gain parameters. Improved results in speech music separation experiments.'],\n",
              " ['The proposed algorithms represent a significant advance in solid geometry reconstruction. The reconstruction process is carried out automatically without user interaction. The methods have been implemented and simulated obtaining a success rate of 100 .'],\n",
              " ['We survey the literature on passive and active object recognition during the last 50years. We survey some of the best performing object recognition algorithms. We discuss the limitations and drawbacks of current methodologies. We discuss future novel research directions.'],\n",
              " ['TouchCut requires only a single touch to bootstrap the object segmentation. It incorporates a new model that fuses edge region geometric and shape cues. A new fast dominant color extraction scheme to generate edge probability. Temporally propagated shape prior enables extension to video segmentation. Comprehensive qualitative and quantitative evaluations on images and videos.'],\n",
              " ['The p norm detector investigated for coherent multilook detection. Compensator used to improve its performance. Criteria established to select p norm plus compensator detector s parameters. Enhance detector performance achieved regardless of the number of looks. Application to detection in compound Gaussian clutter with inverse Gamma texture shows excellent results.'],\n",
              " ['It uses information from radar and ESM to avoid the kinematic only classification. For each class a separate filter is operated in parallel and implemented by IMMRPF. Speed likelihood for each class is calculated and combined with likelihoods from two sensors. Output of classifier is also used for particle reassignment of different classes.'],\n",
              " ['A new technique for acoustic echo cancellation. An improved variable step size normalized least mean square adaptive filtering algorithm. A comparative study using some variable step size algorithms and the proposed algorithm. Experiments in single talk and double talk scenarios using TIMIT database. Standardized performances measures Mean Square Error and Normalized misalignment.'],\n",
              " ['Some shortcomings of some anisotropic filtering process were cited. A new filtering process based on new adaptive diffusion function is proposed. The efficiency of the new anisotropic function in the filtering process was shown. The proposed new function was found very interesting and generates good results. The time of the filtering process was highly decreased.'],\n",
              " ['This paper deals with the concepts of persistence diagram and matching distance. We present multi scale approaches to approximate the matching distance. Experiments show the capability of the proposed methodologies for shape retrieval.'],\n",
              " ['A robust algorithm for 2D visual tracking and 3D pose estimation is proposed. We focus on partial occlusions that distort the region properties of an object. Optimal pose of an object is estimated via particle filters in a decoupled manner. The degree of trust between system s predictions and measurements is controlled. The resulting methodology improves tracking performance in clutter and occlusion.'],\n",
              " ['A new image descriptor RBP is presented for face recognition in this paper. RBP is based on image multi scale analysis and multi order Riesz transform. RBP consists of two complementary components i.e. local Riesz binary pattern LRBP and global Riesz binary pattern GRBP . Experimental results on four databases demonstrate the superiority of our RBP compared with other image representation methods.'],\n",
              " ['A multi spectral dataset is constructed containing RGB and near infrared images. The incorporation of near infrared is proved to be valuable for saliency detection. The best model for integrating RGB and near infrared clues is analyzed.'],\n",
              " ['We present a strategy to create objectively labeled ground truth set of videos. Such a strategy mitigates the subjective biases of the annotation process. Objective labels show superior consistency than subjective labels. A classifier is trained to predict objective labels for 51K unlabeled videos.'],\n",
              " ['Explain the reason for PSNR metric can t consistent with human visual system. Embed the message into texture area preferentially for helping accurate diagnosis. Propose message sparse representation method for decreasing embedding distortion.'],\n",
              " ['Propose a novel target oriented shape prior modeling method. Measure the intrinsic similarity between the target shape and the training shapes. Incorporate the shape model into an optimized search based segmentation method. Exhibit better performance than other existing methods.'],\n",
              " ['Excellent community provides threshold global and local slope information. Global and local slope information further confirm peak distribution information. An adaptive iteratively reweighted genetic programming model the baseline.'],\n",
              " ['We propose a method for the indexing of point matches based on cross ratios. Matches are characterised by the distribution of a scalar measure derived from multiple point relations. Our consistency measure is processed statistically using random sampling. We propose applications both in weekly calibrated and uncalibrated scenarios. Our method exhibits low false positive rates without the use of thresholds.'],\n",
              " ['We present a framework for detection segmentation and tracking of multiple objects. The framework has minimal requirements on input for initialization. The choice of MRF inference method is less important than how scenes are modeled. Proximities are more important than colors as cues for segmentation. For real time application message passing is more feasible than graph cuts.'],\n",
              " ['We compared high and low level features for unsupervised image categorization. We verified that high level features significantly outperform low level features. We assessed how much the performance depends on the dimensionality of the feature vectors. We verified that a simple clustering on supervised features outperform strategies specifically designed for this task.'],\n",
              " ['We introduce the reconstructed residual error a new segmentation evaluation measure. The method provides a spatial map of the errors in a segmented image in tomography. The original projection images are exploited in an unsupervised approach. Validation is performed through simulations and experimental micro CT data. The method can improve gray level estimates and discriminate between segmentations.'],\n",
              " ['Focused and incidental scene text images are processed in a separate manner. Low rank matrix recovery is exploited to process the incidental scene text images. A text confidence map was designed via fuzzy inference system. The proposed algorithm handles both Latin and Farsi Arabic scripts. Farsi Arabic scene texts at arbitrary orientations are localized for the first time.'],\n",
              " ['A new SfM pipeline that uses aerial images as external references is proposed. Good matches between ground and aerial images are found by two stage verification. Consistency of orientation and scale from a feature descriptor is locally verified. Outliers are removed by global verification with sampling based bundle adjustment.'],\n",
              " ['People is grouped based on the spatio temporal features of their trajectories. From people groups new group features for people re identification are extracted. The group features can be employed with any kind of existing appearance features. Experiments demonstrated that the group features improve people re identification.'],\n",
              " ['We present a strategy for automatic image training set generation. We use semantic and statistics to generate strings related to a target label. Our gathered images are good in capturing different visual facets of a concept.'],\n",
              " ['In depth literature review on related subjects. Two regularization strategies interpolation and classification. Regularization on images lacking auxiliar information. Extensive evaluation shows astonishing results on CBIR.'],\n",
              " ['A hierarchical temporal model is used to estimate head pose in real world videos. Head pose classification in un constrained databases shows superior performance. Proposed model is used to classify facial traits in real world videos. Trait classification with and without using the estimated pose angle is performed. Facial trait classification using the proposed model show superior performance.'],\n",
              " ['A method for generating sparse relative depth estimates from video. Depth estimates can be generated without access to past or future frames. Depth maps are created through efficient filter based propagation. Results are favourable when compared to more expensive SfM MVS techniques.'],\n",
              " ['A novel regularized latent Dirichlet allocation approach to tag refinement is proposed. The proposed approach can explore the multiple wise tag relations and visual relations. The tag relevance estimation using the proposed approach is interpreted in the form of the deep structure.'],\n",
              " ['First experimental study on the ridge pattern distribution in the interdigital palm region for biometrics. Novel classification methodology of palms according to five classes. Study of complementarity of the interdigital and traditional palm regions. Evaluation of personal recognition using interdigital region reaching EER 0.01 on database on 416 subjects . Interdigital palm region image database from 416 subjects in this paper is made publicly available.'],\n",
              " ['A decentralized approach for multi view multi person tracking in VSNs. Design overcomplete dictionaries matched to the structure of likelihoods functions. Obtain sparse representation of likelihoods to reduce communication in the network. Comparison on well known L1 solvers to choose a fast and reliable method. Qualitatively and quantitatively our framework outperforms previous approaches.'],\n",
              " ['The informed sampler a general inference technique for Bayesian posterior inference in generative models. This method leverages discriminative computer vision models for faster probabilistic inference in generative models. Three different applications that highlight common challenges of posterior inference. Detailed comparisons and analysis with respect to different baseline sampling based methods. Informed sampling is found to converge faster than all baseline samplers across diverse problems.'],\n",
              " ['Focus has been typically exploited in computer vision as depth cue. A new focus based perceptual cue is introduced the focus signal. The focus signal corresponds to the change in focus level as a function of time. The focus signal can be integrated with classical cues for image segmentation. The proposed focus aided methodology yields improved scene segmentation.'],\n",
              " ['A real time crowd anomaly detection algorithm for video surveillance is proposed. The research has developed a spatio temporal texture model for feature extraction. A redundant texture feature space has been composed by using wavelet transform. Detection algorithm is based on 3 sigma rule which is fast and robust. The system shows improved accuracy and efficiency against many benchmark systems.'],\n",
              " ['We propose a new unbiased method for HDR reconstruction based on evidence theory. Our method allows to locally minimizing the acquisition noise in the HDR image. Our method is adapted for very high dynamic range acquisition. For average user no setting and no additional information are needed. Our method is less sensitive to object or people in motion into the scene.'],\n",
              " ['A method to distinguish photographic images and photorealistic computer graphics is proposed. A customized statistical feature named texture similarity is defined. Proposed method used Homomorphic filtering to highlight the image detail information. Proposed method is robust for content preserving manipulations. Proposed method provides satisfactory detection accuracy and generalization capability.'],\n",
              " ['Introduction of fast algorithm for the exact values of minimum barrier distance MBD. Comparison of the novel algorithm with its approximate versions. Experimental comparison MBD induced segmentations with other segmentation algorithms.'],\n",
              " ['In this paper we propose a 2D based hashing method which could fast extract the binary feature of samples. We successfully apply the hashing method into tracking model by some details. We design an effective and suitable learning model to update hash functions at every frame. Comparison experiments are done to demonstrate the effectiveness and efficiency of our tracker.'],\n",
              " ['Introduction of the phase trajectory fluctuations and gait fluctuation image. Using gait fluctuation as a quality measure or an additional matching score. Evaluation using large scale publicly available gait databases. Suppressing and utilizing gait fluctuations improve the gait recognition performance.'],\n",
              " ['We propose a disparity calculation algorithm based on multi pass aggregation and local optimisation. Disparity calculation is fast and accurate in real world scenarios. We propose the G disparity image which can be used with U V disparity for obstacle detection. Obstacle detection is more efficient and accurate. Free space calculation is simplified after obstacle detection.'],\n",
              " ['We introduce a new dataset where the objective is to recognize a location of an old photograph using modern digital images. We evaluate a large number of existing features and encoding methods for this task. We show that existing domain adaptation methods can help to improve results for this specific task. We further show that there is the need for further research in cross domain image retrieval task.'],\n",
              " ['Multi view model of object categories. Suitable to any type of image features e.g. edges and coarse scale gradients here. Performs detection localization and continuous pose estimation in unified manner. Encode appearance at discrete training viewpoints and in between. Competitive with best task specific methods with framework generally applicable.'],\n",
              " ['An RFID event driven mechanism is used to integrate planning and scheduling. RFID production shopfloor data was used to obtain APPS parameters. Release strategy is efficient to reduce the total tardiness by 44.46 averagely.'],\n",
              " ['An energy minimization based approach for scene text recognition with seamless integration of multiple cues. Applied also to the challenging open vocabulary setting where a word specific lexicon is unavailable. Comprehensive experimental evaluation on several state of the art benchmarks.'],\n",
              " ['We present a model to predict gestational age from 3D fetal brain ultrasound images. A feature based model characterizes spatial and temporal brain development. We capitalize on sonographic image patterns and clinical measures to predict age. Use of clinical measurements and neuroimage information improves age predictions. We identify the most age discriminating brain anatomies in early brain development.'],\n",
              " ['A wide area learner. Efficient sampling for training. Classify with variances.'],\n",
              " ['A multi target tracking is formulated as a dense subgraph discovering problem. Both local and global cues are exploited to represent the tracklet affinity model. The distinguishable appearance based models are learned for the targets.'],\n",
              " ['The concept of cloud asset is first proposed for urban flood control. The framework of cloud asset is worked out from both hardware and software aspects. Cloud asset enabled workflow management is presented. A case is given to verify the effectiveness of cloud asset.'],\n",
              " ['Language is conceived as an intersubjective engagement enabling shared cognition. Experimental studies highlight ways in which language enables intersubjective informational and behavioural synergies. We therefore argue for language as skilful joint activity leading to dialogically extended minds.'],\n",
              " ['A novel method for the calibration of focused plenoptic monocular cameras is proposed. In this way the camera will deliver metric depth information instead of disparities. We detach the intrinsic camera parameters related to either brightness or depth data. We present novel initialization methods for all parameters. The accuracy is demonstrated on independent ground truth validation data.'],\n",
              " ['Sensors and models play vital roles in harnessing Big Data to extract information. Data analytics can help to diminish monitoring burden and support locating sensors. Exploring Big Data is essential to detect universal associations across space and time. Ethical challenges and issues of standards and harmonisation need to be addressed. Citizen science needs robust sensors and models to crowd source and interpret data.'],\n",
              " ['An extended Process to Product Modeling xPPM method has been proposed. xPPM is for efficient integrated and seamless IDM and MVD development. The applicability of xPPM is analyzed through reproduction of existing IDMs. The problems of the current IDM MVD development are discussed. The benefits and limitations of xPPM and lessons are discussed.'],\n",
              " ['Image based 3D modeling method of actual building energy performance is presented. Building environments and their energy performances are jointly visualized in 3D. Automatically 3D registered thermal images assist with localizing energy problems. 3D spatio thermal models can facilitate energy building diagnostics and retrofit analysis.'],\n",
              " ['A robust and fully automatic segmentation framework for MR brain scans is proposed. A heterogeneous cohort of 125 scans of patients who had sustained TBI is segmented. The approach compares favourably to the state of the art on benchmark and TBI data. MRI based biomarkers correlate with outcome relevant clinical variables in TBI. Evidence that subcortical structures are particularly affected in TBI is presented.'],\n",
              " ['We present a multi robot human interaction system with two robots and a depth sensor. It includes a static and dynamic gestures recognition module. The set of gestures is described using arm body and facial head features. Interactive disambiguation for floor and object detection based on pointed location. Tested with several real users as well as with an offline test setting.'],\n",
              " ['We propose a new temporal spatial block based Background estimation approach to compute a foreground free image for video sequences. Threshold free clustering is proposed to discover similar blocks over time which contain the background data. An iterative spatial reconstruction selects blocks to obtain the final background. The performance improvement is validated in two datasets 36 sequences using 13 state of the art algorithms.'],\n",
              " ['The literature provides a wide range of solutions for WWTP influent data generation. The challenge consists of selecting the most suitable solution for each situation. QRAs requires of generating influent data under hypothetical uncertain conditions. The phenomenological modelling integrates knowledge about the generating mechanisms. More detailed descriptions of the components in the catchment area are required.'],\n",
              " ['The river reservoir operations model MODSIM is coupled to MODFLOW. Models simulate physics based hydrology and priority rule based water allocation. Coupling uses a Picard iteration and an implicit mass conservative solution. Model simulates conjunctive use of surface water and groundwater. Helps evaluate groundwater sustainability resulting from alternative management.'],\n",
              " ['We developed an online tool to generate models of Soil Organic Matter SOM . Molecular dynamics simulations of SOM models are now possible. Influence of composition on structure interactions and dynamics is studied. Our work opens the way to new and exciting research in soil sciences.'],\n",
              " ['Scientific and educational experience with the proposed Database Approach To Modelling DATM shows the following It facilitated overview of and insight in the model by developers and users. Allowed for a much more dynamic scientific development of the model. Allowed for a direct implementation of these developments in multiple platforms.'],\n",
              " ['Comprehensive description and validation of three versions of a mechanistic wind risk model for even aged forests. Analysis of the performance of mechanistic and logistic regression models against measured damage in a conifer forest. All models provided acceptable discrimination between damaged and undamaged forest stands. The two version of the mechanistic model with the lowest bias gave very comparable overall results at the forest scale. Statistical analysis showed that increasing tree height and local wind speed were the main factors associated with damage.'],\n",
              " ['A co modelling methodology supported by a co simulation tool frame work is proposed. Fault modelling and a layered structure with fault tolerance mechanisms for the controller software are introduced. The introduced methods and tools are demonstrated on a practical mechatronic device.'],\n",
              " ['The treatment of phosphorus in aquatic models is systematically reviewed. The complexity of lake river and marine models is increasing over time. Catchment river models tend to be simpler than lake and marine models. Performance assessment of lake and marine models is generally inadequate. Processes not included in models are discussed.'],\n",
              " ['Adaptation of the Calculation of Road Traffic Noise method for exposure assessment. Freely available open source software R with PostgreSQL and GRASS GIS . Model estimates compared well to noise measurements r 0.85 0.95 . Noise level exposures modelled for 8.61 million London residents 2003 2010 . Over 1 million residents exposed to high daytime and night time noise levels.'],\n",
              " ['E discovery to define legal evolution using Collective Litigation Intelligence. A scientific approach to improve legal practice litigation research. Derives market entry strategies for global franchise brand expansion.'],\n",
              " ['Molecular dynamics simulations were run on PDB structures containing protein and ligand. Interaction and structural parameters were extracted from the structures. Linear regression was used to check for correlation between these parameters. A neural network NN was used to predict ligand design parameters based on the protein. The NN performance improved when tweaking the protein structural parameters used.'],\n",
              " ['We combined systems and structural modeling to repurpose antibiotics for new hosts. We applied our novel approach to the infectious bacterium Mycoplasma genitalium. Our method suggests that thymidylate kinase is a good potential drug target. Our method suggests that piperidinylthymines are good potential lead compounds. Combined systems and structural modeling is a powerful tool for drug repositioning.'],\n",
              " ['This paper presents a Kansei evaluation approach based on the technique of computing with words. Kansei preferences are modeled by positively worded items with 7 levels of semantic labels. Fuzzy relation based clustering is used to extract a set of Kansei attributes from collected Kansei words. A cluster validation index is proposed to assist evaluators in determining the best number of clusters. Linguistic aggregation is used to synthesize Kansei priority information and rank the order of product alternatives.'],\n",
              " ['A novel computational model for predicting fusion peptide of retroviruses was proposed. A software tool named FP predict.exe has been developed. A large number of new putative FPs of five typical retroviruses were predicted. Property motif and evolutionary relationship about FP were computed and discussed.'],\n",
              " ['We implemented a vectorial representation of residues contacts We implemented an efficient statistical test for machine learnable data Our vectorial model reproduces protein packing A predictor is trained to effectively reproduce CATH and SCOP classifications Our predictor automatically identified inconsistent classification in CATH and SCOP'],\n",
              " ['Access to reliable 3D as built data is a critical issue in civil infrastructure. Applications to production monitoring and automated layout are discussed. Research on applications of as built data in the civil engineering field is reviewed. State of the art and other developments in as built data analysis are surveyed. Unsolved problems and challenges for future improvements in this field are discussed.'],\n",
              " ['It is the first time to study the biological structures and function of Y. NSN using bioinformatics methods. Y. NSN shared high similarity with the nuclease from Yersinia enterocolitica subsp. enterocolitica 8081. Y. NSN showed good thermostability.'],\n",
              " ['We study the structural basis of APPL and Reptin binding in context with APPL1 APPL2 dimerization. We perform a detailed in silico analysis using the knowledge of APPL1 2 heterodimerization and infer that binding of APPLPH to BAR domain and Reptin is mutually exclusive which may facilitate Reptin in nucleocytoplasmic shuttling. We report new insights into the distinctive characterization of APPL catenin Reptin HDACs ternary complex in terms of target gene regulation and transcriptional mechanisms.'],\n",
              " ['We identified potential drug target for Bacillus anthracis A0248 strain. We selected non human homolog essential proteins involved in metabolic pathways. PSORTb ngLOC and CELLO are used to predict membrane bound proteins. 17 proteins in unique pathways and 28 membrane bound proteins are identified. These findings from this current study will pave the way for further extensive in wet lab experiments and in that way assisting to drug design against anthrax.'],\n",
              " ['SVM is explored to predict regulatory interactions in Arabidopsis. Experimentally validated regulatory relationships were collected as the positive samples. Negative training samples were randomly selected TF target pairs under some strategies. Each gene pair was represented by incorporating the expression data and sequence information. Through the jackknife test our method reached an overall accuracy of 98.39 with the sensitivity of 94.88 and the specificity of 93.82 .'],\n",
              " ['The current reported sequence of the almond 2S albumin is a partial 28 aa peptide. The translation into protein of an almond EST sequence matches the partial peptide. The in silico generated aa sequence is a member of the vicilin superfamily. The currently known almond 2S albumin is rather a part of a 7S vicilin like protein.'],\n",
              " ['Image based multiplexing assay was attempted for siRNA library screening. Diverse orthogonal phenotypic parameters were retrieved from a single screening. Image based cell count provided better resolution than the viability measure. Geneset analysis revealed biological implications of various image based parameters.'],\n",
              " ['A protocol is described to graft inhibitors from their cognate kinases to non cognate mTOR. The grafted inhibitor mTOR affinity is virtually evaluated using a consensus scoring strategy. A number of identified inhibitors are assayed to determine their inhibition against mTOR. Diverse nonbonded interactions are found at mTOR inhibitor complex interface.'],\n",
              " ['Sequences analyzed by next generation sequencing contain many errors. We proposed a method of error correction of heterogeneous sequences. We performed a computer experiment of error correction. We confirmed the effectiveness of our method.'],\n",
              " ['Acetone favors partitioning into the water free region of the bilayer. The bilayer can tolerate the presence of acetone in the concentration range 2.8 5.6mol . Drastic disordering of phospholipid packing was observed above the critical acetone concentration.'],\n",
              " ['A computational model to maximize Penicillin G Acylase production is proposed. Bioprocess is modeled using ANN and optimized using ACO. Effect of applying local search techniques within global ACO process is analyzed. Computationally obtained optimized solution was successfully verified in the lab. Proposed method can be employed for modeling and optimization of other bioprocesses.'],\n",
              " ['Xenogeneic silencer like regulators were searched in fully sequenced prokaryote genomes. Putative xenogeneic silencer regulators are limited to some bacteria and are specific for each division. Xenogeneic silencer regulators from the H NS family are the most diversified. Other nucleoid associated proteins were also searched and their distribution was also assessed. Our search updates and extends xenogeneic silencer regulators database. Results showed that xenogeneic silencer like regulators are specific for the bacteria they are found in and exclusive as members of the other families are excluded.'],\n",
              " ['We proposed a hybrid algorithm to increase metabolites production. The hybrid algorithm select best combination of gene knockout. Using 2 microorganisms shows increment in metabolites production.'],\n",
              " ['We improved the ABC algorithm by adding a uniform crossover operation in the onlooker phase. We increased the number of scout bees to two. We adopted a mutation operation during the replacement process at the scout bee phase.'],\n",
              " ['The intrinsic dynamics of exportins were inherent from their cargo bound and cargo free conformations. Dominant modes were found to be relevant to the functional flexibilities and the binding affinities. ENM and FEA were complementary methods to study conformational dynamics of proteins.'],\n",
              " ['Enhanced classification of psychrophilic proteins by rotation forest. Rule extraction from correctly classified sequences. Validation of generated rules on structural data. Biological interpretation of rules. Ranking of amino acids according to their discriminative ability.'],\n",
              " ['64 and 73 putative pollen RLKs chosen from maize and Arabidopsis can be divided into 8 subfamilies. Expansion of pollen RLKs were mainly caused by segmental duplication. Maize might have experienced weaker purifying selection as compare to Arabidopsis. Duplication events of Arabidopsis and maize are between 18 69 million years and 0.67 170 million years ago respectively.'],\n",
              " ['We present a low cost filtering algorithm using algebraic analysis techniques. Our algorithm can significantly reduce the number of elementary collision tests that occur in the narrow stage of continuous collision detection. We demonstrated that cubic solvers augmented by our filtering algorithm are able to achieve up to 99. We observed more than ten times performance improvement against the standard cubic solver without any filters.'],\n",
              " ['We examine the differential geometry properties of the lines of curvature. We visualize the twist of line of curvature using its Frenet frame. We apply our technique to flatten doubly curved plates used in shipbuilding. We apply our technique to thin plate freeform fabrication.'],\n",
              " ['IdealKnock can be employed to efficiently identify excellent knockout strategies for the overproduction of various products. IdealKnock breaks through the common bottleneck of knockout number limitation and gene reaction relationship is well considered. The knockout strategies given by IdealKnock are generally robust which means the maximum and minimum production rate are close.'],\n",
              " ['CD4 T cell subtype master genes and their connected genes are more likely to be associated with a disease or a phenotype. Genes connected to the CD4 T cell subtype master genes are more likely to be transcription factors. CD4 T cell subtype master genes and their connected genes are more likely to be haploinsufficient. CD4 T cell subtype master genes and their connected genes are more likely to be embryonic lethal gene essential genes .'],\n",
              " ['Two sets of naive Bayes classifiers were developed for PICO detection. We trained one set with first sentences and the other with all sentences. The first sentence classifier performs slightly better for patient P elements. The all sentence classifier performs better for intervention I elements. The performances are about the same for outcome O elements.'],\n",
              " ['Describes approach to ensure fundamental principles of a system in reconfigurations. Specifies principles as constraints in an architectural description language. Translates constraints into a two layer graded hybrid logic. Derives interpretation models from specifications of architectures and reconfigurations. Provides equivalence and refinement notions to compare reconfigurations.'],\n",
              " ['Devised balanced multiresolution BMR schemes allow balanced decomposition. Constructed balanced wavelet transform BWT allows perfect reconstruction. BWT provides efficient access to previously extracted details on demand. We eliminate the need for using extraordinary boundary filters. BMR schemes use symmetric antisymmetric extensions at image and detail boundaries.'],\n",
              " ['We modeled communicable disease CD workflow at health department using user centered design. Public health CD work is mobile and episodic yet CD reporting systems are stationary and fixed. Health department efforts are focused on CD investigation and response rather than reporting. Current CD information systems do not conform to PH workflow thus affecting their usefulness. Personas scenarios and user stories provide evidence based representations for designers.'],\n",
              " ['A denotational semantics based on Henkin models is given for a predicative polymorphic calculus of access control. Noninterference as a basic semantic notion of security is semantically defined. It is proven that the language is type sound in the sense that every welltyped program of the language satisfies noninterference.'],\n",
              " ['We propose a language independent symbolic execution framework for languages. The proposed language independent approach is based on language transformations. We prove that the expected formal properties of symbolic execution. We present a prototype and we use it on several languages.'],\n",
              " ['We develop model of spherical parameterization in a rigidity preserving manner. We analyze ARAP energy from smooth description to its corresponding discrete case. We propose a iterative algorithm to efficiently solve the non linear model.'],\n",
              " ['We characterize partial isometries of shapes by single point maps up to first order. A novel matching algorithm for partial intrinsic matching. We use redundancy in our representation for approximate partial iso metric matching. Robustness to strong topological noise geometric noise and missing data.'],\n",
              " ['TRAK ontology models domain specific knowledge about the rehabilitation of knee conditions. TRAK provides terminology definitions and graphical illustrations of knee treatments. TRAK supports shared understanding of standard care in knee rehabilitation. TRAK provides the framework that can be used to collect data for epidemiologic studies of knee conditions. TRAK can support randomized control with a control intervention that is well defined and reflects clinical practice.'],\n",
              " ['We present a viable Python Prolog composition and show four different implementations each using a different composition style. We present the first experiment designed to help understand the effects of different composition styles upon performance. We thoroughly analyse and discuss the results of the experiment breaking down the impact that each composition style has on performance.'],\n",
              " ['The domain model is an extension to the communicating event loop actor model. The domain model retains the safety and liveness properties of the actor model. We provide an operational semantics and validation of the domain model.'],\n",
              " ['The domain of Trapezoid Step Functions is introduced for the static analysis on continuous functions values. The domain is a proper refinement of the Interval Valued Step Function Domain. A constructive abstraction procedure is provided that deals with floating point precision issues.'],\n",
              " ['We review the use of mobile phones for health applied to older adults. The field emerging field contains many feasibility studies with smaller samples. A variety of clinical domains are appropriate for mobile phone interventions. Future work should address generalizability and establish a stronger evidence base.'],\n",
              " ['Predicates called triples contain more structured information than keywords. We propose a new predicate based search engine approach for biomedical texts. We develop a new vector space model and query document similarity function. It achieves higher precision and relevance score than traditional keyword approach. Predicates can support more precise information search than keywords.'],\n",
              " ['Introduction of safety critical subset of the SystemJ language called Safety Critical SC SystemJ. Automata based compilation approach for the SC SystemJ language. A tool chain for verifying correctness properties e.g. liveness and safety of the SC SystemJ programs and generating executable from the verified code for deployment. The new compiler generates both faster and smaller executable compared to the original SystemJ compiler.'],\n",
              " ['TAO provides a declarative framework for automated test and oracle generation. TAO integrates test and oracle generation as a whole to promote better automated software testing. TAO is the first attempt to apply the methodology of denotational semantics in test and oracle generation.'],\n",
              " ['Chaetognath mitogenomes exhibit more than one tRNA like sequence as previously suggested. 16S rRNA genes appear as chaetognath tRNA nurseries. TRNA pairs seem templated by sense antisense strands as previously suggested for multifunctional ribosomal like protogenomes.'],\n",
              " ['Human factors methods were used to investigate No Fault Found NFF incidents. Medical equipment maintenance data was used to identify devices with high NFF rates. Interviews and heuristic analyses revealed usability issues cause NFF incidents. Usability testing validated the results. Our methodology can be used to identify devices with usability related design flaws.'],\n",
              " ['Publications are a key data source for research networking systems. ReCiter extracts bibliographies from PubMed given information about investigators. ReCiter has better precision but worse recall than Scopus a proprietary database. It is challenging to link publications to investigators at a given institution. It is difficult to identify research staff at an institution using publication data.'],\n",
              " ['Anonymization of large scale clinical codes allows for reliable genome phenome analysis. Across various repository sizes full EMR most reliable. Preserves utility for finding genome phenome associations.'],\n",
              " ['False positive removal after gene recognition and normalization. Integration of existing annotations acronym resolution and classification. Based on well established technologies in pharmaceutical industry. Improvement in precision while keeping recall high. We propose to use existing gene taggers to create corpora for common problematic cases.'],\n",
              " ['Psychotherapeutic drugs dominate health OSNs especially non moderated health OSNs. Genitourinary tract agents and nutritional drugs chatter dominates general OSNs. Respiratory and hormonal drugs appear more often in OSNs that require registration. OSNs with a question and answer format are less subjective. Users ask more questions about gastrointestinal and metabolic drugs.'],\n",
              " ['Proposal a model workflow to identify crowding indicators and bottlenecks. Approach based on the optimization of the patient path in the PED. The use of real data to construct the workflow model of the patient path. Build BPMN models to represent the patient journey through the PED.'],\n",
              " ['Bilinear risk prediction model for EHR data. Low dimensional embedding of patients in a learned risk space. Identification of meaningful clinical contexts. Significant improvement in accuracy over existing risk prediction model.'],\n",
              " ['Scenarios are day in the life descriptions designed to represent user needs. Their use for medical device certification differs from user centred design practice. We investigated this discrepancy and provide recommendations for their application. Scenarios elicit variations in use and difference between actual and intended practice.'],\n",
              " ['We present ProNormz as a freely available web based tool for human protein normalization. Besides normalization ProNormz distinguishes human protein kinases from other proteins. ProNormz is the first system to distinguish human protein kinases from other proteins. The methodology can be useful to developers for extensible gene normalization implementation.'],\n",
              " ['We incorporated semantic concept enriched dependence into the medical IR algorithm. The semantic concept enriched dependence model showed robust performance. Performance gain was achieved independently from other previous approaches.'],\n",
              " ['Semantic relationships in XML make the data vulnerable to privacy attacks. We propose a privacy approach for XML that considers semantic relationships. We compare our model and algorithm against other common privacy approaches. Diversification and dissection techniques can be used to protect privacy in XML.'],\n",
              " ['First application of usability methods to evaluate exome analysis software. Key list of user desiderata on exome software from clinical geneticists is compiled. Identified usability challenges and design features for reengineering opportunities.'],\n",
              " ['The all IP WSN architecture based on gateway trees is proposed. The proposed routing algorithm is achieved in the link layer without route discovery. A mobile node does not need to be configured with a care of address. A physician can monitor the vital signs of a patient at any time and at any places.'],\n",
              " ['Poor usability leads to a low adoption rate of telemedicine systems. Mode of input free text or structured report influences usability. Usability and user satisfaction are higher for structured report interfaces in telecardiology.'],\n",
              " ['We delimit a full structure for the utilitarian motivations in online consumption. Measurement scales are proposed for every utilitarian motivation. Qualitative analyses are applied to purify a full structure of motivations. Confirmatory analyses are applied to validate motivational structure and scales. We conclude a nine dimension utilitarian motivational structure for online consumption.'],\n",
              " ['Integrating product and channel preference factors to analyze differences of efficiency in electronic and traditional markets. Applying Data Envelopment Analysis DEA to calculate market efficiency for single channel and multi channel shoppers. Price dispersion and market inefficiency exist in electronic marketplaces. Market efficiencies vary across consumer segments and products. Incorporation of behavioral segmentation and product characteristics enhances the understanding of market efficiency.'],\n",
              " ['We developed and evaluated a rule based natural language processing system. The NLP system extracts mammographic findings from free text mammography reports. Manual review showed that the NLP system performs reasonably well. We developed confidence flags to facilitate further manual review. The NLP system was implemented entirely in SAS Base with SAS code available.'],\n",
              " ['This study reviews the mobile payment research literature from around 2006 to 2015. The authors use a multi perspective framework to classify and analyze the literature. Despite the complexity of the issues that have arisen around mobile payments during the past 10years the related research still lacks diversity. There have been far too many adoption studies that cover the same ground with respect to theory and practice and fail to deliver anything more than incremental knowledge if any. A new agenda for research is proposed based on the analysis and results of the authors critical review that is intended to enhance the quality and relevance of future mobile payment research.'],\n",
              " ['We model personal recording services of TV contents under the fair use doctrine. We examine changes in IT about the copyright holder s profit and the social welfare. Before cloud DVR the copyright holder s profit and the social welfare increased. After cloud DVR the copyright holder s profit and the social welfare decrease. The court is advised to consider the recent IT when applying the fair use doctrine.'],\n",
              " ['A corpus of Swedish clinical notes was annotated for adverse drug event information. Detecting adverse drug events in clinical notes can support pharmacovigilance. Modeling context with distributional semantics yielded better predictive models. Distributed word representations allowed more context information to be incorporated. Inter sentential relations between drugs and disorders findings are hard to detect.'],\n",
              " ['Peripheral cue moderates elaboration on eye movements but not purchase intention. Purchase intention is different between high and low elaborations under positive cue but not negative cue. Under positive cue high elaboration is longer fixation duration than low elaboration. Under negative cue low elaboration is longer fixation duration than high elaboration. The relationship between purchase intention and eye movement is more significant in high elaboration with negative cue and in low elaboration with positive cue.'],\n",
              " ['We proposed a hybrid system to automatically identify heart disease risk factors. We divided different types of risk factors into three categories according to their descriptions. Our system achieves an F score of 92.86 on 2014 i2b2 corpus which is top ranked.'],\n",
              " ['A new advertisement option that allows an advertiser to pay a fixed CPM CPC to purchase impressions or clicks. The fixed payment can be different to the underlying ad format. The proposed option can be priced under the lattice framework for both SV and GBM underlying models. The studied model is validated by two advertising datasets.'],\n",
              " ['Consumers reviewing behavior is affected by culture differences across countries. Chinese are less engaged less negative and value negation more in review systems. Review volume emotional tendency and spotlight review positively affect book sales. Length and Reviewer rank negatively affect sales. Helpful votes have no impact. Effective review systems in China need care about review format and voting system.'],\n",
              " ['We suggest a computational framework to find new uses of existing drugs. We use the complementarity between clinical disease signatures and clinical drug effects. The statistical significance of prediction results is supported through two benchmark datasets.'],\n",
              " ['Apply Topic modeling to group synonyms under a topic to avoid human intervention and improve automatic market structure generation. Develop the WVAP method to filter noises in Topic modeling results to elicit market structure. Besides perceptual maps of product positioning the proposed framework can provide rankings of products.'],\n",
              " ['Patients can access information on healthcare websites to determinate physicians quality. We verify the effects of physicians information on patients online reactions. Physicians information can be divided into patient generated information and system generated information. We find that the physicians information positively impact patients reactions at different stages. Moreover synergies between two kinds of information are positively associated with patients decisions.'],\n",
              " ['We present a novel automated buyer profiling control mechanism. It is based on pseudonym changes performed according to human privacy attitudes. Agents automatically decide whether reusing or changing their pseudonyms. We present the results of an extensive experimental evaluation of the mechanism based on an application scenario.'],\n",
              " ['Cognitive informatics CI research has its foundations in cognitive science. Transformations seen in CI in JBI reflect the changes seen broadly in the field of CI. Key topics include decision making usability comprehension workflow and errors. Recent developments toward use of applied cognition for usability and HCI studies. Future trends point toward consumer health tools and the use of mobile technology.'],\n",
              " ['A method for predicting trainees false positive locations in mammography is proposed. This is the first exploratory study on the topic using computer algorithms. Predictions are made using 133 imaging features and a random forest classifier. The predicted locations are more accurate than the locations selected randomly. The method can select educational material with more challenging locations.'],\n",
              " ['Provide a model based on causal discovery technique ANMCPT for concept drift mining in cross sectional analysis. AVMCPT can discover causal in high dimensional and dynamic environment. ANMCPT outperform the classical Fama French framework. Concept drift phenomenon in China stock market is observed and exhibited clearly.'],\n",
              " ['Utilizing Stanford dependency relation to further analyze the negation status of clinical concepts negated by NegEx. Improvement of NegEx algorithm by decreasing the number of false positives. Comparison of NegEx and DEEPEN on clinical reports from two different clinical settings.'],\n",
              " ['This study investigates the in app purchase intention and mobile game loyalty. The paying players intention is determined by playfulness good price and reward. The nonpaying players is only determined by good price.'],\n",
              " ['We present a privacy preserving protocol to detect concurrent trial participants. We present a name representation scheme resilient to frequency attacks. The accuracy of the protocol is similar to standard non secure methods. For a database size of 20 000 the private query time is under 40s on 32 cores.'],\n",
              " ['A novel approach via the composite of fuzzy controllers and dithers is presented. We can synthesize a set of fuzzy controllers and find appropriate dithers to stabilize nonlinear multiple time delay NMTD interconnected systems. When the designed fuzzy controllers cannot stabilize the NMTD interconnected systems a batch of high frequency signals commonly referred to as dithers is simultaneously introduced to stabilize it.'],\n",
              " ['Indicating the rating scale problems of the Analytic Hierarchy Process AHP and proposing the paired interval scale addressing the limitations. Introducing Primitive Cognitive Network Process P CNP to medical treatment decision making and showing how to use it from laymen perspective. Demonstrating how the current AHP data to medical decision can be converted to P CNP data which is further processed by the P CNP. Applications with the AHP data can be revised by P CNP to explore the more reliable research findings or make more reliable decisions. P CNP can be a promising decision making approach to evaluate medical and healthcare decisions.'],\n",
              " ['Proposing a fuzzy multi objective model for unrelated parallel machine scheduling. Presenting an effective multi objective particle swarm optimization solution method. Comparing the proposed MOPSO algorithm against a conventional MOPSO algorithm.'],\n",
              " ['The objective of the paper is to apply an extended QFD methodology in sustainable supply chain SSC . The paper examines the main elements of SSC and how they serve as a foundation for an evaluation framework. A new GDM approach that takes multiple preference formats and incomplete information into account is introduced. To assess the validity of the proposed approach a case study conducted at HAVI Logistics Turkey is presented.'],\n",
              " ['Genetic algorithm optimization is effective for partial imputation using MAIS. The hybrid MAIS and genetic algorithm improves performance of classifiers. Increased strength and resilience in the presence of escalating missing data.'],\n",
              " ['This paper solves the real world problem of tactical missile guidance in which optimality is sought to be achieved using the evolutionary computing method of differential evolution. A valid criticism against optimization by evolutionary computing methods is that they are computationally intensive as compared to gradient based methods. By posing the problem as that of finding the coefficients of a third order polynomial the dimensionality of the problem is so greatly reduced that online implementation in real time is shown to be possible. The results so obtained are compared against conventional methods in the guidance literature.'],\n",
              " ['Methodology for obtaining fuzzy rule based semi physical models. Possibilistic maximally specific observer of the state of a nonlinear system. Mathematical model of open circuit voltage and state of charge of LiFePO4 batteries.'],\n",
              " ['Efficient and accurate approach to solve nonlinear systems of functional equation. Optimization is carried out by stochastic fuzzy adaptive simulated annealing. Several examples presented and compared with results obtained by other approaches.'],\n",
              " ['Suitable objective function selection is very important for controller design. An objective function using ITAE damping ratio and settling times is proposed. The concept is applied to design an hBFOA PSO based PI controller for AGC system. Linear and nonlinear interconnected power system models are considered. Simulation results show better performance than PSO BFOA GA CRAZYPSO and ANFIS approaches.'],\n",
              " ['Ability of artificial neural networks to model the rainfall discharge relationships of karstic aquifers. Three month before forecast of water resources. Six month before forecast of water resources. Error in forecasting discharge values of just 5 three months before . Error in forecasting discharge values of just 10 six months before .'],\n",
              " ['Proposed a computational stress signal predictor system to estimate a stress signal. System based on support vector machine genetic algorithm and neural network. An experiment was conducted to acquire real world stress data. Features extracted from the stress data were provided as input to the system. The stress signal was most similar to a hyperbolic tangent curve.'],\n",
              " ['Our hybrid intelligent model considers the use of filter and wrapper based feature selection methods. Three qualitative principles are highlighted. The usefulness of our model is demonstrated using relative cluster validities. Better use a subset of salient features for analyzing clinical diagnoses in performing clustering.'],\n",
              " ['We propose a method based on Grammatical Evolution to obtain individualized and customized glycemia models in humans. We have tested this proposal with five in silico patients taken from AIDA simulator. We present a study of four different grammars and five objective functions. In the test phase GE models characterized glucose levels with a mean percentage average error of 13.69 . Models obtained with our method reflect also a good representation of both hyper and hypoglycemic situations.'],\n",
              " ['A modified incremental conduction MPPT algorithm is proposed for solar PV system under partial shading conditions. An adaptive fuzzy modulator is developed to provide PWM pluses to the DC DC converter. An adaptive hysteresis current control algorithm is proposed for DC AC inverter in the PV system. The hardware implementation of proposed algorithms using Xilinx spartran 3 FPGA is presented.'],\n",
              " ['An LRGF neural network with pole assignment technique is proposed to model the dynamic system. An adaptive lifting scheme is used for the improvement of the mechanical fault detection. An adaptive threshold scheme is proposed for the detection of several kinds of faults.'],\n",
              " ['A driver assistance system that correlates obstacles with driver view is proposed. Fuzzy rules based subsystems are used for analysis under different road conditions. One set of fuzzy rules for when a vehicle or pedestrian ahead is detected. One set of fuzzy rules for when no immediate obstacle is detected. Lab experiments and comparisons using real world data show good performance.'],\n",
              " ['Differential Evolution algorithms applied to ANN training suffer from stagnation. The lack of difference vectors of small magnitude is noted during ANN training by Differential Evolution methods. In case of benchmark problems the lack of difference vectors of small magnitude is only occasionally observed. DEGL algorithm outperforms other Differential Evolution variants for ANN training. Best algorithms found for benchmark problems do not perform well for ANN training.'],\n",
              " ['Use historical data and the data investigated by the Scenario and Delphi methods. Propose a two stage fuzzy piecewise logistic growth model for sale forecasting. Forecast the market shares of the optimistic pessimistic and most possible scenarios. Demonstrate two cases in the Television and Telecommunication industries of the global market. Outperform the technology substitution model or the Norton and Bass diffusion model according to MAE MSE and MAPE.'],\n",
              " ['BIRS Biomedical Image Retrieval System framework for diverse medical image collection. Phase Congruency based features for acute image characterizations. Building codebook using GeoSOM for image encoding thereby achieving dimensionality reduction. SIFT features combined with fuzzy function for significant improvement in information retrieval. Performance analysis indicates reduction in retrieval time with efficient indexing and effective retrieval.'],\n",
              " ['Weakest t norm T based approximate intuitionistic fuzzy arithmetic operations on different types of intuitionistic fuzzy numbers to evaluate fault interval and reliability interval. The proposed novel fuzzy arithmetic operations may obtain fitter decision values which have smaller fuzziness accumulating and successfully analyze the system reliability. Also weakest t norm arithmetic operations provide more exact fuzzy results and effectively reduce fuzzy spreads fuzzy intervals . Fuzzy reliability of PCBA fault has been analyzed using the proposed approach.'],\n",
              " ['A multidimensional coupled chaotic map as a pseudo random number generator is proposed. A watermark scheme is presented. The proposed scheme is subjected to some security analysis as well as statistical tests suites. Watermark extraction does not require the original image.'],\n",
              " ['Propose a novel multi swarm cooperative multistage perturbation guiding particle swarm optimizer. Multi swarm information sharing idea aims to improve the evolving efficiency via information communicating and sharing among different sub swarms. Multistage perturbation guiding strategy aims to slow down the learning speed and intensity. Comprehensive studies on algorithm are presented.'],\n",
              " ['The confidence intuitionistic fuzzy weighted averaging CIFWA operator and the confidence intuitionistic fuzzy weighted geometric CIFWG operator are proposed. The confidence intuitionistic fuzzy Einstein weighted averaging CIFEWA operator and the confidence intuitionistic fuzzy Einstein weighted geometric CIFEWG operator are proposed. The properties of the CIFWA and the CIFWG operators are studied in detail. The application of the proposed method to the review of the doctoral dissertation in Chinese universities is provided.'],\n",
              " ['Single chip embedded MIMO autonomous intelligent agent with on line learning capability small footprint low power transparent device. Dynamic deep adaptability through an integrated PCA analyzer. Dynamic on chip significant feature space reduction for adaptation speed up and enhanced generalization capability. Fault tolerance in the presence of sensor failures by minimum input feature space re computing in the integrated PCA module. Real implementation figures and real operation verification.'],\n",
              " ['We propose robust feedforward and recurrent neural network based dynamic weighted combination models. We combine four software reliability growth models by dynamically evaluated weights. We propose genetic algorithm based learning algorithm to train the proposed ANNs. Experimental results demonstrate that proposed models have fairly accurate predictability. PRNNDWCM has best software reliability prediction capability.'],\n",
              " ['This paper proposes a new heuristic algorithm for solving optimization problems. This optimization algorithm is inspired of trading the shares on stock market. The proposed algorithm is successfully implemented on 12 benchmark functions. Result shows the high ability of proposed algorithm in global optimum extraction.'],\n",
              " ['The relevance vector machine is evaluated for estimation of rock compressibility based on physical properties. An iteration strategy is proposed to optimize the hyper parameters of the relevance vector machine. The parameter effect is demonstrated on the performance of the relevance vector machine. The adaptive relevance vector machine is compared to the artificial neural networks and the support vector machine in the estimation.'],\n",
              " ['We propose a performance evaluation method for human resources. We use a Hybrid Multicriteria Decision Making Model fort his purpose. All possible interaction types are considered and handled with the proposed method. Ignoring the interactions may lead to erroneous decisions.'],\n",
              " ['It was modeled a ANFIS speed estimator applied to both vector and scalar induction motor drives. Subtractive clustering was used to generate the membership functions. Subtractive clustering allowed training the ANFIS with experimental data with noise. In the scalar drive the ANFIS estimator used the RMS values of voltages and currents as incoming signals. Magnetizing FOC was used in the vector drive instead of Rotor FOC.'],\n",
              " ['We propose a novel hybrid strategy for applicability of hyper heuristic techniques on dynamic environments. Performance of our method is validated with the dynamic generalized assignment problem and the moving peaks benchmark. Our approach outperforms the related work for various problem instances with respect to quality of solutions.'],\n",
              " ['The major motivation of ACGA2 is to take the bi variate probabilistic models into the consideration. We further provide a theoretical analysis of the two models EDAs. This studied conducted extensive experiments on the single machine scheduling problems with sequence dependent setup times in a common due date environment. The experimental result shows the proposed ACGA2 outperforms ACGA significantly because the average error ratio of ACGA2 is half of ACGA.'],\n",
              " ['The application of Genetic Algorithm and Biogeography Based Optimization to generate near optimal Golomb ruler sequences is being proposed. Both the approaches produce near optimal Golomb ruler sequences very efficiently in reasonable execution time. The performances have been compared with the two other classical methods namely Extended Quadratic Congruence and Search Algorithm. The preliminary results indicate that BBO and GA appear to be most efficient approach to such NP complete problems. BBO approach outperforms the GA slightly for larger mark values.'],\n",
              " ['Multi item inventory models allowing partial backlogging deterioration under random planning horizon. Inventory models with random planning horizon in imprecise environment with the effect of inflation and discounting. A random fuzzy constraint has been successively introduced for the first time in the inventory models. A real life inventory system with limitations on available budget and storing space constraints in both fuzzy and random fuzzy. Maximum profit has been compared by genetic algorithm and fuzzy simulation based genetic algorithm.'],\n",
              " ['We have applied Multiple Criteria Decision Making MCDM to evaluate the service quality of Turkish hospitals. We adopted fuzzy set theory as a research template. Importance weights of the performance criteria have found with AHP. TOPSIS which is MCDM method has applied to find and rank the crisp performance values. OWA operators Compensatory AND operator and Yager s Min Max method have investigated instead of TOPSIS method.'],\n",
              " ['Self adaptive error is added in the initial conditions in every round of which shows the novelty of this manuscript. A rule is designed to let errors be positive or negative. A new control parameter is generated dependent on errors it is also self adaptive. Only diffusion function is applied but it can reach a high security and less time cost. The keystream depends on the plain image which can resist efficiently the known plaintext and chosen plaintext attacks. UACI unified average changing intensity and NPCR number of pixels change rate results confirm that the proposed method can reach the ideal effect.'],\n",
              " ['An orthogonal forward selection algorithm is proposed for constructing radial basis function classifiers based on maximises the leave one out mutual information between the classifier s predicted class labels and the true class labels. Integrated within each OFS step a Bayesian procedure of hyperparameter fitting is introduced to infer the l 2 norm local regularisation parameter from the data. The results obtained demonstrate that the proposed algorithm automatically constructs very parsimonious RBF classifiers with excellent classification generalisation performance.'],\n",
              " ['Group decision making for medical diagnosis. Intuitionistic fuzzy soft set and fuzzy soft matrix. Hamming distance and Euclidean approach. Cardinal of intuitionistic fuzzy soft set to compute the weight. Viral fever related diagnosis.'],\n",
              " ['We propose JCSE SPIHT an algorithm of joint compression and selective encryption based on SPIHT. We design a fast random insertion to accelerate the encryption from taking O n 2 time to O n . JCSE SPIHT can generate plaintext dependent keystream by cryptographically secure PRNG. We exam the security of JCSE SPIHT by traditional cryptanalysis and image processing techniques.'],\n",
              " ['An approach based on evolutionary and bio inspired algorithms is proposed for solving the parameter estimation problem in crop growth dynamic models. Differential Evolution algorithm showed the best performance in solving the parameter estimation problem of a dynamic crop growth model. A statistical analysis and ANOVA were applied to quantitatively evaluate the efficiency and effectiveness of Differential Evolution CMA ES PSO ABC and LSE algorithms.'],\n",
              " ['Nonconvex emission constrained economic dispatch NECED is a complex optimization problem. Many optimization techniques and algorithm have been applied to solve it. A new improved particle swarm optimization technique is proposed for the same. The effectiveness of the proposed method is tested on two practical systems. The results are compared with classical as well as other heuristic technique.'],\n",
              " ['The paper solved different short term hydro thermal scheduling STHS problems. The paper considered both small and large size test systems. Real coded chemical reaction optimization RCCRO is applied to solve the problem. Performance of RCCRO has been compared with different PSO DE modified DE etc. Performance of RCCRO has been found to be more encouraging to solve STHS problems.'],\n",
              " ['An interesting RRE pruning algorithm incorporated with the operation of subtraction is proposed in this work. The WSM is chosen and its votes are subtracted from the votes made by those selected components. The backfitting step of RE algorithm is replaced with the selection step of a WSB in RRE. The problem of ties might be solved more naturally with RRE. Soft voting approach is employed in the testing to RRE algorithm.'],\n",
              " ['A novel fuzzy distance weight matrix based parameter identification method. Fuzzy clustering based algorithm used to find sub models of HDS. WLS algorithm is used to identify parameters of sub models. Results validated through simulation experiments.'],\n",
              " ['Collective behaviors are modeled as aggregations of individual behaviors. Individual behavior is modeled by the minority game. Parameters of individual behavior can be learned using genetic algorithms. The new model is tested based on real world financial data.'],\n",
              " ['Monitoring applications that only use negative selection feature detect too many false positives. The fault injection reinforcement learning process enables the overall model to become robust and efficient. The clonal selection feature promotes adaptation. The model is very flexible because is based on performance indicators and consumption resources.'],\n",
              " ['We propose an image encryption scheme based on a new spatiotemporal chaotic system. The encryption scheme is not the one time pad encryption. The proposed image encryption has a large key space and high security.'],\n",
              " ['Address the potential of learning machine to forecast ground level ozone in urban area. Summarize the existing learning machines used to predict ground level ozone. Compare the performance of commented models via practical case in Hong Kong. Address the underlying philosophy of using learning machine in ozone related prediction.'],\n",
              " ['Fully fuzzy fixed charge multi item solid transportation problem FFFCMISTP is considered. FFFCMISTP with the decision variable are taken as fuzzy. New defuzzification method fuzzy slack and surplus variable is used for FFFCMISTP. Minimization of transportation cost as well as fuzziness of the solution for FFFCMISTP is discussed.'],\n",
              " ['We present the work on efficient classification of multispectral images using soft computing approach. Selection of most discriminative spectral bands and determination of the number of hidden layer neurons are the two most critical issues. We proposed a new multiobjective particle swarm optimization based methodology for adaption of neural network structure for pixel classification of Satellite Imagery. It simultaneously estimates the most discriminative spectral features and the optimal number of nodes in hidden layer. Xie Beni and indexes of proposed algorithm are better than MLC and Euclidean Classifier.'],\n",
              " ['An improvement ranking method for HFEs is proposed. Two induced generalized hesitant fuzzy Shapley hybrid operators are defined. Models for the optimal fuzzy measures are built. An approach to hesitant fuzzy multi attribute decision making is developed.'],\n",
              " ['Bibliometrics in fuzzy research. List of most cited papers on fuzzy topics of all time. An overview of influential authors institutions and countries. Journal analysis in fuzzy research.'],\n",
              " ['We investigate the deviation of the priority weights from HMPRs under GDM. Based on the normalization and normalization we develop two models to derive the weights from HMPRs. Some numerical examples are given to illustrate the proposed models.'],\n",
              " ['The proposed work is a novel DE based clustering scheme for WSNs. The algorithm incorporates an additional step to enhance the performance. Experimental results demonstrate the superiority over existing algorithms. The performance is shown in terms of network life energy consumption etc.'],\n",
              " ['We introduce the concept of opposition based learning in binary spaces.It is proven that utilizing random numbers and their opposite is beneficial in evolutionary algorithms.Opposite numbers are applied to accelerate the convergence rate of Binary Gravitational Search Algorithm BGSA .The results show that OBGSA possesses superior performance in accuracy as compared to the BGSA.'],\n",
              " ['We challenge a popular approach to labelling fMRI data for predictive modelling. We propose a new labelling method based on data stream synchronization. We validate the proposed method experimentally on real fMRI data. We observe major classification accuracy improvement and model complexity reduction.'],\n",
              " ['GDOP stands as a relevant measure of positioning accuracy. We propose hybrid intelligent methods namely ANFIS improved ANFIS and RBF for GPS GDOP classification. Bee algorithm BA and improved BA are proposed for finding the optimum radius vector of the ANFIS. To enhance the classification accuracy PCA is utilized.'],\n",
              " ['The ability of calcareous soil to remove Fe II from aqueous solution is investigated. The maximum adsorption of Fe II of 2.475mg g at 6.0mgL 1 initial Fe II concentration was found at pH 6.0 and a temperature of 303K after 120min. As an inhouse alternative an intelligent technique has been envisaged by a conventional MLP and a proposed QBMLP architecture. Both the MLP and QBMLP are found to be efficient in the compared to the batch study process.'],\n",
              " ['A rapid fuzzy rule clustering method based on granular computing is proposed. Exemplar descriptions are selected from sample s descriptions by relative frequency. Data granulation is guided by the selected exemplar descriptions.'],\n",
              " ['This paper proposes a novel continuous estimation of distribution algorithm EDA . A recent EDA named PMBGNP is extended from discrete domain to continuous domain. Reinforcement Learning RL is applied to construct the probabilistic model. Experiments on real mobile robot control show the superiority of the proposed algorithm. It bridges the gap between EDA and RL.'],\n",
              " ['This paper proposes a new approach for training support vector machines with a bone age determination system. The proposed approach is a combination of particle swarm optimization PSO and support vector machines SVMs . The performance and accuracy of the proposed PSO SVM algorithm are examined on a bone age data set. The results obtained by PSO SVM show that PSO SVM is more effective than the previous study based on conventional SVM.'],\n",
              " ['Hamilton Jacobi Bellman HJB equation based stabilized optimal control of hybrid dynamical systems HDS is presented. The fuzzy validity based method is used to find the number of linear models present in the HDS. Stability proof for the event wise and generalized HJB solution based optimal control is proposed. The proposed modeling and control algorithm have been applied on two HDSs.'],\n",
              " ['We model document summarization as a quadratic Boolean programming problem. We create a modified differential evolution to solve the optimization problem. Experimental study shows that the model improves the summarization results.'],\n",
              " ['MGP INTACTSKY is a fuzzy rule based system for dynamic portfolio trading. Multitree Genetic Programming MGP is applied to learn the TSK fuzzy rule bases. The new TSK structure leads to a more interpretable and accurate system. Input variables are technical indices selected by stepwise regression analysis. The results are based on testing of the model on one emerging and two mature markets.'],\n",
              " ['A new cooperative learning strategy is hybridized with DMS PSO. Information can be exchanged among sub swarms before the regrouping process. Experimental results show that DMS PSO CLS has a superior performance.'],\n",
              " ['Used Grid Based Method to create clusters. Derived cluster head selection method based on Bollinger Bands BB that is a technical trading tool. Simulation results shows significant improvement in the network life time that is measured using First Node Dies FND Half of the Nodes Alive HNA and Last Node Dies LND . Proposed algorithm is compared with seven different algorithms using three different node deployment pattern and two different positions of Sink.'],\n",
              " ['The joint statistics and mutual information of the PDTDFB transform coefficients are studied. The PDTDFB transform coefficients are modeled using a HMT statistical model with Gaussian mixtures. The intra scale inter scale and inter direction dependencies are captured comprehensively. A PDTDFB domain HMT statistical model based color image segmentation scheme is developed.'],\n",
              " ['WCA concepts are inspired by nature and based on water cycle process. New concepts of evaporation rate for rivers and streams are applied in the ER WCA. Variable evaporation rate is utilized in the ER WCA to evaporate the water adaptively. ER WCA forces new generated streams to search near sea using the concept of variance. ER WCA outperforms or equals other methods in terms of function evaluations and solution quality.'],\n",
              " ['Selection of objective function and controller structure is vital for controller design. An objective function using ITAE damping ratio and settling times is proposed. The concept is applied to design an hGSA PS based PI PID controller for LFC. Nonlinear interconnected power system model with GRC GDB and time delay is considered.'],\n",
              " ['The clustering model considers dual centers rather than single centers. The dual centers type 2 clustering model and algorithm are proposed. The relations among parameters of the proposed model are explained. The degrees of belonging to the clusters are defined by type 2 fuzzy numbers. The verification and verification indices are developed for model evaluation.'],\n",
              " ['Focus on a minimisation unplanned dilution and ore loss in underground stope. Comprehensive datasets were collected from three mines. Prediction model was built via artificial neural network ANN . Consultation model was built via Fuzzy expert system FES . ANN and FES are combined as a concurrent neuro fuzzy model.'],\n",
              " ['Feature selection is indispensable when dealing with microarray data. A new method for distributing the filtering process is proposed. The data is distributed by features and then merged in a final subset. The method is tested on 8 microarray datasets. The classification accuracy is maintained and the time considerably shortened.'],\n",
              " ['A switching adaptive control scheme using a Hopfield based dynamic neural network SACHNN for nonlinear systems with external disturbances is proposed. The IAC s limitation of can be solved by simply switching the IAC to the DAC where is a positive desired value. The Hopfield dynamic neural network HDNN is used to not only design DAC but also approximate the unknown plant nonlinearities in IAC design.'],\n",
              " ['Comparison of the behavior of classical basic local search techniques and their ability to reach high local optima in combinatorial fitness landscapes. Two particular aspects of local search are considered pivoting rules first and best improvement strategies neutral move policies how neutral moves are considered during the search . Experimental analysis on four different landscape models non neutral and neutral NK landscapes flow shop QAP MAXSAT .'],\n",
              " ['A fuzzy total margin based support vector machine FTM SVM method to handle the class imbalance learning CIL problem in the presence of outliers and noise was presented. The proposed method incorporates total margin algorithm different cost functions and the proper approach of fuzzification of the penalty into FTM SVM and formulates them in nonlinear case. We thoroughly evaluated the proposed FTM SVM method on two artificial data sets and sixteen real world imbalanced data sets'],\n",
              " ['Support vector machine is sensitive to the outliers. A novel support vector regression together with fuzzification theory inconsistency matrix and neighbors match operator is presented. The objective of this novel support vector regression is to increase the generalization ability for data set with outliers.'],\n",
              " ['A dynamic strategy for screening of ASD is proposed based on patterns of information flow between 8 brain regions. The EEG data is collected from 12 healthy and 6 autistic children in the age of 7 to10 years old. The subjects are then classified as autistic or healthy based on the connectivity features. The connectivity features are also compared with other established methods. The promising recognition rates of 93 were achieved in this study. This study shows that patterns of functional and effective connectivity in ASD subjects are different from healthy subjects.'],\n",
              " ['A new fuzzy peer assessment methodology is proposed. A synthesis of perceptual computing and a fuzzy ranking algorithm. Providing both crisp scores and recommendations. A case study in Universiti Malaysia Sarawak is reported. An illustrative example is adopted to detect possible free riders.'],\n",
              " ['Human visual perception in assessing image quality using type 2 fuzzy sets. Entropy of visually salient regions measure uncertainty in feature space. Transformation of features to interval type 2 fuzzy feature space. Free from type reduction and defuzzification computation. Promising results compared to prominent subjective and objective image quality metrics.'],\n",
              " ['New neighborhood structures for the job shop scheduling problem with operators JSO are proposed and analyzed. A new memetic algorithm is proposed for the JSO that incorporates the neighborhood structures in the local search procedure. An experimental study was conducted showing that the memetic algorithm compares favorably with the state of the art.'],\n",
              " ['A modified Adaline and ANFIS are used for disturbance detection in distribution generation. Training of the Adaline is done using a robust decoupled Gauss Newton algorithm. Impact of wind velocity of the wind farm on islanding and non islanding cases is studied. Power quality indices are used to classify disturbances. Comparison with other techniques is shown to validate the superiority.'],\n",
              " ['Based on previous research in the field of computing with perceptions. Highly interpretable and efficient linguistic model used to recognize the gait phases. Set of parameters that characterize relevant aspects of the gait. Fuzzy rule based classifier that discriminates among different walking patterns. Parameters capable of recognizing among five walking patterns with an accuracy of 84 .'],\n",
              " ['The Kriging based EGO techniques performed better than the baseline LHS approach. The use of re interpolation is crucial to cope with noise. Repeats can be necessary but also decrease the number of possible infill points.'],\n",
              " ['Bat algorithm optimized online ANFIS based speed controller presented for Brushless DC motor. The speed response of Brushless DC motor is analyzed for different operating conditions. The proposed controller eliminates the uncertainty problem due to load disturbance and set speed variations. The proposed controller enhances the time domain specifications and performance indices in all operating conditions.'],\n",
              " ['A replacement strategy that improves the diversity of the population is proposed. A decomposition of a computationally expensive neighborhood is defined. Some methods to speed up the evaluation of the neighbors are proposed and extended. The resulting hybrid algorithm is significantly better than the state of the art.'],\n",
              " ['Embolic signals are used for the identification of active embolic sources in stroke prone individuals. Dual Tree Complex Wavelet Transform DTCWT is used as a new feature extractor from forward and reverse Doppler ultrasound signals. The features acquired from forward and reverse flow directions of the blood are fed into k NN and SVMs. The individual predictions of classifiers are combined using ensemble stacking method considering that the forward and reverse blood flow coefficients carry different characteristics. The results show that the DTCWT is superior to the DWT and FFT.'],\n",
              " ['LSNC automatically evolves the architecture. Real valued data is encoded using a 2 D encoding having spike amplitude and time. Sequential learning algorithm developed for SLSNC. Learning algorithm relies on computationally inexpensive operations.'],\n",
              " ['Immunity based robotic applications are reviewed according to immunological models old and new. Mathematical details of reported literature are tabulated genealogically. Issues pertaining to validity of immunological models are raised. We have suggested immunological equivalents of various support functions in these applications. Modern trends in robotics are emphasized in conjunction with those in immunology.'],\n",
              " ['Gene Suppressor proposed as a new add on phase in GA for attaining self adaption and repairing. This regulates genes dosage and its functional expression with respect to its environment. Identifies suppressor genes to perform suppression activity for attaining specific phenotype. Allows adjustment by gene adaption and repairing to obtain best solution and improving it. Experiment focused on proving single problem but the buildup model can be easily adopted to other problem that uses MKP as a base.'],\n",
              " ['Proposed a fast human action recognition system in H.264 compressed domain. Readily available quantization parameters and motion vectors are used for feature extraction. The functional relationship between the features and action labels are classified using PBL McRBFN which has a cognitive and meta cognitive component. Results are analyzed for various GOP parameters. Achieved more than 90 accuracy using PBL McRBFN on two benchmark datasets KTH and Weizmann.'],\n",
              " ['This paper has presented the our new routing protocol with all results in QoS Quality of service metrics of WSN Wireless Sensor Nodes and it is compared with LEACH SEP genetic HCR and ERP routing protocols. This research is useful for densely deployed networks Terrestrial Environmental and Border surveillance applications . This paper presents the routing protocol in which data is transmitted thorough the CHs and CCOs for intra and inter cluster communication. Our research has optimized many parameters of sensor nodes as Energy Time Reliability Throughput and scalability and results calculated in MATLAB validate our work.'],\n",
              " ['Lipschitz aggregation property and copula characteristic of t norms and implications are discussed and the robustness of rule based fuzzy reasoning is investigated. According to Lipschitz aggregation property and copula characteristic of t norms and implications suitable t norm and implication can be chosen to satisfy the need of robustness of fuzzy reasoning. The approach provides guidance for choosing suitable fuzzy connective operators and decision making application in rule based fuzzy reasoning. The experiments which illustrate the ideas are the applications of intelligent information processing.'],\n",
              " ['We predict maximum and minimum day stock prices of power companies. The methodology is based on attribute selection and time series prediction. The most relevant attributes are determined by correlation analysis. The actual time series prediction is carried out by neural networks. The proposed methodology provides very good results.'],\n",
              " ['Adaptive network anomaly detection strategy based on a batch relevance based fuzzified learning algorithm. Couples the capability of inferring decisional structures from incomplete observations with the flexibility of a fuzzy based uncertainty management strategy. Infers the laws and rules governing normal or abnormal network traffic in order to model its operating dynamics. Based on a rule based detection strategy is more effective against previously unknown phenomena and robust against obfuscation mechanisms.'],\n",
              " ['A novel approach for data stream clustering to linear model prototypes. Good performance robust operation low computational complexity and simple implementation. Validation of results by comparison to well known algorithms.'],\n",
              " ['We use nonlinear and non stationary features of sEMG to identify various gestures. In order to reduce the complexity of EMD nonlinear MLE method is used. In order to improve characteristic stability we use a method based on nonlinear MLE. The proposed method classifies six different hand gestures up to 97.6 accuracy. The method can be used for prosthetics and other rehabilitation applications.'],\n",
              " ['The first systematic literature review on evolutionary rule discovery in stock algorithmic trading. A clear demonstrate of studies in this field based on a classification framework. A precise analysis of gaps and limitations in existing studies based on detail of evaluation scheme. The most important factors influencing profitability of models are presented in detail. Targeted suggestions for future improvements based on the review are proposed.'],\n",
              " ['A new biometric identifier whole MJP pattern is introduced. An effective fast and robust MJP based biometric system is developed and presented. Discriminative common vector based method is firstly applied to obtain the feature sets of MJPs.'],\n",
              " ['The power quality disturbance detection and classification is important for improving the power quality. Various disturbances were taken in to account over 20 events. Wavelet is used to extract features from a MATLAB simulated disturbance waveforms. Radial Basis Function Neural Network has been used to detect and classify the disturbance and compared with other considered approach. The entire work has been presented with particle swarm optimization.'],\n",
              " ['Approach presents classification of ballistic missiles using evidential theory. Airborne objects classified without prior knowledge in few seconds after detection. Decision criterion proposed to categorize airborne objects into six major classes. Approach performs better than k NN and decision tree methods with chosen data sets. Validated with real and simulated data sets from single and multiple radars sources.'],\n",
              " ['An improved variant of the particle swarm optimization algorithm is presented. A new formulation of the location routing problem with stochastic demands is given. A new neighborhood topology for PSO suitable for combinatorial optimization problems is proposed. The proposed algorithm is tested in the CLRP and in the LRPSDs. Comparisons with other algorithms from the literature are performed.'],\n",
              " ['A novel Monte Carlo algorithm is proposed to train Bayesian neural networks. This algorithm is based on the full Bayesian approach of artificial neural networks. Monte Carlo methods are integrated with GAs and fuzzy membership functions. Proposed algorithm is applied to time series and regression analysis in the context of BNNs. Proposed approach is superior to traditional training methods in terms of estimation performance.'],\n",
              " ['Cognitive radio leverages on reinforcement learning RL to enhance network security. There is lack of reviews on the application of RL to based security schemes. We cover the challenges characteristics performance enhancements and others.'],\n",
              " ['We deal with the problem of parameter estimation in Generalized Mallows model GMM . We deal with 22 real datasets all of them but one created by the authors. We have designed two experiments varying the maximum evaluations allowed. Obtained results significantly improve the previous competing approaches.'],\n",
              " ['Flexible flow shop scheduling problem with robotic transportation is presented. Minimization of maximum completion time of all parts is considered as objective function. A mixed integer linear programming model is proposed for the problem. Two meta heuristic algorithm GA and ACO is presented.'],\n",
              " ['Employing ICA as well as ICA combined with chaos as main solvers of PBUC. Utilizing a novel conformational integer coded algorithm for PUBC problem. Proposing a sub ICA cascaded with main solver to determine optimal power of units. Proposing a novel concept to cut down number of integers representing ON OFF status of units. Presenting a heuristic based constraint handling to overcome complexities of PBUC.'],\n",
              " ['We constructed a Linear Deceptive function as the representative of a class of multimodal functions. DE cannot guarantee convergence in probability on the above class of multimodal functions. A random drift model was firstly used to analyze the convergence of a real coded evolutionary algorithm. DE s mutation operators prefer to search in the aggregating region of the target individuals.'],\n",
              " ['An adaptive threshold is determined for segmentation of power quality signals. The power quality signals are based on mathematical models and acquired in field. Wavelet transforms are used to decompose the signals. The intersections between the adaptive threshold and the wavelet transform detail curves determine the start and the end of the segments. The adaptive threshold was accurate in more than 96 percent of the signals.'],\n",
              " ['This paper proposes a hybrid time series ANFIS model based on EMD to forecast stock price. In order to evaluate the forecasting performances the proposed model is compared with other models. The experimental results show that proposed model is superior to the listing models.'],\n",
              " ['We study a criteria evaluation system for outsourcing reverse logistics ORL . A hybrid SWOT and intuitionistic fuzzy AHP model evaluates strategic factors in ORL. Triangular intuitionistic fuzzy numbers are used to model ambiguity and uncertainty. The proposed model is validated through a case study. Focusing on core business is shown to be the organization s strategic priority.'],\n",
              " ['Propose a modified PSO DV algorithm using aging mechanism. The algorithm is tested on 12 benchmark functions. The algorithm is used for determining the optimal parameters of PID controller. Comparisons with different evolutionary algorithms show that the algorithm is efficient and robust.'],\n",
              " ['An evaluation index called Control Strategy PSO is developed. It can be applied to other intelligent algorithms. We present a detailed theoretical and empirical analysis.'],\n",
              " ['Numerical computations were conducted under many conditions including different chromosome lengths and search sizes. The good experimental performance of two schemes indicates that the genetic search proposed in this paper is a better way to locate exactly a constrained point at geometric discontinuity. Our algorithm is invariant of chromosome length and search space size to a certain degree.'],\n",
              " ['In this paper we address Opinion Based Entity Rank OpER task. OpER ranks entities based on how well opinion of entities match with users queries. We have outlined an extensive list of ranking features that can be used to capture the notion of query keyword relevance with individual opinions. Our experiments indicate that these ranking features have significantly high effectiveness for OpER task than standard retrieval models. For further improving the effectiveness we combine these ranking feature using genetic programming GP and learning to rank approach.'],\n",
              " ['In this paper we propose an algorithm named dFIN for frequent itemset mining. dFIN achieves high performance by employing DiffNodesets to represent itemsets. Experiment results on real datasets show that dFIN is effective and outperforms state of the art algorithms.'],\n",
              " ['A cluster ensemble framework based on the group method of data handling was proposed. The components of the CE GMDH can be chosen according to the target of the application. Three novel transfer functions in CE GMDH were proposed. CE GMDH outperforms the other cluster ensemble algorithms and frameworks.'],\n",
              " ['In the case of the singularity of the within class scatter matrix the drawbacks of both MCVSVM and LMLP are analyzed. A novel algorithm TSSVM is proposed to deal with the high dimensional data classification task where the within class scatter matrix is singular. An alternative version of the nonlinear MCVSVM and the nonlinear LMLP are proposed. The nonlinear TSSVM is developed.'],\n",
              " ['Different types of power quality disturbances are discriminated from each other. VMD and ST are used for extraction of dominant features. SVM with simple structure and few adjustable parameters is used as the classifier core. The generalization capability and detection accuracy of the proposed method are increased by elimination of redundant features by using different feature selection methods. The start and end points of PQ events can be detected accurately.'],\n",
              " ['The aggregated artificial neural network was used to investigate the simultaneous effects of printing parameters on the compressive strength and porosity of scaffolds. Particle swarm optimization algorithm was implemented to obtain the optimum topology of the AANN. Pareto front optimization was used to determine the optimal setting parameters. The presented results and discussion can give informative information to practitioners who want to design a porous structure and need to know the impact of influential design parameters.'],\n",
              " ['Type 2 self organizing fuzzy logic controllers for automatic anesthesia control. Type 2 SOFLC use type 2 fuzzy sets to handle anesthesia control uncertainties. Data capturing inter and intra patient variability used to define type 2 fuzzy sets. Simulations show effectiveness of type 2 SOFLC in control of anesthetic infusion under noisy and uncertain surgical conditions. Type 2 SOFLC are able to outperform the existing type 1 SOFLC.'],\n",
              " ['A novel fuzzy controller and a new kind of slack variable approach are developed. The criterion takes the form of an LMI which is computationally tractable. The obtained stabilization conditions are less conservative.'],\n",
              " ['The proposed method integrates SNLM clustering and level set. Able to delineate thyroid nodules in ultrasound images accurately and automatically. Can be applied without preprocessing due to its indeterminacy handling capability. The parameters of SNDRLS are determined adaptively from SNLM clustering. Experimental results show the effectiveness of the proposed method.'],\n",
              " ['Neural network is trained to predict the compressive strength of composites from experimental variables. Input variables are optimized to maximize compressive strength by genetic algorithm. Experimental constraints are considered during optimization. The consistency of the analysis is verified by laboratory experiments.'],\n",
              " ['A Hybrid method for identifying tire parameters in Magic Formula has been applied. Firstly the Hybrid method uses Genetic Algorithm GA with a high exploration power. Then the best result of GA is used as starting values SVs for Levenberg Marquardt. The method shows high convergence speed and accuracy and null sensitivity to SVs.'],\n",
              " ['Fuzzy linear fractional differential equations in form under Riemann Liouville H differentiability are studied. Some of the previous results on solutions of these equations are concreted. New solutions are obtained using the fractional hyperbolic functions and their properties. An application and two examples are given to illustrate our results.'],\n",
              " ['This paper presents a maiden application of EMA to solve power system ORPD problems. Steps of implementation of EMA to solve ORPD are elaborately discussed. The performance of EMA is tested on standard IEEE test systems. The selection of control parameters of EMA is done through exhaustive parametric study.'],\n",
              " ['To automate an ophthalmologic diagnostic system based on signal processing. Wavelet transform is used to extract the features. Entropy analysis is used and we have formulated an Electroretinographic Index to discriminate between controls and suspects. It will assist the physicians to cross check their diagnosis.'],\n",
              " ['Phase synchronization in brain using recurrence plot based technique is studied. A novel application of the technique to multichannel seizure EEG data is made. Contour plots of Correlation between Probability of Recurrence CPR matrix show clear contrast between seizure and pre seizure signals and a classification accuracy of 100 has been achieved. CPR could identify the focus of epilepsy and the identification is much better than that obtained using linear correlation. The paper has shown the utility of a nonparametric nonlinear model in studying synchronization in the brain.'],\n",
              " ['We proposed a novel quadrature clutter rejection approach to suppress the quadrature clutter signals induced by the slowly moving vessel walls and tissues in composite Doppler ultrasound signals. The proposed clutter rejection approach is based on multivariate empirical mode decomposition MEMD . The phase information between the quadrature demodulated Doppler ultrasound signals is keeping well during the decomposition by MEMD. The extra errors induced by the traditional clutter rejection approaches could be removed effectively. The NA TEMD provides the best performance for conserving more bidirectional blood flow components with low velocities.'],\n",
              " ['We propose a new model of non stationary rhythmogram as a frequency modulated signal. Double continuous wavelet transformation is performed to analyze non stationary HRV. Various scenarios of oscillation rearrangement are studied for frequency modulated rhythmogram signal. The transient periods and other spectral characteristics are calculated basing on head up tilt test real records.'],\n",
              " ['We introduced two representation in respect to materials naive representation and expert representation. We proposed a uniforming method of dimensionality of data with different size input vectors using a neural network. The proposed method outperformed the conventional methods the multi layer autoencoder the denoising autoencoder and kernel PCA for the linear regression task on synthetic data. The experimental results showed the robustness for data size and number of constituent elements. In the linear regression task on ion conductivity data of bulk materials and hydrogen storage materials the good fitting performance was obtained in terms of the latent data uniformed by the proposed method.'],\n",
              " ['Parametric estimation of scalp skull and brain conductivities is analyzed. Realistic head models including anisotropy are adopted. Convenient electrode pairs for the current injection are determined. The Maximum Likelihood Estimator is advisable in practical situations.'],\n",
              " ['CKHA algorithm is applied for ORPD problems considering FACTS devices. CKHA is implemented on three IEEE standard test systems. Two different objective functions are considered. The results of CKHA are compared to other algorithms surfaced recently. Effectiveness of CKHA is established for ORPD problem with FACTS devices.'],\n",
              " ['We compared cardiac vectors of ischemic and healthy patients in two different planes. Planes under study the Einthoven s plane FP and a PCA transformed plane PP . We compared the ability to discriminate ischemia from health of both representations. The FP enabled the discrimination of ischemic patients while the PP did not.'],\n",
              " ['An explicit prostate segmentation model utilizing non parametric shape prior is proposed. The method can model the shape that is not statistically significant in the shape space. The method can escape from local minima by using a fairly large detection range during freely deforming. Compared with recently proposed sparse shape composition method training data set with a large number of instances is not necessary. The intrinsic properties of TRUS images are integrated into the model.'],\n",
              " ['Spectral clustering extension to independent component analysis has been proposed. Synthetic and clinical MRI data analyzed to verify the potential of the algorithm. High performance tissue classification observed in multispectral brain MRI study. The proposed method shows 98.8 accuracy in clinical abnormality analysis.'],\n",
              " ['A novel feature representation is proposed for biomedical time series classification. The representation is extended from the bag of words model in text document analysis. The proposed bag of words representation is effective and discriminative. The representation is able to capture both global and local structural information.'],\n",
              " ['We propose a technology credit scoring model based on fuzzy logistic regression. Fuzzy predictor fuzzy binary responses with crisp coefficients are considered. Fuzzy least square method is used to estimate parameters. The performance of proposed fuzzy logistic regression model is improved compared to the logistic regression.'],\n",
              " ['A novel de noising method is proposed to remove the ECG interference from noisy EMGdi signals. Experimental results achieved on practical clinical data show that the proposed approach is better than traditional methods include wavelet transform WT ICA digital filter and adaptive filter in ECG interference removing. The original independent components of contaminated EMGdi signal were firstly obtained with independent component analysis ICA . Then the ECG components contained were removed by a specially designed wavelet domain filter. Finally the purified independent components were reconstructed back to the original signal space by ICA to obtain the clean EMGdi signals.'],\n",
              " ['We study the NP hard problem of finding a minimum vertex cover of graphs based on rough sets. The problem of finding a minimum vertex cover of graphs can be translated into the problem of finding an optimal reduct of a decision information table in rough sets. A new method based on rough sets is proposed to compute the minimum vertex cover of a given graph.'],\n",
              " ['This paper presents an approach for breast cancer diagnosis in digital mammograms using shearlet transform. A new approach is represented for feature extraction using shearlet transform t test statistic and dynamic thresholding. The system uses two datasets ROIs obtained from two different database MIAS and DDSM database. The datasets include all abnormalites for breast cancer. The classification section was performed to distinguish the ROIs as normal abnormal and benign malignant.'],\n",
              " ['A combination of ICA and WNN is introduced to eliminate EOG artefacts. This method corrects only EOG artefacts within a contaminated component. Minimal low frequency and underlying cerebral activity is lost. EEG quality is increased.'],\n",
              " ['Hilbert transform method can be used for detecting both the onset and systolic peak in pulse wave signals. It achieves good performance and precision when compared to expert annotation. It is robust when noise and interference are present in pulse wave and also. Shows a low detection error rate. It is computationally simple.'],\n",
              " ['An optimization study on EMG based hand gesture recognition is proposed. We compare unsupervised PCA vs. supervised CSP preprocessing technique. Artificial neural network achieves best classification accuracies in both cases. The study identifies the best parameters to choose in pattern recognition stages. The optimization algorithm is robust enough to be used on amputees as well.'],\n",
              " ['PCA based ICA is proposed to exploit the inter channel correlations. 1 D signal is arranged in the form of matrix to exploit the intra channel correlations. SPIHT algorithm is used to compress the signals in matrix form.'],\n",
              " ['HuWSF is computed from local regions of a spectrogram by Hu moments. HuWSF can evaluates how the energy aggregate to some frequencies in a spectrogram. HuWSF extracts the features among neighbor coefficients of Mel filters of a frame. HuWSF extracts the features among coefficients of Mel filters of neighbor frames. HuWSF can reduce the changes brought by sentences speakers and speaking styles.'],\n",
              " ['The multi ganglion ANN based feature learning ANNFL method is an unsupervised feature extraction method. This method can find an effective feature representation automatically for single trial P300 signal. The ANNFL method reduces the training time of conventional three layer auto encoder and leads to better classification results in the P300 BCI paradigm of our study.'],\n",
              " ['Yawning is detected upon mouth opening mouth covering and facial feature distortion. Mouth covered detection is based on classification of LBP features extracted from mouth covering hand. Proposed approach has been tested on real video sequences of sleep deprived volunteers.'],\n",
              " ['Session to session EEG variation is addressed and tackled by the proposed method. Interval type 2 fuzzy classification approach adopted for the purpose. Studied on discrimination of wrist and fingers motor imagery EEG signals. Offline and online experiments establish the efficacy of the proposed method. Using Extreme Energy Ratio as features 86.45 and 78.44 accuracies are obtained.'],\n",
              " ['The combination of EEMD and ICA proposed and used for multichannel MCG signal denoising. Reduces the computational complexity associated with EEMD alone and effectively enhances the signal quality. The effect of ICA alone wICA and EEMD ICA on the magnetic field map and pseudo current density maps is investigated. The EEMD ICA combination yield robust source estimate on the PCD map. The EEMD ICA can be applied to beat to beat analysis of MCG signals.'],\n",
              " ['We develop a novel decentralised feedback controller for a hydrodynamic human circulatory system simulator. We present a detailed model of the human circulatory system simulator. Nonlinear simulation and experimental results underline the performance of the proposed controller. In vitro experiments with a blood pump show sufficiently fast reference tracking to realise aortic and ventricle pressures.'],\n",
              " ['Brain s induced response was measured in response to auditory tones and words. Power and coherence analyses were used for this purpose. Power analyses detected induced activity in response to words. Coherence analyses showed significant induced activity in response to both stimuli. The results may be of interest in studies involving hearing impaired patients.'],\n",
              " ['We present an automatic method for estimating features describing speech F0 contour. Analysis on acted emotional speeches distinguished high from low arousal emotions. Intra subject analysis on bipolar patients found out differences between mood states. The results on bipolar patients are subject specific and task dependent. Results on controls confirmed a good specificity of the proposed features.'],\n",
              " ['Muscle fiber conduction velocity estimators during dynamic exercises are proposed. Time varying delay estimators are compared by Monte Carlo simulations. TFTS methods are robust to noise and accurate for the conduction velocity tracking.'],\n",
              " ['Identification of steatosis through three distinct approaches. We show that it is possible to detect fatty liver using classifiers with a reduced number of features. We prove that the hepatorenal coefficient and the attenuation coefficient are capable to successfully discriminate steatotic from normal livers. Our methodologies could be also used to detect the degrees of steatosis useful for analyzing its evolution over time.'],\n",
              " ['Motivation Synchronize biomedical signals from several acquisition systems is a difficult task. Methodology Use of white noise and correlation methods to synchronize biomedical signals. Results Synchronization with an error of 0.2ms in an experimental case. Conclusions This is a reliable and flexible method for several biomedical acquisition systems.'],\n",
              " ['Critical issues and practices for myoelectric interfaces are discussed. Robust classification in real use is the main challenge of sEMG interfaces. Expected trend is toward regression and factorization methods and sensor fusion. Simplified sEMG setup may be adequate in real life environment. Potential uses of sEMG interfaces are rapidly increasing.'],\n",
              " ['A minimal recruitment model which captures alveolar opening and closing pressures is presented. The model is computationally intense and not suitable for bedside application. The model is simplified while maintaining its physiological relevance. A new algorithm is used to improve computational performance.'],\n",
              " ['The DWT SVD based algorithm achieves imperceptibility and robustness. The multiple watermarking techniques achieve high security of medical images more robustness and to preserve the privacy. The transparency and robustness are considered as an optimization problem and solved by applying Genetic algorithms.'],\n",
              " ['A mathematical model for 3 D computer graphs is presented. Structures of preference who is collaborating with whom are shown in 3 D version. The methods are applied on male and female networks.'],\n",
              " ['Photogrammetry is suitable for 3D modeling at macroscopic scale. Most relevant criteria for the traceology of petroglyphs are in the scope of photogrammetry. Focus stacking enlarges the depth of field of photomicrography and gives data for relief elevation. Consumers devices and software provide effective solutions for macro and microscopic recording.'],\n",
              " ['ABECm may efficiently and non intrusively predict speech intelligibility in noise. High correlation was obtained between ABECm values and intelligibility scores. The computation of ABECm does not need access to the clean reference signal.'],\n",
              " ['Developing a knowledge based framework to assist the designer in design decisions. Opitz feature recognition and code generation from STEP for data standardization. An efficient similarity recognition algorithm to retrieve models from database.'],\n",
              " ['Features a specific quantitative a priori analytical criterion as expansion ratio.. Injector capacity and desired moulding features are synchronised to augment functionality. In situ spatio temporal injectant state perplexity described as explicit function. Comprehensive sensitisation showed discrete slope and altitude for each polymeric character. All thermoplastic characteristics were assorted across congruent range of apparent viscosity and shear thinning index. Congruent ranges of apprent viscosity and shear thinning index were conceived to characteristically assort most thermoplastics.'],\n",
              " ['We analyse MCMC methods for cosmology regarding parallelisability and efficiency. We present the Python framework CosmoHammer for parallelised MCMC sampling. It enables us to estimate cosmological parameters on high performance clusters. To test the efficiency of CosmoHammer we use an elastic cloud computing environment.'],\n",
              " ['Internet based cognitive behavior therapy for procrastination was considered credible by a majority of the participants. Many participants were able to gain momentum and raise their self efficacy as a result of the treatment interventions. Conflicting commitments prevented a number of the participants from completing the treatment program. Feedback might have to be adapted to the individual needs of the participants. Short and manageable modules could be of particular importance in the treatment of procrastination.'],\n",
              " ['This paper focus on analytical numerical and experimental design optimization of shifter. Performed design optimization by finite element analysis and experimental of live industrial problem. The results can applicable as a basis of design and optimization of new type of the automotive parts. The results of the current work present the actual behavior of induced stresses.'],\n",
              " ['We present an efficient technique for sketch based 3D modeling using automatically extracted image features. An automatic and real time method is proposed to align a user s hand drawn sketch line to the contour lines of an image facilitating a considerable level of ease for 3D modeling. We use a geometric method to align a sketch line to the outlines of an image using the features of the sketch line and contour lines of an image and some operations are proposed to refine the result of alignment. In the sketch based 3D modeling method the sketch line is represented by a editable spline therefore the aligned sketch line can be further adjusted interactively.'],\n",
              " ['A total function on an initial algebra is a homomorphism iff its kernel is a congruence. An earlier paper CMCS 2001 elaborates on that classical result. We extend the result from total to partial functions. We simplify the proofs using the relational calculus. We generalise the setting to regular categories.'],\n",
              " ['The 90 9 1 principle provides a model for participation in an Internet support group. Alternately a power curve accounts for 98.6 of variance in frequency of posts. We confirm the application of Zipf s Law for the Internet support group.'],\n",
              " ['Permutation entropy PE is a broadly used algorithm to measure the complexity of signals. Multiscale PE MPE is based on assessing the PE for a number of coarse grained sequences representing temporal scales. To increase the stability and reliability of MPE improved MPE IMPE is proposed. Several signal processing concepts are used to show the ability of IMPE. We also apply MPE and IMPE for real publicly available electroencephalogram EEG signals.'],\n",
              " ['Problems of traditional rock art recordings were discussed Laser scans of rock art carvings were processed in ArcGIS Detailed DSMs of rock art surfaces were generated from TIN interpolation Rasters with different surface illuminations were summarised with PCA ArcGIS offers the possibility to visualise and enhance rock art carvings'],\n",
              " ['Process variables in eHealth interventions such as choices made during an interactive intervention are readily available. Such variables can provide insight into how participants use eHealth interventions. Correlations between choices and outcomes may also guide intervention optimization and future research. Among screen positive adults in primary care 108 60.7 denied any problems as a result of their drug use. Participant ratings of the importance of change were unrelated to changes in substance use frequency.'],\n",
              " ['This paper provides a nontechnical overview of growth models. I advocate for these methods use in the field of internet interventions. Recommendations for design analysis and reporting of results are provided.'],\n",
              " ['I CBT has not been used as an early intervention for adolescents with coexisting pain and emotional distress The aim of this pilot study was to explore the effects of early I CBT for adolescents with pain and emotional distress The intervention consisted of 5 9 modules of customized I CBT combined with personal support The effects were quite modest and larger studies are needed to further explore the outcomes'],\n",
              " ['Asteroids@home is a distributed computing project. It runs in the framework of Berkeley Open Infrastructure for Network Computing. Shape models of asteroids are reconstructed from disk integrated photometry.'],\n",
              " ['This study presents baseline data on screening scales in a large sample of Australian adolescents. This study presents baseline data on suicidal ideation in Australian adolescents. This school based intervention reduced symptoms of anxiety and depression. Given the study limitations these findings can only be considered tentative.'],\n",
              " ['A new methodology in predicting a CNC machining output has been investigated. A data mining technique and a hybrid type II fuzzy system are applied. Two different types of membership functions were created to generate a hybrid system. Fuzzy rules are automatically modified in the process of genetic algorithm training. The results showed that the hybrid system generated a far better accuracy.'],\n",
              " ['We examined the effectiveness of internet CBT in routine clinical care. Patients underwent an automated assessment or an assessment by a psychiatrist. Both groups showed significant improvement in symptoms of anxiety and depression. There were no differences between groups at post treatment. Automated assessments can lead to good treatment outcomes in etherapy.'],\n",
              " ['Depressive signs in concurrent cognitive decline are detected through mining EEG resting state activity. Random Forest Random Tree MLP Network and Support Vector Machines SVM are employed for data classification. Random Forest demonstrated the highest accuracy. Synchronization features significantly contributed to the decision tree formation.'],\n",
              " ['Parents act as gatekeepers to mental health services for children and adolescents. Parents reported their attitudes towards computer based therapies for young people. 94 of parents would access a computer based therapy if their child needed support. Computer based therapy knowledge predicted perceived helpfulness and use intentions. Parents hold positive attitudes to the use of computer based therapies.'],\n",
              " ['We model the automotive twist beam. We developed an algorithm to solve multicriteria optimization problem. We solve our industrial problem with our developed algorithm. New profiles of the twist beam are proposed.'],\n",
              " ['Qualitative study on experiences of non adherence to Internet delivered treatment Explores treatment features relevant to participants experience of premature treatment termination Explores negative effects'],\n",
              " ['A parallel open source implementation of Liley s mean field cortex model in PETSc. We implement fully implicit time integration of nonlinear and variational equations. We perform equilibrium continuation with computation of inhomogeneous eigenmodes. We compute periodic solutions with Newton Krylov iteration.'],\n",
              " ['A new approach to detect sources in long wavelength images is presented. It is based on a multiscale decomposition and a method called Distilled Sensing. The method is tested against real infrared and radio images. The performance of the method is compared to reference state of the art algorithms.'],\n",
              " ['Development of a dynamic replication strategy of segments for popular video. Proposing an approach that can provide admissions control for all VoD systems. It addresses the following challenges resource allocation and several algorithms for scheduling policies. Application of a measurement algorithm. Application of a decision algorithm. To demonstrate the efficiency and feasibility of our approaches and validate the results obtained we conducted a series of experiments.'],\n",
              " ...]"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b10719e",
      "metadata": {
        "id": "0b10719e"
      },
      "outputs": [],
      "source": [
        "text=[]\n",
        "for i in range(2000):\n",
        "   a=[]\n",
        "   a.append(df.at[i,'Abstract'])\n",
        "   text.append(a)\n",
        "   a=[]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3194bc2e",
      "metadata": {
        "id": "3194bc2e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c587a24a-ea91-4aeb-8f38-76ce225af8e2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[' This paper introduces four classes of rotation invariant orthogonal moments by generalizing four existing moments that use harmonic functions in their radial kernels . Members of these classes share beneficial properties for image representation and pattern recognition like orthogonality and rotation invariance . The kernel sets of these generic harmonic function based moments are complete in the Hilbert space of square integrable continuous complex valued functions . Due to their resemble definition the computation of these kernels maintains the simplicity and numerical stability of existing harmonic function based moments . In addition each member of one of these classes has distinctive properties that depend on the value of a parameter making it more suitable for some particular applications . Comparison with existing orthogonal moments defined based on Jacobi polynomials and eigenfunctions has been carried out and experimental results show the effectiveness of these classes of moments in terms of representation capability and discrimination power . \\n'],\n",
              " [' In this paper a MATLAB based graphical user interface software tool for general biomedical signal processing and analysis of functional neuroimaging data is introduced . Specifically electroencephalography and electrocardiography signals can be processed and analyzed by the developed tool which incorporates commonly used temporal and frequency analysis methods . In addition to common methods the tool also provides non linear chaos analysis with Lyapunov exponents and entropies multivariate analysis with principal and independent component analyses and pattern classification with discriminant analysis . This tool can also be utilized for training in biomedical engineering education . This easy to use and easy to learn intuitive tool is described in detail in this paper . \\n'],\n",
              " [' Background and objective A markerless low cost prototype has been developed for the determination of some spatio temporal parameters of human gait step length step width and cadence have been considered . Only a smartphone and a high definition webcam have been used . Methods The signals obtained by the accelerometer embedded in the smartphone are used to recognize the heel strike events while the feet positions are calculated through image processing of the webcam stream . Step length and width are computed during gait trials on a treadmill at various speeds . Results Six subjects have been tested for a total of 504 steps . Results were compared with those obtained by a stereo photogrammetric system . The maximum average errors were 3.7cm for the right step length and 1.63cm for the right step width at 5km h. The maximum average error for step duration was 0.02s at 5km h for the right steps . Conclusion The system is characterized by a very high level of automation that allows its use by non expert users in non structured environments . A low cost system able to automatically provide a reliable and repeatable evaluation of some gait events and parameters during treadmill walking is relevant also from a clinical point of view because it allows the analysis of hundreds of steps and consequently an analysis of their variability . \\n'],\n",
              " [' This paper presents an improved multiple instance learning tracker representing target with Distribution Fields and building a weighted geometric mean MIL classifier . Firstly we adopt DF layer as feature instead of traditional Haar like one to model the target thanks to the DF specificity and the landscape smoothness . Secondly we integrate sample importance into the weighted geometric mean MIL model and derive an online approach to maximize the bag likelihood by AnyBoost gradient framework to select the most discriminative layers . Due to the target model consisting of selected discriminative layers our tracker is more robust while needing fewer features than the traditional Haar like one and the original DFs one . The experimental results show higher performances of our tracker than those of five state of the art ones on several challenging video sequences . \\n'],\n",
              " [' Text based image retrieval may perform poorly due to the irrelevant and or incomplete text surrounding the images in the web pages . In such situations visual content of the images can be leveraged to improve the image ranking performance . In this paper we look into this problem of image re ranking and propose a system that automatically constructs multiple candidate multi instance bags which are likely to contain relevant images . These automatically constructed bags are then utilized by ensembles of Multiple Instance Learning classifiers and the images are re ranked according to the final classification responses . Our method is unsupervised in the sense that the only input to the system is the text query itself without any user feedback or annotation . The experimental results demonstrate that constructing multiple instance bags based on the retrieval order and utilizing ensembles of MIL classifiers greatly enhance the retrieval performance achieving on par or better results compared to the state of the art . \\n'],\n",
              " [' Background and objective Non compartmental analysis calculates pharmacokinetic metrics related to the systemic exposure to a drug following administration e.g . area under the concentration time curve and peak concentration . We developed a new package in R called ncappc to perform a NCA and simulation based posterior predictive checks for a population PK model using NCA metrics . Methods The nca feature of ncappc package estimates the NCA metrics by NCA . The ppc feature of ncappc estimates the NCA metrics from multiple sets of simulated concentration time data and compares them with those estimated from the observed data . The diagnostic analysis is performed at the population as well as the individual level . The distribution of the simulated population means of each NCA metric is compared with the corresponding observed population mean . The individual level comparison is performed based on the deviation of the mean of any NCA metric based on simulations for an individual from the corresponding NCA metric obtained from the observed data . The ncappc package also reports the normalized prediction distribution error of the simulated NCA metrics for each individual and their distribution within a population . Results The ncappc produces two default outputs depending on the type of analysis performed i.e . NCA and PopPK diagnosis . The PopPK diagnosis feature of ncappc produces 8 sets of graphical outputs to assess the ability of a population model to simulate the concentration time profile of a drug and thereby evaluate model adequacy . In addition tabular outputs are generated showing the values of the NCA metrics estimated from the observed and the simulated data along with the deviation NPDE regression parameters used to estimate the elimination rate constant and the related population statistics . Conclusions The ncappc package is a versatile and flexible tool set written in R that successfully estimates NCA metrics from concentration time data and produces a comprehensive set of graphical and tabular output to summarize the diagnostic results including the model specific outliers . The output is easy to interpret and to use in evaluation of a population PK model . ncappc is freely available on CRAN and GitHub . \\n'],\n",
              " [' The cardiovascular and respiratory autonomic nervous regulation has been studied mainly by hemodynamic responses during different physical stressors . In this study dynamics of autonomic response to an orthostatic challenge was investigated by hemodynamic variables and by diverse linear and nonlinear indices calculated from time series of beat to beat intervals respiratory cycle duration systolic and diastolic blood pressure . This study included 16 young female patients with vasovagal syncope and 12 age matched female controls . The subjects were enrolled in a head up tilt test breathing normally including 5min of baseline and 18min of 70 orthostatic phase . To increase the time resolution of the analysis the time series were segmented in five minute overlapping windows with a shift of 1min . Hemodynamic parameters did not show any statistical differences between SYN and CON . Time domain linear analysis revealed increased respiratory frequency and increased blood pressure variability in patients during OP meaning increased sympathetic activity and vagal withdrawal . Frequency domain analysis confirmed a predominance of sympathetic tone by steadily increased values of low over high frequency power in BBI and of low frequency power in SYS and DIA in patients during OP . The nonlinear analysis by symbolic dynamics seemed to be highly suitable for differentiation of SYN and CON in the early beginning of OP i.e . 5min after tilt up . In particular the index SYS plvar3 showed less patterns of low variability in patients reflecting a steadily increase in both BPV and sympathetic activity . The proposed dynamical analysis could lead to a better understanding of the temporal underlying mechanisms in healthy subjects and patients under orthostatic stress . \\n'],\n",
              " [' Flooding is a widely occurring natural hazard that noticeably damages property people and the environment . In the context of climate change the integration of spatial planning with flood risk management has gained prominence as an approach to mitigating the risks of flooding . The absence of easy access to integrated and high quality information and the technologies and tools to use information are among the factors that impede this integration . Limited research has been conducted to develop a framework and to investigate the role of information and technologies in this integration . This study draws primarily on the European experiences and literature and identifies three dimensions of the integration of spatial planning with flood risk management territorial policy and institutional . To facilitate integration and in accord with these three dimensions a Spatially Integrated Policy Infrastructure is conceptualised that encompasses data and information decision support and analysis tools and access tools and protocols . This study presents the connections between SIPI elements and integration dimensions which is important for a better understanding of roles of geographic information and technologies in integration . The conceptual framework of SIPI will govern further development and evaluation of SIPI . \\n'],\n",
              " [' Dense disparity map is required by many great 3D applications. In this paper a novel stereo matching algorithm is presented. The main contributions of this work are three fold. Firstly a new cost volume filtering method is proposed. A novel concept named two level local adaptation is introduced to guide the proposed filtering approach. Secondly a novel post processing method is proposed to handle both occlusions and textureless regions. Thirdly a parallel algorithm is proposed to efficiently calculate an integral image on GPU and it accelerates the whole cost volume filtering process. The overall stereo matching algorithm generates the state of the art results. At the time of submission it ranks the 10th among about 152 algorithms on the Middlebury stereo evaluation benchmark and takes the 1st place in all local methods. By implementing the entire algorithm on the NVIDIA Tesla C2050 GPU it can achieve over 30 million disparity estimates per second MDE s . \\n'],\n",
              " [' Since 2005 human and computer performance has been systematically compared as part of face recognition competitions with results being reported for both still and video imagery. The key results from these competitions are reviewed. To analyze performance across studies the cross modal performance analysis CMPA framework is introduced. The CMPA framework is applied to experiments that were part of face a recognition competition. The analysis shows that for matching frontal faces in still images algorithms are consistently superior to humans. For video and difficult still face pairs humans are superior. Finally based on the CMPA framework and a face performance index we outline a challenge problem for developing algorithms that are superior to humans for the general face recognition problem. \\n'],\n",
              " [' Background and objectives The diagnosis of Developmental Dysplasia of the Hip in infants is currently made primarily by ultrasound . However two dimensional ultrasound images capture only an incomplete portion of the acetabular shape and the alpha and beta angles measured on 2DUS for the Graf classification technique show high inter scan and inter observer variability . This variability relates partly to the manual determination of the apex point separating the acetabular roof from the ilium during index measurement . This study proposes a new 2DUS image processing technique for semi automated tracing of the bony surface followed by automatic calculation of two indices a contour based alpha angle and a new modality independent quantitative rounding index . The new index M is independent of the apex point and can be directly extended to 3D surface models . Methods We tested the proposed indices on a dataset of 114 2DUS scans of infant hips aged between 4 and 183 days scanned using a 12MHz linear transducer . We calculated the manual alpha angle coverage contour based alpha angle and rounding index for each of the recordings and statistically evaluated these indices based on regression analysis area under the receiver operating characteristic curve and analysis of variance . Results Processing time for calculating and M was similar to manual alpha angle measurement 30s per image . Reliability of the new indices was high with inter observer intraclass correlation coefficients 0.90 for and 0.89 for M. For a diagnostic test classifying hips as normal or dysplastic AUC was 93.0 for vs. 92.7 for 91.6 for M alone and up to 95.7 for combination of M with or coverage . Conclusions The rounding index provides complimentary information to conventional indices such as alpha angle and coverage . Calculation of the contour based alpha angle and rounding index is rapid shows potential to improve the reliability and accuracy of DDH diagnosis from 2DUS and could be extended to 3D ultrasound in future . \\n'],\n",
              " [' Background and objective Progress in biomedical engineering has improved the hardware available for diagnosis and treatment of cardiac arrhythmias . But although huge amounts of intracardiac electrograms can be acquired during electrophysiological examinations there is still a lack of software aiding diagnosis . The development of novel algorithms for the automated analysis of EGMs has proven difficult due to the highly interdisciplinary nature of this task and hampered data access in clinical systems . Thus we developed a software platform which allows rapid implementation of new algorithms verification of their functionality and suitable visualization for discussion in the clinical environment . Methods A software for visualization was developed in Qt5 and C utilizing the class library of VTK . The algorithms for signal analysis were implemented in MATLAB . Clinical data for analysis was exported from electroanatomical mapping systems . Results The visualization software KaPAVIE was implemented and tested on several clinical datasets . Both common and novel algorithms were implemented which address important clinical questions in diagnosis of different arrhythmias . It proved useful in discussions with clinicians due to its interactive and user friendly design . Time after export from the clinical mapping system to visualization is below 5min . Conclusion KaPAVIE See http www.ibt.kit.edu hardundsoftware.php . is a powerful platform for the development of novel algorithms in the clinical environment . Simultaneous and interactive visualization of measured EGM data and the results of analysis will aid diagnosis and help understanding the underlying mechanisms of complex arrhythmias like atrial fibrillation . \\n'],\n",
              " ['In this paper we propose an album oriented face recognition model that exploits the album structure for face recognition in online social networks. Albums usually associated with pictures of a small group of people at a certain event or occasion provide vital information that can be used to effectively reduce the possible list of candidate labels. We show how this intuition can be formalized into a model that expresses a prior on how albums tend to have many pictures of a small number of people. We also show how it can be extended to include other information available in a social network. Using two real world datasets independently drawn from Facebook we show that this model is broadly applicable and can significantly improve recognition rates. \\n'],\n",
              " [' Background and objective Methods used in image processing should reflect any multilevel structures inherent in the image dataset or they run the risk of functioning inadequately . We wish to test the feasibility of multilevel principal components analysis to build active shape models for cases relevant to medical and dental imaging . Methods Multilevel PCA was used to carry out model fitting to sets of landmark points and it was compared to the results of standard PCA . Proof of principle was tested by applying mPCA to model basic peri oral expressions approximated to the junction between the mouth lips . Monte Carlo simulations were used to create this data which allowed exploration of practical implementation issues such as the number of landmark points number of images and number of groups . To further test the robustness of the method mPCA was subsequently applied to a dental imaging dataset utilising landmark points along the boundary of mandibular cortical bone in panoramic radiographs of the face . Results Changes of expression that varied between groups were modelled correctly at one level of the model and changes in lip width that varied within groups at another for the Monte Carlo dataset . Extreme cases in the test dataset were modelled adequately by mPCA but not by standard PCA . Similarly variations in the shape of the cortical bone were modelled by one level of mPCA and variations between the experts at another for the panoramic radiographs dataset . Results for mPCA were found to be comparable to those of standard PCA for point to point errors via miss one out testing for this dataset . These errors reduce with increasing number of eigenvectors values retained as expected . Conclusions We have shown that mPCA can be used in shape models for dental and medical image processing . mPCA was found to provide more control and flexibility when compared to standard single level PCA . Specifically mPCA is preferable to standard PCA when multiple levels occur naturally in the dataset . \\n'],\n",
              " [' Background Accurate segmentation of human head on medical images is an important process in a wide array of applications such as diagnosis facial surgery planning prosthesis design and forensic identification . Objectives In this study a Bayesian method for segmentation of facial tissues is presented . Segmentation classes include muscle bone fat air and skin . Methods The method presented incorporates information fusion from multiple modalities modelling of image resolution image noise two priors helping to reduce noise and partial volume . Image resolution modelling employed facilitates resolution enhancement and superresolution capabilities during image segmentation . Regularization based on isotropic and directional Markov Random Field priors is integrated . The Bayesian model is solved iteratively yielding tissue class labels at every voxel of the image . Sub methods as variations of the main method are generated by using a combination of the models . Results Testing of the sub methods is performed on two patients using single modality three dimensional image as well as registered MR CT images with information fusion . Numerical visual and statistical analyses of the methods are conducted . High segmentation accuracy values are obtained by the use of image resolution and partial volume models as well as information fusion from MR and CT images . The methods are also compared with our Bayesian segmentation method proposed in a previous study . The performance is found to be similar to our previous Bayesian approach but the presented methods here eliminates ad hoc parameter tuning needed by the previous approach which is system and data acquisition setting dependent . Conclusions The Bayesian approach presented provides resolution enhanced segmentation of very thin structures of the human head . Meanwhile free parameters of the algorithm can be adjusted for different imaging systems and data acquisition settings in a more systematic way as compared with our previous study . \\n'],\n",
              " [' The assessment of microcirculation spatial heterogeneity on the hand skin is the main objective of this work . Near infrared spectroscopy based 2D imaging is a non invasive technique for the assessment of tissue oxygenation . The haemoglobin oxygen saturation images were acquired by a dedicated camera during baseline ischaemia and reperfusion . Acquired images underwent a preliminary restoration process aimed at removing degradations occurring during signal capturing . Then wavelet transform based multiscale analysis was applied to identify edges by detecting local maxima and minima across successive scales . Segmentation of test areas during different conditions was obtained by thresholding based region growing approach . The method identifies the differences in microcirculatory control of blood flow in different regions of the hand skin . The obtained results demonstrate the potential use of NIRS images for the clinical evaluation of skin disease and microcirculatory dysfunction . \\n'],\n",
              " [' Background and objective We live our lives by the calendar and the clock but time is also an abstraction even an illusion . The sense of time can be both domain specific and complex and is often left implicit requiring significant domain knowledge to accurately recognize and harness . In the clinical domain the momentum gained from recent advances in infrastructure and governance practices has enabled the collection of tremendous amount of data at each moment in time . Electronic health records have paved the way to making these data available for practitioners and researchers . However temporal data representation normalization extraction and reasoning are very important in order to mine such massive data and therefore for constructing the clinical timeline . The objective of this work is to provide an overview of the problem of constructing a timeline at the clinical point of care and to summarize the state of the art in processing temporal information of clinical narratives . Methods This review surveys the methods used in three important area modeling and representing of time medical NLP methods for extracting time and methods of time reasoning and processing . The review emphasis on the current existing gap between present methods and the semantic web technologies and catch up with the possible combinations . Results The main findings of this review are revealing the importance of time processing not only in constructing timelines and clinical decision support systems but also as a vital component of EHR data models and operations . Conclusions Extracting temporal information in clinical narratives is a challenging task . The inclusion of ontologies and semantic web will lead to better assessment of the annotation task and together with medical NLP techniques will help resolving granularity and co reference resolution problems . \\n'],\n",
              " [' In crisis situations a seamless ubiquitous communication is necessary to provide emergency medical service to save people s lives . An excellent prehospital emergency medicine provides immediate medical care to increase the survival rate of patients . On their way to the hospital ambulance personnel must transmit real time and uninterrupted patient information to the hospital to apprise the physician of the situation and provide options to the ambulance personnel . In emergency and crisis situations many communication channels can be unserviceable because of damage to equipment or loss of power . Thus data transmission over wireless communication to achieve uninterrupted network services is a major obstacle . This study proposes a mobile middleware for cognitive radio for improving the wireless communication link . CRs can sense their operating environment and optimize the spectrum usage so that the mobile middleware can integrate the existing wireless communication systems with a seamless communication service in heterogeneous network environments . Eventually the proposed seamless mobile communication middleware was ported into an embedded system which is compatible with the actual network environment without the need for changing the original system architecture . \\n'],\n",
              " [' Cataract is defined as a lenticular opacity presenting usually with poor visual acuity . It is one of the most common causes of visual impairment worldwide . Early diagnosis demands the expertise of trained healthcare professionals which may present a barrier to early intervention due to underlying costs . To date studies reported in the literature utilize a single learning model for retinal image classification in grading cataract severity . We present an ensemble learning based approach as a means to improving diagnostic accuracy . Three independent feature sets i.e . wavelet sketch and texture based features are extracted from each fundus image . For each feature set two base learning models i.e . Support Vector Machine and Back Propagation Neural Network are built . Then the ensemble methods majority voting and stacking are investigated to combine the multiple base learning models for final fundus image classification . Empirical experiments are conducted for cataract detection and cataract grading tasks . The best performance of the ensemble classifier is 93.2 and 84.5 in terms of the correct classification rates for cataract detection and grading tasks respectively . The results demonstrate that the ensemble classifier outperforms the single learning model significantly which also illustrates the effectiveness of the proposed approach . \\n'],\n",
              " [' Background and objective The automatic classification of breast imaging lesions is currently an unsolved problem . This paper describes an innovative representation learning framework for breast cancer diagnosis in mammography that integrates deep learning techniques to automatically learn discriminative features avoiding the design of specific hand crafted image based feature detectors . Methods A new biopsy proven benchmarking dataset was built from 344 breast cancer patients cases containing a total of 736 film mammography views representative of manually segmented lesions associated with masses 426 benign lesions and 310 malignant lesions . The developed method comprises two main stages preprocessing to enhance image details and supervised training for learning both the features and the breast imaging lesions classifier . In contrast to previous works we adopt a hybrid approach where convolutional neural networks are used to learn the representation in a supervised way instead of designing particular descriptors to explain the content of mammography images . Results Experimental results using the developed benchmarking breast cancer dataset demonstrated that our method exhibits significant improved performance when compared to state of the art image descriptors such as histogram of oriented gradients and histogram of the gradient divergence increasing the performance from 0.787 to 0.822 in terms of the area under the ROC curve . Interestingly this model also outperforms a set of hand crafted features that take advantage of additional information from segmentation by the radiologist . Finally the combination of both representations learned and hand crafted resulted in the best descriptor for mass lesion classification obtaining 0.826 in the AUC score . Conclusions A novel deep learning based framework to automatically address classification of breast mass lesions in mammography was developed . \\n'],\n",
              " [' Nowadays the diagnosis and treatment of pelvic sarcoma pose a major surgical challenge for reconstruction in orthopedics . With the development of manufacturing technology the metal 3D printed customized implants have brought revolution for the limb salvage resection and reconstruction surgery . However the tumor resection is not without risk and the precise implant placement is very difficult due to the anatomic intricacies of the pelvis . In this study a surgical navigation system including the implant calibration algorithm has been developed so that the surgical instruments and the 3D printed customized implant can be tracked and rendered on the computer screen in real time minimizing the risks and improving the precision of the surgery . Both the phantom experiment and the pilot clinical case study presented the feasibility of our computer aided surgical navigation system . According to the accuracy evaluation experiment the precision of customized implant installation can be improved three to five times compared with the non navigated implant installation after the guided osteotomy which means it is sufficient to meet the clinical requirements of the pelvic reconstruction . However more clinical trials will be conducted in the future work for the validation of the reliability and efficiency of our navigation system . \\n'],\n",
              " [' Iterative proportional fitting is a widely used method for spatial microsimulation . The technique results in non integer weights for individual rows of data . This is problematic for certain applications and has led many researchers to favour combinatorial optimisation approaches such as simulated annealing . An alternative to this is integerisation of IPF weights the translation of the continuous weight variable into a discrete number of unique or cloned individuals . We describe four existing methods of integerisation and present a new one . Our method truncate replicate sample recognises that IPF weights consist of both replication weights and conventional weights the effects of which need to be separated . The procedure consists of three steps separate replication and conventional weights by truncation replication of individuals with positive integer weights and probabilistic sampling . The results which are reproducible using supplementary code and data published alongside this paper show that TRS is fast and more accurate than alternative approaches to integerisation . \\n'],\n",
              " [' Background and objectives Computer aided analysis of mammograms has been employed by radiologists as a vital tool to increase the precision in the diagnosis of breast cancer . The efficiency of such an analysis is dependent on the employed mammogram enhancement approach as its major role is to yield a visually improved image for radiologists . Methods Non linear Polynomial Filtering framework has been explored previously as a robust approach for contrast improvement of mammographic images . This paper presents the extension of NPF framework for sharpening and edge enhancement of mammogram lesions . Proposed NPF serves to provide enhancement of edges and sharpness of the lesion region in mammograms in a manner to minimize the dependencies on pre selected thresholds . In the present work Logarithmic Image Processing model has been employed for the purpose of improvement in visualization of mammograms based on Human Visual System characteristics . Results The proposed NPF filtering framework yields mammograms with significant improvement in contrast edges as well as sharpness of the lesion region . The performance of the proposed approach has been validated using state of art objective evaluation measures like Contrast Improvement Index Peak Signal to Noise Ratio Average Signal to Noise Ratio and Combined Enhancement Measure as well as subjective evaluation by radiologists opinions . Conclusions The proposed NPF provides a robust solution to perform noise controlled contrast as well as edge enhancement using a single filtering model . This leads to a better visualization of the fine lesion details predictive of their severity . The applicability of single filtering methodology for carrying out denoising contrast and edge enhancement improves the worth of the overall framework . \\n'],\n",
              " [' Background Diabetes mellitus is associated with an increased risk of liver cancer and these two diseases are among the most common and important causes of morbidity and mortality in Taiwan . Purpose To use data mining techniques to develop a model for predicting the development of liver cancer within 6 years of diagnosis with type II diabetes . Methods Data were obtained from the National Health Insurance Research Database of Taiwan which covers approximately 22 million people . In this study we selected patients who were newly diagnosed with type II diabetes during the 2000 2003 periods with no prior cancer diagnosis . We then used encrypted personal ID to perform data linkage with the cancer registry database to identify whether these patients were diagnosed with liver cancer . Finally we identified 2060 cases and assigned them to a case group and a control group . The risk factors were identified from the literature review and physicians suggestion then chi square test was conducted on each independent variable for a comparison between patients with liver cancer and those without those found to be significant were selected as the factors . We subsequently performed data training and testing to construct artificial neural network and logistic regression prediction models . The dataset was randomly divided into 2 groups a training group and a test group . The training group consisted of 1442 cases and the prediction model was developed on the basis of the training group . The remaining 30 were assigned to the test group for model validation . Results The following 10 variables were used to develop the ANN and LR models sex age alcoholic cirrhosis nonalcoholic cirrhosis alcoholic hepatitis viral hepatitis other types of chronic hepatitis alcoholic fatty liver disease other types of fatty liver disease and hyperlipidemia . The performance of the ANN was superior to that of LR according to the sensitivity specificity and the area under the receiver operating characteristic curve . After developing the optimal prediction model we base on this model to construct a web based application system for liver cancer prediction which can provide support to physicians during consults with diabetes patients . Conclusion In the original dataset 33 of diabetes patients were diagnosed with liver cancer . After using 70 of the original data to training the model and other 30 for testing the sensitivity and specificity of our model were 0.757 and 0.755 respectively this means that 75.7 of diabetes patients can be predicted correctly to receive a future liver cancer diagnosis and 75.5 can be predicted correctly to not be diagnosed with liver cancer . These results reveal that this model can be used as effective predictors of liver cancer for diabetes patients after discussion with physicians they also agreed that model can assist physicians to advise potential liver cancer patients and also helpful to decrease the future cost incurred upon cancer treatment . \\n'],\n",
              " [' The mathematical modeling of physical and biologic systems represents an interesting alternative to study the behavior of these phenomena . In this context the development of mathematical models to simulate the dynamic behavior of tumors is configured as an important theme in the current days . Among the advantages resulting from using these models is their application to optimization and inverse problem approaches . Traditionally the formulated Optimal Control Problem has the objective of minimizing the size of tumor cells by the end of the treatment . In this case an important aspect is not considered namely the optimal concentrations of drugs may affect the patients health significantly . In this sense the present work has the objective of obtaining an optimal protocol for drug administration to patients with cancer through the minimization of both the cancerous cells concentration and the prescribed drug concentration . The resolution of this multi objective problem is obtained through the Multi objective Optimization Differential Evolution algorithm . The Pareto s Curve obtained supplies a set of optimal protocols from which an optimal strategy for drug administration can be chosen according to a given criterion . \\n'],\n",
              " [' To facilitate the performance comparison of new methods for sleep patterns analysis datasets with quality content publicly available are very important and useful . We introduce an open access comprehensive sleep dataset called ISRUC Sleep . The data were obtained from human adults including healthy subjects subjects with sleep disorders and subjects under the effect of sleep medication . Each recording was randomly selected between PSG recordings that were acquired by the Sleep Medicine Centre of the Hospital of Coimbra University . The dataset comprises three groups of data data concerning 100 subjects with one recording session per subject data gathered from 8 subjects two recording sessions were performed per subject and data collected from one recording session related to 10 healthy subjects . The polysomnography recordings associated with each subject were visually scored by two human experts . Comparing the existing sleep related public datasets ISRUC Sleep provides data of a reasonable number of subjects with different characteristics such as data useful for studies involving changes in the PSG signals over time and data of healthy subjects useful for studies involving comparison of healthy subjects with the patients suffering from sleep disorders . This dataset was created aiming to complement existing datasets by providing easy to apply data collection with some characteristics not covered yet . ISRUC Sleep can be useful for analysis of new contributions in biomedical signal processing in development of ASSC methods and on sleep physiology studies . To evaluate and compare new contributions which use this dataset as a benchmark results of applying a subject independent automatic sleep stage classification method on ISRUC Sleep dataset are presented . \\n'],\n",
              " [' Dictionary learning plays a crucial role in sparse representation based image classification . In this paper we propose a novel approach to learn a discriminative dictionary with low rank regularization on the dictionary . Specifically we apply Fisher discriminant function to the coding coefficients to make the dictionary more discerning that is a small ratio of the within class scatter to between class scatter . In practice noisy information in the training samples will undermine the discriminative ability of the dictionary . Inspired by the recent advances in low rank matrix recovery theory we apply low rank regularization on the dictionary to tackle this problem . The iterative projection method and inexact augmented Lagrange multiplier algorithm are adopted to solve our objective function . The proposed discriminative dictionary learning with low rank regularization approach is evaluated on four face and digit image datasets in comparison with existing representative dictionary learning and classification algorithms . The experimental results demonstrate the superiority of our approach . \\n'],\n",
              " [' Background and objective Cosmetic outcome of breast cancer conservative treatment remains without a standard evaluation method . Subjective methods in spite of their low reproducibility continue to be the most frequently used . Objective methods although more reproducible seem unable to translate all the subtleties involved in cosmetic outcome . The breast cancer conservative treatment cosmetic results software was developed in 2007 to try to overcome these pitfalls . The software is a semi automatic objective tool that evaluates asymmetry color differences and scar visibility using patient s digital pictures . The purpose of this work is to review the use of the BCCT.core software since its availability in 2007 and to put forward future developments . Methods All the online requests for BCCT.core use were registered from June 2007 to December 2014 . For each request the department city and country as well as user intention were questioned . A literature search was performed in Medline Google Scholar and ISI Web of Knowledge for all publications using and citing BCCT.core . Results During this period 102 centers have requested the software essentially for clinical use . The BCCT.core software was used in 19 full published papers and in 29 conference abstracts . Conclusions The BCCT.core is a user friendly semi automatic method for the objective evaluation of BCCT . The number of online requests and publications have been steadily increasing turning this computer program into the most frequently used tool for the objective cosmetic evaluation of BCCT . \\n'],\n",
              " [' This paper deals with model based pose estimation . We propose a direct approach that takes into account the image as a whole . For this we consider a similarity measure the mutual information . Mutual information is a measure of the quantity of information shared by two signals . Exploiting this measure allows our method to deal with different image modalities . Furthermore it handles occlusions and illumination changes . Results with synthetic and real image sequences with static or mobile camera demonstrate the robustness of the method and its ability to produce stable and precise pose estimations . \\n'],\n",
              " [' To extend the use of wearable sensor networks for stroke patients training and assessment in non clinical settings this paper proposes a novel remote quantitative Fugl Meyer assessment framework in which two accelerometer and seven flex sensors were used to monitoring the movement function of upper limb wrist and fingers . The extreme learning machine based ensemble regression model was established to map the sensor data to clinical FMA scores while the RRelief algorithm was applied to find the optimal features subset . Considering the FMA scale is time consuming and complicated seven training exercises were designed to replace the upper limb related 33 items in FMA scale . 24 stroke inpatients participated in the experiments in clinical settings and 5 of them were involved in the experiments in home settings after they left the hospital . Both the experimental results in clinical and home settings showed that the proposed quantitative FMA model can precisely predict the FMA scores based on wearable sensor data the coefficient of determination can reach as high as 0.917 . It also indicated that the proposed framework can provide a potential approach to the remote quantitative rehabilitation training and evaluation . \\n'],\n",
              " [' This paper addresses the general problem of robust parametric model estimation from data that has both an unknown and possibly majority fraction of outliers as well as an unknown scale of measurement noise. We focus on computer vision applications from image correspondences such as camera resectioning estimation of the fundamental matrix or relative pose for 3D reconstruction and estimation of 2D homographies for image registration and motion segmentation although there are many other applications. In practice these methods typically rely on a predefined inlier thresholds because automatic scale detection is usually too unreliable or too slow. We propose a new method for robust estimation with automatic scale detection that is faster more precise and more robust than previous alternatives and show that it can be practically applied to these problems. \\n'],\n",
              " [' Psoriasis is an autoimmune skin disease with red and scaly plaques on skin and affecting about 125 million people worldwide . Currently dermatologist use visual and haptic methods for diagnosis the disease severity . This does not help them in stratification and risk assessment of the lesion stage and grade . Further current methods add complexity during monitoring and follow up phase . The current diagnostic tools lead to subjectivity in decision making and are unreliable and laborious . This paper presents a first comparative performance study of its kind using principal component analysis based CADx system for psoriasis risk stratification and image classification utilizing 11 higher order spectra features 60 texture features and 86 color feature sets and their seven combinations . Aggregate 540 image samples from 30 psoriasis patients of Indian ethnic origin are used in our database . Machine learning using PCA is used for dominant feature selection which is then fed to support vector machine classifier to obtain optimized performance . Three different protocols are implemented using three kinds of feature sets . Reliability index of the CADx is computed . Among all feature combinations the CADx system shows optimal performance of 100 accuracy 100 sensitivity and specificity when all three sets of feature are combined . Further our experimental result with increasing data size shows that all feature combinations yield high reliability index throughout the PCA cutoffs except color feature set and combination of color and texture feature sets . HOS features are powerful in psoriasis disease classification and stratification . Even though independently all three set of features HOS texture and color perform competitively but when combined the machine learning system performs the best . The system is fully automated reliable and accurate . \\n'],\n",
              " [' With the exponential growth in the world population and the constant increase in human mobility the possible impact of outbreaks of epidemics on cities is increasing especially in high density urban areas such as public transportation and transfer points . The volume and proximity of people in these areas can lead to an observed dramatic increase in the transmission of airborne viruses and related pathogens . Due to the critical role these areas play in transmission it is vital that we have a comprehensive understanding of the transmission highways in these areas to predict or prevent the spreading of infectious diseases in general . The principled approach of this paper is to combine and utilize as much information as possible from relevant sources and to integrate these data in a simulated environment that allows for scenario testing and decision support . In this paper we describe a novel approach to study the spread of airborne diseases in cities by combining traffic information with geo spatial data infection dynamics and spreading characteristics . The system is currently being used in an attempt to understand the outbreak of influenza in densely populated cities in China . \\n'],\n",
              " [' Background and objective In oral and maxillofacial surgery conventional radiographic cephalometry is one of the standard auxiliary tools for diagnosis and surgical planning . While contemporary computer assisted cephalometric systems and methodologies support cephalometric analysis they tend neither to be practical nor intuitive for practitioners . This is particularly the case for 3D methods since the associated landmarking process is difficult and time consuming . In addition to this there are no 3D cephalometry norms or standards defined therefore new landmark selection methods are required which will help facilitate their establishment . This paper presents and evaluates a novel haptic enabled landmarking approach to overcome some of the difficulties and disadvantages of the current landmarking processes used in 2D and 3D cephalometry . Method In order to evaluate this new system s feasibility and performance 21 dental surgeons performed a range of case studies using a haptic enabled 2D 2 D and 3D digital cephalometric analyses . Results The results compared the 2D 2 D and 3D cephalometric values errors and standard deviations for each case study and associated group of participants and revealed that 3D cephalometry significantly reduced landmarking errors and variability compared to 2D methods . Conclusions Through enhancing the process by providing a sense of touch the haptic enabled 3D digital cephalometric approach was found to be feasible and more intuitive than its counterparts as well effective at reducing errors the variability of the measurements taken and associated task completion times . \\n'],\n",
              " [' In this paper how to calibrate a fixed multi camera system and simultaneously achieve a Euclidean reconstruction from a set of segments is addressed . It is well known that only a projective reconstruction could be achieved without any prior information . Here the known segment lengths are exploited to upgrade the projective reconstruction to a Euclidean reconstruction and simultaneously calibrate the intrinsic and extrinsic camera parameters . At first a DLT like algorithm for the Euclidean upgrading from segment lengths is derived in a very simple way . Although the intermediate results in the DLT like algorithm are essentially equivalent to the quadric of segments the DLT like algorithm is of higher accuracy than the existing linear algorithms derived from the QoS because of a more accurate way to extract the plane at infinity from the intermediate results . Then to further improve the accuracy of Euclidean upgrading two weighted DLT like algorithms are presented by weighting the linear constraint equations in the original DLT like algorithm . Finally using the results of these linear algorithms as the initial values a new weighted nonlinear algorithm for Euclidean upgrading is explored to recover the Euclidean structure more accurately . Extensive experimental results on both the synthetic data and the real image data demonstrate the effectiveness of our proposed algorithms in Euclidean upgrading and multi camera calibration . \\n'],\n",
              " ['Recent research emphasizes more on analyzing multiple features to improve face recognition FR performance. One popular scheme is to extend the sparse representation based classification framework with various sparse constraints. Although these methods jointly study multiple features through the constraints they just process each feature individually such that they overlook the possible high level relationship among different features. It is reasonable to assume that the low level features of facial images such as edge information and smoothed low frequency image can be fused into a more compact and more discriminative representation based on the latent high level relationship. FR on the fused features is anticipated to produce better performance than that on the original features since they provide more favorable properties. Focusing on this we propose two different strategies which start from fusing multiple features and then exploit the dictionary learning DL framework for better FR performance. The first strategy is a simple and efficient two step model which learns a fusion matrix from training face images to fuse multiple features and then learns class specific dictionaries based on the fused features. The second one is a more effective model requiring more computational time that learns the fusion matrix and the class specific dictionaries simultaneously within an iterative optimization procedure. Besides the second model considers to separate the shared common components from class specified dictionaries to enhance the discrimination power of the dictionaries. The proposed strategies which integrate multi feature fusion process and dictionary learning framework for FR realize the following goals 1 exploiting multiple features of face images for better FR performances 2 learning a fusion matrix to merge the features into a more compact and more discriminative representation 3 learning class specific dictionaries with consideration of the common patterns for better classification performance. We perform a series of experiments on public available databases to evaluate our methods and the experimental results demonstrate the effectiveness of the proposed models. \\n'],\n",
              " [' In this article a novel technique for fixation prediction and saccade generation will be introduced . The proposed model simulates saccadic eye movement to incorporate the underlying eye movement mechanism into saliency estimation . To this end a simple salience measure is introduced . Afterwards we derive a system model for saccade generation and apply it in a stochastic filtering framework . The proposed model will dynamically make a saccade toward the next predicted fixation and produces saliency maps . Evaluation of the proposed model is carried out in terms of saccade generation performance and saliency estimation . Saccade generation evaluation reveals that the proposed model outperforms inhibition of return . Also experiments signify integration of eye movement mechanism into saliency estimation boosts the results . Finally comparison with several saliency models shows the proposed model performs aptly . \\n'],\n",
              " [' Intensity inhomogeneity often appears in medical images such as X ray tomography and magnetic resonance images due to technical limitations or artifacts introduced by the object being imaged . It is difficult to segment such images by traditional level set based segmentation models . In this paper we propose a new level set method integrating local and global intensity information adaptively to segment inhomogeneous images . The local image information is associated with the intensity difference between the average of local intensity distribution and the original image which can significantly increase the contrast between foreground and background . Thus the images with intensity inhomogeneity can be efficiently segmented . What is more to avoid the re initialization of the level set function and shorten the computational time a simple and fast level set evolution formulation is used in the numerical implementation . Experimental results on synthetic images as well as real medical images are shown in the paper to demonstrate the efficiency and robustness of the proposed method . \\n'],\n",
              " [' In this paper we present the dfcomb R package for the implementation of a single prospective clinical trial or simulation studies of phase I combination trials in oncology . The aim is to present the features of the package and to illustrate how to use it in practice though different examples . The use of combination clinical trials is growing but the implementation of existing model based methods is complex so this package should promote the use of innovative adaptive designs for early phases combination trials . \\n'],\n",
              " [' Interventional cardiologists have a deep interest in risk stratification prior to stenting and percutaneous coronary intervention procedures . Intravascular ultrasound is most commonly adapted for screening but current tools lack the ability for risk stratification based on grayscale plaque morphology . Our hypothesis is based on the genetic makeup of the atherosclerosis disease that there is evidence of a link between coronary atherosclerosis disease and carotid plaque built up . This novel idea is explored in this study for coronary risk assessment and its classification of patients between high risk and low risk . This paper presents a strategy for coronary risk assessment by combining the IVUS grayscale plaque morphology and carotid B mode ultrasound carotid intima media thickness a marker of subclinical atherosclerosis . Support vector machine learning paradigm is adapted for risk stratification where both the learning and testing phases use tissue characteristics derived from six feature combinational spaces which are then used by the SVM classifier with five different kernels sets . These six feature combinational spaces are designed using 56 novel feature sets . K fold cross validation protocol with 10 trials per fold is used for optimization of best SVM kernel and best feature combination set . IRB approved coronary IVUS and carotid B mode ultrasound were jointly collected on 15 patients via 40MHz catheter utilizing iMap with 2865 frames per patient and linear probe B mode carotid ultrasound . Using the above protocol the system shows the classification accuracy of 94.95 and AUC of 0.95 using optimized feature combination . This is the first system of its kind for risk stratification as a screening tool to prevent excessive cost burden and better patients cardiovascular disease management while validating our two hypotheses . \\n'],\n",
              " [' One of the key elements of e learning platforms is the content provided to the students . Content creation is a time demanding task that requires teachers to prepare material taking into account that it will be accessed on line . Moreover the teacher is restricted by the functionalities provided by the e learning platforms . In contexts such as radiology where images have a key role the required functionalities are still more specific and difficult to be provided by these platforms . Our purpose is to create a framework to make teacher s tasks easier specially when he has to deal with contents where images have a main role . In this paper we present RadEd a new web based teaching framework that integrates a smart editor to create case based exercises that support image interaction such as changing the window width and the grey scale used to render the image taking measurements on the image attaching labels to images and selecting parts of the images amongst others . It also provides functionalities to prepare courses with different topics exercises and theory material and also functionalities to control students work . Different experts have used RadEd and all of them have considered it a very useful and valuable tool to prepare courses where radiological images are the main component . RadEd provides teachers functionalities to prepare more realistic cases and students the ability to make a more specific diagnosis . \\n'],\n",
              " [' Background and objectives Because skin cancer affects millions of people worldwide computational methods for the segmentation of pigmented skin lesions in images have been developed in order to assist dermatologists in their diagnosis . This paper aims to present a review of the current methods and outline a comparative analysis with regards to several of the fundamental steps of image processing such as image acquisition pre processing and segmentation . Methods Techniques that have been proposed to achieve these tasks were identified and reviewed . As to the image segmentation task the techniques were classified according to their principle . Results The techniques employed in each step are explained and their strengths and weaknesses are identified . In addition several of the reviewed techniques are applied to macroscopic and dermoscopy images in order to exemplify their results . Conclusions The image segmentation of skin lesions has been addressed successfully in many studies however there is a demand for new methodologies in order to improve the efficiency . \\n'],\n",
              " [' Background and objective This study proposes an infrastructure with a reporting workflow optimization algorithm in order to interconnect facilities reporting units and radiologists on a single access interface to increase the efficiency of the reporting process by decreasing the medical report turnaround time and to increase the quality of medical reports by determining the optimum match between the inspection and radiologist in terms of subspecialty workload and response time . Methods Workflow centric network architecture with an enhanced caching querying and retrieving mechanism is implemented by seamlessly integrating Grid Agent and Grid Manager to conventional digital radiology systems . The inspection and radiologist attributes are modelled using a hierarchical ontology structure . Attribute preferences rated by radiologists and technical experts are formed into reciprocal matrixes and weights for entities are calculated utilizing Analytic Hierarchy Process . The assignment alternatives are processed by relation based semantic matching and Integer Linear Programming . Results The results are evaluated based on both real case applications and simulated process data in terms of subspecialty response time and workload success rates . Results obtained using simulated data are compared with the outcomes obtained by applying Round Robin Shortest Queue and Random distribution policies . The proposed algorithm is also applied to a real case teleradiology application process data where medical reporting workflow was performed based on manual assignments by the chief radiologist for 6225 inspections . Conclusions RBSM gives the highest subspecialty success rate and integrating ILP with RBSM ratings as RWOA provides a better response time and workload distribution success rate . RWOA based image delivery also prevents bandwidth storage or hardware related stuck and latencies . When compared with a real case teleradiology application where inspection assignments were performed manually the proposed solution was found to increase the experience success rate by 13.25 workload success rate by 63.76 and response time success rate by 120 . The total response time in the real case application data was improved by 22.39 . \\n'],\n",
              " [' The human visual system is quite adept at swiftly detecting objects of interest in complex visual scene . Simulating human visual system to detect visually salient regions of an image has been one of the active topics in computer vision . Inspired by random sampling based bagging ensemble learning method an ensemble dictionary learning framework for saliency detection is proposed in this paper . Instead of learning a universal dictionary requiring a large number of training samples to be collected from natural images multiple over complete dictionaries are independently learned with a small portion of randomly selected samples from the input image itself resulting in more flexible multiple sparse representations for each of the image patches . To boost the distinctness of salient patch from background region we present a reconstruction residual based method for dictionary atom reduction . Meanwhile with the obtained multiple probabilistic saliency responses for each of the patches the combination of them is finally carried out from the probabilistic perspective to achieve better predictive performance on saliency region . Experimental results on several open test datasets and some natural images demonstrate that the proposed EDL for saliency detection is much more competitive compared with some existing state of the art algorithms . \\n'],\n",
              " [' 3D shape descriptor has been used widely in the field of 3D object retrieval . However the performance of object retrieval greatly depends on the shape descriptor used . The aims of this study is to review and compare the common 3D shape descriptors proposed in 3D object retrieval literature for object recognition and classification based on Kinect like depth image obtained from RGB D object dataset . In this paper we introduce inter class and intra class evaluation in order to study the feasibility of such descriptors in object recognition . Based on these evaluations local spin image outperforms the rest in discriminating different classes when several depth images from an instance per class are used in inter class evaluation . This might be due to the slightly consistent local shape property of such images and due to the proposed local similarity measurement that manages to extract the local based descriptor . However shape distribution performs excellent for intra class evaluation may be due to the global shape from different instances per class is slightly unchanged . These results indicate a remarkable feasibility analysis of the 3D shape descriptor in object recognition that can be potentially used for Kinect like sensor . \\n'],\n",
              " [' Despite the successes in the last two decades the state of the art face detectors still have problems in dealing with images in the wild due to large appearance variations . Instead of leaving appearance variations directly to statistical learning algorithms we propose a hierarchical part based structural model to explicitly capture them . The model enables part subtype option to handle local appearance variations such as closed and open month and part deformation to capture the global appearance variations such as pose and expression . In detection candidate window is fitted to the structural model to infer the part location and part subtype and detection score is then computed based on the fitted configuration . In this way the influence of appearance variation is reduced . Besides the face model we exploit the co occurrence between face and body which helps to handle large variations such as heavy occlusions to further boost the face detection performance . We present a phrase based representation for body detection and propose a structural context model to jointly encode the outputs of face detector and body detector . Benefit from the rich structural face and body information as well as the discriminative structural learning algorithm our method achieves state of the art performance on FDDB AFW and a self annotated dataset under wide comparisons with commercial and academic methods . \\n'],\n",
              " [' Background and objectives The lack of benchmark data in computational ophthalmology contributes to the challenging task of applying disease assessment and evaluate performance of machine learning based methods on retinal spectral domain optical coherence tomography scans . Presented here is a general framework for constructing a benchmark dataset for retinal image processing tasks such as cyst vessel and subretinal fluid segmentation and as a result a benchmark dataset for cyst segmentation has been developed . Method First a dataset captured by different SD OCT vendors with different numbers of scans and pathology qualities are selected . Then a robust and intelligent method is used to evaluate performance of readers partitioning the dataset into subsets . Subsets are then assigned to complementary readers for annotation with respect to a novel confidence based annotation protocol . Finally reader annotations are combined based on their performance to generate final annotations . Result The generated benchmark dataset for cyst segmentation comprises 26 SD OCT scans with differing cyst qualities collected from 4 different SD OCT vendors to cover a wide variety of data . The dataset is partitioned into three subsets which are annotated by complementary readers based on a confidence based annotation protocol . Experimental results show annotations of complementary readers are combined efficiently with respect to their performance generating accurate annotations . Conclusion Our results facilitate the process of generating benchmark datasets . Moreover the generated benchmark data set for cyst segmentation can be used reliably to train and test machine learning based methods . \\n'],\n",
              " ['This paper focuses on activity recognition when multiple views are available. In the literature this is often performed using two different approaches. In the first one the systems build a 3D reconstruction and match that. However there are practical disadvantages to this methodology since a sufficient number of overlapping views is needed to reconstruct and one must calibrate the cameras. A simpler alternative is to match the frames individually. This offers significant advantages in the system architecture e.g. it is easy to incorporate new features and camera dropouts can be tolerated . In this paper the second approach is employed and a novel fusion method is proposed. Our fusion method collects the activity labels over frames and cameras and then fuses activity judgments as the sequence label. It is shown that there is no performance penalty when a straightforward weighted voting scheme is used. In particular when there are enough overlapping views to generate a volumetric reconstruction our recognition performance is comparable with that produced by volumetric reconstructions. However if the overlapping views are not adequate the performance degrades fairly gracefully even in cases where test and training views do not overlap. \\n'],\n",
              " [' Objective The analysis of treatment effects in clinical trials usually focus on efficacy and safety in separate descriptive statistical analyses . The Q TWiST method has been proposed by Gelber in the 90s to enable a statistical comparison between two groups with a graphical representation by incorporating benefit and risk into a single analysis . Although the method has been programmed in SAS it is rarely used . The availability of the method in the freely software environment system like R would greatly enhanced the accessibility by researchers . The objective of this paper is to present a program for Q TWiST analyses within R software environment . Methods The qtwist function was developed in order to estimate and compare Q TWiST for two groups . Two individual patient data files are required used for input one for visits and one for follow up . Q TWiST is obtained as a sum of time spent in three health states period in toxicity period without relapse and toxicity and period in relapse weighted by associated utility scores restricted to median overall survival for example . The bootstrap method is used for testing statistical significance . Threshold analysis and gain functions allow a group comparison for different utility values . Results Input data is checked for consistency . Descriptive statistics and mean durations for each health state are provided allowing statistical comparisons . Graphical results are presented in a PDF file . The use of the function is illustrated with data from a simulated data set and a randomized clinical trial . Conclusions qtwist is an easy to use R function allowing a quality adjusted survival analysis with the Q TWiST method . \\n'],\n",
              " [' A method to obtain accurate hand gesture classification and fingertip localization from depth images is proposed . The Oriented Radial Distribution feature is utilized exploiting its ability to globally describe hand poses but also to locally detect fingertip positions . Hence hand gesture and fingertip locations are characterized with a single feature calculation . We propose to divide the difficult problem of locating fingertips into two more tractable problems by taking advantage of hand gesture as an auxiliary variable . Along with the method we present the ColorTip dataset a dataset for hand gesture recognition and fingertip classification using depth data . ColorTip contains sequences where actors wear a glove with colored fingertips allowing automatic annotation . The proposed method is evaluated against recent works in several datasets achieving promising results in both gesture classification and fingertip localization . \\n'],\n",
              " [' Background and Objective Iterative reconstruction from Compton scattered data is known to be computationally more challenging than that from conventional line projection based emission data in that the gamma rays that undergo Compton scattering are modeled as conic projections rather than line projections . In conventional tomographic reconstruction to parallelize the projection and backprojection operations using the graphics processing unit approximated methods that use an unmatched pair of ray tracing forward projector and voxel driven backprojector have been widely used . In this work we propose a new GPU accelerated method for Compton camera reconstruction which is more accurate by using exactly matched pair of projector and backprojector . Methods To calculate conic forward projection we first sample the cone surface into conic rays and accumulate the intersecting chord lengths of the conic rays passing through voxels using a fast ray tracing method . For conic backprojection to obtain the true adjoint of the conic forward projection while retaining the computational efficiency of the GPU we use a voxel driven RTM which is essentially the same as the standard RTM used for the conic forward projector . Results Our simulation results show that while the new method is about 3 times slower than the approximated method it is still about 16 times faster than the CPU based method without any loss of accuracy . Conclusions The net conclusion is that our proposed method is guaranteed to retain the reconstruction accuracy regardless of the number of iterations by providing a perfectly matched projector backprojector pair which makes iterative reconstruction methods for Compton imaging faster and more accurate . \\n'],\n",
              " [' In this paper initially the impact of mask spoofing on face recognition is analyzed . For this purpose one baseline technique is selected for both 2D and 3D face recognition . Next novel countermeasures which are based on the analysis of different shape texture and reflectance characteristics of real faces and mask faces are proposed to detect mask spoofing . In this paper countermeasures are developed using both 2D data and 3D data available in the mask database . The results show that each of the proposed countermeasures is successful in detecting mask spoofing and the fusion of these countermeasures further improves the results compared to using a single countermeasure . Since there is no publicly available mask database studies on mask spoofing are limited . This paper provides significant results by proposing novel countermeasures to protect face recognition systems against mask spoofing . \\n'],\n",
              " [' Finding regions of interest is a fundamentally important problem in the area of computer vision and image processing . Previous studies addressing this issue have mainly focused on investigating chromatic cues to characterize visually salient image regions while less attention has been devoted to monochromatic cues . The purpose of this paper is the study of monochromatic cues which have the potential to complement chromatic cues for the detection of ROIs in an image . This paper first presents a taxonomy of existing ROI detection approaches using monochromatic cues ranging from well known algorithms to the most recently published techniques . We then propose a novel monochromatic cue for ROI detection . Finally a comparative evaluation has been conducted on large scale challenging test sets of real world natural scenes . Experimental results demonstrate that the use of our proposed monochromatic cue yields a more accurate identification of ROIs . This paper serves as a benchmark for future research on this particular topic and a steppingstone for developers and practitioners interested in adopting monochromatic cues to ROI detection systems and methodologies . \\n'],\n",
              " ['Concurrently obtaining an accurate robust and fast global registration of multiple 3D scans is still an open issue for modern 3D modeling pipelines especially when high metric precision as well as easy usage of high end devices structured light or laser scanners are required. Various solutions have been proposed either heuristic iterative and or closed form solutions which present some compromise concerning the fulfillment of the above contrasting requirements. Our purpose here compared to existing reference solutions is to go a step further in this perspective by presenting a new technique able to provide improved alignment performance even on large datasets both in terms of number of views and or point density of range images. Relying on the Optimization on a Manifold OOM approach originally proposed by Krishnan et al. we propose a set of methodological and computational upgrades that produce an operative impact on both accuracy robustness and computational performance compared to the original solution. In particular always basing on an unconstrained error minimization over the manifold of rotations instead of relying on a static set of point correspondences our algorithm updates the optimization iterations with a dynamically modified set of correspondences in a computationally effective way leading to substantial improvements in terms of registration accuracy and convergence trend. Other proposed improvements are directed to a substantial reduction of the computational load without sacrificing the alignment performance. Stress tests with increasing view misalignment allowed us to appreciate the convergence robustness of the proposed solution. Eventually we demonstrate that for very large datasets a further computational speedup can be reached by the adoption of a hybrid local heuristic followed by global optimization registration approach. \\n'],\n",
              " [' An extensive in depth study of cardiovascular risk factors seems to be of crucial importance in the research of cardiovascular disease in order to prevent the chance of developing or dying from CVD . The main focus of data analysis is on the use of models able to discover and understand the relationships between different CVRF . In this paper a report on applying Bayesian network modeling to discover the relationships among thirteen relevant epidemiological features of heart age domain in order to analyze cardiovascular lost years cardiovascular risk score and metabolic syndrome is presented . Furthermore the induced BN was used to make inference taking into account three reasoning patterns causal reasoning evidential reasoning and intercausal reasoning . Application of BN tools has led to discovery of several direct and indirect relationships between different CVRF . The BN analysis showed several interesting results among them CVLY was highly influenced by smoking being the group of men the one with highest risk in CVLY MetS was highly influence by physical activity being again the group of men the one with highest risk in MetS and smoking did not show any influence . BNs produce an intuitive transparent graphical representation of the relationships between different CVRF . The ability of BNs to predict new scenarios when hypothetical information is introduced makes BN modeling an Artificial Intelligence tool of special interest in epidemiological studies . As CVD is multifactorial the use of BNs seems to be an adequate modeling tool . \\n'],\n",
              " [' Geographical masking is the conventional solution to protect the privacy of individuals involved in confidential spatial point datasets . The masking process displaces confidential locations to protect individual privacy while maintaining a fine level of spatial resolution . The adaptive form of this process aims to further minimize the displacement error by taking into account the underlying population density . We describe an alternative adaptive geomasking method referred to as Adaptive Areal Elimination . AAE creates areas of a minimum K anonymity and then original points are either randomly perturbed within the areas or aggregated to the median centers of the areas . In addition to the masked points K anonymized areas can be safely disclosed as well without increasing the risk of re identification . Using a burglary dataset from Vienna AAE is compared with an existing adaptive geographical mask the donut mask . The masking methods are evaluated for preserving a predefined K anonymity and the spatial characteristics of the original points . The spatial characteristics are assessed with four measures of spatial error displaced distance correlation coefficient of density surfaces hotspots divergence and clusters specificity . Masked points from point aggregation of AAE have the highest spatial error in all the measures but the displaced distance . In contrast masked points from the donut mask are displaced the least preserve the original spatial clusters better have the highest clusters specificity and correlation coefficient of density surfaces . However when the donut mask is adapted to achieve an actual K anonymity the random perturbation of AAE introduces less spatial error than the donut mask for all the measures of spatial error . \\n'],\n",
              " [' To determine initial velocities of enzyme catalyzed reactions without theoretical errors it is necessary to consider the use of the integrated Michaelis Menten equation . When the reaction product is an inhibitor this approach is particularly important . Nevertheless kinetic studies usually involved the evaluation of other inhibitors beyond the reaction product . The occurrence of these situations emphasizes the importance of extending the integrated Michaelis Menten equation assuming the simultaneous presence of more than one inhibitor because reaction product is always present . This methodology is illustrated with the reaction catalyzed by alkaline phosphatase inhibited by phosphate and urea . The approach is explained in a step by step manner using an Excel spreadsheet . Curve fitting by nonlinear regression was performed with the Solver add in . Discrimination of the kinetic models was carried out based on Akaike information criterion . This work presents a methodology that can be used to develop an automated process to discriminate in real time the inhibition type and kinetic constants as data are achieved by the spectrophotometer . \\n'],\n",
              " [' Background and Objective Viruses are infectious agents that replicate inside organisms and reveal a plethora of distinct characteristics . Viral infections spread in many ways but often have devastating consequences and represent a huge danger for public health . It is important to design statistical and computational techniques capable of handling the available data and highlighting the most important features . Methods This paper reviews the quantitative and qualitative behaviour of 22 infectious diseases caused by viruses . The information is compared and visualized by means of the multidimensional scaling technique . Results The results are robust to uncertainties in the data and revealed to be consistent with clinical practice . Conclusions The paper shows that the proposed methodology may represent a solid mathematical tool to tackle a larger number of virus and additional information about these infectious agents . \\n'],\n",
              " [' An electrocardiogram measures the electric activity of the heart and has been widely used for detecting heart diseases due to its simplicity and non invasive nature . By analyzing the electrical signal of each heartbeat i.e . the combination of action impulse waveforms produced by different specialized cardiac tissues found in the heart it is possible to detect some of its abnormalities . In the last decades several works were developed to produce automatic ECG based heartbeat classification methods . In this work we survey the current state of the art methods of ECG based automated abnormalities heartbeat classification by presenting the ECG signal preprocessing the heartbeat segmentation techniques the feature description methods and the learning algorithms used . In addition we describe some of the databases used for evaluation of methods indicated by a well known standard developed by the Association for the Advancement of Medical Instrumentation and described in ANSI AAMI EC57 1998 2008 . Finally we discuss limitations and drawbacks of the methods in the literature presenting concluding remarks and future challenges and also we propose an evaluation process workflow to guide authors in future works . \\n'],\n",
              " [' Background and objective Injury of knee joint cartilage may result in pathological vibrations between the articular surfaces during extension and flexion motions . The aim of this paper is to analyze and quantify vibroarthrographic signal irregularity associated with articular cartilage degeneration and injury in the patellofemoral joint . Methods The symbolic entropy approximate entropy fuzzy entropy and the mean standard deviation and root mean squared values of the envelope amplitude were utilized to quantify the signal fluctuations associated with articular cartilage pathology of the patellofemoral joint . The quadratic discriminant analysis generalized logistic regression analysis and support vector machine methods were used to perform signal pattern classifications . Results The experimental results showed that the patients with cartilage pathology possess larger SyEn and ApEn but smaller FuzzyEn over the statistical significance level of the Wilcoxon rank sum test than the healthy subjects . The mean standard deviation and RMS values computed from the amplitude difference between the upper and lower signal envelopes are also consistently and significantly larger for the group of CP patients than for the HS group . The SVM based on the entropy and envelope amplitude features can provide superior classification performance as compared with QDA and GLRA with an overall accuracy of 0.8356 sensitivity of 0.9444 specificity of 0.8 Matthews correlation coefficient of 0.6599 and an area of 0.9212 under the receiver operating characteristic curve . Conclusions The SyEn ApEn and FuzzyEn features can provide useful information about pathological VAG signal irregularity based on different entropy metrics . The statistical parameters of signal envelope amplitude can be used to characterize the temporal fluctuations related to the cartilage pathology . \\n'],\n",
              " [' We study the impact of settlement sizes on network connectivity in a spatial setting . First we develop a model of geometric urban networks that posits a positive relationship between connectivity and size . Empirical evidence is then presented validating the model prediction that local links exhibit super linear scaling with the exponent greater than 1 while long range connections scale linearly with the unit exponent . The scaling exponents thus suggest that the impact of population size on connectivity is stronger within cities than between cities . We next combine the geometric framework with a computational model of interacting agents to generate a realistic settlement distribution and urban networks from the bottom up . Calibrated simulation results demonstrate the consistency between hierarchical rank size distribution and scale free connectivity . Finally coupling the spatial network with a tipping diffusion model allows us to consolidate the evolution of network connectivity city sizes and social practices in a unified computational framework . \\n'],\n",
              " [' One of the greatest challenges while working on image segmentation algorithms is a comprehensive measure to evaluate their accuracy . Although there are some measures for doing this task but they can consider only one aspect of segmentation in evaluation process . The performance of evaluation measures can be improved using a combination of single measures . However combination of single measures does not always lead to an appropriate criterion . Besides its effectiveness the efficiency of the new measure should be considered . In this paper a new and combined evaluation measure based on genetic programming has been sought . Because of the nature of evolutionary approaches the proposed approach allows nonlinear and linear combinations of other single evaluation measures and can search within many and different combinations of basic operators to find a good enough one . We have also proposed a new fitness function to make GP enable to search within search space effectively and efficiently . To test the method Berkeley and Weizmann datasets besides several different experiments have been used . Experimental results demonstrate that the GP based approach is suitable for effective combination of single evaluation measures . \\n'],\n",
              " ['Confronted with the explosive growth of web images the web image annotation has become a critical research issue for image search and index. Sparse feature selection plays an important role in improving the efficiency and performance of web image annotation. Meanwhile it is beneficial to developing an effective mechanism to leverage the unlabeled training data for large scale web image annotation. In this paper we propose a novel sparse feature selection framework for web image annotation namely sparse Feature Selection based on Graph Laplacian FSLG 2. FSLG applies the l2 1 2 matrix norm into the sparse feature selection algorithm to select the most sparse and discriminative features. Additional graph Laplacian based semi supervised learning is used to exploit both labeled and unlabeled data for enhancing the annotation performance. An efficient iterative algorithm is designed to optimize the objective function. Extensive experiments on two web image datasets are performed and the results illustrate that our method is promising for large scale web image annotation. \\n'],\n",
              " [' Hair segmentation is challenging due to the diverse appearance irregular region boundary and the influence of complex background . To deal with this problem we propose a novel data driven method named Isomorphic Manifold Inference . The IMI method assumes the coarse probability map and the binary segmentation map as a couple of isomorphic manifolds and tries to learn hair specific priors from manually labeled training images . For an input image firstly the method calculates a coarse probability map . Then it exploits regression techniques to obtain the relationship between the coarse probability map of the test image and those of training images . Finally this relationship i.e . a coefficient set is transferred to the binary segmentation maps and a soft segmentation of the test image will be achieved by a linear combination of those binary maps . Further we employ this soft segmentation as a shape cue and integrate it with color and texture cues into a unified segmentation framework . A better segmentation is achieved by the Graph Cuts optimization . Extensive experiments are conducted to validate effectiveness of the IMI method compare contributions of different cues and investigate the generalization of IMI method . The results strongly encourage our method . \\n'],\n",
              " [' Automatic pain recognition from videos is a vital clinical application and owing to its spontaneous nature poses interesting challenges to automatic facial expression recognition research . Previous pain vs no pain systems have highlighted two major challenges ground truth is provided for the sequence but the presence or absence of the target expression for a given frame is unknown and the time point and the duration of the pain expression event in each video are unknown . To address these issues we propose a novel framework where each sequence is represented as a bag containing multiple segments and multiple instance learning is employed to handle this weakly labeled data in the form of sequence level ground truth . These segments are generated via multiple clustering of a sequence or running a multi scale temporal scanning window and are represented using a state of the art Bag of Words representation . This work extends the idea of detecting facial expressions through concept frames to concept segments and argues through extensive experiments that algorithms such as MIL are needed to reap the benefits of such representation . The key advantages of our approach are joint detection and localization of painful frames using only sequence level ground truth incorporation of temporal dynamics by representing the data not as individual frames but as segments and extraction of multiple segments which is well suited to signals with uncertain temporal location and duration in the video . Extensive experiments on UNBC McMaster Shoulder Pain dataset highlight the effectiveness of the approach by achieving competitive results on both tasks of pain classification and localization in videos . We also empirically evaluate the contributions of different components of MS MIL . The paper also includes the visualization of discriminative facial patches important for pain detection as discovered by our algorithm and relates them to Action Units that have been associated with pain expression . We conclude the paper by demonstrating that MS MIL yields a significant improvement on another spontaneous facial expression dataset the FEEDTUM dataset . \\n'],\n",
              " ['Multipath interference of light is the cause of important errors in Time of Flight ToF depth estimation. This paper proposes an algorithm that removes multipath distortion from a single depth map obtained by a ToF camera. Our approach does not require information about the scene apart from ToF measurements. The method is based on fitting ToF measurements with a radiometric model. Model inputs are depth values free from multipath interference whereas model outputs consist of synthesized ToF measurements. We propose an iterative optimization algorithm that obtains model parameters that best reproduce ToF measurements recovering the depth of the scene without distortion. We show results with both synthetic and real scenes captured by commercial ToF sensors. In all cases our algorithm accurately corrects the multipath distortion obtaining depth maps that are very close to ground truth data. \\n'],\n",
              " [' Background and objective In clinical examinations and brain computer interface research a short electroencephalogram measurement time is ideal . The use of event related potentials relies on both estimation accuracy and processing time . We tested a particle filter that uses a large number of particles to construct a probability distribution . Methods We constructed a simple model for recording EEG comprising three components ERPs approximated via a trend model background waves constructed via an autoregressive model and noise . We evaluated the performance of the particle filter based on mean squared error P300 peak amplitude and latency . We then compared our filter with the Kalman filter and a conventional simple averaging method . To confirm the efficacy of the filter we used it to estimate ERP elicited by a P300 BCI speller . Results A 400 particle filter produced the best MSE . We found that the merit of the filter increased when the original waveform already had a low signal to noise ratio . We calculated the amount of averaging necessary after applying a particle filter that produced a result equivalent to that associated with conventional averaging and determined that the particle filter yielded a maximum 42.8 reduction in measurement time . The particle filter performed better than both the Kalman filter and conventional averaging for a low SNR in terms of both MSE and P300 peak amplitude and latency . For EEG data produced by the P300 speller we were able to use our filter to obtain ERP waveforms that were stable compared with averages produced by a conventional averaging method irrespective of the amount of averaging . Conclusions We confirmed that particle filters are efficacious in reducing the measurement time required during simulations with a low SNR . Additionally particle filters can perform robust ERP estimation for EEG data produced via a P300 speller . \\n'],\n",
              " [' Recent advances in public sector open data and online mapping software are opening up new possibilities for interactive mapping in research applications . Increasingly there are opportunities to develop advanced interactive platforms with exploratory and analytical functionality . This paper reviews tools and workflows for the production of online research mapping platforms alongside a classification of the interactive functionality that can be achieved . A series of mapping case studies from government academia and research institutes are reviewed . The conclusions are that online cartography s technical hurdles are falling due to open data releases open source software and cloud services innovations . The data exploration functionality of these new tools is powerful and complements the emerging fields of big data and open GIS . International data perspectives are also increasingly feasible . Analytical functionality for web mapping is currently less developed but promising examples can be seen in areas such as urban analytics . For more presentational research communication applications there has been progress in story driven mapping drawing on data journalism approaches that are capable of connecting with very large audiences . \\n'],\n",
              " [' Automatically focusing and seeing occluded moving object in cluttered and complex scene is a significant challenging task for many computer vision applications . In this paper we present a novel synthetic aperture imaging approach to solve this problem . The unique characteristics of this work include the following To the best of our knowledge this work is the first to simultaneously solve camera array auto focusing and occluded moving object imaging problem . A unified framework is designed to achieve seamless interaction between the focusing and imaging modules . In the focusing module a local and global constraint based optimization algorithm is presented to dynamically estimate the focus plane of the moving object . In the imaging module a novel visibility analysis based active synthetic aperture imaging approach is proposed to remove the occluder and significantly improve the quality of occluded object imaging . An active camera array system has been set up and evaluated in challenging indoor and outdoor scenes . Extensive experimental results with qualitative and quantitative analyses demonstrate the superiority of the proposed approach compared with state of the art approaches . \\n'],\n",
              " [' This paper proposes a new method for self calibrating a set of stationary non rotating zooming cameras . This is a realistic configuration usually encountered in surveillance systems in which each zooming camera is physically attached to a static structure . In particular a linear yet effective method to recover the affine structure of the observed scene from two or more such stationary zooming cameras is presented . The proposed method solely relies on point correspondences across images and no knowledge about the scene is required . Our method exploits the mostly translational displacement of the so called principal plane of each zooming camera to estimate the location of the plane at infinity . The principal plane of a camera at any given setting of its zoom is encoded in its corresponding perspective projection matrix from which it can be easily extracted . As a displacement of the principal plane of a camera under the effect of zooming allows the identification of a pair of parallel planes each zooming camera can be used to locate a line on the plane at infinity . Hence two or more such zooming cameras in general positions allow the obtainment of an estimate of the plane at infinity making it possible under the assumption of zero skew and or known aspect ratio to linearly calculate the camera s parameters . Finally the parameters of the camera and the coordinates of the plane at infinity are refined through a nonlinear least squares optimization procedure . The results of our extensive experiments using both simulated and real data are also reported in this paper . \\n'],\n",
              " [' This paper proposes a novel active learning framework and combines it with semi supervised learning for segmenting Crohns disease tissues from abdominal magnetic resonance images . Robust fully supervised learning based classifiers require lots of labeled data of different disease severities . Obtaining such data is time consuming and requires considerable expertise . SSL methods use a few labeled samples and leverage the information from many unlabeled samples to train an accurate classifier . AL queries labels of most informative samples and maximizes gain from the labeling effort . Our primary contribution is in designing a query strategy that combines novel context information with classification uncertainty and feature similarity . Combining SSL and AL gives a robust segmentation method that optimally uses few labeled samples and many unlabeled samples and requires lower training time . Experimental results show our method achieves higher segmentation accuracy than FSL methods with fewer samples and reduced training effort . \\n'],\n",
              " [' Aim Medical data mining processes for extracting patterns from large datasets . In the current study we intend to assess different medical data mining approaches to predict ischemic stroke . Materials and methods The collected dataset from Turgut Ozal Medical Centre Inonu University Malatya Turkey comprised the medical records of 80 patients and 112 healthy individuals with 17 predictors and a target variable . As data mining approaches support vector machine stochastic gradient boosting and penalized logistic regression were employed . 10 fold cross validation resampling method was utilized and model performance evaluation metrics were accuracy area under ROC curve sensitivity specificity positive predictive value and negative predictive value . The grid search method was used for optimizing tuning parameters of the models . Results The accuracy values with 95 CI were 0.9789 for SVM 0.9737 for SGB and 0.8947 for PLR . The AUC values with 95 CI were 0.9783 for SVM 0.9757 for SGB and 0.8953 for PLR . Conclusions The results of the current study demonstrated that the SVM produced the best predictive performance compared to the other models according to the majority of evaluation metrics . SVM and SGB models explained in the current study could yield remarkable predictive performance in the classification of ischemic stroke . \\n'],\n",
              " [' Human action recognition has lots of real world applications such as natural user interface virtual reality intelligent surveillance and gaming . However it is still a very challenging problem . In action recognition using the visible light videos the spatiotemporal interest point based features are widely used with good performance . Recently with the advance of depth imaging technology a new modality has appeared for human action recognition . It is important to assess the performance and usefulness of the STIP features for action analysis on the new modality of 3D depth map . In this paper we evaluate the spatiotemporal interest point based features for depth based action recognition . Different interest point detectors and descriptors are combined to form various STIP features . The bag of words representation and the SVM classifiers are used for action learning . Our comprehensive evaluation is conducted on four challenging 3D depth databases . Further we use two schemes to refine the STIP features one is to detect the interest points in RGB videos and apply to the aligned depth sequences and the other is to use the human skeleton to remove irrelevant interest points . These refinements can help us have a deeper understanding of the STIP features on 3D depth data . Finally we investigate a fusion of the best STIP features with the prevalent skeleton features to present a complementary use of the STIP features for action recognition on 3D data . The fusion approach gives significantly higher accuracies than many state of the art results . \\n'],\n",
              " [' Background and objectives Angle closure disease in the eye can be detected using time domain Anterior Segment Optical Coherence Tomography . The Anterior Chamber characteristics can be quantified from AS OCT image which is dependent on the image quality at the image acquisition stage . To date to the best of our knowledge there are no objective or automated subjective measurements to assess the quality of AS OCT images . Methods To address AS OCT image quality assessment issue we define a method for objective assessment of AS OCT images using complex wavelet based local binary pattern features . These features are pooled using the Na ve Bayes classifier to obtain the final quality parameter . To evaluate the proposed method a subjective assessment has been performed by clinical AS OCT experts who graded the quality of AS OCT images on a scale of good fair and poor . This was done based on the ability to identify the AC structures including the position of the scleral spur . Results We compared the results of the proposed objective assessment with the subjective assessments . From this comparison it is validated that the proposed objective assessment has the ability of differentiating the good and fair quality AS OCT images for glaucoma diagnosis from the poor quality AS OCT images . Conclusions This proposed algorithm is an automated approach to evaluate the AS OCT images with the intention for collecting of high quality data for further medical diagnosis . Our proposed quality index has the ability of automatic objective and quantitative assessment of AS OCT image quality and this quality index is similar to glaucoma specialist . \\n'],\n",
              " [' Anatomical cine cardiovascular magnetic resonance imaging is widely used to assess the systolic cardiac function because of its high soft tissue contrast . Assessment of diastolic LV function has not regularly been performed due the complex and time consuming procedures . This study presents a semi automated assessment of the left ventricular diastolic function using anatomical short axis cine CMR images . The proposed method is based on three main steps non rigid registration which yields a sequence of endocardial boundary points over the cardiac cycle based on a user provided contour on the first frame LV volume and filling rate computations over the cardiac cycle and automated detection of the peak values of early and late ventricular filling waves . In 47 patients cine CMR imaging and Doppler echocardiographic imaging were performed . CMR measurements of peak values of the E and A waves as well as the deceleration time were compared with the corresponding values obtained in Doppler Echocardiography . For the E A ratio the proposed algorithm for CMR yielded a Cohen s kappa measure of 0.70 and a Gwet s AC1 coefficient of 0.70 . Conclusion Semi automated assessment of the left ventricular diastolic function using anatomical short axis cine CMR images provides mitral inflow measurements comparable to Doppler Echocardiography . \\n'],\n",
              " [' We propose a measure of information gained through biometric matching systems . Firstly we discuss how the information about the identity of a person is derived from biometric samples through a biometric system and define the biometric system entropy or BSE based on mutual information . We present several theoretical properties and interpretations of the BSE and show how to design a biometric system which maximizes the BSE . Then we prove that the BSE can be approximated asymptotically by the relative entropy D fI where fG and fI are probability mass functions of matching scores between samples from individuals and among population . We also discuss how to evaluate the BSE of a biometric system and show experimental evaluation of the BSE of face fingerprint and multimodal biometric systems . \\n'],\n",
              " [' Brain ageing is followed by changes of the connectivity of white matter and changes of the grey matter concentration . Neurodegenerative disease is more vulnerable to an accelerated brain ageing which is associated with prospective cognitive decline and disease severity . Accurate detection of accelerated ageing based on brain network analysis has a great potential for early interventions designed to hinder atypical brain changes . To capture the brain ageing we proposed a novel computational approach for modeling the 112 normal older subjects brain age by connectivity analyses of networks of the brain . Our proposed method applied principal component analysis to reduce the redundancy in network topological parameters . Back propagation artificial neural network improved by hybrid genetic algorithm and Levenberg Marquardt algorithm is established to model the relation among principal components and brain age . The predicted brain age is strongly correlated with chronological age . The model has mean absolute error of 4.29 years . Therefore we believe the method can provide a possible way to quantitatively describe the typical and atypical network organization of human brain and serve as a biomarker for presymptomatic detection of neurodegenerative diseases in the future . \\n'],\n",
              " [' Background and objective Signal segmentation and spike detection are two important biomedical signal processing applications . Often non stationary signals must be segmented into piece wise stationary epochs or spikes need to be found among a background of noise before being further analyzed . Permutation entropy has been proposed to evaluate the irregularity of a time series . PE is conceptually simple structurally robust to artifacts and computationally fast . It has been extensively used in many applications but it has two key shortcomings . First when a signal is symbolized using the Bandt Pompe procedure only the order of the amplitude values is considered and information regarding the amplitudes is discarded . Second in the PE the effect of equal amplitude values in each embedded vector is not addressed . To address these issues we propose a new entropy measure based on PE the amplitude aware permutation entropy . Methods AAPE is sensitive to the changes in the amplitude in addition to the frequency of the signals thanks to it being more flexible than the classical PE in the quantification of the signal motifs . To demonstrate how the AAPE method can enhance the quality of the signal segmentation and spike detection a set of synthetic and realistic synthetic neuronal signals electroencephalograms and neuronal data are processed . We compare the performance of AAPE in these problems against state of the art approaches and evaluate the significance of the differences with a repeated ANOVA with post hoc Tukey s test . Results In signal segmentation the accuracy of AAPE based method is higher than conventional segmentation methods . AAPE also leads to more robust results in the presence of noise . The spike detection results show that AAPE can detect spikes well even when presented with single sample spikes unlike PE . For multi sample spikes the changes in AAPE are larger than in PE . Conclusion We introduce a new entropy metric AAPE that enables us to consider amplitude information in the formulation of PE . The AAPE algorithm can be used in almost every irregularity based application in various signal and image processing fields . We also made freely available the Matlab code of the AAPE . \\n'],\n",
              " [' Background and objective Heart failure due to iron overload cardiomyopathy is one of the main causes of mortality . The cardiomyopathy is reversible if intensive iron chelation treatment is done in time but the diagnosis is often delayed because the cardiac iron deposition is unpredictable and the symptoms are lately detected . There are many ways to assess iron overload . However the widely used and approved method is by using MRI which is performed by calculating the T2 . In order to compute the T2 value the region of interest is manually selected by an expert which may require considerable time and skills . The aim of this work is hence to develop the cardiac T2 measurement by using region growing algorithm for automatically segmenting the ROI in cardiac MR images . Mathematical morphologies are also used to reduce some errors . Methods Thirty MR images with free breathing and respiratory trigger technique were used in this work . The segmentation algorithm yields good results when compared with the manual segmentation performed by two experts . Results The averages of positive predictive value the sensitivity the Hausdorff distance and the Dice similarity coefficient are 0.76 0.84 7.78 pixels and 0.80 when compared with the two experts opinions . The T2 values were carried out based on the automatically segmented ROI s. The mean difference of T2 values between the proposed technique and the experts opinion is about 1.40ms . Conclusions The results demonstrate the accuracy of the proposed method in T2 value estimation . Some previous methods were implemented for comparisons . The results show that the proposed method yields better segmentation and T2 value estimation performances . \\n'],\n",
              " [' Background and objectives Text mining and semantic analysis approaches can be applied to the construction of biomedical domain specific search engines and provide an attractive alternative to create personalized and enhanced search experiences . Therefore this work introduces the new open source BIOMedical Search Engine Framework for the fast and lightweight development of domain specific search engines . The rationale behind this framework is to incorporate core features typically available in search engine frameworks with flexible and extensible technologies to retrieve biomedical documents annotate meaningful domain concepts and develop highly customized Web search interfaces . Methods The BIOMedical Search Engine Framework integrates taggers for major biomedical concepts such as diseases drugs genes proteins compounds and organisms and enables the use of domain specific controlled vocabulary . Technologies from the Typesafe Reactive Platform the AngularJS JavaScript framework and the Bootstrap HTML CSS framework support the customization of the domain oriented search application . Moreover the RESTful API of the BIOMedical Search Engine Framework allows the integration of the search engine into existing systems or a complete web interface personalization . Results The construction of the Smart Drug Search is described as proof of concept of the BIOMedical Search Engine Framework . This public search engine catalogs scientific literature about antimicrobial resistance microbial virulence and topics alike . The keyword based queries of the users are transformed into concepts and search results are presented and ranked accordingly . The semantic graph view portraits all the concepts found in the results and the researcher may look into the relevance of different concepts the strength of direct relations and non trivial indirect relations . The number of occurrences of the concept shows its importance to the query and the frequency of concept co occurrence is indicative of biological relations meaningful to that particular scope of research . Conversely indirect concept associations i.e . concepts related by other intermediary concepts can be useful to integrate information from different studies and look into non trivial relations . Conclusions The BIOMedical Search Engine Framework supports the development of domain specific search engines . The key strengths of the framework are modularity and extensibilityin terms of software design the use of open source consolidated Web technologies and the ability to integrate any number of biomedical text mining tools and information resources . Currently the Smart Drug Search keeps over 1 186 000 documents containing more than 11 854 000 annotations for 77 200 different concepts . The Smart Drug Search is publicly accessible at http sing.ei.uvigo.es sds . The BIOMedical Search Engine Framework is freely available for non commercial use at https github.com agjacome biomsef . \\n'],\n",
              " [' Background and objective Hypokinetic dysarthria is a frequent speech disorder associated with idiopathic Parkinson s disease . It affects all dimensions of speech production . One of the most common features of HD is dysprosody that is characterized by alterations of rhythm and speech rate flat speech melody and impairment of speech intensity control . Dysprosody has a detrimental impact on speech naturalness and intelligibility . Methods This paper deals with quantitative prosodic analysis of neutral stress modified and rhymed speech in patients with PD . The analysis of prosody is based on quantification of monopitch monoloudness and speech rate abnormalities . Experimental dataset consists of 98 patients with PD and 51 healthy speakers . For the purpose of HD identification sequential floating feature selection algorithm and random forests classifier is used . In this paper we also introduce a concept of permutation test applied in the field of acoustic analysis of dysarthric speech . Results Prosodic features obtained from stress modified reading task provided higher classification accuracies compared to the ones extracted from reading task with neutral emotion demonstrating the importance of stress in speech prosody . Features calculated from poem recitation task outperformed both reading tasks in the case of gender undifferentiated analysis showing that rhythmical demands can in general lead to more precise identification of HD . Additionally some gender related patterns of dysprosody has been observed . Conclusions This paper confirms reduced variation of fundamental frequency in PD patients with HD . Interestingly increased variability of speech intensity compared to healthy speakers has been detected . Regarding speech rate disturbances our results does not report any particular pattern . We conclude further development of prosodic features quantifying the relationship between monopitch monoloudness and speech rate disruptions in HD can have a great potential in future PD analysis . \\n'],\n",
              " [' This paper presents a matching strategy to improve the discriminative power of histogram based keypoint descriptors by constraining the range of allowable dominant orientations according to the context of the scene under observation . This can be done when the descriptor uses a circular grid and quantized orientation steps by computing or providing a global reference orientation based on the feature matches . The proposed matching strategy is compared with the standard approaches used with the SIFT and GLOH descriptors and the recent rotation invariant MROGH and LIOP descriptors . A new evaluation protocol based on an approximated overlap error is presented to provide an effective analysis in the case of non planar scenes thus extending the current state of the art results . \\n'],\n",
              " [' The analysis of regular texture images is cast in a model comparison framework . Texel lattice hypotheses are used to define statistical models which are compared in terms of their ability to explain the images . This approach is used to estimate lattice geometry from patterns that exhibit translational symmetry . It is also used to determine whether images consist of such regular textures . A method based on this approach is described in which lattice hypotheses are generated using analysis of peaks in the image autocorrelation function statistical models are based on Gaussian or Gaussian mixture clusters and model comparison is performed using the marginal likelihood as approximated by the Bayes Information Criterion . Experiments on public domain images and a commercial textile image archive demonstrate substantially improved accuracy compared to several alternative methods . \\n'],\n",
              " [' Eye blinks are one of the most influential artifact sources in electroencephalogram recorded from frontal channels and thereby detecting and rejecting eye blink artifacts is regarded as an essential procedure for improving the quality of EEG data . In this paper a novel method to detect eye blink artifacts from a single channel frontal EEG signal was proposed by combining digital filters with a rule based decision system and its performance was validated using an EEG dataset recorded from 24 healthy participants . The proposed method has two main advantages over the conventional methods . First it uses single channel EEG data without the need for electrooculogram references . Therefore this method could be particularly useful in brain computer interface applications using headband type wearable EEG devices with a few frontal EEG channels . Second this method could estimate the ranges of eye blink artifacts accurately . Our experimental results demonstrated that the artifact range estimated using our method was more accurate than that from the conventional methods and thus the overall accuracy of detecting epochs contaminated by eye blink artifacts was markedly increased as compared to conventional methods . The MATLAB package of our library source codes and sample data named Eyeblink Master is open for free download . \\n'],\n",
              " [' We introduce a new computational phonetic modeling framework for sign language recognition . This is based on dynamic static statistical subunits and provides sequentiality in an unsupervised manner without prior linguistic information . Subunit sequentiality refers to the decomposition of signs into two types of parts varying and non varying that are sequentially stacked across time . Our approach is inspired by the Movement Hold SL linguistic model that refers to such sequences . First we segment signs into intra sign primitives and classify each segment as dynamic or static i.e . movements and non movements . These segments are then clustered appropriately to construct a set of dynamic and static subunits . The dynamic static discrimination allows us employing different visual features for clustering the dynamic or static segments . Sequences of the generated subunits are used as sign pronunciations in a data driven lexicon . Based on this lexicon and the corresponding segmentation each subunit is statistically represented and trained on multimodal sign data as a hidden Markov model . In the proposed approach dynamic static sequentiality is incorporated in an unsupervised manner . Further handshape information is integrated in a parallel hidden Markov modeling scheme . The novel sign language modeling scheme is evaluated in recognition experiments on data from three corpora and two sign languages Boston University American SL which is employed pre segmented at the sign level Greek SL Lemmas and American SL Large Vocabulary Dictionary including both signer dependent and unseen signers testing . Results show consistent improvements when compared with other approaches demonstrating the importance of dynamic static structure in sub sign phonetic modeling . \\n'],\n",
              " [' Background and objective Cell migration differentiation proliferation and apoptosis are the main processes in tissue regeneration . Mesenchymal Stem Cells have the potential to differentiate into many cell phenotypes such as tissue or organ specific cells to perform special functions . Experimental observations illustrate that differentiation and proliferation of these cells can be regulated according to internal forces induced within their Extracellular Matrix . The process of how exactly they interpret and transduce these signals is not well understood . Methods A previously developed three dimensional computational model is here extended and employed to study how force free substrates and force induced substrate control cell differentiation and or proliferation during the mechanosensing process . Consistent with experimental observations it is assumed that cell internal deformation in correlation with the cell maturation state directly triggers cell differentiation and or proliferation . The Extracellular Matrix is modeled as Neo Hookean hyperelastic material assuming that cells are cultured within 3D nonlinear hydrogels . Results In agreement with well known experimental observations the findings here indicate that within neurogenic chondrogenic and osteogenic substrates Mesenchymal Stem Cells differentiation and proliferation can be precipitated by inducing the substrate with an internal force . Therefore cells require a longer time to grow and maturate within force free substrates than within force induced substrates . In the instance of Mesenchymal Stem Cells differentiation into a compatible phenotype the magnitude of the net traction force increases within chondrogenic and osteogenic substrates while it reduces within neurogenic substrates . This is consistent with experimental studies and numerical works recently published by the same authors . However in all cases the magnitude of the net traction force considerably increases at the instant of cell proliferation because of cell cell interaction . Conclusions The present model provides new perspectives to delineate the role of force induced substrates in remotely controlling the cell fate during cell matrix interaction which open the door for new tissue regeneration methodologies . \\n'],\n",
              " [' Conventional particle filtering based visual ego motion estimation or visual odometry often suffers from large local linearization errors in the case of abrupt camera motion . The main contribution of this paper is to present a novel particle filtering based visual ego motion estimation algorithm that is especially robust to the abrupt camera motion . The robustness to the abrupt camera motion is achieved by multi layered importance sampling via particle swarm optimization which iteratively moves particles to higher likelihood region without local linearization of the measurement equation . Furthermore we make the proposed visual ego motion estimation algorithm in real time by reformulating the conventional vector space PSO algorithm in consideration of the geometry of the special Euclidean group SE which is a Lie group representing the space of 3 D camera poses . The performance of our proposed algorithm is experimentally evaluated and compared with the local linearization and unscented particle filter based visual ego motion estimation algorithms on both simulated and real data sets . \\n'],\n",
              " [' Glaucoma is a disease of the retina which is one of the most common causes of permanent blindness worldwide . This paper presents an automatic image processing based method for glaucoma diagnosis from the digital fundus image . In this paper wavelet feature extraction has been followed by optimized genetic feature selection combined with several learning algorithms and various parameter settings . Unlike the existing research works where the features are considered from the complete fundus or a sub image of the fundus this work is based on feature extraction from the segmented and blood vessel removed optic disc to improve the accuracy of identification . The experimental results presented in this paper indicate that the wavelet features of the segmented optic disc image are clinically more significant in comparison to features of the whole or sub fundus image in the detection of glaucoma from fundus image . Accuracy of glaucoma identification achieved in this work is 94.7 and a comparison with existing methods of glaucoma detection from fundus image indicates that the proposed approach has improved accuracy of classification . \\n'],\n",
              " ['Automatic facial landmarking is a crucial prerequisite of many applications dedicated to face analysis. In this paper we describe a two step method. In a first step each landmark position in the image is predicted independently. To achieve fast and accurate localizations we implement detectors based on a two stage classifier and we use multiple kernel learning algorithms to combine multi scale features. In a second step to increase the robustness of the system we introduce spatial constraints between landmarks. To this end parameters of a deformable shape model are optimized using the first step outputs through a Gauss Newton algorithm. Extensive experiments have been carried out on different databases PIE LFPW Cohn Kanade Face Pix and BioID assessing the accuracy and the robustness of the proposed approach. They show that the proposed algorithm is not significantly affected by small rotations facial expressions or natural occlusions and can be favorably compared with the current state of the art landmarking systems. \\n'],\n",
              " [' Abnormal values of vital parameters such as hypotension or tachycardia may occur during anesthesia and may be detected by analyzing time series data collected during the procedure by the Anesthesia Information Management System . When crossed with other data from the Hospital Information System abnormal values of vital parameters have been linked with postoperative morbidity and mortality . However methods for the automatic detection of these events are poorly documented in the literature and differ between studies making it difficult to reproduce results . In this paper we propose a methodology for the automatic detection of abnormal values of vital parameters . This methodology uses an algorithm allowing the configuration of threshold values for any vital parameters as well as the management of missing data . Four examples illustrate the application of the algorithm after which it is applied to three vital signs to all 2014 anesthetic records at our institution . \\n'],\n",
              " [' Background The simultaneous acquisition of electroencephalogram and functional magnetic resonance imaging provides both high temporal and spatial resolution when measuring brain activity . A real time analysis during a simultaneous EEG fMRI acquisition is essential when studying neurofeedback and conducting effective brain activity monitoring . However the ballistocardiogram artifacts which are induced by heartbeat related electrode movements in an MRI scanner severely contaminate the EEG signals and hinder a reliable real time analysis . New method The optimal basis sets method is an effective candidate for removing BCG artifacts in a traditional offline EEG fMRI analysis but has yet to be applied to a real time EEG fMRI analysis . Here a novel real time technique based on OBS method is proposed to remove BCG artifacts on a moment to moment basis . Real time electrocardiogram R peak detection procedure and sliding window OBS method were adopted . Results A series of simulated data was constructed to verify the feasibility of the rtOBS technique . Furthermore this method was applied to real EEG fMRI data to remove BCG artifacts . The results of both simulated data and real EEG fMRI data from eight healthy human subjects demonstrate the effectiveness of rtOBS in both the time and frequency domains . Comparison with existing methods A comparison between rtOBS and real time averaged artifact subtraction was conducted . The results suggest the efficacy and advantage of rtOBS in the real time removal of BCG artifacts . Conclusions In this study a novel real time OBS technique was proposed for the real time removal of BCG artifacts . The proposed method was tested using simulated data and applied to real simultaneous EEG fMRI data . The results suggest the effectiveness of this method . \\n'],\n",
              " [' Transfer function design is a key issue in direct volume rendering . Many sophisticated transfer functions have been proposed to visualize boundaries in volumetric data sets such as computed tomography and magnetic resonance imaging . However it is still conventionally challenging to reliably detect boundaries . Meanwhile the interactive strategy is complicated for new users or even experts . In this paper we first propose the human centric boundary extraction criteria and our boundary model . Based on the model we present a boundary visualization method through a what material you pick is what boundary you see approach . Users can pick out the material of interest to directly convey semantics . In addition the 3 D canny edge detection is utilized to ensure the good localization of boundaries . Furthermore we establish a point to material distance measure to guarantee the accuracy and integrity of boundaries . The proposed boundary visualization is intuitive and flexible for the exploration of volumetric data . \\n'],\n",
              " [' Background and objective Integrative approaches for the study of biological systems have gained popularity in the realm of statistical genomics . For example The Cancer Genome Atlas has applied integrative clustering methodologies to various cancer types to determine molecular subtypes within a given cancer histology . In order to adequately compare integrative or systems biology type methods realistic and related datasets are needed to assess the methods . This involves simulating multiple types of omic data with realistic correlation between features of the same type and across data types . Methods We present the software application tool InterSIM for simulating multiple interrelated data types with realistic intra and inter relationships based on the DNA methylation mRNA gene expression and protein expression from the TCGA ovarian cancer study . Results The resulting simulated datasets can be used to assess and compare the operating characteristics of newly developed integrative bioinformatics methods to existing methods . Application of InterSIM is presented with an example of heatmaps of the simulated datasets . Conclusions InterSIM allows researchers to evaluate and test new integrative methods with realistically simulated interrelated genomic datasets . The software tool InterSIM is implemented in R and is freely available from CRAN . \\n'],\n",
              " [' Current image matting methods based on color sampling use color to distinguish between foreground and background pixels . However they fail when the corresponding color distributions overlap . Other methods that define correlation between neighboring pixels based on color aim to propagate the opacity parameter from known pixels to unknown pixels . However strong edges of textured regions may block the propagation of . In this paper a new matting strategy is proposed that delivers an accurate matte by considering texture as a feature that can complement color even if the foreground and background color distributions overlap and the image is a complex one with highly textured regions . The texture feature is extracted in such a way as to increase distinction between foreground and background regions . An objective function containing color and texture components is optimized to find the best foreground and background pair among a set of candidate pairs . The effectiveness of proposed method is compared quantitatively as well as qualitatively with other matting methods by evaluating their results on a benchmark dataset and a set of complex images . The evaluations show that the proposed method presented the best among state of the art matting methods . \\n'],\n",
              " [' Background and objectives The automated analysis of indirect immunofluorescence images for Anti Nuclear Autoantibody testing is a fairly recent field that is receiving ever growing interest from the research community . ANA testing leverages on the categorization of intensity level and fluorescent pattern of IIF images of HEp 2 cells to perform a differential diagnosis of important autoimmune diseases . Nevertheless it suffers from tremendous lack of repeatability due to subjectivity in the visual interpretation of the images . The automatization of the analysis is seen as the only valid solution to this problem . Several works in literature address individual steps of the work flow nonetheless integrating such steps and assessing their effectiveness as a whole is still an open challenge . Methods We present a modular tool ANAlyte able to characterize a IIF image in terms of fluorescent intensity level and fluorescent pattern without any user interactions . For this purpose ANAlyte integrates the following Intensity Classifier module that categorizes the intensity level of the input slide based on multi scale contrast assessment Cell Segmenter module that splits the input slide into individual HEp 2 cells Pattern Classifier module that determines the fluorescent pattern of the slide based on the pattern of the individual cells . Results To demonstrate the accuracy and robustness of our tool we experimentally validated ANAlyte on two different public benchmarks of IIF HEp 2 images with rigorous leave one out cross validation strategy . We obtained overall accuracy of fluorescent intensity and pattern classification respectively around 85 and above 90 . We assessed all results by comparisons with some of the most representative state of the art works . Conclusions Unlike most of the other works in the recent literature ANAlyte aims at the automatization of all the major steps of ANA image analysis . Results on public benchmarks demonstrate that the tool can characterize HEp 2 slides in terms of intensity and fluorescent pattern with accuracy better or comparable with the state of the art techniques even when such techniques are run on manually segmented cells . Hence ANAlyte can be proposed as a valid solution to the problem of ANA testing automatization . \\n'],\n",
              " [' Although direct volume rendering has become a commodity effective rendering of interesting features is still a challenge . In one of active DVR application fields the medicine radiologists have used DVR for the diagnosis of lesions or diseases that should be visualized distinguishably from other surrounding anatomical structures . One of most frequent and important radiologic tasks is the detection of lesions usually constrictions in complex tubular structures . In this paper we propose a 3D spatial field for the effective visualization of constricted tubular structures called as a stenosis map which stores the degree of constriction at each voxel . Constrictions within tubular structures are quantified by using newly proposed measures based on the localized structure analysis and classified with a proposed transfer function mapping the degree of constriction to color and opacity . We show the application results of our method to the visualization of coronary artery stenoses . We present performance evaluations using twenty eight clinical datasets demonstrating high accuracy and efficacy of our proposed method . The ability of our method to saliently visualize the constrictions within tubular structures and interactively adjust the visual appearance of the constrictions proves to deliver a substantial aid in radiologic practice . \\n'],\n",
              " [' Forecasting the variability of dwellings and residential land is important for estimating the future potential of environmental technologies . This paper presents an innovative method of converting average residential density into a set of one hectare 3D tiles to represent the dwelling stock . These generic tiles include residential land as well as the dwelling characteristics . The method was based on a detailed analysis of the English House Condition Survey data and density was calculated as the inverse of the plot area per dwelling . This found that when disaggregated by age band urban morphology and area type the frequency distribution of plot density per dwelling type can be represented by the gamma distribution . The shape parameter revealed interesting characteristics about the dwelling stock and how this has changed over time . It showed a consistent trend that older dwellings have greater variability in plot density than newer dwellings and also that apartments and detached dwellings have greater variability in plot density than terraced and semi detached dwellings . Once calibrated the shape parameter of the gamma distribution was used to convert the average density per housing type into a frequency distribution of plot density . These were then approximated by systematically selecting a set of generic tiles . These tiles are particularly useful as a medium for multidisciplinary research on decentralized environmental technologies or climate adaptation which requires this understanding of the variability of dwellings occupancies and urban space . It thereby links the socioeconomic modeling of city regions with the physical modeling of dwellings and associated infrastructure across the spatial scales . The tiles method has been validated by comparing results against English regional housing survey data and dwelling footprint area data . The next step would be to explore the possibility of generating generic residential area types and adapt the method to other countries that have similar housing survey data . \\n'],\n",
              " [' Background and objective Ankle motion and proprioception in multiple axis movements are crucial for daily activities . However few studies have developed and used a multiple axis system for measuring ankle motion and proprioception . This study was designed to validate a novel ankle haptic interface system that measures the ankle range of motion and joint position sense in multiple plane movements investigating the proprioception deficits during joint position sense tasks for patients with ankle instability . Methods Eleven healthy adults and thirteen patients with ankle instability were recruited in this study . All subjects were asked to perform tests to evaluate the validity of the ankle ROM measurements and underwent tests for validating the joint position sense measurements conducted during multiple axis movements of the ankle joint . Pearson correlation was used for validating the angular position measurements obtained using the developed system the independent t test was used to investigate the differences in joint position sense task performance for people with or without ankle instability . Results The ROM measurements of the device were linearly correlated with the criterion standards . The ankle instability and healthy groups were significantly different in direction absolute and variable errors of plantar flexion dorsiflexion inversion and eversion . Conclusions The results demonstrate that the novel ankle joint motion and position sense measurement system is valid and can be used for measuring the ankle ROM and joint position sense in multiple planes and indicate proprioception deficits for people with ankle instability . \\n'],\n",
              " [' Mobile devices namely phones and tablets have long gone smart . Their growing use is both a cause and an effect of their technological advancement . Among the others their increasing ability to store and exchange sensitive information has caused interest in exploiting their vulnerabilities and the opposite need to protect users and their data through secure protocols for access and identification on mobile platforms . Face and iris recognition are especially attractive since they are sufficiently reliable and just require the webcam normally equipping the involved devices . On the contrary the alternative use of fingerprints requires a dedicated sensor . Moreover some kinds of biometrics lend themselves to uses that go beyond security . Ambient intelligence services bound to the recognition of a user as well as social applications such as automatic photo tagging on social networks can especially exploit face recognition . This paper describes FIRME as a biometric application based on a multimodal recognition of face and iris which is designed to be embedded in mobile devices . Both design and implementation of FIRME rely on a modular architecture whose workflow includes separate and replaceable packages . The starting one handles image acquisition . From this point different branches perform detection segmentation feature extraction and matching for face and iris separately . As for face an antispoofing step is also performed after segmentation . Finally results from the two branches are fused . In order to address also security critical applications FIRME can perform continuous reidentification and best sample selection . To further address the possible limited resources of mobile devices all algorithms are optimized to be low demanding and computation light . \\n'],\n",
              " [' Background Dynamic measurements of human muscle fascicle length from sequences of B mode ultrasound images have become increasingly prevalent in biomedical research . Manual digitisation of these images is time consuming and algorithms for automating the process have been developed . Here we present a freely available software implementation of a previously validated algorithm for semi automated tracking of muscle fascicle length in dynamic ultrasound image recordings UltraTrack . Methods UltraTrack implements an affine extension to an optic flow algorithm to track movement of the muscle fascicle end points throughout dynamically recorded sequences of images . The underlying algorithm has been previously described and its reliability tested but here we present the software implementation with features for tracking multiple fascicles in multiple muscles simultaneously correcting temporal drift in measurements manually adjusting tracking results saving and re loading of tracking results and loading a range of file formats . Results Two example runs of the software are presented detailing the tracking of fascicles from several lower limb muscles during a squatting and walking activity . Conclusion We have presented a software implementation of a validated fascicle tracking algorithm and made the source code and standalone versions freely available for download . \\n'],\n",
              " [' In this paper an efficient method for text independent writer identification using a codebook method is proposed . The method uses the occurrence histogram of the shapes in a codebook to create a feature vector for each specific manuscript . For cursive handwritings a wide variety of different shapes exist in the connected components obtained from the handwriting . Small fragments of connected components are used to avoid complex patterns . Two efficient methods for extracting codes from contours are introduced . One method uses the actual pixel coordinates of contour fragments while the other one uses a linear piece wise approximation using segment angles and lengths . To evaluate the methods writer identification is conducted on two English and three Farsi handwriting databases . Both methods show promising performances with the performance of second method being better than the first one . \\n'],\n",
              " [' In this paper a statistical approach to static texture description is developed which combines a local pattern coding strategy with a robust global descriptor to achieve highly discriminative power invariance to photometric transformation and strong robustness against geometric changes . Built upon the local binary patterns that are encoded at multiple scales a statistical descriptor called pattern fractal spectrum characterizes the self similar behavior of the local pattern distributions by calculating fractal dimension on each type of pattern . Compared with other fractal based approaches the proposed descriptor is compact highly distinctive and computationally efficient . We applied the descriptor to texture classification . Our method has demonstrated excellent performance in comparison with state of the art approaches on four challenging benchmark datasets . \\n'],\n",
              " [' The Non local means denoising filter has been established as gold standard for image denoising problem in general and particularly in medical imaging due to its efficiency . However its computation time limited its applications in real world application especially in medical imaging . In this paper a distributed version on parallel hybrid architecture is proposed to solve the computation time problem and a new method to compute the filters coefficients is also proposed where we focused on the implementation and the enhancement of filters parameters via taking the neighborhood of the current voxel more accurately into account . In terms of implementation our key contribution consists in reducing the number of shared memory accesses . The different tests of the proposed method were performed on the brain web database for different levels of noise . Performances and the sensitivity were quantified in terms of speedup peak signal to noise ratio execution time the number of floating point operations . The obtained results demonstrate the efficiency of the proposed method . Moreover the implementation is compared to that of other techniques recently published in the literature . \\n'],\n",
              " [' In this paper we introduce a novel framework for low level image processing and analysis . First we process images with very simple difference based filter functions . Second we fit the 2 parameter Weibull distribution to the filtered output . This maps each image to the 2D Weibull manifold . Third we exploit the information geometry of this manifold and solve low level image processing tasks as minimisation problems on point sets . For a proof of concept example we examine the image autofocusing task . We propose appropriate cost functions together with a simple implicitly constrained manifold optimisation algorithm and show that our framework compares very favourably against common autofocus methods from literature . In particular our approach exhibits the best overall performance in terms of combined speed and accuracy . \\n'],\n",
              " [' Methods designed for tracking in dense crowds typically employ prior knowledge to make this difficult problem tractable. In this paper we show that it is possible to handle this problem without any priors by utilizing the visual and contextual information already available in such scenes. We propose a novel tracking method tailored to dense crowds which provides an alternative and complementary approach to methods that require modeling of crowd flow and simultaneously is less likely to fail in the case of dynamic crowd flows and anomalies by minimally relying on previous frames. Our method begins with the automatic identification of prominent individuals from the crowd that are easy to track. Then we use Neighborhood Motion Concurrence to model the behavior of individuals in a dense crowd this predicts the position of an individual based on the motion of its neighbors. When the individual moves with the crowd flow we use Neighborhood Motion Concurrence to predict motion while leveraging five frame instantaneous flow in case of dynamically changing flow and anomalies. All these aspects are then embedded in a framework which imposes hierarchy on the order in which positions of individuals are updated. Experiments on a number of sequences show that the proposed solution can track individuals in dense crowds without requiring any pre processing making it a suitable online tracking algorithm for dense crowds. \\n'],\n",
              " ['There are many machine vision models of the visual saliency mechanism which controls the process of selecting and allocating attention to the most prominent locations in the scene and helps humans interact with the visual environment efficiently Itti and C. Koch 2001 Gao et al. 2000 . It is important to know which models perform the best in mimicking the saliency mechanism of the human visual system. There are several metrics to compare saliency models however results from different metrics vary widely in evaluating models. In this paper a procedure is proposed for evaluating metrics for comparing saliency maps using a database of human fixations on approximately 1000 images. This procedure is then employed to identify the best metric. This best metric is then used to evaluate ten published bottom up saliency models. An optimized level of the blurriness and center bias is found for each visual saliency model. Performance of the models is also analyzed on a dataset of 54 synthetic images. \\n'],\n",
              " ['Tracking vehicles using a network of cameras with non overlapping views is a challenging problem of great importance in traffic surveillance. One of the main challenges is accurate vehicle matching across the cameras. Even if the cameras have similar views on vehicles vehicle matching remains a difficult task due to changes of their appearance between observations and inaccurate detections and occlusions which often occur in real scenarios. To be executed on smart cameras the matching has also to be efficient in terms of needed data and computations. To address these challenges we present a low complexity method for vehicle matching robust against appearance changes and inaccuracies in vehicle detection. We efficiently represent vehicle appearances using signature vectors composed of Radon transform like projections of the vehicle images and compare them in a coarse to fine fashion using a simple combination of 1 D correlations. To deal with appearance changes we include multiple observations in each vehicle appearance model. These observations are automatically collected along the vehicle trajectory. The proposed signature vectors can be calculated in low complexity smart cameras by a simple scan line algorithm of the camera software itself and transmitted to the other smart cameras or to the central server. Extensive experiments based on real traffic surveillance videos recorded in a tunnel validate our approach. \\n'],\n",
              " [' Background and objective Carpal fusions are useful for treating specific carpal disorders maximizing postoperative wrist motion hand strength reducing pain and instability of the joint . The surgeon selects the appropriate treatment by considering the degree of stability the chronicity of the injury functional demands of the patient and former patient s outcomes as well . However there are not many studies regarding the load distribution provided by the treatment . So the purpose of this study is to analyze the load distribution through the wrist joint with an arthrodesis treatment and compare the results with a normal wrist . Method To this end the rigid body spring model method was used on a three dimensional model of the wrist joint . The cartilage and ligaments were simulated as springs acting under compression and tension respectively while the bones were considered as rigid bodies . To simulate the arthrodesis the fused bones were considered as a single rigid body . Results The changes on the load distribution for each arthrodesis agree with the treatment objective reducing load transmission through a specific articular surface . For example for SLAC SNAC II most of the treatments reduced the load transmitted through the radioscaphoid fossae almost by 8 . However the capitolunate arthrodesis was the treatment that managed to keep the load transmitted through the radiolunate joint closer to normal conditions . Also in treatments where the scaphoid was excised the joint surface between the lunate surface compensates by doubling the transmitted force to the radius . Conclusions The common arthrodesis for treating SLAC SNAC II III reduces in fact the load on the radioscaphoid joint . Alternative treatments that reduce load distribution on the radiocarpal joint should be three corner and capitolunate arthrodesis for treating SLAC SNAC II and for SLAC SNAC III four corners with scaphoid excision . On Kienbock s disease . Scaphocapitate arthrodesis is more effective on reducing the load transmission through the radiolunate and ulnolunate joints . All arthrodesis treatment should consider changes on the load transmission and also bones fusion rates and pain reduction on patient s outcomes . \\n'],\n",
              " [' Urbanisation environmental risks and resource scarcity are but three of many challenges that cities must address if they are to become more sustainable . However the policies and spatial development strategies implemented to achieve individual sustainability objectives frequently interact and conflict presenting decision makers a multi objective spatial optimisation problem . This work presents a developed spatial optimisation framework which optimises the location of future residential development against several sustainability objectives . The framework is applied to a case study over Middlesbrough in the North East of the United Kingdom . In this context the framework optimises five sustainability objectives from our case study site minimising risk from heat waves minimising the risk from flood events minimising travel costs to minimise transport emissions minimising the expansion of urban sprawl and preventing development on green spaces . A series of optimised spatial configurations of future development strategies are presented . The results compare strategies that are optimal against individual pairs and multiple sustainability objectives such that each of these optimal strategies out performs all other development strategies in at least one sustainability objective . Moreover the resulting spatial strategies significantly outperform the current local authority strategy for all objectives with for example a relative improvement of up to 68 in the performance of distance to CBD . Based on these results it suggests that spatial optimisation can provide a powerful decision support tool to help planners to identify spatial development strategies that satisfy multiple sustainability objectives . \\n'],\n",
              " [' The peristaltic flow of a copper oxide water fluid investigates the effects of heat generation and magnetic field in permeable tube is studied . The mathematical formulation is presented the resulting equations are solved exactly . The obtained expressions for pressure gradient pressure rise temperature velocity profile are described through graphs for various pertinent parameters . It is found that pressure gradient is reduce with enhancement of particle concentration and velocity profile is upturn beside it is observed that temperature increases as more volume fraction of copper oxide . The streamlines are drawn for some physical quantities to discuss the trapping phenomenon . \\n'],\n",
              " [' Background and Objective The HIV AIDS related issue has given rise to a priority concern in which potential new therapies are increasingly highlighted to lessen the negative impact of highly active anti retroviral therapy in the healthcare industry . With the motivation of medical applications this study focuses on the main advanced feature selection techniques and classification approaches that reflect a new architecture and a trial to build a hybrid model for interested parties . Methods This study first uses an integrated linear nonlinear feature selection technique to identify the determinants influencing HAART medication and utilizes organizations of different condition attributes to generate a hybrid model based on a rough set classifier to study evolving HIV AIDS research in order to improve classification performance . Results The proposed model makes use of a real data set from Taiwan s specialist medical center . The experimental results show that the proposed model yields a satisfactory result that is superior to the listed methods and the core condition attributes PVL CD4 Code Age Year PLT and Sex were identified in the HIV AIDS data set . In addition the decision rule set created can be referenced as a knowledge based healthcare service system as the best of evidence based practices in the workflow of current clinical diagnosis . Conclusions This study highlights the importance of these key factors and provides the rationale that the proposed model is an effective alternative to analyzing sustained HAART medication in follow up studies of HIV AIDS treatment in practice . \\n'],\n",
              " [' Facial expression recognition systems must ultimately work on real data in uncontrolled environments although most research studies have been conducted on lab based data with posed or evoked facial expressions obtained in pre set laboratory environments . It is very difficult to obtain data in real world situations because privacy laws prevent unauthorized capture and use of video from events such as funerals birthday parties marriages etc . It is a challenge to acquire such data on a scale large enough for benchmarking algorithms . Although video obtained from TV or movies or postings on the World Wide Web may also contain acted emotions and facial expressions they may be more realistic than lab based data currently used by most researchers . Or is it One way of testing this is to compare feature distributions and FER performance . This paper describes a database that has been collected from television broadcasts and the World Wide Web containing a range of environmental and facial variations expected in real conditions and uses it to answer this question . A fully automatic system that uses a fusion based approach for FER on such data is introduced for performance evaluation . Performance improvements arising from the fusion of point based texture and geometry features and the robustness to image scale variations are experimentally evaluated on this image and video dataset . Differences in FER performance between lab based and realistic data between different feature sets and between different train test data splits are investigated . \\n'],\n",
              " ['In this paper we tackle the problem of gait recognition based on the model free approach. Numerous methods exist they all lead to high dimensional feature spaces. To address the problem of high dimensional feature space we propose the use of the Random Forest algorithm to rank features importance. In order to efficiently search throughout subspaces we apply a backward feature elimination search strategy. Our first experiments are carried out on unknown covariate conditions. Our first results suggest that the selected features contribute to increase the CCR of different existing classification methods. Secondary experiments are performed on unknown covariate conditions and viewpoints. Inspired by the location of our first experiments features we proposed a simple mask. Experimental results demonstrate that the proposed mask gives satisfactory results for all angles of the probe and consequently is not view specific. We also show that our mask performs well when an uncooperative experimental setup is considered as compared to the state of the art methods. As a consequence we propose a panoramic gait recognition framework on unknown covariate conditions. Our results suggest that panoramic gait recognition can be performed under unknown covariate conditions. Our approach can greatly reduce the complexity of the classification problem while achieving fair correct classification rates when gait is captured with unknown conditions. \\n'],\n",
              " ['A large number of methods have been published that aim to evaluate various components of multi view geometry systems. Most of these have focused on the feature extraction description and matching stages the visual front end since geometry computation can be evaluated through simulation. Many data sets are constrained to small scale scenes or planar scenes that are not challenging to new algorithms or require special equipment. This paper presents a method for automatically generating geometry ground truth and challenging test cases from high spatio temporal resolution video. The objective of the system is to enable data collection at any physical scale in any location and in various parts of the electromagnetic spectrum. The data generation process consists of collecting high resolution video computing accurate sparse 3D reconstruction video frame culling and down sampling and test case selection. The evaluation process consists of applying a test 2 view geometry method to every test case and comparing the results to the ground truth. This system facilitates the evaluation of the whole geometry computation process or any part thereof against data compatible with a realistic application. A collection of example data sets and evaluations is included to demonstrate the range of applications of the proposed system. \\n'],\n",
              " ['In this paper we propose a visual tracking algorithm by incorporating the appearance information gathered from two collaborative feature sets and exploiting its geometric structures. A structured visual dictionary SVD can be learned from both appearance and geometric structure thereby enhancing its discriminative strength between the foreground object and the background. Experimental results show that the proposed tracking algorithm using SVD SVDTrack performs favorably against the state of the art methods. \\n'],\n",
              " [' We present a novel method for on line joint object tracking and segmentation in a monocular video captured by a possibly moving camera . Our goal is to integrate tracking and fine segmentation of a single previously unseen potentially non rigid object of unconstrained appearance given its segmentation in the first frame of an image sequence as the only prior information . To this end we tightly couple an existing kernel based object tracking method with Random Walker based image segmentation . Bayesian inference mediates between tracking and segmentation enabling effective data fusion of pixel wise spatial and color visual cues . The fine segmentation of an object at a certain frame provides tracking with reliable initialization for the next frame closing the loop between the two building blocks of the proposed framework . The effectiveness of the proposed methodology is evaluated experimentally by comparing it to a large collection of state of the art tracking and video based object segmentation methods on the basis of a data set consisting of several challenging image sequences for which ground truth data is available . \\n'],\n",
              " [' Background and objectives Gene splicing is a vital source of protein diversity . Perfectly eradication of introns and joining exons is the prominent task in eukaryotic gene expression as exons are usually interrupted by introns . Identification of splicing sites through experimental techniques is complicated and time consuming task . With the avalanche of genome sequences generated in the post genomic age it remains a complicated and challenging task to develop an automatic robust and reliable computational method for fast and effective identification of splicing sites . Methods In this study a hybrid model iSS Hyb mRMR is proposed for quickly and accurately identification of splicing sites . Two sample representation methods namely pseudo trinucleotide composition and pseudo tetranucleotide composition were used to extract numerical descriptors from DNA sequences . Hybrid model was developed by concatenating PseTNC and PseTetraNC . In order to select high discriminative features minimum redundancy maximum relevance algorithm was applied on the hybrid feature space . The performance of these feature representation methods was tested using various classification algorithms including K nearest neighbor probabilistic neural network general regression neural network and fitting network . Jackknife test was used for evaluation of its performance on two benchmark datasets S 1 and S 2 respectively . Results The predictor proposed in the current study achieved an accuracy of 93.26 sensitivity of 88.77 and specificity of 97.78 for S 1 and the accuracy of 94.12 sensitivity of 87.14 and specificity of 98.64 for S 2 respectively . Conclusion It is observed that the performance of proposed model is higher than the existing methods in the literature so for and will be fruitful in the mechanism of RNA splicing and other research academia . \\n'],\n",
              " [' Facial expression is central to human experience . Its efficiency and valid measurement are challenges that automated facial image analysis seeks to address . Most publically available databases are limited to 2D static images or video of posed facial behavior . Because posed and un posed facial expressions differ along several dimensions including complexity and timing well annotated video of un posed facial behavior is needed . Moreover because the face is a three dimensional deformable object 2D video may be insufficient and therefore 3D video archives are required . We present a newly developed 3D video database of spontaneous facial expressions in a diverse group of young adults . Well validated emotion inductions were used to elicit expressions of emotion and paralinguistic communication . Frame level ground truth for facial actions was obtained using the Facial Action Coding System . Facial features were tracked in both 2D and 3D domains . To the best of our knowledge this new database is the first of its kind for the public . The work promotes the exploration of 3D spatiotemporal features in subtle facial expression better understanding of the relation between pose and motion dynamics in facial action units and deeper understanding of naturally occurring facial action . \\n'],\n",
              " [' Background and Objectives Light sharing PET detector configuration coupled with thick light guide and Geiger mode avalanche photodiode with large area microcells was proposed to overcome the energy non linearity problem and to obtain high light collection efficiency . Methods A Monte Carlo simulation was conducted for the three types of LSO block 4 4 array of 3 3 20 mm3 discrete crystals 6 6 array of 2 2 20 mm3 discrete crystals and 12 12 array of 1 1 20 mm3 discrete crystals to investigate the scintillation light distribution after conversion of the rays in LSO . The incident photons were read out by three types of 4 4 array photosensors which were PSPMT of 25 quantum efficiency GAPD1 with 50 50 m2 microcells of 30 photon detection efficiency and GAPD2 with 100 100 m2 of 45 PDE . The number of counted photons in each photosensor was analytically calculated . The LCE linearity and flood histogram were examined for each PET detector module having 99 different configurations as a function of light guide thickness ranging from 0 to 10 mm . Results The performance of PET detector modules based on GAPDs was considerably improved by using the thick light guide . The LCE was increased from 24 to 30 and from 14 to 41 and the linearity was also improved from 0.97 to 0.99 and from 0.75 to 0.99 for GAPD1 and GAPD2 respectively . As expected the performance of PSPMT based detector did not change . The flood histogram of 12 12 array PET detector modules using 3 mm light guide coupled with GAPDs was obtained by simulation and all crystals of 1 1 20 mm3 size were clearly identified . PET detector module coupled with thick light guide and GAPD array with large area microcells was proposed to obtain high QE and high spatial resolution and its feasibility was verified . Conclusions This study demonstrated that the overall PET performance of the proposed design was considerably improved and this approach will provide opportunities to develop GAPD based PET detector with a high LCE . \\n'],\n",
              " ['Range imaging sensors such as Kinect and time of flight cameras can produce aligned depth and color images in real time. However the depth maps captured by such sensors contain numerous invalid regions and suffer from heavy noise. These defects more or less influence the use of depth information in practical applications. In order to enhance the depth maps this paper proposes a new inpainting approach based on the fast marching method FMM . We extend the inpainting model and the propagation strategy of FMM to incorporate color information for depth inpainting. An edge preserving guided filter is further applied for noise reduction. To validate our algorithm we perform experiments on both Kinect data and Middlebury dataset which respectively provide qualitative and quantitative results. Meanwhile we also compare it to the original FMM and other two state of the art depth enhancement methods. Experimental results show that our method performs better than the local methods in terms of both visual and metric qualities and it achieves visually comparable results to the time consuming global method. \\n'],\n",
              " [' Motor unit action potential which consists of individual muscle fiber action potentials represents the electrical activity of the motor unit . The values of the MUAP features are changed by denervation and reinnervation in neurogenic involvement as well as muscle fiber loss with increased diameter variability in myopathic diseases . The present study is designed to investigate how increased muscle fiber diameter variability affects MUAP parameters in simulated motor units . In order to detect this variation simulated MUAPs were calculated both at the innervation zone where the MFAPs are more synchronized and near the tendon where they show increased temporal dispersion . Reinnervation in neurogenic state increases MUAP amplitude for the recordings at both the innervation zone and near the tendon . However MUAP duration and the number of peaks significantly increased in a case of myopathy for recordings near the tendon . Furthermore of the new features number of peaks spike duration was found as the strongest indicator of MFAP dispersion in myopathy . MUAPs were also recorded from healthy participants in order to investigate the biological counterpart of the simulation data . MUAPs which were recorded near to tendon revealed significantly prolonged duration and decreased amplitude . Although the number of peaks was increased by moving the needle near to tendon this was not significant . \\n'],\n",
              " [' Current telehealth services are dominated by conventional 2D video conferencing systems which are limited in their capabilities in providing a satisfactory communication experience due to the lack of realism . The immersiveness provided by 3D technologies has the potential to promote telehealth services to a wider range of applications . However conventional stereoscopic 3D technologies are deficient in many aspects including low resolution and the requirement for complicated multi camera setup and calibration and special glasses . The advent of light field photography enables us to record light rays in a single shot and provide glasses free 3D display with continuous motion parallax in a wide viewing zone which is ideally suited for 3D telehealth applications . As far as our literature review suggests there have been no reports of 3D telemedicine systems using LF technology . In this paper we propose a cross platform solution for a LF based 3D telemedicine system . Firstly a novel system architecture based on LF technology is established which is able to capture the LF of a patient and provide an immersive 3D display at the doctor site . For 3D modeling we further propose an algorithm which is able to convert the captured LF to a 3D model with a high level of detail . For the software implementation on different platforms a cross platform solution is proposed . Demo applications have been developed for 2D 3D video conferencing 3D model display and edit blood pressure and heart rate monitoring and patient data viewing functions . The demo software can be extended to multi discipline telehealth applications such as tele dentistry tele wound and tele psychiatry . The proposed 3D telemedicine solution has the potential to revolutionize next generation telemedicine technologies by providing a high quality immersive tele consultation experience . \\n'],\n",
              " [' An acetabular cup with larger abduction angles is able to affect the normal function of the cup seriously that may cause early failure of the total hip replacement . Complexity of the finite element simulation in the wear analysis of the THR is usually concerned with the contact status the computational effort and the possible divergence of results which become more difficult on THRs with larger cup abduction angles . In the study we propose a FE approach with contact transformation that offers less computational effort . Related procedures such as Lagrangian Multiplier partitioned matrix inversion detection of contact forces continuity of contact surface nodal area estimation etc . are explained in this report . Through the transformed methodology the computer round off error is tremendously reduced and the embedded repetitive procedure can be processed precisely and quickly . Here wear behaviors of THR with various abduction angles are investigated . The most commonly used combination i.e . metal on polyethylene is adopted in the current study where a cobalt chromium femoral head is paired with an Ultra High Molecular Weight Polyethylene cup . In all illustrations wear coefficients are estimated by self averaging strategy with available experimental datum reported elsewhere . The results reveal that the THR with larger abduction angles may produce deeper depth of wear but the volume of wear presents an opposite tendency these results are comparable with clinical and experimental reports . The current approach can be widely applied easily to fields such as the study of the wear behaviors on ante version impingement and time dependent behaviors of prostheses etc . \\n'],\n",
              " [' In the spirit of recent work on contextual recognition and estimation we present a method for estimating the pose of human hands employing information about the shape of the object in the hand . Despite the fact that most applications of human hand tracking involve grasping and manipulation of objects the majority of methods in the literature assume a free hand isolated from the surrounding environment . Occlusion of the hand from grasped objects does in fact often pose a severe challenge to the estimation of hand pose . In the presented method object occlusion is not only compensated for it contributes to the pose estimation in a contextual fashion this without an explicit model of object shape . Our hand tracking method is non parametric performing a nearest neighbor search in a large database of hand poses with and without grasped objects . The system that operates in real time is robust to self occlusions object occlusions and segmentation errors and provides full hand pose reconstruction from monocular video . Temporal consistency in hand pose is taken into account without explicitly tracking the hand in the high dim pose space . Experiments show the non parametric method to outperform other state of the art regression methods while operating at a significantly lower computational cost than comparable model based hand tracking methods . \\n'],\n",
              " [' In this paper we present a comparative study of two approaches for road traffic density estimation . The first approach uses the microscopic parameters which are extracted using both motion detection and tracking methods from a video sequence and the second approach uses the macroscopic parameters which are directly estimated by analyzing the global motion in the video scene . The extracted parameters are applied to three classifiers the K Nearest Neighbor classifier the LVQ classifier and the SVM classifier in order to classify the road traffic in three categories light medium and heavy . The methods are compared based on their robustness to the classification of different road traffic states . The goal of this study is to propose an algorithm for road traffic density estimation with a high precision . \\n'],\n",
              " [' Shape from focus is a passive technique widely used in image processing for obtaining depth maps . This technique is attractive since it only requires a single monocular camera with focus control thus avoiding correspondence problems typically found in stereo as well as more expensive capturing devices . However one of its main drawbacks is its poor performance when the change in the focus level is difficult to detect . Most research in SFF has focused on improving the accuracy of the depth estimation . Less attention has been paid to the problem of providing quality measures in order to predict the performance of SFF without prior knowledge of the recovered scene . This paper proposes a reliability measure aimed at assessing the quality of the depth map obtained using SFF . The proposed reliability measure analyzes the shape of the focus measure function and estimates the likelihood of obtaining an accurate depth estimation without any previous knowledge of the recovered scene . The proposed R measure is then applied for determining the image regions where SFF will not perform correctly in order to discard them . Experiments with both synthetic and real scenes are presented . \\n'],\n",
              " [' Background and objectives Mathematical models are suitable to simulate complex biological processes by a set of non linear differential equations . These simulation models can be used as an e learning tool in medical education . However in many cases these mathematical systems have to be treated numerically which is computationally intensive . The aim of the study was to develop a system for numerical simulation to be used in an online e learning environment . Methods In the software system the simulation is located on the server as a CGI application . The user selects the boundary conditions for the simulation on the browser . With these parameters the simulation on the server is started and the simulation result is re transferred to the browser . Results With this system two examples of e learning units were realized . The first one uses a multi compartment model of the glucose insulin control loop for the simulation of the plasma glucose level after a simulated meal or during diabetes . The second one simulates the ion transport leading to the resting and action potential in nerves . The student can vary parameters systematically to explore the biological behavior of the system . Conclusions The described system is able to simulate complex biological processes and offers the possibility to use these models in an online e learning environment . As far as the underlying principles can be described mathematically this type of system can be applied to a broad spectrum of biomedical or natural scientific topics . \\n'],\n",
              " ['Motion segmentation refers to the problem of separating the objects in a video sequence according to their motion. It is a fundamental problem of computer vision since various systems focusing on the analysis of dynamic scenes include motion segmentation algorithms. In this paper we present a novel approach where a video shot is temporally divided in successive and overlapping windows and motion segmentation is performed on each window respectively. This attribute renders the algorithm suitable even for long video sequences. In the last stage of the algorithm the segmentation results for every window are aggregated into a final segmentation. The presented algorithm can handle effectively asynchronous trajectories on each window even when they have no temporal intersection. The evaluation of the proposed algorithm on the Berkeley motion segmentation benchmark demonstrates its scalability and accuracy compared to the state of the art. \\n'],\n",
              " [' In this paper the unsteady pulsatile magneto hydrodynamic blood flows through porous arteries concerning the influence of externally imposed periodic body acceleration and a periodic pressure gradient are numerically simulated . Blood is taken into account as the third grade non Newtonian fluid . Besides the numerical solution for small Womersley parameter the analytical perturbation method is used to solve the nonlinear governing equations . Consequently analytical expressions for the velocity profile wall shear stress and blood flow rate are obtained . Excellent agreement between the analytical and numerical predictions is evident . Also the effects of body acceleration magnetic field third grade non Newtonian parameter pressure gradient and porosity on the flow behaviors are examined . Some important conclusions are that when the Womersley parameter is low viscous forces tend to dominate the flow velocity profiles are parabolic in shape and the center line velocity oscillates in phase with the driving pressure gradient . In addition by increasing the pressure gradient the mean value of the velocity profile increases and the amplitude of the velocity remains constant . Also when non Newtonian effect increases the amplitude of the velocity profile . \\n'],\n",
              " [' When estimating human gaze directions from captured eye appearances most existing methods assume a fixed head pose because head motion changes eye appearance greatly and makes the estimation inaccurate. To handle this difficult problem in this paper we propose a novel method that performs accurate gaze estimation without restricting the user s head motion. The key idea is to decompose the original free head motion problem into subproblems including an initial fixed head pose problem and subsequent compensations to correct the initial estimation biases. For the initial estimation automatic image rectification and joint alignment with gaze estimation are introduced. Then compensations are done by either learning based regression or geometric based calculation. The merit of using such a compensation strategy is that the training requirement to allow head motion is not significantly increased only capturing a 5 s video clip is required. Experiments are conducted and the results show that our method achieves an average accuracy of around 3 by using only a single camera. \\n'],\n",
              " [' Models that simulate land use patterns often use either inductive data driven approaches or deductive theory based methods to describe the relative strength of the social economic and biophysical forces that drive the various sectors in the land system . An integrated framework is proposed here that incorporates both approaches based on a unified assessment for local land suitability following a monetary utility based logic . The framework is illustrated with a hedonic pricing analysis of urban land values and a net present value assessment for agricultural production system in combination with statistics based assessments of land suitability for other sectors . The results show that limited difference exists between the most commonly applied inductive approaches that use either multinomial or binomial logistic regression specifications of suitability . Land use simulations following the binomial regression based suitability values that were rescaled to bid prices perform better for all individual land use types . Performance improves even further when a land value based description of urban bid prices is added to this approach . Interestingly enough the better fitting description of suitability for urban areas also improves the ability of the model to simulate correct locations for business estates and greenhouses . The simulation alternatives that consider the net present values for agricultural types of land use show the relevance of this approach for understanding the spatial distribution of these types of land use . The combined use of urban land values and net present values for agricultural land use in defining land suitability performs best in our validation exercise . The proposed methodology can also be used to incorporate information from other research frameworks that describe the utility of land for different types of use . \\n'],\n",
              " [' Discriminative human pose estimation is the problem of inferring the 3D articulated pose of a human directly from an image feature . This is a challenging problem due to the highly non linear and multi modal mapping from the image feature space to the pose space . To address this problem we propose a model employing a mixture of Gaussian processes where each Gaussian process models a local region of the pose space . By employing the models in this way we are able to overcome the limitations of Gaussian processes applied to human pose estimation their O time complexity and their uni modal predictive distribution . Our model is able to give a multi modal predictive distribution where each mode is represented by a different Gaussian process prediction . A logistic regression model is used to give a prior over each expert prediction in a similar fashion to previous mixture of expert models . We show that this technique outperforms existing state of the art regression techniques on human pose estimation data sets for ballet dancing sign language and the HumanEva data set . \\n'],\n",
              " ['Cheap ubiquitous high resolution digital cameras have led to opportunities that demand camera based text understanding such as wearable computing or assistive technology. Perspective distortion is one of the main challenges for text recognition in camera captured images since the camera may often not have a fronto parallel view of the text. We present a method for perspective recovery of text in natural scenes where text can appear as isolated words short sentences or small paragraphs as found on posters billboards shop and street signs etc. . It relies on the geometry of the characters themselves to estimate a rectifying homography for every line of text irrespective of the view of the text over a large range of orientations. The horizontal perspective foreshortening is corrected by fitting two lines to the top and bottom of the text while the vertical perspective foreshortening and shearing are estimated by performing a linear regression on the shear variation of the individual characters within the text line. The proposed method is efficient and fast. We present comparative results with improved recognition accuracy against the current state of the art. \\n'],\n",
              " [' Avoiding the use of complicated pre processing steps such as accurate face and body part segmentation or image normalization this paper proposes a novel face person image representation which can properly handle background and illumination variations . Denoted as gBiCov this representation relies on the combination of Biologically Inspired Features and Covariance descriptors . More precisely gBiCov is obtained by computing and encoding the difference between BIF features at different scales . The distance between two persons can then be efficiently measured by computing the Euclidean distance of their signatures avoiding some time consuming operations in Riemannian manifold required by the use of Covariance descriptors . In addition the recently proposed KISSME framework is adopted to learn a metric adapted to the representation . To show the effectiveness of gBiCov experiments are conducted on three person re identification tasks and one face verification task on which competitive results are obtained . As an example the matching rate at rank 1 on the VIPeR dataset is of 31.11 improving the best previously published result by more than 10 . \\n'],\n",
              " [' Many recent image retrieval methods are based on the bag of words model with some additional spatial consistency checking . This paper proposes a more accurate similarity measurement that takes into account spatial layout of visual words in an offline manner . The similarity measurement is embedded in the standard pipeline of the BoW model and improves two features of the model i latent visual words are added to a query based on spatial co occurrence to improve query recall and ii weights of reliable visual words are increased to improve the precision . The combination of these methods leads to a more accurate measurement of image similarity . This is similar in concept to the combination of query expansion and spatial verification but does not require query time processing which is too expensive to apply to full list of ranked results . Experimental results demonstrate the effectiveness of our proposed method on three public datasets . \\n'],\n",
              " [' Background and objective Percutaneous coronary interventional procedures need advance planning prior to stenting or an endarterectomy . Cardiologists use intravascular ultrasound for screening risk assessment and stratification of coronary artery disease . We hypothesize that plaque components are vulnerable to rupture due to plaque progression . Currently there are no standard grayscale IVUS tools for risk assessment of plaque rupture . This paper presents a novel strategy for risk stratification based on plaque morphology embedded with principal component analysis for plaque feature dimensionality reduction and dominant feature selection technique . The risk assessment utilizes 56 grayscale coronary features in a machine learning framework while linking information from carotid and coronary plaque burdens due to their common genetic makeup . Method This system consists of a machine learning paradigm which uses a support vector machine combined with PCA for optimal and dominant coronary artery morphological feature extraction . Carotid artery proven intima media thickness biomarker is adapted as a gold standard during the training phase of the machine learning system . For the performance evaluation K fold cross validation protocol is adapted with 20 trials per fold . For choosing the dominant features out of the 56 grayscale features a polling strategy of PCA is adapted where the original value of the features is unaltered . Different protocols are designed for establishing the stability and reliability criteria of the coronary risk assessment system . Results Using the PCA based machine learning paradigm and cross validation protocol a classification accuracy of 98.43 with K 10 folds using an SVM radial basis function kernel was achieved . A reliability index of 97.32 and machine learning stability criteria of 5 were met for the cRAS . Conclusions This is the first Computer aided design system of its kind that is able to demonstrate the ability of coronary risk assessment and stratification while demonstrating a successful design of the machine learning system based on our assumptions . \\n'],\n",
              " [' We introduce a robust framework for learning and fusing of orientation appearance models based on both texture and depth information for rigid object tracking . Our framework fuses data obtained from a standard visual camera and dense depth maps obtained by low cost consumer depth cameras such as the Kinect . To combine these two completely different modalities we propose to use features that do not depend on the data representation angles . More specifically our framework combines image gradient orientations as extracted from intensity images with the directions of surface normals computed from dense depth fields . We propose to capture the correlations between the obtained orientation appearance models using a fusion approach motivated by the original Active Appearance Models . To incorporate these features in a learning framework we use a robust kernel based on the Euler representation of angles which does not require off line training and can be efficiently implemented online . The robustness of learning from orientation appearance models is presented both theoretically and experimentally in this work . This kernel enables us to cope with gross measurement errors missing data as well as other typical problems such as illumination changes and occlusions . By combining the proposed models with a particle filter the proposed framework was used for performing 2D plus 3D rigid object tracking achieving robust performance in very difficult tracking scenarios including extreme pose variations . \\n'],\n",
              " [' Objectives The present work has the goal of developing a secure medical imaging information system based on a combined steganography and cryptography technique . It attempts to securely embed patient s confidential information into his her medical images . Methods The proposed information security scheme conceals coded Electronic Patient Records into medical images in order to protect the EPRs confidentiality without affecting the image quality and particularly the Region of Interest which is essential for diagnosis . The secret EPR data is converted into ciphertext using private symmetric encryption method . Since the Human Visual System is less sensitive to alterations in sharp regions compared to uniform regions a simple edge detection method has been introduced to identify and embed in edge pixels which will lead to an improved stego image quality . In order to increase the embedding capacity the algorithm embeds variable number of bits in edge pixels based on the strength of edges . Moreover to increase the efficiency two message coding mechanisms have been utilized to enhance the 1 steganography . The first one which is based on Hamming code is simple and fast while the other which is known as the Syndrome Trellis Code is more sophisticated as it attempts to find a stego image that is close to the cover image through minimizing the embedding impact . The proposed steganography algorithm embeds the secret data bits into the Region of Non Interest where due to its importance the ROI is preserved from modifications . Results The experimental results demonstrate that the proposed method can embed large amount of secret data without leaving a noticeable distortion in the output image . The effectiveness of the proposed algorithm is also proven using one of the efficient steganalysis techniques . Conclusion The proposed medical imaging information system proved to be capable of concealing EPR data and producing imperceptible stego images with minimal embedding distortions compared to other existing methods . In order to refrain from introducing any modifications to the ROI the proposed system only utilizes the Region of Non Interest in embedding the EPR data . \\n'],\n",
              " [' This paper presents a thorough study of gender classification methodologies performing on neutral expressive and partially occluded faces when they are used in all possible arrangements of training and testing roles . A comprehensive comparison of two representation approaches three types of features three classifiers and two performance measures is provided over single and cross database experiments . Experiments revealed some interesting findings which were supported by three non parametric statistical tests when training and test sets contain different types of faces local models using the 1 NN rule outperform global approaches even those using SVM classifiers however with the same type of faces even if the acquisition conditions are diverse the statistical tests could not reject the null hypothesis of equal performance of global SVMs and local 1 NNs . \\n'],\n",
              " [' The development of adequate mathematical models for blood glucose dynamics may improve early diagnosis and control of diabetes mellitus . We have developed a stochastic nonlinear second order differential equation to describe the response of blood glucose concentration to food intake using continuous glucose monitoring data . A variational Bayesian learning scheme was applied to define the number and values of the system s parameters by iterative optimisation of free energy . The model has the minimal order and number of parameters to successfully describe blood glucose dynamics in people with and without DM . The model accounts for the nonlinearity and stochasticity of the underlying glucose insulin dynamic process . Being data driven it takes full advantage of available CGM data and at the same time reflects the intrinsic characteristics of the glucose insulin system without detailed knowledge of the physiological mechanisms . We have shown that the dynamics of some postprandial blood glucose excursions can be described by a reduced model previously seen in the literature . A comprehensive analysis demonstrates that deterministic system parameters belong to different ranges for diabetes and controls . Implications for clinical practice are discussed . This is the first study introducing a continuous data driven nonlinear stochastic model capable of describing both DM and non DM profiles . \\n'],\n",
              " ['Robust high dimensional data processing has witnessed an exciting development in recent years. Theoretical results have shown that it is possible using convex programming to optimize data fit to a low rank component plus a sparse outlier component. This problem is also known as robust PCA and it has found application in many areas of computer vision. In image and video processing and face recognition the opportunity to process massive image databases is emerging as people upload photo and video data online in unprecedented volumes. However data quality and consistency is not controlled in any way and the massiveness of the data poses a serious computational challenge. In this paper we present t GRASTA or Transformed GRASTA Grassmannian robust adaptive subspace tracking algorithm . t GRASTA iteratively performs incremental gradient descent constrained to the Grassmann manifold of subspaces in order to simultaneously estimate three components of a decomposition of a collection of images a low rank subspace a sparse part of occlusions and foreground objects and a transformation such as rotation or translation of the image. We show that t GRASTA is 4 faster than state of the art algorithms has half the memory requirement and can achieve alignment for face images as well as jittered camera surveillance images. \\n'],\n",
              " [' This work provides a performance comparison of four different machine learning classifiers multinomial logistic regression with ridge estimators classifier k nearest neighbours support vector machine and na ve Bayes as applied to terahertz transient time domain sequences associated with pixelated images of different powder samples . The six substances considered although have similar optical properties their complex insertion loss at the THz part of the spectrum is significantly different because of differences in both their frequency dependent THz extinction coefficient as well as differences in their refractive index and scattering properties . As scattering can be unquantifiable in many spectroscopic experiments classification solely on differences in complex insertion loss can be inconclusive . The problem is addressed using two dimensional cross correlations between background and sample interferograms these ensure good noise suppression of the datasets and provide a range of statistical features that are subsequently used as inputs to the above classifiers . A cross validation procedure is adopted to assess the performance of the classifiers . Firstly the measurements related to samples that had thicknesses of 2mm were classified then samples at thicknesses of 4mm and after that 3mm were classified and the success rate and consistency of each classifier was recorded . In addition mixtures having thicknesses of 2 and 4mm as well as mixtures of 2 3 and 4mm were presented simultaneously to all classifiers . This approach provided further cross validation of the classification consistency of each algorithm . The results confirm the superiority in classification accuracy and robustness of the MLR and KNN algorithms which consistently outperformed the SVM and NB classifiers for the same number of feature vectors across all studies . The work establishes a general methodology for assessing the performance of other hyperspectral dataset classifiers on the basis of 2 D cross correlations in far infrared spectroscopy or other parts of the electromagnetic spectrum . It also advances the wider proliferation of automated THz imaging systems across new application areas e.g . biomedical imaging industrial processing and quality control where interpretation of hyperspectral images is still under development . \\n'],\n",
              " [' Graphical abstract Figure illustrates the role of person re identification in a typical surveillance scenario . An area monitored by multiple cameras is depicted by top view of a building floor plan and the relative placement of the cameras with respect to the building . Colored dots depict different people and numbers besides the dots are the IDs assigned to the people . As a person moves from one camera s FOV into another camera s FOV re identification is required to establish correspondence between disconnected tracks to accomplish multiple camera tracking . This paper explores the problem of person re identification and discusses the current solutions . Open issues and challenges of the problem are highlighted with a discussion on potential directions for further research . \\n'],\n",
              " [' We propose a novel symmetry driven Bayesian framework to incorporate structural shape into conventional geometrical shape descriptor of an image indexing and retrieval . We use rotation and reflection symmetries for structural shape description . Symmetry detection on each shape image provides a qualitative and a quantitative categorization of the types and the degrees of symmetry level . The posterior shape similarity enhances the shape matching performance based on the symmetry structural discrimination . Experimental results show statistically significant improvement on retrieval accuracy over the state of the art methods on MPEG 7 data set . \\n'],\n",
              " ['Recently Universum data that does not belong to any class of the training data has been applied for training better classifiers. In this paper we address a novel boosting algorithm called AdaBoost that can improve the classification performance of AdaBoost with Universum data. AdaBoost chooses a function by minimizing the loss for labeled data and Universum data. The cost function is minimized by a greedy stagewise functional gradient procedure. Each training stage of AdaBoost is fast and efficient. The standard AdaBoost weights labeled samples during training iterations while AdaBoost gives an explicit weighting scheme for Universum samples as well. In addition this paper describes the practical conditions for the effectiveness of Universum learning. These conditions are based on the analysis of the distribution of ensemble predictions over training samples. Experiments on handwritten digits classification and gender classification problems are presented. As exhibited by our experimental results the proposed method can obtain superior performances over the standard AdaBoost by selecting proper Universum data. \\n'],\n",
              " [' The purpose of this study was to evaluate the use of fractional order modeling in asthma . To this end three FrOr models were compared with traditional parameters and an integer order model . We investigated which model would best fit the data the correlation with traditional lung function tests and the contribution to the diagnostic of airway obstruction . The data consisted of forced oscillation measurements obtained from healthy and asthmatic volunteers with mild moderate and severe obstructions . The first part of this study showed that a FrOr was the model that best fit the data . The correlation analysis resulted in reasonable to very good associations between FrOr parameters and spirometry . The closest associations were observed between parameters related to peripheral airway obstruction showing a clear relationship between the FrOr models and lung mechanics . Receiver operator analysis showed that FrOr parameters presented a high potential to contribute to the detection of the mild obstruction in a clinical setting . The accuracy observed in these parameters was higher than that observed in traditional FO parameters and that obtained from the InOr model . Patients with moderate and severe obstruction were identified with high accuracy . In conclusion the results obtained are in close agreement with asthma pathology and provide evidence that FO measurement associated with FrOr models is a non invasive simple and radiation free method for the detection of biomechanical abnormalities in asthma . \\n'],\n",
              " [' Today smart mobile devices are very commonly used due to their powerful hardware and useful features . According to an eMarketer report in 2014 there were 1.76 billion smartphone users in the world it is predicted that this number will rise by 15.9 to 2.04 billion in 2015 . It is thought that these devices can be used successfully in biomedical applications . A wireless blood pressure measuring device used together with a smart mobile device was developed in this study . By means of an interface developed for smart mobile devices with Android and iOS operating systems a smart mobile device was used both as an indicator and as a control device . The cuff communicating with this device through Bluetooth was designed to measure blood pressure via the arm . A digital filter was used on the cuff instead of the traditional analog signal processing and filtering circuit . The newly developed blood pressure measuring device was tested on 18 patients and 20 healthy individuals of different ages under a physician s supervision . When the test results were compared with the measurements made using a sphygmomanometer it was shown that an average 93.52 accuracy in sick individuals and 94.53 accuracy in healthy individuals could be achieved with the new device . \\n'],\n",
              " ['This paper proposes a weighted scheme for elastic graph matching hand posture recognition. Visual features scattered on the elastic graph are assigned corresponding weights according to their relative ability to discriminate between gestures. The weights values are determined using adaptive boosting. A dictionary representing the variability of each gesture class is expressed in the form of a bunch graph. The positions of the nodes in the bunch graph are determined using three techniques manually semi automatically and automatically. Experimental results also show that the semi automatic annotation method is efficient and accurate in terms of three performance measures assignment cost accuracy and transformation error. In terms of the recognition accuracy our results show that the hierarchical weighting on features has more significant discriminative power than the classic method uniform weighting . The hierarchical elastic graph matching WEGM approach was used to classify a lexicon of ten hand postures and it was found that the poses were recognized with a recognition accuracy of 97.08 on average. Using the weighted scheme computing cycles can be decreased by only computing the features for those nodes whose weight is relatively high and ignoring the remaining nodes. It was found that only 30 of the nodes need to be computed to obtain a recognition accuracy of over 90 . \\n'],\n",
              " [' This paper proposes an unsupervised variational segmentation approach of color texture images . To improve the description ability the compact multi scale structure tensor total variation flow and color information are integrated to extract color texture information . Since heterogeneous image object and nonlinear variation exist in color texture image it is not appropriate to use one single multiple constant in the Chan and Vese model to describe each phase . Therefore a multiphase successive active contour model based on the multivariable Gaussian distribution is presented to describe each phase . As geodesic active contour has a stronger ability in capturing boundary . To inherit the advantages of edge based model and region based model we incorporate the GAC into the MSACM to enhance the detection ability for concave edge . Although multiphase optimization of our proposed MSACM is a NP hard problem we can discretely and approximately solve it by a multilayer graph method . In addition to segment the color texture image automatically an adaptive iteration convergence criterion is designed by incorporating the local Kullback Leibler distance and global phase label so that we can control the segmentation process converges . Comparing to state of the art unsupervised segmentation methods on a substantial of color texture images our approach achieves a significantly better performance on capture ability of homogeneous region smooth boundary and accuracy . \\n'],\n",
              " [' The present work attempts to build a bio cryptographic system that combines transformed minutiae pairwise feature and user generated password fuzzy vault . The fingerprint fuzzy vault is based on a new minutiae pairwise structure which overcomes the fingerprint feature publication while the secret binary vault code is generated according to the fingerprint fuzzy vault result . The authentication process involves two stages fuzzy vault matching and secret vault code validation . Our minutiae pairwise transformation produces different templates thus resolving the problem of cross matching attacks in fingerprint fuzzy vault . So the original fingerprint template can not be recreated because it is protected by the key generated from the user password . In addition the proposed bio cryptographic system ensures an acceptable security level for user authentication . \\n'],\n",
              " [' This paper presents a new heuristic algorithm for reduct selection based on credible index in the rough set theory applications . This algorithm is efficient and effective in selecting the decision rules particularly the problem to be solved in a large scale . This algorithm is capable to derive the rules with multi outcomes and identify the most significant features simultaneously which is unique and useful in solving predictive medical problems . The end results of the proposed approach are a set of decision rules that illustrates the causes for solitary pulmonary nodule and results of the long term treatment . \\n'],\n",
              " [' Human Nonverbal Communication Computing aims to investigate how people exploit nonverbal aspects of their communication to coordinate their activities and social relationships . Nonverbal behavior plays important roles in message production and processing relational communication social interaction and networks deception and impression management and emotional expression . This is a fundamental yet challenging research topic . To effectively analyze Nonverbal Communication Computing motion analysis methods have been widely investigated and employed . In this paper we introduce the concept and applications of Nonverbal Communication Computing and also review some of the motion analysis methods employed in this area . They include face tracking expression recognition body reconstruction and group activity analysis . In addition we also discuss some open problems and the future directions of this area . \\n'],\n",
              " [' In this paper we present a robust and lightweight method for the automatic fitting of deformable 3D face models on facial images . Popular fitting techniques such as those based on statistical models of shape and appearance require a training stage based on a set of facial images and their corresponding facial landmarks which have to be manually labeled . Therefore new images in which to fit the model can not differ too much in shape and appearance from those used for training . By contrast our approach can fit a generic face model in two steps the detection of facial features based on local image gradient analysis and the backprojection of a deformable 3D face model through the optimization of its deformation parameters . The proposed approach can retain the advantages of both learning free and learning based approaches . Thus we can estimate the position orientation shape and actions of faces and initialize user specific face tracking approaches such as Online Appearance Models which have shown to be more robust than generic user tracking approaches . Experimental results show that our method outperforms other fitting alternatives under challenging illumination conditions and with a computational cost that allows its implementation in devices with low hardware specifications such as smartphones and tablets . Our proposed approach lends itself nicely to many frameworks addressing semantic inference in face images and videos . \\n'],\n",
              " ['Building facade detection is an important problem in computer vision with applications in mobile robotics and semantic scene understanding. In particular mobile platform localization and guidance in urban environments can be enabled with accurate models of the various building facades in a scene. Toward that end we present a system for detection segmentation and parameter estimation of building facades in stereo imagery. The proposed method incorporates multilevel appearance and disparity features in a binary discriminative model and generates a set of candidate planes by sampling and clustering points from the image with Random Sample Consensus RANSAC using local normal estimates derived from Principal Component Analysis PCA to inform the planar models. These two models are incorporated into a two layer Markov Random Field MRF an appearance and disparity based discriminative classifier at the mid level and a geometric model to segment the building pixels into facades at the high level. By using object specific stereo features our discriminative classifier is able to achieve substantially higher accuracy than standard boosting or modeling with only appearance based features. Furthermore the results of our MRF classification indicate a strong improvement in accuracy for the binary building detection problem and the labeled planar surface models provide a good approximation to the ground truth planes. \\n'],\n",
              " [' Segregation models often focus on private racial preference but overlook the institutional context . This paper represents an effort to move beyond the preference centricity . In this paper an ideal Pigovian regulatory intervention is emulated and added into Schelling s classic spatial proximity model of racial segregation with an aim to preserve collective welfare against the negative externalities induced by the changing local racial compositions after individual relocations . A key discovery from a large number of cellular automata is that the Pigovian regulation tends to result in less segregated but also less efficient residential patterns than laissez faire . This finding albeit from a highly stylized model bears intellectual relations to an important practical question What are the potential racial effects of Pigovian local planning interventions such as financially motivated anti density zoning or the collection of a development impact fee On top of its modest policy implications this paper demonstrates a bottom up computational modelling approach to reconcile the preference based and institution orientated academic perspectives regarding racial residential segregation . \\n'],\n",
              " [' Over the last few years much online volunteered geographic information has emerged and has been increasingly analyzed to understand places and cities as well as human mobility and activity . However there are concerns about the quality and usability of such VGI . In this study we demonstrate a complete process that comprises the collection unification classification and validation of a type of VGI online point of interest data and develop methods to utilize such POI data to estimate disaggregated land use at a very high spatial resolution using part of the Boston metropolitan area as an example . With recent advances in activity based land use transportation and environment models such disaggregated land use data become important to allow LUTE models to analyze and simulate a person s choices of work location and activity destinations and to understand policy impacts on future cities . These data can also be used as alternatives to explore economic activities at the local level especially as government published census based disaggregated employment data have become less available in the recent decade . Our new approach provides opportunities for cities to estimate land use at high resolution with low cost by utilizing VGI while ensuring its quality with a certain accuracy threshold . The automatic classification of POI can also be utilized for other types of analyses on cities . \\n'],\n",
              " [' Background and objective Retinal blood vessel segmentation is a prominent task for the diagnosis of various retinal pathology such as hypertension diabetes glaucoma etc . In this paper a novel matched filter approach with the Gumbel probability distribution function as its kernel is introduced to improve the performance of retinal blood vessel segmentation . Methods Before applying the proposed matched filter the input retinal images are pre processed . During pre processing stage principal component analysis based gray scale conversion followed by contrast limited adaptive histogram equalization are applied for better enhancement of retinal image . After that an exhaustive experiments have been conducted for selecting the appropriate value of parameters to design a new matched filter . The post processing steps after applying the proposed matched filter include the entropy based optimal thresholding and length filtering to obtain the segmented image . Results For evaluating the performance of proposed approach the quantitative performance measures an average accuracy average true positive rate and average false positive rate are calculated . The respective values of the quantitative performance measures are 0.9522 0.7594 0.0292 for DRIVE data set and 0.9270 0.7939 0.0624 for STARE data set . To justify the effectiveness of proposed approach receiver operating characteristic curve is plotted and the average area under the curve is calculated . The average AUC for DRIVE and STARE data sets are 0.9287 and 0.9140 respectively . Conclusions The obtained experimental results confirm that the proposed approach performance better with respect to other prominent Gaussian distribution function and Cauchy PDF based matched filter approaches . \\n'],\n",
              " [' We present a method for the recognition of complex actions . Our method combines automatic learning of simple actions and manual definition of complex actions in a single grammar . Contrary to the general trend in complex action recognition that consists in dividing recognition into two stages our method performs recognition of simple and complex actions in a unified way . This is performed by encoding simple action HMMs within the stochastic grammar that models complex actions . This unified approach enables a more effective influence of the higher activity layers into the recognition of simple actions which leads to a substantial improvement in the classification of complex actions . We consider the recognition of complex actions based on person transits between areas in the scene . As input our method receives crossings of tracks along a set of zones which are derived using unsupervised learning of the movement patterns of the objects in the scene . We evaluate our method on a large dataset showing normal suspicious and threat behaviour on a parking lot . Experiments show an improvement of 30 in the recognition of both high level scenarios and their composing simple actions with respect to a two stage approach . Experiments with synthetic noise simulating the most common tracking failures show that our method only experiences a limited decrease in performance when moderate amounts of noise are added . \\n'],\n",
              " [' The estimation of camera orientation from image lines using the anthropic environment restriction is a well known problem but traditional methods to solve it depend on line extraction a relatively complex procedure that is also incompatible with distorted images . We propose Corisco a monocular orientation estimation method based on edgels instead of lines . Edgels are points sampled from image edges with their tangential directions extracted in Corisco using a grid mask . The estimation aligns the measured edgel directions with the predicted directions calculated from the orientation using a known camera model . Corisco uses the M estimation technique to define an objective function that is optimized by two algorithms in sequence RANSAC which gives robustness and flexibility to Corisco and FilterSQP which performs a continuous optimization to refine the initial estimate using closed formulas for the function derivatives . Corisco is the first edgel based method able to analyze images with any camera model and it also allows for a compromise between speed and accuracy so that its performance can be tuned according to the application requirements . Our experiments demonstrate the effectiveness of Corisco with various camera models and its performance surpasses similar edgel based methods . The accuracy displayed a mean error below 2 for execution times above 8s in a conventional computer and above 3 for less than 2s . \\n'],\n",
              " [' Background and objective Neuroimaging studies have demonstrated dysfunction in the brain reward circuit in individuals with online gaming addiction . We hypothesized that virtual reality therapy for OGA would improve the functional connectivity of the cortico striatal limbic circuit by stimulating the limbic system . Methods Twenty four adults with OGA were randomly assigned to a cognitive behavior therapy group or VRT group . Before and after the four week treatment period the severity of OGA was evaluated with Young s Internet Addiction Scale . Using functional magnetic resonance imaging the amplitude of low frequency fluctuation and FC from the posterior cingulate cortex seed to other brain areas were evaluated . Twelve casual game users were also recruited and underwent only baseline assessment . Results After treatment both CBT and VRT groups showed reductions in YIAS scores . At baseline the OGA group showed a smaller ALFF within the right middle frontal gyrus and reduced FC in the cortico striatal limbic circuit . In the VRT group connectivity from the PCC seed to the left middle frontal and bilateral temporal lobe increased after VRT . Conclusion VRT seemed to reduce the severity of OGA showing effects similar to CBT and enhanced the balance of the cortico striatal limbic circuit . \\n'],\n",
              " [' Background and objective Classification of gene expression data is the common denominator of various biomedical recognition tasks . However obtaining class labels for large training samples may be difficult or even impossible in many cases . Therefore semi supervised classification techniques are required as semi supervised classifiers take advantage of unlabeled data . Methods Gene expression data is high dimensional which gives rise to the phenomena known under the umbrella of the curse of dimensionality one of its recently explored aspects being the presence of hubs or hubness for short . Therefore hubness aware classifiers have been developed recently such as Naive Hubness Bayesian k Nearest Neighbor . In this paper we propose a semi supervised extension of NHBNN which follows the self training schema . As one of the core components of self training is the certainty score we propose a new hubness aware certainty score . Results We performed experiments on publicly available gene expression data . These experiments show that the proposed classifier outperforms its competitors . We investigated the impact of each of the components separately and showed that each of these components are relevant to the performance of the proposed approach . Conclusions Our results imply that our approach may increase classification accuracy and reduce computational costs . Based on the promising results presented in the paper we envision that hubness aware techniques will be used in various other biomedical machine learning tasks . In order to accelerate this process we made an implementation of hubness aware machine learning techniques publicly available in the PyHubs software package implemented in Python one of the most popular programming languages of data science . the set of k nearest neighbors of x. probability that x belongs to class C given its nearest neighbors the probability of the event that x appears as one of the k nearest neighbors of any labeled training instance belonging to class C the prior probability of the event that an instance belongs to class C how many times x occurs as one of the k nearest neighbors of labeled training instances belonging to class C how many times x occurs as one of the k nearest neighbors of other instances when considering lab \\n'],\n",
              " [' This paper presents a novel approach for action recognition localization and video matching based on a hierarchical codebook model of local spatio temporal video volumes. Given a single example of an activity as a query video the proposed method finds similar videos to the query in a target video dataset. The method is based on the bag of video words BOV representation and does not require prior knowledge about actions background subtraction motion estimation or tracking. It is also robust to spatial and temporal scale changes as well as some deformations. The hierarchical algorithm codes a video as a compact set of spatio temporal volumes while considering their spatio temporal compositions in order to account for spatial and temporal contextual information. This hierarchy is achieved by first constructing a codebook of spatio temporal video volumes. Then a large contextual volume containing many spatio temporal volumes ensemble of volumes is considered. These ensembles are used to construct a probabilistic model of video volumes and their spatio temporal compositions. The algorithm was applied to three available video datasets for action recognition with different complexities KTH Weizmann and MSR II and the results were superior to other approaches especially in the case of a single training example and cross dataset1 action recognition. \\n'],\n",
              " ['The relationship between nonverbal behavior and severity of depression was investigated by following depressed participants over the course of treatment and video recording a series of clinical interviews. Facial expressions and head pose were analyzed from video using manual and automatic systems. Both systems were highly consistent for FACS action units AUs and showed similar effects for change over time in depression severity. When symptom severity was high participants made fewer affiliative facial expressions AUs 12 and 15 and more non affiliative facial expressions AU 14 . Participants also exhibited diminished head motion i.e. amplitude and velocity when symptom severity was high. These results are consistent with the Social Withdrawal hypothesis that depressed individuals use nonverbal behavior to maintain or increase interpersonal distance. As individuals recover they send more signals indicating a willingness to affiliate. The finding that automatic facial expression analysis was both consistent with manual coding and revealed the same pattern of findings suggests that automatic facial expression analysis may be ready to relieve the burden of manual coding in behavioral and clinical science. \\n'],\n",
              " [' Engineers have proposed many watermark mechanisms for protecting the content of digital media from unauthorized use . The visible watermark scheme indicates the copyright of digital media posted over the Internet by embedding an inconspicuous but recognizable pattern into media . However the embedding process often results in serious distortion of the protected image . Since the strength of the watermark in conventional methods mainly depends on the feature of protected media this may lead to unsatisfactory transparency of watermarked images . This paper proposes a removable solution for visible watermark mechanism . By adopting the subsampling technique the method proposes a contrast adaptive strategy to solve this problem . This method can also guarantee the essentials of general visible watermark schemes . Experimental results show that the proposed method outperforms related works in terms of preserving the quality of the restored image . \\n'],\n",
              " [' Equivalence testing is recommended as a better alternative to the traditional difference based methods for demonstrating the comparability of two or more treatment effects . Although equivalent tests of two groups are widely discussed the natural extensions for assessing equivalence between several groups have not been well examined . This article provides a detailed and schematic comparison of the ANOVA F and the studentized range tests for evaluating the comparability of several standardized effects . Power and sample size appraisals of the two grossly distinct approaches are conducted in terms of a constraint on the range of the standardized means when the standard deviation of the standardized means is fixed . Although neither method is uniformly more powerful the studentized range test has a clear advantage in sample size requirements necessary to achieve a given power when the underlying effect configurations are close to the priori minimum difference for determining equivalence . For actual application of equivalence tests and advance planning of equivalence studies both SAS and R computer codes are available as supplementary files to implement the calculations of critical values p values power levels and sample sizes . \\n'],\n",
              " ['Increasingly realistic virtual three dimensional 3D models have been created that demonstrate a variety of landscape designs. They have supported a more collaborative and participative approach in planning and design. However these 3D landscape models are often developed for use in bespoke virtual reality labs that tie the models to expensive graphics hardware or complex arrays of screens with the viewer spatially detached from the actual site. Given the increase in prevalence of advanced smartphone and tablet technology with GPS and compass functionality this paper demonstrates two methods for on demand dissemination of existing virtual 3D landscape models using 1 a touch based interface with integrated mapping 2 a standard web browser interface on mobile phones. The latter method demonstrates the potential to reduce the complexity of accessing an existing 3D landscape model on site to simply pointing a smartphone in a particular direction loading a web page and seeing the relevant view of the model as an image. A prototype system was developed to demonstrate both methods successfully but it was also ascertained that the accuracy of GPS positional data can have a negative effect on the browser based method. Finally potential developments are presented exploring the future of the technology underpinning the method and possible extensions to the prototype as a technique for increasing public participation in planning and design. \\n'],\n",
              " [' Social media data are increasingly perceived as alternative sources to public attitude surveys because of the volume of available data that are time stamped and precisely located . Such data can be mined to provide planners marketers and researchers with useful information about activities and opinions across time and space . However in their raw form textual data are still difficult to analyse coherently and Twitter streams pose particular interpretive challenges because they are restricted to just 140 characters . This paper explores the use of an unsupervised learning algorithm to classify geo tagged Tweets from Inner London recorded during typical weekdays throughout 2013 into a small number of groups following extensive text cleaning techniques . Our classification identifies 20 distinctive and interpretive topic groupings which represent key types of Tweets from describing activities or informal conversations between users to the use of check in applets . Our motivation is to use the classification to demonstrate how the nature of the content posted on Twitter varies according to the characteristics of places and users . Topics and attitudes expressed through Tweets are found to vary substantially across Inner London and by time of day . Some observed variations in behaviour on Twitter can be attributed to the inferred demographic and socio economic characteristics of users but place and local activities can also exert a considerable influence . Overall the classification was found to provide a valuable framework for investigating the content and coverage of Twitter usage across Inner London . \\n'],\n",
              " [' This paper proposes a method for keyword spotting in off line Chinese handwritten documents using a contextual word model which measures the similarity between the query word and every candidate word in the document by combining a character classifier and the geometric context as well as linguistic context . The geometric context model characterizes the single character likeliness and between character relationship . The linguistic model utilizes the dependency of the word with the external adjacent characters . The combining weights are optimized on training documents . Experiments on a large handwriting database CASIA HWDB demonstrate the effectiveness of the proposed method and justify the benefits of geometric and linguistic contexts . Compared to transcription based text search the proposed method can provide higher recall rate and for spotting words of four characters the proposed method provides both higher precision and recall rate . \\n'],\n",
              " [' Background and objectives Mammography analysis is an effective technology for early detection of breast cancer . Micro calcification clusters are a vital indicator of breast cancer so detection of MCs plays an important role in computer aided detection system this paper proposes a new hybrid method to improve MCs detection rate in mammograms . Methods The proposed method comprises three main steps firstly remove label and pectoral muscle adopting the largest connected region marking and region growing method and enhance MCs using the combination of double top hat transform and grayscale adjustment function secondly remove noise and other interference information and retain the significant information by modifying the contourlet coefficients using nonlinear function thirdly we use the non linking simplified pulse coupled neural network to detect MCs . Results In our work we choose 118 mammograms including 38 mammograms with micro calcification clusters and 80 mammograms without micro calcification to demonstrate our algorithm separately from two open and common database including the MIAS and JSMIT and we achieve the higher specificity of 94.7 sensitivity of 96.3 AUC of 97.0 accuracy of 95.8 MCC of 90.4 MCC PS of 61.3 and CEI of 53.5 these promising results clearly demonstrate that the proposed approach outperforms the current state of the art algorithms . In addition this method is verified on the 20 mammograms from the People s Hospital of Gansu Province the detection results reveal that our method can accurately detect the calcifications in clinical application . Conclusions This proposed method is simple and fast furthermore it can achieve high detection rate it could be considered used in CAD systems to assist the physicians for breast cancer diagnosis in the future . \\n'],\n",
              " [' This paper presents a novel skeleton pruning approach based on a 2D empirical mode like decomposition . The EMD algorithm can decompose any nonlinear and non stationary data into a number of intrinsic mode functions . When the object contour is decomposed by empirical mode like decomposition the IMFs of the object provide a workspace with very good properties for obtaining the object s skeleton . The theoretical properties and the performed experiments demonstrate that the obtained skeletons match to hand labeled skeletons provided by human subjects . Even in the presence of significant noise and shape variations cuts and tears the resulted skeletons have the same topology as the original skeletons . In particular the proposed approach produces no spurious branches as many existing skeleton pruning methods and moreover does not displace the skeleton points which are all centers of maximal disks . \\n'],\n",
              " [' Human faces encode plenty of useful information . Recent studies in psychology and human perception have found that facial features have relations to human weight or body mass index . These studies focus on finding the correlations between facial features and the BMI . Motivated by the recent psychology studies we develop a computational method to predict the BMI from face images automatically . We formulate the BMI prediction from facial features as a machine vision problem and evaluate our approach on a large database with more than 14 500 face images . A promising result has been obtained which demonstrates the feasibility of developing a computational system for BMI prediction from face images at a large scale . \\n'],\n",
              " ['Using image hierarchies for visual categorization has been shown to have a number of important benefits. Doing so enables a significant gain in efficiency e.g. logarithmic with the number of categories 16 12 or the construction of a more meaningful distance metric for image classification 17 . A critical question however still remains controversial would structuring data in a hierarchical sense also help classification accuracy In this paper we address this question and show that the hierarchical structure of a database can be indeed successfully used to enhance classification accuracy using a sparse approximation framework. We propose a new formulation for sparse approximation where the goal is to discover the sparsest path within the hierarchical data structure that best represents the query object. Extensive quantitative and qualitative experimental evaluation on a number of branches of the Imagenet database 7 as well as on the Caltech 256 12 demonstrate our theoretical claims and show that our approach produces better hierarchical categorization results than competing techniques. \\n'],\n",
              " [' This paper presents a tool for automatic assessment of skeletal bone age according to a modified version of the Tanner and Whitehouse clinical method . The tool is able to provide an accurate bone age assessment in the range 0 6 years by processing epiphysial metaphysial ROIs with image processing techniques and assigning TW2 stage to each ROI by means of hidden Markov models . The system was evaluated on a set of 360 X rays achieving a high success rate in bone age evaluation as well as outperforming other effective methods . The paper also describes the graphical user interface of the tool which is also released thus to support and speed up clinicians practices when dealing with bone age assessment . \\n'],\n",
              " ['This article discusses the motion analysis based on dense optical flow fields and for a new generation of robotic moving systems with real time constraints. It focuses on a surveillance scenario where an especially designed autonomous mobile robot uses a monocular camera for perceiving motion in the environment. The computational resources and the processing time are two of the most critical aspects in robotics and therefore two non parametric techniques are proposed namely the Hybrid Hierarchical Optical Flow Segmentation and the Hybrid Density Based Optical Flow Segmentation. Both methods are able to extract the moving objects by performing two consecutive operations refining and collecting. During the refining phase the flow field is decomposed in a set of clusters and based on descriptive motion properties. These properties are used in the collecting stage by a hierarchical or density based scheme to merge the set of clusters that represent different motion models. In addition a model selection method is introduced. This novel method analyzes the flow field and estimates the number of distinct moving objects using a Bayesian formulation. The research evaluates the performance achieved by the methods in a realistic surveillance situation. The experiments conducted proved that the proposed methods extract reliable motion information in real time and without using specialized computers. Moreover the resulting segmentation is less computationally demanding compared to other recent methods and therefore they are suitable for most of the robotic or surveillance applications. \\n'],\n",
              " [' Background Structural changes of the brain s third ventricle have been acknowledged as an indicative measure of the brain atrophy progression in neurodegenerative and endocrinal diseases . To investigate the ventricular enlargement in relation to the atrophy of the surrounding structures shape analysis is a promising approach . However there are hurdles in modeling the third ventricle shape . First it has topological variations across individuals due to the inter thalamic adhesion . In addition as an interhemispheric structure it needs to be aligned to the midsagittal plane to assess its asymmetric and regional deformation . Method To address these issues we propose a model based shape assessment . Our template model of the third ventricle consists of a midplane and a symmetric mesh of generic shape . By mapping the template s midplane to the individuals brain midsagittal plane we align the symmetric mesh on the midline of the brain before quantifying the third ventricle shape . To build the vertex wise correspondence between the individual third ventricle and the template mesh we employ a minimal distortion surface deformation framework . In addition to account for topological variations we implement geometric constraints guiding the template mesh to have zero width where the inter thalamic adhesion passes through preventing vertices crossing between left and right walls of the third ventricle . The individual shapes are compared using a vertex wise deformity from the symmetric template . Results Experiments on imaging and demographic data from a study of aging showed that our model was sensitive in assessing morphological differences between individuals in relation to brain volume gender and the fluid intelligence at age 72 . It also revealed that the proposed method can detect the regional and asymmetrical deformation unlike the conventional measures volume and width of the third ventricle . Similarity measures between binary masks and the shape model showed that the latter reconstructed shape details with high accuracy . Conclusions We have demonstrated that our approach is suitable to morphometrical analyses of the third ventricle providing high accuracy and inter subject consistency in the shape quantification . This shape modeling method with geometric constraints based on anatomical landmarks could be extended to other brain structures which require a consistent measurement basis in the morphometry . \\n'],\n",
              " [' A major difficulty with chest radiographic analysis is the invisibility of abnormalities caused by the superimposition of normal anatomical structures such as ribs over the main tissue to be examined . Suppressing the ribs with no information loss about the original tissue would therefore be helpful during manual identification or computer aided detection of nodules on a chest radiographic image . In this study we introduce a two step algorithm for eliminating rib shadows in chest radiographic images . The algorithm first delineates the ribs using a novel hybrid self template approach and then suppresses these delineated ribs using an unsupervised regression model that takes into account the change in proximal thickness of bone in the vertical axis . The performance of the system is evaluated using a benchmark set of real chest radiographic images . The experimental results determine that proposed method for rib delineation can provide higher accuracy than existing methods . The knowledge of rib delineation can remarkably improve the nodule detection performance of a current computer aided diagnosis system . It is also shown that the rib suppression algorithm can increase the nodule visibility by eliminating rib shadows while mostly preserving the nodule intensity . \\n'],\n",
              " [' Background In the last few years the use of social media in medicine has grown exponentially providing a new area of research based on the analysis and use of Web 2.0 capabilities . In addition the use of social media in medical education is a subject of particular interest which has been addressed in several studies . One example of this application is the medical quizzes of The New England Journal of Medicine that regularly publishes a set of questions through their Facebook timeline . Objective We present an approach for the automatic extraction of medical quizzes and their associated answers on a Facebook platform by means of a set of computer based methods and algorithms . Methods We have developed a tool for the extraction and analysis of medical quizzes stored on Facebook timeline at the NEJM Facebook page based on a set of computer based methods and algorithms using Java . The system is divided into two main modules Crawler and Data retrieval . Results The system was launched on December 31 2014 and crawled through a total of 3004 valid posts and 200 081 valid comments . The first post was dated on July 23 2009 and the last one on December 30 2014 . 285 quizzes were analyzed with 32 780 different users providing answers to the aforementioned quizzes . Of the 285 quizzes patterns were found in 261 . From these 261 quizzes where trends were found we saw that users follow trends of incorrect answers in 13 quizzes and trends of correct answers in 248 . Conclusions This tool is capable of automatically identifying the correct and wrong answers to a quiz provided on Facebook posts in a text format to a quiz with a small rate of false negative cases and this approach could be applicable to the extraction and analysis of other sources after including some adaptations of the information on the Internet . \\n'],\n",
              " [' Background and objective Probabilistic topic models provide an unsupervised method for analyzing unstructured text . These models discover semantically coherent combinations of words that could be integrated in a clinical automatic summarization system for primary care physicians performing chart review . However the human interpretability of topics discovered from clinical reports is unknown . Our objective is to assess the coherence of topics and their ability to represent the contents of clinical reports from a primary care physician s point of view . Methods Three latent Dirichlet allocation models were fit to a large collection of clinical reports . Topics were manually evaluated by primary care physicians and graduate students . Wilcoxon Signed Rank Tests for Paired Samples were used to evaluate differences between different topic models while differences in performance between students and primary care physicians were tested using Mann Whitney U tests for each of the tasks . Results While the 150 topic model produced the best log likelihood participants were most accurate at identifying words that did not belong in topics learned by the 100 topic model suggesting that 100 topics provides better relative granularity of discovered semantic themes for the data set used in this study . Models were comparable in their ability to represent the contents of documents . Primary care physicians significantly outperformed students in both tasks . Conclusion This work establishes a baseline of interpretability for topic models trained with clinical reports and provides insights on the appropriateness of using topic models for informatics applications . Our results indicate that PCPs find discovered topics more coherent and representative of clinical reports relative to students warranting further research into their use for automatic summarization . \\n'],\n",
              " [' Recently various non invasive tools such as the magnetic resonance image ultrasound imaging computed tomography and the computational fluid dynamics have been widely utilized to enhance our current understanding of the physiological parameters that affect the initiation and the progression of the cardiovascular diseases associated with heart failure . In particular the hemodynamics of left ventricle has attracted the attention of the researchers due to its significant role in the heart functionality . In this study CFD owing its capability of predicting detailed flow field was adopted to model the blood flow in images based patient specific LV over cardiac cycle . In most published studies the blood is modeled as Newtonian that is not entirely accurate as the blood viscosity varies with the shear rate in non linear manner . In this paper we studied the effect of Newtonian assumption on the degree of accuracy of intraventricular hemodynamics . In doing so various non Newtonian models and Newtonian model are used in the analysis of the intraventricular flow and the viscosity of the blood . Initially we used the cardiac MRI images to reconstruct the time resolved geometry of the patient specific LV . After the unstructured mesh generation the simulations were conducted in the CFD commercial solver FLUENT to analyze the intraventricular hemodynamic parameters . The findings indicate that the Newtonian assumption can not adequately simulate the flow dynamic within the LV over the cardiac cycle which can be attributed to the pulsatile and recirculation nature of the flow and the low blood shear rate . \\n'],\n",
              " [' Background and objective In this paper we have tested the suitability of using different artificial intelligence based algorithms for decision support when classifying the risk of congenital heart surgery . In this sense classification of those surgical risks provides enormous benefits as the a priori estimation of surgical outcomes depending on either the type of disease or the type of repair and other elements that influence the final result . This preventive estimation may help to avoid future complications or even death . Methods We have evaluated four machine learning algorithms to achieve our objective multilayer perceptron self organizing map radial basis function networks and decision trees . The architectures implemented have the aim of classifying among three types of surgical risk low complexity medium complexity and high complexity . Results Accuracy outcomes achieved range between 80 and 99 being the multilayer perceptron method the one that offered a higher hit ratio . Conclusions According to the results it is feasible to develop a clinical decision support system using the evaluated algorithms . Such system would help cardiology specialists paediatricians and surgeons to forecast the level of risk related to a congenital heart disease surgery . \\n'],\n",
              " [' Background and objective The adoption of computerized physician order entry is an important cornerstone of using health information technology in health care . The transition from paper to computer forms presents a change in physicians practices . The main objective of this study was to investigate the impact of implementing a computer based order entry system without clinical decision support on the number of radiographs ordered for patients admitted in the emergency department . Methods This single center pre post intervention study was conducted in January 2013 and January 2014 at the emergency department at N mes University Hospital . All patients admitted in the emergency department who had undergone medical imaging were included in the study . Results Emergency department admissions have increased since the implementation of CPOE . In the period before CPOE implementation 2345 patients had undergone medical imaging in the period after CPOE implementation 2306 patients had undergone medical imaging . In the period before CPOE 2916 medical imaging procedures were ordered in the period after CPOE 2876 medical imaging procedures were ordered . In the period before CPOE 1885 radiographs were ordered in the period after CPOE 1776 radiographs were ordered . The time between emergency department admission and medical imaging did not vary between the two periods . Conclusions Our results show a decrease in the number of radiograph requests after a CPOE system without clinical decision support was implemented in our emergency department . \\n'],\n",
              " [' Object tracking quality usually depends on video scene conditions . In order to overcome this limitation this article presents a new control approach to adapt the object tracking process to the scene condition variations . More precisely this approach learns how to tune the tracker parameters to cope with the tracking context variations . The tracking context or context of a video sequence is defined as a set of six features density of mobile objects their occlusion level their contrast with regard to the surrounding background their contrast variance their 2D area and their 2D area variance . In an offline phase training video sequences are classified by clustering their contextual features . Each context cluster is then associated to satisfactory tracking parameters . In the online control phase once a context change is detected the tracking parameters are tuned using the learned values . The approach has been experimented with three different tracking algorithms and on long complex video datasets . This article brings two significant contributions a classification method of video sequences to learn offline tracking parameters and a new method to tune online tracking parameters using tracking context . \\n'],\n",
              " [' Glomerulus diameter and Bowman s space width in renal microscopic images indicate various diseases . Therefore the detection of the renal corpuscle and related objects is a key step in histopathological evaluation of renal microscopic images . However the task of automatic glomeruli detection is challenging due to their wide intensity variation besides the inconsistency in terms of shape and size of the glomeruli in the renal corpuscle . Here a novel solution is proposed which includes the Particles Analyzer technique based on median filter for morphological image processing to detect the renal corpuscle objects . Afterwards the glomerulus diameter and Bowman s space width are measured . The solution was tested with a dataset of 21 rats renal corpuscle images acquired using light microscope . The experimental results proved that the proposed solution can detect the renal corpuscle and its objects efficiently . As well as the proposed solution has the ability to manage any input images assuring its robustness to the deformations of the glomeruli even with the glomerular hypertrophy cases . Also the results reported significant difference between the control and affected of fructose groups in terms of glomerulus diameter . \\n'],\n",
              " [' Background and objective Optimal experimental design approaches are seldom used in preclinical drug discovery . The objective is to develop an optimal design software tool specifically designed for preclinical applications in order to increase the efficiency of drug discovery in vivo studies . Methods Several realistic experimental design case studies were collected and many preclinical experimental teams were consulted to determine the design goal of the software tool . The tool obtains an optimized experimental design by solving a constrained optimization problem where each experimental design is evaluated using some function of the Fisher Information Matrix . The software was implemented in C using the Qt framework to assure a responsive user software interaction through a rich graphical user interface and at the same time achieving the desired computational speed . In addition a discrete global optimization algorithm was developed and implemented . Results The software design goals were simplicity speed and intuition . Based on these design goals we have developed the publicly available software PopED lite . Optimization computation was on average over 14 test problems 30 times faster in PopED lite compared to an already existing optimal design software tool . PopED lite is now used in real drug discovery projects and a few of these case studies are presented in this paper . Conclusions PopED lite is designed to be simple fast and intuitive . Simple to give many users access to basic optimal design calculations . Fast to fit a short design execution cycle and allow interactive experimental design . Intuitive so that the input to and output from the software tool can easily be understood by users without knowledge of the theory of optimal design . In this way PopED lite is highly useful in practice and complements existing tools . \\n'],\n",
              " [' Wildfire smoke detection is particularly important for early warning systems because smoke usually rises before flames arise . Therefore this paper presents an automatic wildfire smoke detection method using computer vision and pattern recognition techniques . First candidate blocks are identified using key frame differences and nonparametric smoke color models to detect smoke colored moving objects . Subsequently three dimensional spatiotemporal volumes are built by combining the candidate blocks in the current key frame with the corresponding blocks in previous frames . A histogram of oriented gradient is extracted and a histogram of oriented optical flow is extracted as a temporal feature based on the fact that the direction of smoke diffusion is upward owing to thermal convection . From spatiotemporal features of training data a visual codebook and a bag of features histogram are generated using our proposed weighting scheme . For smoke verification a random forest classifier is built during the training phase using the BoF histogram . The random forest with the BoF histogram can increase the detection accuracy performance when compared with related methods and allow smoke detection to be carried out in near real time . \\n'],\n",
              " [' This paper examines the issue of face speaker and bi modal authentication in mobile environments when there is significant condition mismatch . We introduce this mismatch by enrolling client models on high quality biometric samples obtained on a laptop computer and authenticating them on lower quality biometric samples acquired with a mobile phone . To perform these experiments we develop three novel authentication protocols for the large publicly available MOBIO database . We evaluate state of the art face speaker and bi modal authentication techniques and show that inter session variability modelling using Gaussian mixture models provides a consistently robust system for face speaker and bi modal authentication . It is also shown that multi algorithm fusion provides a consistent performance improvement for face speaker and bi modal authentication . Using this bi modal multi algorithm system we derive a state of the art authentication system that obtains a half total error rate of 6.3 and 1.9 for Female and Male trials respectively . \\n'],\n",
              " [' Background and objectives Automatic electrocardiogram heartbeat classification is substantial for diagnosing heart failure . The aim of this paper is to evaluate the effect of machine learning methods in creating the model which classifies normal and congestive heart failure on the long term ECG time series . Methods The study was performed in two phases feature extraction and classification phase . In feature extraction phase autoregressive Burg method is applied for extracting features . In classification phase five different classifiers are examined namely C4.5 decision tree k nearest neighbor support vector machine artificial neural networks and random forest classifier . The ECG signals were acquired from BIDMC Congestive Heart Failure and PTB Diagnostic ECG databases and classified by applying various experiments . Results The experimental results are evaluated in several statistical measures and showed that the random forest method gives 100 classification accuracy . Conclusions Impressive performance of random forest method proves that it plays significant role in detecting congestive heart failure and can be valuable in expressing knowledge useful in medicine . \\n'],\n",
              " ['Text contained in scene images provides the semantic context of the images. For that reason robust extraction of text regions is essential for successful scene text understanding. However separating text pixels from scene images still remains as a challenging issue because of uncontrolled lighting conditions and complex backgrounds. In this paper we propose a two stage conditional random field TCRF approach to robustly extract text regions from the scene images. The proposed approach models the spatial and hierarchical structures of the scene text and it finds text regions based on the scene text model. In the first stage the system generates multiple character proposals for the given image by using multiple image segmentations and a local CRF model. In the second stage the system selectively integrates the generated character proposals to determine proper character regions by using a holistic CRF model. Through the TCRF approach we cast the scene text separation problem as a probabilistic labeling problem which yields the optimal label configuration of pixels that maximizes the conditional probability of the given image. Experimental results indicate that our framework exhibits good performance in the case of the public databases. \\n'],\n",
              " [' Vibroarthographic signals emitted from the knee joint disorder provides an early diagnostic tool . The nonstationary and nonlinear nature of VAG signal makes an important aspect for feature extraction . In this work we investigate VAG signals by proposing a wavelet based decomposition . The VAG signals are decomposed into sub band signals of different frequencies . Nonlinear features such as recurrence quantification analysis approximate entropy and sample entropy are extracted as features of VAG signal . A total of twenty four features form a vector to characterize a VAG signal . Two feature selection techniques apriori algorithm and genetic algorithm selects six and four features as the most significant features . Least square support vector machines and random forest are proposed as classifiers to evaluate the performance of FS techniques . Results indicate that the classification accuracy was more prominent with features selected from FS algorithms . Results convey that LS SVM using the apriori algorithm gives the highest accuracy of 94.31 with false discovery rate of 0.0892 . The proposed work also provided better classification accuracy than those reported in the previous studies which gave an accuracy of 88 . This work can enhance the performance of existing technology for accurately distinguishing normal and abnormal VAG signals . And the proposed methodology could provide an effective non invasive diagnostic tool for knee joint disorders . \\n'],\n",
              " [' We present a new method for multi agent activity analysis and recognition that uses low level motion features and exploits the inherent structure and recurrence of motion present in multi agent activity scenarios. Our representation is inspired by the need to circumvent the difficult problem of tracking in multi agent scenarios and the observation that for many visual multi agent recognition tasks the spatiotemporal description of events irrespective of agent identity is sufficient for activity classification. We begin by learning generative models describing motion induced by individual actors or groups which are considered to be agents. These models are Gaussian mixture distributions learned by linking clusters of optical flow to obtain contiguous regions of locally coherent motion. These possibly overlapping regions or segments known as motion patterns are then used to analyze a scene by estimating their spatial and temporal relationships. The geometric transformations between two patterns are obtained by iteratively warping one pattern onto another whereas the temporal relationships are obtained from their relative times of occurrence within videos. These motion segments and their spatio temporal relationships are represented as a graph where the nodes are the statistical distributions and the edges have geometric transformations between motion patterns transformed to Lie space as their attributes. Two activity instances are then compared by estimating the cost of attributed inexact graph matching. We demonstrate the application of our framework in the analysis of American football plays a typical multi agent activity. The performance analysis of our method shows that it is feasible and easily generalizable. \\n'],\n",
              " [' Objective Cancer is the primary disease responsible for death and disability worldwide . Currently prevention and early detection represents the best hope for cure . Knowing the expected diseases that occur with a particular cancer in advance could lead to physicians being able to better tailor their treatment for cancer . The aim of this study was to build an animated visualization tool called as Cancer Associations Map Animation to chart the association of cancers with other disease over time . Methods The study population was collected from the Taiwan National Health Insurance Database during the period January 2000 to December 2002 782 million outpatient visits were used to compute the associations of nine major cancers with other diseases . A motion chart was used to quantify and visualize the associations between diseases and cancers . Results The CAMA motion chart that was built successfully facilitated the observation of cancer disease associations across ages and genders . The CAMA system can be accessed online at http 203.71.86.98 web runq16.html . Conclusion The CAMA animation system is an animated medical data visualization tool which provides a dynamic time lapse animated view of cancer disease associations across different age groups and gender . Derived from a large nationwide healthcare dataset this exploratory data analysis tool can detect cancer comorbidities earlier than is possible by manual inspection . Taking into account the trajectory of cancer specific comorbidity development may facilitate clinicians and healthcare researchers to more efficiently explore early stage hypotheses develop new cancer treatment approaches and identify potential effect modifiers or new risk factors associated with specific cancers . Motion chart parameters Mapping variables in this study Time Presents age of patients X axis Presents the scale of association s strength Y axis Presents the scale of count number of relative disease Size of circle Presents the number of co occurrence of both diseases A and B Color Presents the category of disease \\n'],\n",
              " [' A toolkit has been developed for calculating the 3 dimensional biological effective dose distributions in multi phase external beam radiotherapy treatments such as those applied in liver stereotactic body radiation therapy and in multi prescription treatments . This toolkit also provides a wide range of statistical results related to dose and BED distributions . MATLAB 2010a version 7.10 was used to create this GUI toolkit . The input data consist of the dose distribution matrices organ contour coordinates and treatment planning parameters from the treatment planning system . The toolkit has the capability of calculating the multi phase BED distributions using different formulas . Following the calculations of the BED distributions the dose and BED distributions can be viewed in different projections . The different elements of this toolkit are presented and the important steps for the execution of its calculations are illustrated . The toolkit is applied on brain head neck and prostate cancer patients who received primary and boost phases in order to demonstrate its capability in calculating BED distributions as well as measuring the inaccuracy and imprecision of the approximate BED distributions . Finally the clinical situations in which the use of the present toolkit would have a significant clinical impact are indicated . \\n'],\n",
              " [' Background and objectives In computed tomography statistical iterative reconstruction approaches can produce images of higher quality compared to the conventional analytical methods such as filtered backprojection algorithm . Effective noise modeling and possibilities to incorporate priors in the image reconstruction problem are the main advantages that lead to continuous development of SIR methods . Oriented by low dose CT requirements several methods are recently developed to obtain a high quality image reconstruction from down sampled or noisy projection data . In this paper a new prior information obtained from probabilistic atlas is proposed for low dose CT image reconstruction . Methods The proposed approach consists of two main phases . In learning phase a dataset of images obtained from different patients is used to construct a 3D atlas with Laplacian mixture model . The expectation maximization algorithm is used to estimate the mixture parameters . In reconstruction phase prior information obtained from the probabilistic atlas is used to construct the cost function for image reconstruction . Results We investigate the low dose imaging by considering the reduction of X ray beam intensity and by acquiring the projection data through a small number of views or limited view angles . Experimental studies using simulated data and chest screening CT data demonstrate that the probabilistic atlas prior is a practically promising approach for the low dose CT imaging . Conclusions The prior information obtained from probabilistic atlas constructed from earlier scans of different patients is useful in low dose CT imaging . \\n'],\n",
              " [' Background M2M communications represent one of the main pillars of the new paradigm of the Internet of Things and is making possible new opportunities for the eHealth business . Nevertheless the large number of M2M protocols currently available hinders the election of a suitable solution that satisfies the requirements that can demand eHealth applications . Objectives In the first place to develop a tool that provides a benchmarking analysis in order to objectively select among the most relevant M2M protocols for eHealth solutions . In the second place to validate the tool with a particular use case the respiratory rehabilitation . Methods A software tool called Distributed Computing Framework has been designed and developed to execute the benchmarking tests and facilitate the deployment in environments with a large number of machines with independence of the protocol and performance metrics selected . Results DDS MQTT CoAP JMS AMQP and XMPP protocols were evaluated considering different specific performance metrics including CPU usage memory usage bandwidth consumption latency and jitter . The results obtained allowed to validate a case of use respiratory rehabilitation of chronic obstructive pulmonary disease patients in two scenarios with different types of requirement Home Based and Ambulatory . Conclusions The results of the benchmark comparison can guide eHealth developers in the choice of M2M technologies . In this regard the framework presented is a simple and powerful tool for the deployment of benchmark tests under specific environments and conditions . \\n'],\n",
              " [' Background and objective Transfer function is an important parameter for the analysis and understanding of hemodynamics when arterial stenosis exists in human arterial tree . Aimed to validate the feasibility of using TF to diagnose arterial stenosis the forward problem and inverse problem were simulated and discussed . Methods A calculation method of TF between ascending aorta and any other artery was proposed based on a 55 segment transmission line model of human artery tree . The effects of artery stenosis on TF were studied in two aspects stenosis degree and position . The degree of arterial stenosis was specified to be 10 90 in three representative arteries carotid aorta and iliac artery respectively . In order to validate the feasibility of diagnosis of artery stenosis using TF and support vector machine a database of TF was established to simulate the real conditions of artery stenosis based on the TLM model . And a diagnosis model of artery stenosis was built by using SVM and the database . Results The simulating results showed the modulus and phase of TF were decreasing sharply from frequency 2 to 10Hz with the stenosis degree increasing and displayed their unique and nonlinear characteristics when frequency is higher than 10Hz . The diagnosis results showed the average accuracy was above 76 for the stenosis from 10 to 90 degree and the diagnosis accuracies of moderate and serious stenosis were 87 and 99 respectively . When the stenosis degree increased to 90 the accuracy of stenosis localization reached up to 94 for most of arteries . Conclusions The proposed method of combining TF and SVM is a theoretically feasible method for diagnosis of artery stenosis . \\n'],\n",
              " [' In observational studies without random assignment of the treatment the unadjusted comparison between treatment groups may be misleading due to confounding . One method to adjust for measured confounders is inverse probability of treatment weighting . This method can also be used in the analysis of time to event data with competing risks . Competing risks arise if for some individuals the event of interest is precluded by a different type of event occurring before or if only the earliest of several times to event corresponding to different event types is observed or is of interest . In the presence of competing risks time to event data are often characterized by cumulative incidence functions one for each event type of interest . We describe the use of inverse probability of treatment weighting to create adjusted cumulative incidence functions . This method is equivalent to direct standardization when the weight model is saturated . No assumptions about the form of the cumulative incidence functions are required . The method allows studying associations between treatment and the different types of event under study while focusing on the earliest event only . We present a SAS macro implementing this method and we provide a worked example . \\n'],\n",
              " ['We propose a scheme for comparing local neighborhoods window of image points to estimate optical flow using discrete optimization. The proposed approach is based on using large correlation windows with adaptive support weights. We present three new types of weighting constraints derived from image gradient color statistics and occlusion information. The first type provides gradient structure constraints that favor flow consistency across strong image gradients. The second type imposes perceptual color constraints that reinforce relationship among pixels in a window according to their color statistics. The third type yields occlusion constraints that reject pixels that are seen in one window but not seen in the other. All these constraints contribute to suppress the effect of cluttered background which is unavoidably included in the large correlation windows. Experimental results demonstrate that each of the proposed constraints appreciably elevates the quality of estimations and that they jointly yield results that compare favorably to current techniques especially on object boundaries. \\n'],\n",
              " [' In this paper the problem of human ear recognition in the Mid wave infrared spectrum is studied in order to illustrate the advantages and limitations of the ear based biometrics that can operate in day and night time environments . The main contributions of this work are two fold First a dual band database is assembled that consists of visible and mid wave IR left and right profile face images . Profile face images were collected using a high definition mid wave IR camera that is capable of acquiring thermal imprints of human skin . Second a fully automated thermal imaging based ear recognition system is proposed that is designed and developed to perform real time human identification . The proposed system tests several feature extraction methods namely intensity based such as independent component analysis principal component analysis and linear discriminant analysis shape based such as scale invariant feature transform as well as texture based such as local binary patterns and local ternary patterns . Experimental results suggest that LTP yields the best performance on manually segmented ears and on ear images that are automatically detected and segmented . By fusing the matching scores obtained by LBP and LTP the identification performance increases by about 5 . Although these results are promising the outcomes of our study suggest that the design and development of automated ear based recognition systems that can operate efficiently in the lower part of the passive IR spectrum are very challenging tasks . \\n'],\n",
              " [' On the Semantic Web the types of resources and the semantic relationships between resources are defined in an ontology . By using that information the accuracy of information retrieval can be improved . In this paper we present effective ranking and search techniques considering the semantic relationships in an ontology . Our technique retrieves top k resources which are the most relevant to query keywords through the semantic relationships . To do this we propose a weighting measure for the semantic relationship . Based on this measure we propose a novel ranking method which considers the number of meaningful semantic relationships between a resource and keywords as well as the coverage and discriminating power of keywords . In order to improve the efficiency of the search we prune the unnecessary search space using the length and weight thresholds of the semantic relationship path . In addition we exploit Threshold Algorithm based on an extended inverted index to answer top k results efficiently . The experimental results using real data sets demonstrate that our retrieval method using the semantic information generates accurate results efficiently compared to the traditional methods . \\n'],\n",
              " [' Transfer learning utilizes labeled data available from some related domain for achieving effective knowledge transformation to the target domain . However most state of the art cross domain classification methods treat documents as plain text and ignore the hyperlink relationship existing among the documents . In this paper we propose a novel cross domain document classification approach called Link Bridged Topic model . LBT consists of two key steps . Firstly LBT utilizes an auxiliary link network to discover the direct or indirect co citation relationship among documents by embedding the background knowledge into a graph kernel . The mined co citation relationship is leveraged to bridge the gap across different domains . Secondly LBT simultaneously combines the content information and link structures into a unified latent topic model . The model is based on an assumption that the documents of source and target domains share some common topics from the point of view of both content information and link structure . By mapping both domains data into the latent topic spaces LBT encodes the knowledge about domain commonality and difference as the shared topics with associated differential probabilities . The learned latent topics must be consistent with the source and target data as well as content and link statistics . Then the shared topics act as the bridge to facilitate knowledge transfer from the source to the target domains . Experiments on different types of datasets show that our algorithm significantly improves the generalization performance of cross domain document classification . \\n'],\n",
              " ['Background modeling is widely used in visual surveillance systems aiming to facilitate analysis of real world video scenes. The goal is to discriminate between pixels from foreground objects and those ones from the background. However real world scenarios tend to have time and spatial non stationary variations being difficult to reveal the foreground and background entities from video data. Here we propose a novel adaptive background modeling termed Object based Selective Updating with Correntropy OSUC to support video based surveillance systems. Our approach that is developed within an adaptive learning framework unveils existing spatio temporal pixel relationships making use of a single Gaussian for the model representation stage. Moreover we introduce a background updating scheme composed of an updating rule that is based on the stochastic gradient algorithm and Correntropy cost function. As a result this scheme can extract the temporal statistical pixel distribution at the same time dealing with non stationary pixel value fluctuations that affect the background model. Here an automatic tuning strategy of the cost function bandwidth parameter is carried out that can handle both Gaussian and non Gaussian noise environments. Besides to include pixel spatial relationships in the background modeling processing we introduce an object based selective learning rate strategy for enhancing the background modeling accuracy. Particularly an object motion analysis stage is presented to detect and track foreground entities based on pixel intensities and motion direction attained via optical flow computation. Testing is provided on well known datasets for discriminating between foreground and background that include stationary and non stationary behaviors. Achieved results show that the OSUC outperforms in most of the considered cases the state of the art approaches with an affordable computational cost. Therefore the proposed approach is suitable for supporting real world video based surveillance systems. \\n'],\n",
              " ['Video segmentation is a fundamental problem in computer vision and aims to extract meaningful entities from a video. One of the most useful cues in this quest is motion as is described by the trajectories of tracked points. In this paper we present a motion segmentation method attempting to address some of the major issues in the area. Namely we propose an efficient framework where more complex motion models can be seamlessly integrated both maintaining computational tractability and not penalizing non translational motion. Moreover we expose in depth the problem of object leakage due to occlusion and highlight that motion segmentation could be treated as a graph coloring problem. Our algorithm uses an approach based on graph theory and resolves occlusion cases in a robust manner. To endow our method with scalability we follow the previously presented subsequence architecture and test it in a streaming setup. Extensive experiments demonstrate the flexibility and robustness of the method. The segmentation results are competitive compared to the state of the art. \\n'],\n",
              " [' Both general and domain specific search engines have adopted query suggestion techniques to help users formulate effective queries . In the specific domain of literature search the initial queries are usually based on a draft paper or abstract rather than short lists of keywords . In this paper we investigate phrasal concept query suggestions for literature search . These suggestions explicitly specify important phrasal concepts related to an initial detailed query . The merits of phrasal concept query suggestions for this domain are their readability and retrieval effectiveness phrasal concepts are natural for academic authors because of their frequent use of terminology and subject specific phrases and academic papers describe their key ideas via these subject specific phrases and thus phrasal concepts can be used effectively to find those papers . We propose a novel phrasal concept query suggestion technique that generates queries by identifying key phrasal concepts from pseudo labeled documents and combines them with related phrases . Our proposed technique is evaluated in terms of both user preference and retrieval effectiveness . We conduct user experiments to verify a preference for our approach in comparison to baseline query suggestion methods and demonstrate the effectiveness of the technique with retrieval experiments . \\n'],\n",
              " [' This paper presents an orthonormal dictionary learning method for low rank representation . The orthonormal property encourages the dictionary atoms to be as dissimilar as possible which is beneficial for reducing the ambiguities of representations and computation cost . To make the dictionary more discriminative we enhance the ability of the class specific dictionary to well represent samples from the associated class and suppress the ability of representing samples from other classes and also enforce the representations that have small within class scatter and big between class scatter . The learned orthonormal dictionary is used to obtain low rank representations with fast computation . The performances of face recognition demonstrate the effectiveness and efficiency of the method . \\n'],\n",
              " [' Most existing search engines focus on document retrieval . However information needs are certainly not limited to finding relevant documents . Instead a user may want to find relevant entities such as persons and organizations . In this paper we study the problem of related entity finding . Our goal is to rank entities based on their relevance to a structured query which specifies an input entity the type of related entities and the relation between the input and related entities . We first discuss a general probabilistic framework derive six possible retrieval models to rank the related entities and then compare these models both analytically and empirically . To further improve performance we study the problem of feedback in the context of related entity finding . Specifically we propose a mixture model based feedback method that can utilize the pseudo feedback entities to estimate an enriched model for the relation between the input and related entities . Experimental results over two standard TREC collections show that the derived relation generation model combined with a relation feedback method performs better than other models . \\n'],\n",
              " [' Recently a video representation based on dense trajectories has been shown to outperform other human action recognition methods on several benchmark datasets . The trajectories capture the motion characteristics of different moving objects in space and temporal dimensions . In dense trajectories points are sampled at uniform intervals in space and time and then tracked using a dense optical flow field over a fixed length of L frames spread overlapping over the entire video . However among these base trajectories a few may continue for longer than duration L capturing motion characteristics of objects that may be more valuable than the information from the base trajectories . Thus we propose a technique that searches for trajectories with a longer duration and refer to these as ordered trajectories . Experimental results show that ordered trajectories perform much better than the base trajectories both standalone and when combined . Moreover the uniform sampling of dense trajectories does not discriminate objects of interest from the background or other objects . Consequently a lot of information is accumulated which actually may not be useful . This can especially escalate when there is more data due to an increase in the number of action classes . We observe that our proposed trajectories remove some background clutter too . We use a Bag of Words framework to conduct experiments on the benchmark HMDB51 UCF50 and UCF101 datasets containing the largest number of action classes to date . Further we also evaluate three state of the art feature encoding techniques to study their performance on a common platform . \\n'],\n",
              " [' In this paper a novel and effective lip based biometric identification approach with the Discrete Hidden Markov Model Kernel is developed . Lips are described by shape features on two different grid layouts rectangular and polar . These features are then specifically modeled by a DHMMK and learnt by a support vector machine classifier . Our experiments are carried out in a ten fold cross validation fashion on three different datasets GPDS ULPGC Face Dataset PIE Face Dataset and RaFD Face Dataset . Results show that our approach has achieved an average classification accuracy of 99.8 97.13 and 98.10 using only two training images per class on these three datasets respectively . Our comparative studies further show that the DHMMK achieved a 53 improvement against the baseline HMM approach . The comparative ROC curves also confirm the efficacy of the proposed lip contour based biometrics learned by DHMMK . We also show that the performance of linear and RBF SVM is comparable under the frame work of DHMMK . \\n'],\n",
              " [' The emergence of large scale human action datasets poses a challenge to efficient action labeling. Hand labeling large scale datasets is tedious and time consuming thus a more efficient labeling method would be beneficial. One possible solution is to make use of the knowledge of a known dataset to aid the labeling of a new dataset. To this end we propose a new transfer learning method for cross dataset human action recognition. Our method aims at learning generalized feature representation for effective cross dataset classification. We propose a novel dual many to one encoder architecture to extract generalized features by mapping raw features from source and target datasets to the same feature space. Benefiting from the favorable property of the proposed many to one encoder cross dataset action data are encouraged to possess identical encoded features if the actions share the same class labels. Experiments on pairs of benchmark human action datasets achieved state of the art accuracy proving the efficacy of the proposed method. \\n'],\n",
              " [' Textual entailment is a task for which the application of supervised learning mechanisms has received considerable attention as driven by successive Recognizing Data Entailment data challenges . We developed a linguistic analysis framework in which a number of similarity dissimilarity features are extracted for each entailment pair in a data set and various classifier methods are evaluated based on the instance data derived from the extracted features . The focus of the paper is to compare and contrast the performance of single and ensemble based learning algorithms for a number of data sets . We showed that there is some benefit to the use of ensemble approaches but based on the extracted features Na ve Bayes proved to be the strongest learning mechanism . Only one ensemble approach demonstrated a slight improvement over the technique of Na ve Bayes . \\n'],\n",
              " [' Nowadays using increasingly granular data from real time location information and detailed demographics to consumers generated content on the social networking sites businesses are starting to offer precise location based product recommendation services through mobile devices . Based on the technology acceptance model this paper develops a theoretical model to examine the adoption intention of active SNS users toward location based recommendation agents . The research model was tested by using the Partial Least Squares technique.The results show that perceived usefulness perceived control and perceived institutional assurance are important in developing adoption intention . Perceived effort saving special treatment and social benefit have influences on the adoption intention through the mediating effect of perceived usefulness . Perceived accuracy has direct influence on adoption intention . \\n'],\n",
              " [' Arabic is a widely spoken language but few mining tools have been developed to process Arabic text . This paper examines the crime domain in the Arabic language using text mining techniques . The development and application of a Crime Profiling System is presented . The system is able to extract meaningful information in this case the type of crime location and nationality from Arabic language crime news reports . The system has two unique attributes firstly information extraction that depends on local grammar and secondly dictionaries that can be automatically generated . It is shown that the CPS improves the quality of the data through reduction where only meaningful information is retained . Moreover the Self Organising Map approach is adopted in order to perform the clustering of the crime reports based on crime type . This clustering technique is improved because only refined data containing meaningful keywords extracted through the information extraction process are inputted into it i.e . the data are cleansed by removing noise . The proposed system is validated through experiments using a corpus collated from different sources it was not used during system development . Precision recall and F measure are used to evaluate the performance of the proposed information extraction approach . Also comparisons are conducted with other systems . In order to evaluate the clustering performance three parameters are used data size loading time and quantization error . \\n'],\n",
              " ['Wide field of view panoramic videos have recently become popular due to the availability of high resolution displays. These panoramic videos are generated by stitching video frames captured from a panoramic video acquisition system typically comprising of multiple video cameras arranged on a static or mobile platform. A mobile panoramic video acquisition system may suffer from global mechanical vibrations as well as independent inter camera vibrations resulting in a jittery panoramic video. While existing stabilization schemes generally tackle single camera vibrations they do not account for these inter camera vibrations. In this paper we propose a video stabilization technique for multi camera panoramic videos under the consideration that independent jitter may be exhibited by content of each camera. The proposed method comprises of three steps the first step removes the global jitter in the video by estimating collective motion and subsequently removing the high frequency component from it. The second step removes the independent i.e. local jitter of each camera by estimating motion of each camera content separately. Pixels that are located in the overlapping regions of panoramic video are contributed by neighboring cameras therefore the estimated camera motion for these pixels is weighted using the blend masks generated by the stitching process. The final step applies local geometric warping to the stitched frames and removes any residual jitter induced due to parallax. Experimental results prove that proposed scheme performs better than existing panoramic stabilization schemes. \\n'],\n",
              " [' Much of the valuable information in supporting decision making processes originates in text based documents . Although these documents can be effectively searched and ranked by modern search engines actionable knowledge need to be extracted and transformed in a structured form before being used in a decision process . In this paper we describe how the discovery of semantic information embedded in natural language documents can be viewed as an optimization problem aimed at assigning a sequence of labels to a set of interdependent variables . Dependencies among variables are efficiently modeled through Conditional Random Fields an indirected graphical model able to represent the distribution of labels given a set of observations . The Markov property of these models prevent them to take into account long range dependencies among variables which are indeed relevant in Natural Language Processing . In order to overcome this limitation we propose an inference method based on Integer Programming formulation of the problem where long distance dependencies are included through non deterministic soft constraints . \\n'],\n",
              " [' IDR QR which is an incremental dimension reduction algorithm based on linear discriminant analysis and QR decomposition has been successfully employed for feature extraction and incremental learning . IDR QR can update the discriminant vectors with light computation when new training samples are inserted into the training data set . However IDR QR has two limitations 1 IDR QR can only process new samples one instance after another even if a chunk of training samples is available at a time and 2 the approximate trick is used in IDR QR . Then there exists a gap in performance between incremental and batch IDR QR solutions . To address the problems of IDR QR in this paper we propose a new chunk IDR method which is capable of processing multiple data instances at a time and can accurately update the discriminant vectors when new data items are added dynamically . Experiments on some real databases demonstrate the effectiveness of the proposed algorithm over the original one . \\n'],\n",
              " [' This paper aims at identifying the factors influencing the implementation of Web accessibility by European banks . We studied a database made up of 49 European banks whose shares are included in the Dow Jones EURO STOXX TMI Banks Index . Regarding the factors for the implementation we considered three feasible reasons . Firstly WA adoption can be motivated by operational factors as WA can aid in increasing operational efficiency . Secondly we expect large banks to have higher WA levels as small firms face competitive disadvantages with regard to technology adoption . Lastly WA can also be understood as a part of the Corporate Social Responsibility strategy so the more committed a bank is to CSR the more prone it will be to implement WA . Our results indicate that neither the operational factors nor the firm size seem to have exerted a significant influence on WA adoption . Regarding CSR commitment results indicate a significant influence on WA adoption . However the effect of the influence is contrary to that hypothesized since more CSR committed banks have less accessible Web sites . A possible reason for this result is that banks not included in the CSR indexes try to overcome this drawback by engaging in alternative CSR activities such as WA . Nowadays Internet banking is a must have service for financial entities . Several authors conclude that it contributes to increase profitability and is a necessary investment to keep the custom of younger members . So Web sites are a key instrument for improving the competitive edge of banks as well as nonfinancial firms . Banks have an additional incentive to have a high quality Web site as Internet is a means of providing information to investors and other parties interacting with the bank . This factor is especially important in the case of banks whose shares are publicly traded . To assess the quality of a Web design many frameworks have been proposed by researchers . Ho proposed a framework that included criteria such as timeliness custom logistics and seasonal factors . Elliot Morup Petersen and Bjon Andersen also proposed a model which considers company information product service promotion transaction processing and customer service . Olsina Godoy Lafuente and Rossi proposed a Web site Quality Evaluation Model based on functionality usability efficiency and site reliability . Other similar proposals are those by DeLone and McLean Katerattanakul and Siau Kim Kishore and Sanders and Miranda and Ba egil . Regarding the specific case of banks Miranda Cort s and Barriuso elaborated an index for the quantitative evaluation of e banking Web sites and Bose and Leung proposed a framework to assess anti phishing preparedness of banks Web sites . However Web accessibility was not explicitly included in any of the proposed frameworks . Web accessibility entails overcoming all disabilities that prejudice Internet access it means that people with disabilities can use it and perceive understand navigate and interact with the Web and they can contribute to the Web. \\n'],\n",
              " [' Disaster Management is a diffused area of knowledge . It has many complex features interconnecting the physical and the social views of the world . Many international and national bodies create knowledge models to allow knowledge sharing and effective DM activities . But these are often narrow in focus and deal with specified disaster types . We analyze thirty such models to uncover that many DM activities are actually common even when the events vary . We then create a unified view of DM in the form of a metamodel . We apply a metamodelling process to ensure that this metamodel is complete and consistent . We validate it and present a representational layer to unify and share knowledge as well as combine and match different DM activities according to different disaster situations . \\n'],\n",
              " [' Automatic face alignment is a fundamental step in facial image analysis . However this problem continues to be challenging due to the large variability of expression illumination occlusion pose and detection drift in the real world face images . In this paper we present a multi view multi scale and multi component cascade shape regression model for robust face alignment . Firstly face view is estimated according to the deformable facial parts for learning view specified CSR which can decrease the shape variance alleviate the drift of face detection and accelerate shape convergence . Secondly multi scale HoG features are used as the shape index features to incorporate local structure information implicitly and a multi scale optimization strategy is adopted to avoid trapping in local optimum . Finally a component based shape refinement process is developed to further improve the performance of face alignment . Extensive experiments on the IBUG dataset and the 300 W challenge dataset demonstrate the superiority of the proposed method over the state of the art methods . \\n'],\n",
              " [' Multi document discourse parsing aims to automatically identify the relations among textual spans from different texts on the same topic . Recently with the growing amount of information and the emergence of new technologies that deal with many sources of information more precise and efficient parsing techniques are required . The most relevant theory to multi document relationship Cross document Structure Theory has been used for parsing purposes before though the results had not been satisfactory . CST has received many critics because of its subjectivity which may lead to low annotation agreement and consequently to poor parsing performance . In this work we propose a refinement of the original CST which consists in formalizing the relationship definitions pruning and combining some relations based on their meaning and organizing the relations in a hierarchical structure . The hypothesis for this refinement is that it will lead to better agreement in the annotation and consequently to better parsing results . For this aim it was built an annotated corpus according to this refinement and it was observed an improvement in the annotation agreement . Based on this corpus a parser was developed using machine learning techniques and hand crafted rules . Specifically hierarchical techniques were used to capture the hierarchical organization of the relations according to the proposed refinement of CST . These two approaches were used to identify the relations among texts spans and to generate multi document annotation structure . Results outperformed other CST parsers showing the adequacy of the proposed refinement in the theory . \\n'],\n",
              " [' Feature correspondence lays the foundation for many tasks in computer vision and pattern recognition . In this paper the directed structural model is utilized to represent the feature set and the correspondence problem is then formulated as the structural model matching . Compared with the undirected structural model the proposed directed model provides more discriminating ability and invariance against rotation and scale transformations . Finally the recently proposed convex concave relaxation procedure is generalized to approximately solve the problem . Extensive experiments on synthetic and real data witness the effectiveness of the proposed method . \\n'],\n",
              " [' A user study of aNobii was conducted with an aim to exploring possible criteria for evaluating social navigational tools . A set of measures designed to capture various aspects of the benefits provided by the tools was proposed . To test the applicability of these measures a within subject experimental design was adopted where fifty regular aNobii users searched alternately with three book finding tools browsing friends bookshelves similar bookshelves and books by known authors . Other than the self report user experience and search result measures the choice set model was used as a novel framework for navigational effectiveness . Further analyses were conducted to explore whether three aspects of reader preference preference insight preference diversity and reading involvement might influence the performance of the tools . Some major findings are as follows . While the author browsing function was shown to be most efficient browsing friends bookshelves was shown to generate more interesting and informative browsing experiences . Three evaluative dimensions were derived from our study search experience search efficiency and result quality . The disagreement of these measures shows a need for a multi faceted evaluative framework for these exploration based navigational tools . Furthermore interaction effects on performance were found between users preference characteristics and tools . While users with high preference insight relied more heavily on author browsing to obtain more accurate results highly involved readers tended percentage wise to examine and select more titles when browsing friends bookshelves . \\n'],\n",
              " [' Visual tracking is an important task in various computer vision applications including visual surveillance human computer interaction event detection video indexing and retrieval . Recent state of the art sparse representation based trackers show better robustness than many of the other existing trackers . One of the issues with these SR trackers is low execution speed . The particle filter framework is one of the major aspects responsible for slow execution and is common to most of the existing SR trackers . In this paper An earlier brief version of the paper has appeared in ICIP 13 Melbourne Australia 2013 . we propose a robust interest point based tracker in l 1 minimization framework that runs at real time with performance comparable to the state of the art trackers . In the proposed tracker the target dictionary is obtained from the patches around target interest points . Next the interest points from the candidate window of the current frame are obtained . The correspondence between target and candidate points is obtained via solving the proposed l 1 minimization problem . In order to prune the noisy matches a robust matching criterion is proposed where only the reliable candidate points that mutually match with target and candidate dictionary elements are considered for tracking . The object is localized by measuring the displacement of these interest points . The reliable candidate patches are used for updating the target dictionary . The performance and accuracy of the proposed tracker is benchmarked with several complex video sequences . The tracker is found to be considerably fast as compared to the reported state of the art trackers . The proposed tracker is further evaluated for various local patch sizes number of interest points and regularization parameters . The performance of the tracker for various challenges including illumination change occlusion and background clutter has been quantified with a benchmark dataset containing 50 videos . \\n'],\n",
              " ['The goals of this paper are 1 to enhance the quality of images of faces 2 to enable 3D Morphable Models 3DMMs to cope with severely degraded images and 3 to reconstruct textured 3D faces with details that are not in the input images. Details that are lost in the input images due to blur low resolution or occlusions are filled in by the 3DMM and an additional texture enhancement algorithm that adds high resolution details from example faces. By leveraging class specific knowledge this restoration process goes beyond what general image operations such as deblurring or inpainting can achieve. The benefit of the 3DMM for image restoration is that it can be applied to any pose and illumination unlike image based methods. However it is only with the new fitting algorithm that 3DMMs can produce realistic faces from severely degraded images. The new method includes the blurring or downsampling operator explicitly into the analysis by synthesis algorithm. \\n'],\n",
              " [' This paper investigates the problem of cross domain action recognition . Specifically we present a cross domain action recognition framework by utilizing some labeled data from other data sets as the auxiliary source domain . It is a challenging task as data from different domains may have different feature distribution . To map data from different domains into the same abstract space and boost the action recognition performance we propose a method named collective matrix factorization with graph Laplacian regularization . Our approach is built upon the technique of collective matrix factorization which simultaneously learns a common latent space linear projection matrices for obtaining semantic representations and an optimal linear classifier . Moreover we explore the label consistency across different domain and the local geometric consistency in each domain and obtain a graph Laplacian regularization term to enhance the discrimination of learned features . Experimental results verify that CMFGLR significantly outperforms several state of the art methods . \\n'],\n",
              " [' The authors of this paper investigate terms of consumers diabetes based on a log from the Yahoo Answers social question and answers forum ascertain characteristics and relationships among terms related to diabetes from the consumers perspective and reveal users diabetes information seeking patterns . In this study the log analysis method data coding method and visualization multiple dimensional scaling analysis method were used for analysis . The visual analyses were conducted at two levels terms analysis within a category and category analysis among the categories in the schema . The findings show that the average number of words per question was 128.63 the average number of sentences per question was 8.23 the average number of words per response was 254.83 and the average number of sentences per response was 16.01 . There were 12 categories in the diabetes related schema which emerged from the data coding analysis . The analyses at the two levels show that terms and categories were clustered and patterns were revealed . Future research directions are also included . \\n'],\n",
              " [' An example based face hallucination system is proposed in which given a low resolution facial image a corresponding high resolution image is automatically obtained . In practice such a problem is extremely challenging since it is often the case that two discriminative high resolution images may have similar low resolution inputs . To address this issue this study proposes an ensemble of image feature representations including various local patch or block based representations a one dimensional vector image representation a two dimensional matrix image representation and a global matrix image representation . Notably some of these representations are designed to preserve the global facial geometry of the low resolution input while others are designed to preserve the local detailed texture . For each feature representation a regression function is constructed to synthesize a high resolution image from the low resolution input image . The synthesis process is conducted in a layer by layer fashion with the output from one layer serving as the input to the following layer . Importantly each regression function is associated with a classifier in order to determine which regression functions are required in the synthesis procedure in accordance with the particular characteristics of the input image . Furthermore these classifiers can also help to deal with the individual ambiguity of system low resolution inputs . The experimental results show that the proposed framework is capable of synthesizing high resolution images from low resolution input images with a wide variety of facial poses geometry misalignments and facial expressions even when such images are not included within the original training dataset . \\n'],\n",
              " [' Automatic text summarization has been an active field of research for many years . Several approaches have been proposed ranging from simple position and word frequency methods to learning and graph based algorithms . The advent of human generated knowledge bases like Wikipedia offer a further possibility in text summarization they can be used to understand the input text in terms of salient concepts from the knowledge base . In this paper we study a novel approach that leverages Wikipedia in conjunction with graph based ranking . Our approach is to first construct a bipartite sentence concept graph and then rank the input sentences using iterative updates on this graph . We consider several models for the bipartite graph and derive convergence properties under each model . Then we take up personalized and query focused summarization where the sentence ranks additionally depend on user interests and queries respectively . Finally we present a Wikipedia based multi document summarization algorithm . An important feature of the proposed algorithms is that they enable real time incremental summarization users can first view an initial summary and then request additional content if interested . We evaluate the performance of our proposed summarizer using the ROUGE metric and the results show that leveraging Wikipedia can significantly improve summary quality . We also present results from a user study which suggests that using incremental summarization can help in better understanding news articles . \\n'],\n",
              " [' As a result of the growing demand for health services China s large city hospitals have become markedly overstretched resulting in delicate and complex operating room scheduling problems . While the operating rooms are struggling to meet demand they face idle times because of resources being pulled away for other urgent demands and cancellations for economic and health reasons . In this research we analyze the resulting stochastic operating room scheduling problems and the improvements attainable by scheduled cancellations to accommodate the large demand while avoiding the negative consequences of excessive overtime work . We present a three stage recourse model which formalizes the scheduled cancellations and is anticipative to further uncertainty . We develop a solution method for this three stage model which relies on the sample average approximation and the L shaped method . The method exploits the structure of optimal solutions to speed up the optimization . Scheduled cancellations can significantly and substantially improve the operating room schedule when the costs of cancellations are close to the costs of overtime work . Moreover the proposed methods illustrate how the adverse impact of cancellations for economic and health reasons can be largely controlled . The resource unavailability however is shown to cause a more than proportional loss of solution value for the surgery scheduling problems occurring in China s large city hospitals even when applying the proposed solution techniques and requires different management measures . \\n'],\n",
              " [' In this paper we consider the two stage scheduling problem in which n jobs are first processed on m identical machines at a manufacturing facility and then delivered to their customers by one vehicle which can deliver one job at each shipment . In the problem a set of n delivery times is given in advance and in a schedule the n delivery times should be assigned to the n jobs respectively . The objective is to minimize the maximum delivery completion time i.e . the time when all jobs are delivered to their respective customers and the vehicle returns to the facility . For this problem we present a approximation algorithm and a polynomial time approximation scheme . \\n'],\n",
              " [' Bipartite graph matching has been demonstrated to be one of the most efficient algorithms to solve error tolerant graph matching . This algorithm is based on defining a cost matrix between the whole nodes of both graphs and solving the nodes correspondence through a linear assignment method . Recently two versions of this algorithm have been published called Fast Bipartite and Square Fast Bipartite . They compute the same distance value than Bipartite but with a reduced runtime if some restrictions on the edit costs are considered . In this paper we do not present a new algorithm but we compare the three versions of Bipartite algorithm and show how the violation of the theoretically imposed restrictions in Fast Bipartite and Square Fast Bipartite do not affect the algorithm s performance . That is in practice we show that these restrictions do not affect the optimality of the algorithm and so the three algorithms obtain similar distances and recognition ratios in classification applications although the restrictions do not hold . Moreover we conclude that the Square Fast Bipartite with the Jonker Volgenant solver is the fastest algorithm . \\n'],\n",
              " ['We present a multi view face detector based on Cascade Deformable Part Models CDPM . Over the last decade there have been several attempts to extend the well established Viola Jones face detector algorithm to solve the problem of multi view face detection. Recently a tree structure model for multi view face detection was proposed. This method is primarily designed for facial landmark detection and consequently a face detection is provided. However the effort to model inner facial structures by using a detailed facial landmark labelling resulted on a complex and suboptimal system for face detection. Instead we adopt CDPMs where the models are learned from partially labelled images using Latent Support Vector Machines LSVM . Furthermore LSVM is enhanced with data mining and bootstrapping procedures to enrich models during the training. Furthermore a post optimization procedure is derived to improve the performance. This semi supervised methodology allows us to build models based on weakly labelled data while incrementally learning latent positive and negative samples. Our results show that the proposed model can deal with highly expressive and partially occluded faces while outperforming the state of the art face detectors by a large margin on challenging benchmarks such as the Face Detection Data Set and Benchmark FDDB 1 and the Annotated Facial Landmarks in the Wild AFLW 2 databases. In addition we validate the accuracy of our models under large head pose variation and facial occlusions in the Head Pose Image Database HPID 3 and Caltech Occluded Faces in the Wild COFW datasets 4 respectively. We also outline the suitability of our models to support facial landmark detection algorithms. \\n'],\n",
              " ['We present a deeply integrated method of exploiting low cost gyroscopes to improve general purpose feature tracking. Most previous methods use gyroscopes to initialize and bound the search for features. In contrast we use them to regularize the tracking energy function so that they can directly assist in the tracking of ambiguous and poor quality features. We demonstrate that our simple technique offers significant improvements in performance over conventional template based tracking methods and is in fact competitive with more complex and computationally expensive state of the art trackers but at a fraction of the computational cost. Additionally we show that the practice of initializing template based feature trackers like KLT Kanade Lucas Tomasi using gyro predicted optical flow offers no advantage over using a careful optical only initialization method suggesting that some deeper level of integration like the method we propose is needed in order to realize a genuine improvement in tracking performance from these inertial sensors. \\n'],\n",
              " ['In recent years much effort has been put into the development of novel algorithms to solve the person re identification problem. The goal is to match a given person s image against a gallery of people. In this paper we propose a single shot supervised method to compute a scoring function that when applied to a pair of images provides a score expressing the likelihood that they depict the same individual. The method is characterized by i the usage of a set of local image descriptors based on Fisher vectors ii the training of a pool of scoring functions based on the local descriptors and iii the construction of a strong scoring function by means of an adaptive boosting procedure. The method has been tested on four data sets and results have been compared with state of the art methods clearly showing superior performance. \\n'],\n",
              " [' Modeling collaboration processes is a challenging task . Existing modeling approaches are not capable of expressing the unpredictable non routine nature of human collaboration which is influenced by the social context of involved collaborators . We propose a modeling approach which considers collaboration processes as the evolution of a network of collaborative documents along with a social network of collaborators . Our modeling approach accompanied by a graphical notation and formalization allows to capture the influence of complex social structures formed by collaborators and therefore facilitates such activities as the discovery of socially coherent teams social hubs or unbiased experts . We demonstrate the applicability and expressiveness of our approach and notation and discuss their strengths and weaknesses . \\n'],\n",
              " [' Time of flight depth cameras have widely been used in many applications such as 3D imaging 3D reconstruction human interaction and robot navigation . However conventional depth cameras are incapable of imaging a translucent object which occupies a substantial portion of a real world scene . Such a limitation prohibits realistic imaging using depth cameras . In this work we propose a new skewed stereo ToF camera for detecting and imaging translucent objects under minimal prior of environment . We find that the depth calculation of a ToF camera with a translucent object presents a systematic distortion due to the superposed reflected light ray observation from multiple surfaces . We propose to use a stereo ToF camera setup and derive a generalized depth imaging formulation for translucent objects . Distorted depth value is refined using an iterative optimization . Experimental evaluation shows that our proposed method reasonably recovers the depth image of translucent objects . \\n'],\n",
              " [' In this paper we address the document image binarization problem with a three stage procedure . First possible stains and general document background information are removed from the image through a background removal stage . The remaining misclassified background and character pixels are then separated using a Local Co occurrence Mapping local contrast and a two state Gaussian Mixture Model . Finally some isolated misclassified components are removed by a morphology operator . The proposed scheme offers robust and fast performance especially for both handwritten and printed documents which compares favorably with other binarization methods . \\n'],\n",
              " [' The Bag of Words framework is well known in image classification . In the framework there are two essential steps 1 coding which encodes local features by a visual vocabulary and 2 pooling which pools over the response of all features into image representation . Many coding and pooling methods are proposed and how to apply them better in different conditions has become a practical problem . In this paper to better use BoW in different applications we study the relation between many typical coding methods and two popular pooling methods . Specifically complete combinations of coding and pooling are evaluated based on an extremely large range of vocabulary sizes on five primary and popular datasets . Three typical ones are 15 Scenes Caltech 101 and PASCAL VOC 2007 while the other two large scale ones are Caltech 256 and ImageNet . Based on the systematic evaluation some interesting conclusions are drawn . Some conclusions are the extensions of previous viewpoints while some are different but important to understand BoW model . Based on these conclusions we provide detailed application criterions by evaluating coding and pooling based on precision efficiency and memory requirements in different applications . We hope that this study can be helpful to evaluate different coding and pooling methods the conclusions can be beneficial to better understand BoW and the application criterions can be valuable to use BoW better in different applications . \\n'],\n",
              " [' Modern appearance based object recognition systems typically involve feature descriptor extraction and matching stages . The extracted descriptors are expected to be robust to illumination changes and to reasonable image object transformations . Some descriptors work well for general object matching but gray scale key point based methods may be suboptimal for matching line rich color scenes objects such as cars buildings and faces . We present a Rotation and Scale Invariant Line based Color aware descriptor which allows matching of objects scenes in terms of their key lines line region properties and line spatial arrangements . An important special application is face matching since face characteristics are best captured by lines curves . We tested RSILC performance on publicly available datasets and compared it with other descriptors . Our experiments show that RSILC is more accurate in line rich object description than other well known generic object descriptors . \\n'],\n",
              " [' Improper camera orientation produces convergent vertical lines and skewed horizon lines in digital pictures an a posteriori processing is then necessary to obtain appealing pictures . We show here that after accurate calibration the camera on board accelerometer can be used to automatically generate an alternative perspective view from a virtual camera leading to images with residual keystone and horizon distortions that are essentially imperceptible at visual inspection . Furthermore we describe the uncertainty on the position of each pixel in the corrected image with respect to the accelerometer noise . Experimental results show a similar accuracy for a smartphone and for a digital reflex camera . The method can find application in customer imaging devices as well as in the computer vision field especially when reference vertical and horizontal features are not easily detectable in the image . \\n'],\n",
              " [' Several studies of Web server workloads have hypothesized that these workloads are self similar . The explanation commonly advanced for this phenomenon is that the distribution of Web server requests may be heavy tailed . However there is another possible explanation self similarity can also arise from deterministic chaotic processes . To our knowledge this possibility has not previously been investigated and so existing studies on Web workloads lack an adequate comparison against this alternative . We conduct an empirical study of workloads from two different Web sites one public university and one private company using the largest datasets that have been described in the literature . Our study employs methods from nonlinear time series analysis to search for chaotic behavior in the web logs of these two sites . While we do find that the deterministic components are significant components in these time series we do not find evidence of chaotic behavior . Predictive modeling experiments contrasting heavy tailed with deterministic models showed that both approaches were equally effective in modeling our datasets . \\n'],\n",
              " ['We present a novel method that tackles the problem of facial landmarking in unconstrained conditions within the part based framework. Part based methods alternate the evaluation of local appearance models to produce a per point response map and a shape fitting step which finds a valid face shape that maximises the sum of the per point responses. Our approach focuses on obtaining better appearance models for the creation of the response maps and it can be used in combination with any shape fitting strategy. Local appearance models need to tackle very heterogeneous data when dealing with in the wild imagery due to factors as varying head poses facial expressions identity lighting conditions or image quality among others. Pose wise experts are typically used in this scenario so that each expert deals with more homogeneous data. However the computation cost at test time is significantly increased. Furthermore choosing the right expert is not straightforward which can lead to gross errors. We propose to dynamically select at test time the training examples used for inference. We use a global similarity measure to select the most adequate training examples for inference and create a single test sample specific expert using a localised inference technique. To illustrate the validity of these ideas we capitalise on the recently proposed use of regression to generate local appearance models. In particular we use Gaussian processes as their non parametric nature easily allows for localised regression. This novel way of constructing the response maps is combined with two state of the art standard shape fitting algorithms the popular Constrained Local Models framework and the Consensus of Exemplars method. We validate our method on two publicly available datasets as well as on a cross dataset experiment showing a considerable performance improvement of the proposed approach. \\n'],\n",
              " [' In sponsored search many advertisers have not achieved their expected performances while the search engine also has a large room to improve their revenue . Specifically due to the improper keyword bidding many advertisers can not survive the competitive ad auctions to get their desired ad impressions meanwhile a significant portion of search queries have no ads displayed in their search result pages even if many of them have commercial values . We propose recommending a group of relevant yet less competitive keywords to an advertiser . Hence the advertiser can get the chance to win some ad slots and accumulate a number of impressions . At the same time the revenue of the search engine can also be boosted since many empty ad shots are filled . Mathematically we model the problem as a mixed integer programming problem which maximizes the advertiser revenue and the relevance of the recommended keywords while minimizing the keyword competitiveness subject to the bid and budget constraints . By solving the problem we can offer an optimal group of keywords and their optimal bid prices to an advertiser . Simulation results have shown the proposed method is highly effective in increasing ad impressions expected clicks advertiser revenue and search engine revenue . \\n'],\n",
              " [' People re identification has been a very active research topic recently in computer vision . It is an important application in surveillance systems with disjoint cameras . In this paper a framework is proposed to extract descriptors of people in videos which are based on soft biometric traits and can be further used for people re identification or other applications . Soft biometric based description is more invariant to changing factors than directly using low level features such as color and texture . The ensemble of a set of soft biometric traits can achieve good performance in people re identification . In the proposed method the body of detected people is divided into three parts and the selected soft biometric traits are extracted from each part . All traits are then combined to form the final descriptor and people re identification is performed based on the descriptor and Nearest Neighbor matching strategy . The experiments are carried out on SAIVT SoftBio database which consists of videos from disjoint surveillance cameras as well as some static image based datasets . An open ID recognition problem is also evaluated for the proposed method . Comparisons with some state of the art methods are provided as well . The experiment results show the good performance of the proposed framework . \\n'],\n",
              " [' Privacy preserving collaborative filtering is an emerging web adaptation tool to cope with information overload problem without jeopardizing individuals privacy . However collaborative filtering with privacy schemes commonly suffer from scalability and sparseness as the content in the domain proliferates . Moreover applying privacy measures causes a distortion in collected data which in turn defects accuracy of such systems . In this work we propose a novel privacy preserving collaborative filtering scheme based on bisecting k means clustering in which we apply two preprocessing methods . The first preprocessing scheme deals with scalability problem by constructing a binary decision tree through a bisecting k means clustering approach while the second produces clones of users by inserting pseudo self predictions into original user profiles to boost accuracy of scalability enhanced structure . Sparse nature of collections are handled by transforming ratings into item features based profiles . After analyzing our scheme with respect to privacy and supplementary costs we perform experiments on benchmark data sets to evaluate it in terms of accuracy and online performance . Our empirical outcomes verify that combined effects of the proposed preprocessing schemes relieve scalability and augment accuracy significantly . \\n'],\n",
              " [' Accurate reconstruction of 3D geometrical shape from a set of calibrated 2D multiview images is an active yet challenging task in computer vision . The existing multiview stereo methods usually perform poorly in recovering deeply concave and thinly protruding structures and suffer from several common problems like slow convergence sensitivity to initial conditions and high memory requirements . To address these issues we propose a two phase optimization method for generalized reprojection error minimization where a generalized framework of reprojection error is proposed to integrate stereo and silhouette cues into a unified energy function . For the minimization of the function we first introduce a convex relaxation on 3D volumetric grids which can be efficiently solved using variable splitting and Chambolle projection . Then the resulting surface is parameterized as a triangle mesh and refined using surface evolution to obtain a high quality 3D reconstruction . Our comparative experiments with several state of the art methods show that the performance of TwGREM based 3D reconstruction is among the highest with respect to accuracy and efficiency especially for data with smooth texture and sparsely sampled viewpoints . \\n'],\n",
              " ['This paper proposes to model an action as the output of a sequence of atomic Linear Time Invariant LTI systems. The sequence of LTI systems generating the action is modeled as a Markov chain where a Hidden Markov Model HMM is used to model the transition from one atomic LTI system to another. In turn the LTI systems are represented in terms of their Hankel matrices. For classification purposes the parameters of a set of HMMs one for each action class are learned via a discriminative approach. This work proposes a novel method to learn the atomic LTI systems from training data and analyzes in detail the action representation in terms of a sequence of Hankel matrices. Extensive evaluation of the proposed approach on two publicly available datasets demonstrates that the proposed method attains state of the art accuracy in action classification from the 3D locations of body joints skeleton . \\n'],\n",
              " ['One of the leading time of flight imaging technologies for depth sensing is based on Photonic Mixer Devices PMD . In PMD sensors each pixel samples the correlation between emitted and received light signals. Current PMD cameras compute eight correlation samples per pixel in four sequential stages to obtain depth with invariance to signal amplitude and offset variations. With motion PMD pixels capture different depths at each stage. As a result correlation samples are not coherent with a single depth producing artifacts. We propose to detect and remove motion artifacts from a single frame taken by a PMD camera. The algorithm we propose is very fast simple and can be easily included in camera hardware. We recover depth of each pixel by exploiting consistency of the correlation samples and local neighbors of the pixel. In addition our method obtains the motion flow of occluding contours in the image from a single frame. The system has been validated in real scenes using a commercial low cost PMD camera and high speed dynamics. In all cases our method produces accurate results and it highly reduces motion artifacts. \\n'],\n",
              " [' Downloading software via Web is a major solution for publishers to deliver their software products . In this context user interfaces for software downloading play a key role . Actually they have to allow usable interactions as well as support users in taking conscious and coherent decisions about whether to accept to download a software product or not . This paper presents different design alternatives for software download interfaces i.e . the interface that prompts the user if he wishes to actually complete its download and evaluates their ability to improve the quality of user interactions while reducing errors in user decisions . More precisely we compare Authenticode the leading software download interface for Internet Explorer to Question Answer a software download interface previously proposed by the authors Dini Foglia Prete Zanda . Furthermore we evaluate the effect of extending both interfaces by means of a reputation system similar to the eBay Feedback Forum . The results of the usability studies show that the pure Question Answer interface is the most effective in minimizing users incoherent behaviors and the differences in reputation rankings significantly influence users . Overall results suggest guidelines to design the best interface depending on the context . \\n'],\n",
              " [' Preprocessing is one of the key components in a typical text classification framework . This paper aims to extensively examine the impact of preprocessing on text classification in terms of various aspects such as classification accuracy text domain text language and dimension reduction . For this purpose all possible combinations of widely used preprocessing tasks are comparatively evaluated on two different domains namely e mail and news and in two different languages namely Turkish and English . In this way contribution of the preprocessing tasks to classification success at various feature dimensions possible interactions among these tasks and also dependency of these tasks to the respective languages and domains are comprehensively assessed . Experimental analysis on benchmark datasets reveals that choosing appropriate combinations of preprocessing tasks rather than enabling or disabling them all may provide significant improvement on classification accuracy depending on the domain and language studied on . \\n'],\n",
              " [' This paper introduces an action recognition system based on a multiscale local part model . This model includes both a coarse primitive level root patch covering local global information and higher resolution overlapping part patches incorporating local structure and temporal relations . Descriptors are then computed over the local part models by applying fast random sampling at very high density . We also improve the recognition performance using a discontinuity preserving optical flow algorithm . The evaluation shows that the feature dimensions can be reduced by 7 8 through PCA while preserving high accuracy . Our system achieves state of the art results on large challenging realistic datasets namely 61.0 on HMDB51 92.0 on UCF50 86.6 on UCF101 and 65.3 on Hollywood2 . \\n'],\n",
              " [' The process of creating a photo product such as a photobook calendar or collage from a large personal image collection requires intensive user effort . The primary goal of the current research was to develop an end to end solution to the problem of photo product generation that enables the user to complete the process with minimal edits where the system intelligently selects assets and groups them before presenting the output to the user . The automation is driven by metadata extracted both from individual images as well as from sets of assets in a collection . In particular we use an automatically detected event hierarchy to establish meaningful groupings in the assets and to determine an appropriate grouping and pagination for the final product . We propose a novel intermediate construct called a storyboard which can be translated to different product types without recomputing the underlying metadata . In addition to chronological storyboards we also describe a novel hybrid storyboard that joins chronological image presentation with groups of images of a common theme . A pagination algorithm uses the information in the storyboard and the product constraints to generate a product . Finally the user is provided with a metadata driven editing mechanism that makes it easy to change the auto populated product . Given that the proposed system envisions user interaction in creating the final product user studies are conducted to judge the usefulness of the system where consumers use the system to generate a photobook with their own images . \\n'],\n",
              " ['Advanced segmentation techniques in the surveillance domain deal with shadows to avoid distortions when detecting moving objects. Most approaches for shadow detection are still typically restricted to penumbra shadows and cannot cope well with umbra shadows. Consequently umbra shadow regions are usually detected as part of moving objects thus affecting the performance of the final detection. In this paper we address the detection of both penumbra and umbra shadow regions. First a novel bottom up approach is presented based on gradient and colour models which successfully discriminates between chromatic moving cast shadow regions and those regions detected as moving objects. In essence those regions corresponding to potential shadows are detected based on edge partitioning and colour statistics. Subsequently i temporal similarities between textures and ii spatial similarities between chrominance angle and brightness distortions are analysed for each potential shadow region for detecting the umbra shadow regions. Our second contribution refines even further the segmentation results a tracking based top down approach increases the performance of our bottom up chromatic shadow detection algorithm by properly correcting non detected shadows. To do so a combination of motion filters in a data association framework exploits the temporal consistency between objects and shadows to increase the shadow detection rate. Experimental results exceed current state of the art in shadow accuracy for multiple well known surveillance image databases which contain different shadowed materials and illumination conditions. \\n'],\n",
              " [' Sentiment analysis from data streams is aimed at detecting authors attitude emotions and opinions from texts in real time . To reduce the labeling effort needed in the data collection phase active learning is often applied in streaming scenarios where a learning algorithm is allowed to select new examples to be manually labeled in order to improve the learner s performance . Even though there are many on line platforms which perform sentiment analysis there is no publicly available interactive on line platform for dynamic adaptive sentiment analysis which would be able to handle changes in data streams and adapt its behavior over time . This paper describes ClowdFlows a cloud based scientific workflow platform and its extensions enabling the analysis of data streams and active learning . Moreover by utilizing the data and workflow sharing in ClowdFlows the labeling of examples can be distributed through crowdsourcing . The advanced features of ClowdFlows are demonstrated on a sentiment analysis use case using active learning with a linear Support Vector Machine for learning sentiment classification models to be applied to microblogging data streams . \\n'],\n",
              " [' We propose a real time multi view landmark detector based on Deformable Part Models . The detector is composed of a mixture of tree based DPMs each component describing landmark configurations in a specific range of viewing angles . The usage of view specific DPMs allows to capture a large range of poses and to deal with the problem of self occlusions . Parameters of the detector are learned from annotated examples by the Structured Output Support Vector Machines algorithm . The learning objective is directly related to the performance measure used for detector evaluation . The tree based DPM allows to find a globally optimal landmark configuration by the dynamic programming . We propose a coarse to fine search strategy which allows real time processing by the dynamic programming also on high resolution images . Empirical evaluation on in the wild images shows that the proposed detector is competitive with the state of the art methods in terms of speed and accuracy yet it keeps the guarantee of finding a globally optimal estimate in contrast to other methods . \\n'],\n",
              " [' Efficient feature description and classification of dynamic texture is an important problem in computer vision and pattern recognition . Recently the local binary pattern based dynamic texture descriptor has been proposed to classify DTs by extending the LBP operator used in static texture analysis to the temporal domain . However the extended LBP operator can not characterize the intrinsic motion of dynamic texture well . In this paper we propose a novel video set based collaborative representation dynamic texture classification method . First we divide the dynamic texture sequence into subsequences along the temporal axis to form the video set . For each DT we extract the video set based LBP histogram to describe it . We then propose a regularized collaborative representation model to code the LBP histograms of the query video sets over the LBP histograms of the training video sets . Finally with the coding coefficients the distance between the query video set and the training video sets can be calculated for classification . Experimental results on the benchmark dynamic texture datasets demonstrate that the proposed method can yield good performance in terms of both classification accuracy and efficiency . \\n'],\n",
              " [' The detection of vanishing points in a monoscopic image is a first step to the extraction of 3D data . This article shows a partition of the image space in order to determine the type of perspective which is present thereby allowing the determination of the vanishing point associated with each of the axes of the special reference system . Additionally the Thales second theorem allows us to determine the position of the vanishing points of the image and to automatize the process . An algorithm has been used with the data provided by the selected edge detector which provides the location of the vanishing points contained on the image plane . The comparative study includes two variables the number of vanishing points and the image resolution . The results show that in general the Prewitt s edge detector provides the best results both positional and statistical . Increasing the image resolution improves the results although the results obtained for a resolution of 640 480 pixels and another of 1024 768 pixels are very similar . \\n'],\n",
              " [' Automatic optical inspection plays an important role to control the appearance quality of wide range of products in the product process . Recently the high popularity of smartphones and information appliances drives significant demand of touch panels . However the traditional frequency based method which exploits the line structure feature of texture images is not effective for the defect detection of touch panels . The paper presents a novel spatial domain algorithm to inspect the defects on touch panel . By utilizing the characteristics of periodic patterns of the sensing circuits an adaptive model for each pattern is learned online to effectively extract defects . The experimental results indicate that our proposed method achieves accurate detection with efficient computation . In addition the users pay very little effort for the testing of different panel products . \\n'],\n",
              " ['The difficulty of face recognition FR systems to operate efficiently in diverse operational environments e.g. day and night time is aided by employing sensors covering different spectral bands i.e. visible and infrared . Biometric practitioners have identified a framework of band specific algorithms which can contribute to both assessment and intervention. While these motions are proven to achieve improvement of identification performance they traditionally result in solutions that typically fail to work efficiently across multiple spectrums. In this work we designed and developed an efficient fully automated direct matching based FR approach that is designed to operate efficiently when face data is captured using either visible or passive infrared IR sensors. Thus it can be applied in both daytime and nighttime environments. First input face images are geometrically normalized using our pre processing pipeline prior to feature extraction. Then face based features including wrinkles veins as well as edges of facial characteristics are detected and extracted for each operational band visible MWIR and LWIR . Finally global and local face based matching is applied before fusion is performed at the score level. Our approach achieves a rank 1 identification rate of at least 99.43 regardless of the spectrum of operation. This suggests that our approach results in better performance than other tested standard commercial and academic face based matchers on all spectral bands used. \\n'],\n",
              " [' A trie is one of the data structures for keyword matching . It is used in natural language processing IP address routing and so on . It is represented by the matrix form the link form the double array and LOUDS . The double array representation combines retrieval speed of the matrix form with compactness of the list form . LOUDS is a succinct data structure using bit string . Retrieval speed of LOUDS is not faster than that of the double array but its space usage is smaller . This paper proposes a compressed version of the double array by dividing the trie into multiple levels and removing the BASE array from the double array . Moreover a retrieval algorithm and a construction algorithm are proposed . According to the presented experimental results for pseudo and real data sets the retrieval speed of the presented method is almost the same as the double array and its space usage is compressed to 66 comparing with LOUDS for a large set of keywords with fixed length . \\n'],\n",
              " [' Pose estimation is a key concern in 3D urban surveying mapping and navigation . Although Global Positioning System technologies can be used to estimate a robot s or vehicle s pose there are many urban environments in which GPS functions poorly or not at all . For these situations we offer a novel approach based on a careful fusion of panoramic camera data and 2D laser scanner input . First a Constrained Bundle Adjustment is introduced to handle scale and loop closure constraints . The fusion of a panoramic image series and laser data then enables an accurate scale to be estimated and loop closures detected . Finally the two geometric constraints are enforced on the global CBA solution which in turn produces a robust pose estimate . Experiments show that the proposed method is practicable and more accurate than vision only methods with an average error of just 0.2m in the horizontal plane over a 580m trajectory . \\n'],\n",
              " ['Locally affine transformation with globally elastic interpolation is a common strategy for non rigid registration. Current techniques improve the registration accuracy by only processing the sub images that contain well defined structures quantified by Moran s spatial correlation. As an indicator Moran s metric successfully excludes noisy structures that result in misleading global optimum in terms of similarity. However some well defined structures with intensity only varying in one direction may also cause mis registration. In this paper we propose a new metric based on the response of a similarity function to quantify the ability of being correctly registered for each sub image. Using receiver operating characteristic analysis we show that the proposed metric more accurately reflects such ability than Moran s metric. Incorporating the proposed metric into a hierarchical non rigid registration scheme we show that registration accuracy is improved relative to Moran s metric. \\n'],\n",
              " [' In this paper we propose a method for face recognition by using the two dimensional discrete wavelet transform and a new patch strategy . Based on the average image of all training samples by using integral projection technique for two top level s high frequency sub bands of 2D DWT we propose a non uniform patch strategy for the top level s low frequency sub band . This patch strategy is more suitable to reflect the structure feature of face image and it is better for retaining the integrity of local information . By applying the obtained patch strategy to all samples we obtain patches of training samples and testing samples and then give the final decision by using the nearest neighbor classifier and the majority voting . Experiments are run on the AR FERET Extended Yale B and LFW face databases . The obtained numerical results show that the new face recognition method outperforms the traditional 2D DWT method and some state of the art patch based methods . \\n'],\n",
              " [' The norm of practice in estimating graph properties is to use uniform random node samples whenever possible . Many graphs are large and scale free inducing large degree variance and estimator variance . This paper shows that random edge sampling and the corresponding harmonic mean estimator for average degree can reduce the estimation variance significantly . First we demonstrate that the degree variance and consequently the variance of the RN estimator can grow almost linearly with data size for typical scale free graphs . Then we prove that the RE estimator has a variance bounded from above . Therefore the variance ratio between RN and RE samplings can be very large for big data . The analytical result is supported by both simulation studies and 18 real networks . We observe that the variance reduction ratio can be more than a hundred for some real networks such as Twitter . Furthermore we show that random walk sampling is always worse than RE sampling and it can reduce the variance of RN method only when its performance is close to that of RE sampling . \\n'],\n",
              " [' Several Web 2.0 applications allow users to assign keywords to provide better organization and description of the shared content . Tag recommendation methods may assist users in this task improving the quality of the available information and thus the effectiveness of various tag based information retrieval services such as searching content recommendation and classification . This work addresses the tag recommendation problem from two perspectives . The first perspective centered at the object aims at suggesting relevant tags to a target object jointly exploiting the following three dimensions tag co occurrences terms extracted from multiple textual features and various metrics to estimate tag relevance . The second perspective centered at both object and user aims at performing personalized tag recommendation to a target object user pair exploiting in addition to the three aforementioned dimensions a metric that captures user interests . In particular we propose new heuristic methods that extend state of the art strategies by including new metrics that estimate how accurately a candidate tag describes the target object . We also exploit three learning to rank based techniques namely RankSVM Genetic Programming and Random Forest for generating ranking functions that exploit multiple metrics as attributes to estimate the relevance of a tag to a given object or object user pair . We evaluate the proposed methods using data from four popular Web 2.0 applications namely Bibsonomy LastFM YouTube and YahooVideo . Our new heuristics for object centered tag recommendation provide improvements in precision over the best state of the art alternative of 12 on average while our new heuristics for personalized tag recommendation produce average gains in precision of 121 over the baseline . Similar performance gains are also achieved in terms of other metrics notably recall Normalized Discounted Cumulative Gain and Mean Reciprocal Rank . Further improvements for both object centered and personalized tag recommendation can also be achieved with our new L2R based strategies which are flexible and can be easily extended to exploit other aspects of the tag recommendation problem . Finally we also quantify the benefits of personalized tag recommendation to provide better descriptions of the target object when compared to object centered recommendation by focusing only on the relevance of the suggested tags to the object . We find that our best personalized method outperforms the best object centered strategy with average gains in precision of 10 . \\n'],\n",
              " ['In this paper a tracking method based on sequential Bayesian inference is proposed. The proposed method focuses on solving both the problem of tracking under partial occlusions and the problem of non rigid object tracking in real time on a desktop personal computer PC . The proposed method is mainly composed of two parts 1 modeling the target object using elastic structure of local patches for robust performance and 2 efficient hierarchical diffusion method to perform the tracking procedure in real time. The elastic structure of local patches allows the proposed method to handle partial occlusions and non rigid deformations through the relationship among neighboring patches. The proposed hierarchical diffusion method generates samples from the region where the posterior is concentrated to reduce computation time. The method is extensively tested on a number of challenging image sequences with occlusion and non rigid deformation. The experimental results show the real time capability and the robustness of the proposed method under various situations. \\n'],\n",
              " ['In robot localization particle filtering can estimate the position of a robot in a known environment with the help of sensor data. In this paper we present an approach based on particle filtering for accurate stereo matching. The proposed method consists of three parts. First we utilize multiple disparity maps in order to acquire a very distinctive set of features called landmarks and then we use segmentation as a grouping technique. Secondly we apply scan line particle filtering using the corresponding landmarks as a virtual sensor data to estimate the best disparity value. Lastly we reduce the computational redundancy of particle filtering in our stereo correspondence with a Markov chain model given the previous scan line values. More precisely we assist particle filtering convergence by adding a proportional weight in the predicted disparity value estimated by Markov chains. In addition to this we optimize our results by applying a plane fitting algorithm along with a histogram technique to refine any outliers. This work provides new insights into stereo matching methodologies by taking advantage of global geometrical and spatial information from distinctive landmarks. Experimental results show that our approach is capable of providing high quality disparity maps comparable to other well known contemporary techniques. \\n'],\n",
              " [' In this paper a local approach for 3D object recognition is presented . It is based on the topological invariants provided by the critical points of the 3D object . The critical points and the links between them are represented by a set of size functions obtained after splitting the 3D object into portions . A suitable similarity measure is used to compare the sets of size functions associated with the 3D objects . In order to validate our approach s recognition performance we used different collections of 3D objects . The obtained scores are favourably comparable to the related work . \\n'],\n",
              " ['This paper introduces a novel topic model for learning a robust object model. In this hierarchical model the layout topic is used to capture the local relationships among a limited number of parts when the part topic is used to locate the potential part regions. Naturally an object model is represented as a probability distribution over a set of parts with certain layouts. Rather than a monolithic model our object model is composed of multiple sub category models designed to capture the significant variations in appearance and shape of an object category. Given a set of object instances with a bounding box an iterative learning process is proposed to divide them into several sub categories and learn the corresponding sub category models without any supervision. Through an experiment in object detection the learned object model is examined and the results highlight the advantages of our present method compared with others. \\n'],\n",
              " ['This paper proposes a new method to extract a gait feature from a raw gait video directly. The Space Time Interest Points STIPs are detected where there are significant movements of human body along both spatial and temporal directions in local spatio temporal volumes of a raw gait video. Then a histogram of STIP descriptors HSD is constructed as a gait feature. In the classification stage the support vector machine SVM is applied to recognize gaits based on HSDs. In this study the standard multi class i.e. multiple subjects classification can often be computationally infeasible at test phase when gait recognition is performed by using every possible classifiers i.e. SVM models trained for all individual subjects. In this paper the attribute based classification is applied to reduce the number of SVM models needed for recognizing each probe gait. This process will significantly reduce the test time computational complexity and also retain or even improve the recognition accuracy. When compared with other existing methods in the literature the proposed method is shown to have the promising performance for the case of normal walking and the outstanding performance for the cases of walking with variations such as walking with carrying a bag and walking with varying a type of clothes. \\n'],\n",
              " [' Many studies have confirmed gait as a robust biometric feature for identification of individuals . However direction changes cause difficulties for most of the gait recognition systems due to appearance changes . This study presents an efficient multi view gait recognition method that allows curved trajectories on unconstrained paths in indoor environments . The recognition is based on volumetric analysis of the human gait to exploit most of the 3D information enclosed in it . Appearance based gait descriptors are extracted from 3D gait volumes and temporal patterns of them are classified using a Support Vector Machine with a sliding temporal window for majority voting . The proposed approach is experimentally validated on the AVA Multi View Dataset and on the Kyushu University 4D Gait Database . The results show that this new approach is able to identify people walking on curved paths . \\n'],\n",
              " [' This paper proposes a novel method to address the registration of images with affine transformation . Firstly the Maximally Stable Extremal Region detection method is performed on the reference image and the image to be registered respectively . And the coarse affine transformation matrix between the two images is estimated by the matched MSER pairs . Two circular regions containing roughly the same image content are also obtained by fitting and normalizing the centroids of the matched MSERs from the two images . Secondly a scale invariant and approximate affine transformation invariant feature point detection algorithm based on the Gabor filter decomposition and phase congruency is performed on the two coarsely aligned regions and two feature point sets are achieved respectively . Finally the affine transformation matrix between the two feature point sets is obtained by using a probabilistic point set registration algorithm and the final affine transformation matrix between the reference image and the image to be registered is achieved according to the coarse affine transformation matrix and the affine transformation matrix between the two feature point sets . Several sets of experiments demonstrate that our proposed method performs competitively with the classical scale invariant feature transform method for images with scale changes and performs better than the traditional MSER and Affine SIFT methods for images with affine distortions . Moreover the proposed method shows higher computation efficiency and robustness to illumination change than some existing area based or feature based methods do . \\n'],\n",
              " [' Influence theories constitute formal models that identify those individuals that are able to affect and guide their peers through their activity . There is a large body of work on developing such theories as they have important applications in viral marketing recommendations as well as information retrieval . Influence theories are typically evaluated through a manual process that can not scale to data voluminous enough to draw safe representative conclusions . To overcome this issue we introduce in this paper a formalized framework for large scale automatic evaluation of topic specific influence theories that are specialized in Twitter . Basically it consists of five conjunctive conditions that are indicative of real influence exertion the first three determine which influence theories are compatible with our framework while the other two estimate their relative effectiveness . At the core of these two conditions lies a novel metric that assesses the aggregate sentiment of a group of users and allows for estimating how close the behavior of influencers is to that of the entire community . We put our framework into practice using a large scale test bed with real data from 75 Twitter communities . In order to select the theories that can be employed in our analysis we introduce a generic two dimensional taxonomy that elucidates their functionality . With its help we ended up with five established topic specific theories that are applicable to our settings . The outcomes of our analysis reveal significant differences in their performance . To explain them we introduce a novel methodology for delving into the internal dynamics of the groups of influencers they define . We use it to analyze the implications of the selected theories and based on the resulting evidence we propose a novel partition of influence theories in three major categories with divergent performance . \\n'],\n",
              " [' Despite the fact that both the Efficient Market Hypothesis and Random Walk Theory postulate that it is impossible to predict future stock prices based on currently available information recent advances in empirical research have been proving the opposite by achieving what seems to be better than random prediction performance . We discuss some of the advantages of the most widely used performance metrics and conclude that is difficult to assess the external validity of performance using some of these measures . Moreover there remain many questions as to the real world applicability of these empirical models . In the first part of this study we design novel stock price prediction models based on state of the art text mining techniques to assert whether we can predict the movement of stock prices more accurately by including indicators of irrationality . Along with this we discuss which metrics are most appropriate for which scenarios in order to evaluate the models . Finally we discuss how to gain insight into text mining based stock price prediction models in order to evaluate validate and refine the models . \\n'],\n",
              " [' Relevance Based Language Models commonly known as Relevance Models are successful approaches to explicitly introduce the concept of relevance in the statistical Language Modelling framework of Information Retrieval . These models achieve state of the art retrieval performance in the pseudo relevance feedback task . On the other hand the field of recommender systems is a fertile research area where users are provided with personalised recommendations in several applications . In this paper we propose an adaptation of the Relevance Modelling framework to effectively suggest recommendations to a user . We also propose a probabilistic clustering technique to perform the neighbour selection process as a way to achieve a better approximation of the set of relevant items in the pseudo relevance feedback process . These techniques although well known in the Information Retrieval field have not been applied yet to recommender systems and as the empirical evaluation results show both proposals outperform individually several baseline methods . Furthermore by combining both approaches even larger effectiveness improvements are achieved . \\n'],\n",
              " [' In this paper we propose improved variants of the sentence retrieval method TF ISF . The improvement is achieved by using context consisting of neighboring sentences and at the same time promoting the retrieval of longer sentences . We thoroughly compare new modified TF ISF methods to the TF ISF baseline to an earlier attempt to include context into TF ISF named tfmix and to a language modeling based method that uses context and promoting retrieval of long sentences named 3MMPDS . Experimental results show that the TF ISF method can be improved using local context . Results also show that the TF ISF method can be improved by promoting the retrieval of longer sentences . Finally we show that the best results are achieved when combining both modifications . All new methods also show statistically significant better results than the other tested methods . \\n'],\n",
              " [' The evaluation of the scale of an object in a cluttered background is a serious problem in computer vision . The most existing contour based approaches relevant to object detection address this problem by normalizing descriptor or multi scale searching such as sliding window searching spatial pyramid model etc . Besides Hough voting framework can predict the scale of an object according to some meaning fragments . However utilizing scale variant descriptor or complicated structure in these measures reduces the efficiency of detection . In the present paper we propose a novel shape feature called scale invariant contour segment context . This feature is based on the angle between contour line segments . It remains unchanged as scale varies . Most importantly it evaluates the scale of objects located in cluttered images and facilitates localization of the boundary of the object in unseen images simultaneously . In this way we need to focus on just the shape matching algorithm without considering the variant scale of the object in an image . This is a procedure which absolutely differs from voting and sliding window searching . We do experiments on ETHZ shape dataset Weizmann horses dataset and the bottle subset from PASCAL datasets . The results confirm that the present model of object detection based on CSC outperforms state of the art of shape based detection methods . \\n'],\n",
              " [' An analysis of the relative motion and point feature model configurations leading to solution degeneracy is presented for the case of a Simultaneous Localization and Mapping system using multicamera clusters with non overlapping fields of view . The SLAM optimization system seeks to minimize image space reprojection error and is formulated for a cluster containing any number of component cameras observing any number of point features over two keyframes . The measurement Jacobian is transformed to expose a reduced dimension representation such that the degeneracy of the system can be determined by the rank of a dense submatrix . A set of relative motions sufficient for degeneracy are identified for certain cluster configurations independent of target model geometry . Furthermore it is shown that increasing the number of cameras within the cluster and observing features across different cameras over the two keyframes reduces the size of the degenerate motion sets significantly . \\n'],\n",
              " [' The article describes a reconstruction pipeline that generates piecewise planar models of man made environments using two calibrated views . The 3D space is sampled by a set of virtual cut planes that intersect the baseline of the stereo rig and implicitly define possible pixel correspondences across views . The likelihood of these correspondences being true matches is measured using signal symmetry analysis which enables to obtain profile contours of the 3D scene that become lines whenever the virtual cut planes intersect planar surfaces . The detection and estimation of these lines cuts is formulated as a global optimization problem over the symmetry matching cost and pairs of reconstructed lines are used to generate plane hypotheses that serve as input to PEARL clustering . The PEARL algorithm alternates between a discrete optimization step which merges planar surface hypotheses and discards detections with poor support and a continuous optimization step which refines the plane poses taking into account surface slant . The pipeline outputs an accurate semi dense Piecewise Planar Reconstruction of the 3D scene . In addition the input images can be segmented into piecewise planar regions using a standard labeling formulation for assigning pixels to plane detections . Extensive experiments with both indoor and outdoor stereo pairs show significant improvements over state of the art methods with respect to accuracy and robustness . \\n'],\n",
              " [' Fully automatic annotation of tennis game using broadcast video is a task with a great potential but with enormous challenges . In this paper we describe our approach to this task which integrates computer vision machine listening and machine learning . At the low level processing we improve upon our previously proposed state of the art tennis ball tracking algorithm and employ audio signal processing techniques to detect key events and construct features for classifying the events . At high level analysis we model event classification as a sequence labelling problem and investigate four machine learning techniques using simulated event sequences . Finally we evaluate our proposed approach on three real world tennis games and discuss the interplay between audio vision and learning . To the best of our knowledge our system is the only one that can annotate tennis game at such a detailed level . \\n'],\n",
              " [' Feature pooling is a key component in modern visual classification system . However the conventional two prevailing pooling techniques namely average and max poolings are not theoretically optimal due to the unrecoverable loss of the spatial information during the statistical summarization and the underlying over simplified assumption about the feature distribution . Addressing these issues this paper proposes to generalize previous pooling methods toward a weighted norm spatial pooling function tailored for class specific feature spatial distribution . Optimizing such a pooling function toward discriminative class separability that is subject to a spatial smoothness constraint yields a so called geometric norm pooling method . Furthermore to handle the variation of object scale position which would affect not only the learning of discriminative pooling weights but also the applicability of the learned weights we propose a simple yet effective self alignment step during both learning and testing to adaptively adjust the pooling weights for individual images . Image segmentation and visual saliency map are utilized to construct a directed pixel adjacency graph . The discriminative pooling weights are diffused using random walk on the constructed graph and therefore the discriminative pooling weights are propagated onto the salient and foreground region . This leads to a robust version of GLP which can cope with the misalignment of object position and scale in images . Comprehensive experiments validate the effectiveness of the proposed GLP feature pooling framework . The proposed random walk based self alignment step can effectively alleviate the image misalignment issue and further boost classification accuracy . State of the art image classification and action recognition performances are attained on several benchmarks . \\n'],\n",
              " ['Representation of facial expressions using continuous dimensions has shown to be inherently more expressive and psychologically meaningful than using categorized emotions and thus has gained increasing attention over recent years. Many sub problems have arisen in this new field that remain only partially understood. A comparison of the regression performance of different texture and geometric features and the investigation of the correlations between continuous dimensional axes and basic categorized emotions are two of these. This paper presents empirical studies addressing these problems and it reports results from an evaluation of different methods for detecting spontaneous facial expressions within the arousal valence AV dimensional space. The evaluation compares the performance of texture features SIFT Gabor LBP against geometric features FAP based distances and the fusion of the two. It also compares the prediction of arousal and valence obtained using the best fusion method to the corresponding ground truths. Spatial distribution shift similarity and correlation are considered for the six basic categorized emotions i.e. anger disgust fear happiness sadness surprise . Using the NVIE database results show that the fusion of LBP and FAP features performs the best. The results from the NVIE and FEEDTUM databases reveal novel findings about the correlations of arousal and valence dimensions to each of six basic emotion categories. \\n'],\n",
              " [' This paper reports on an approach to the analysis of form during genre recognition recorded using eye tracking . The researchers focused on eight different types of e mail such as calls for papers newsletters and spam which were chosen to represent different genres . The study involved the collection of oculographic behavior data based on the scanpath duration and scanpath length based metric to highlight the ways in which people view the features of genres . We found that genre analysis based on purpose and form was an effective means of identifying the characteristics of these e mails . The research carried out on a group of 24 participants highlighted their interaction and interpretation of the e mail texts and the visual cues or features perceived . In addition the ocular strategies of scanning and skimming they employed for the processing of the texts by block genre and representation were evaluated . \\n'],\n",
              " [' Named entity recognition is mostly formalized as a sequence labeling problem in which segments of named entities are represented by label sequences . Although a considerable effort has been made to investigate sophisticated features that encode textual characteristics of named entities little attention has been paid to segment representations for multi token named entities . In this paper we investigate the effects of different SRs on NER tasks and propose a feature generation method using multiple SRs . The proposed method allows a model to exploit not only highly discriminative features of complex SRs but also robust features of simple SRs against the data sparseness problem . Since it incorporates different SRs as feature functions of Conditional Random Fields we can use the well established procedure for training . In addition the tagging speed of a model integrating multiple SRs can be accelerated equivalent to that of a model using only the most complex SR of the integrated model . Experimental results demonstrate that incorporating multiple SRs into a single model improves the performance and the stability of NER . We also provide the detailed analysis of the results . \\n'],\n",
              " [' Surface defects are important factors of surface quality of industrial products . Most of the traditional machine vision based methods for surface defect recognition have some shortcomings such as low detection rate of defects and high rate of false alarms . Different types of defects have special information at some directions and scales of their images while the traditional methods of feature extraction such as Wavelet transform are unable to get the information at all directions . In this study Shearlet transform is introduced to provide efficient multi scale directional representation and a general framework has been developed to analyze and represent surface defect images with anisotropic information . The metal surface images captured from production lines are decomposed into multiple directional subbands with Shearlet transform and features are extracted from all subbands and combined into a high dimensional feature vector . Kernel Locality Preserving Projection is applied to the dimension reduction of the feature vector . The proposed method is tested with the surface images captured from different production lines and the results show that the classification rates of surface defects of continuous casting slabs hot rolled steels and aluminum sheets are 94.35 95.75 and 92.5 respectively . \\n'],\n",
              " [' Knowledge organization and bibliometrics have traditionally been seen as separate subfields of library and information science but bibliometric techniques make it possible to identify candidate terms for thesauri and to organize knowledge by relating scientific papers and authors to each other and thereby indicating kinds of relatedness and semantic distance . It is therefore important to view bibliometric techniques as a family of approaches to KO in order to illustrate their relative strengths and weaknesses . The subfield of bibliometrics concerned with citation analysis forms a distinct approach to KO which is characterized by its social historical and dynamic nature its close dependence on scholarly literature and its explicit kind of literary warrant . The two main methods co citation analysis and bibliographic coupling represent different things and thus neither can be considered superior for all purposes . The main difference between traditional knowledge organization systems and maps based on citation analysis is that the first group represents intellectual KOSs whereas the second represents social KOSs . For this reason bibliometric maps can not be expected ever to be fully equivalent to scholarly taxonomies but they are along with other forms of KOSs valuable tools for assisting users to orient themselves to the information ecology . Like other KOSs citation based maps can not be neutral but will always be based on researchers decisions which tend to favor certain interests and views at the expense of others . \\n'],\n",
              " [' In this paper we present our solution to the 300 Faces in the Wild Facial Landmark Localization Challenge . We demonstrate how to achieve very competitive localization performance with a simple deep learning based system . Human study is conducted to show that the accuracy of our system has been very close to human performance . We discuss how this finding would affect our future direction to improve our system . \\n'],\n",
              " [' The estimation of query model is an important task in language modeling approaches to information retrieval . The ideal estimation is expected to be not only effective in terms of high mean retrieval performance over all queries but also stable in terms of low variance of retrieval performance across different queries . In practice however improving effectiveness can sacrifice stability and vice versa . In this paper we propose to study this tradeoff from a new perspective i.e . the bias variance tradeoff which is a fundamental theory in statistics . We formulate the notion of bias variance regarding retrieval performance and estimation quality of query models . We then investigate several estimated query models by analyzing when and why the bias variance tradeoff will occur and how the bias and variance can be reduced simultaneously . A series of experiments on four TREC collections have been conducted to systematically evaluate our bias variance analysis . Our approach and results will potentially form an analysis framework and a novel evaluation strategy for query language modeling . \\n'],\n",
              " [' In this work we deal with the problem of high level event detection in video . Specifically we study the challenging problems of i learning to detect video events from solely a textual description of the event without using any positive video examples and ii additionally exploiting very few positive training samples together with a small number of related videos . For learning only from an event s textual description we first identify a general learning framework and then study the impact of different design choices for various stages of this framework . For additionally learning from example videos when true positive training samples are scarce we employ an extension of the Support Vector Machine that allows us to exploit related event videos by automatically introducing different weights for subsets of the videos in the overall training set . Experimental evaluations performed on the large scale TRECVID MED 2014 video dataset provide insight on the effectiveness of the proposed methods . \\n'],\n",
              " [' Social networking sites enable user to personalize their contents and functions . This feature has been assumed as causing positive effects on the use of online information services through enhancing user satisfaction . However unlike other online information services due to the results of personalization in a certain situation SNS users can not help using the SNS even though they feel dissatisfaction on using it . SNSs are different from other information services in the sense that they create and sustain their own value based on the number of participating members . In SNSs personalization reflected by updates and maintenance of profile pages results in such participation . This study hypothesizes that personalization influences on the continued use of SNSs through two factors switching cost and satisfaction . Web based survey was conducted with the samples of 677 SNS users from six universities in the US . In person interviews were conducted with 25 university students to elicit their thoughts on the SNSs . Quantitative analysis employed by testing the proposed model with five hypotheses through a structural equation modeling technique . The transcribed interview data was analyzed following the constant comparative technique . The main findings indicate that as expected the personalization increases its switching cost as well as satisfaction which results in further use of SNSs . These findings suggest that it is necessary to consider both extrinsic and intrinsic factors of user perceptions when adding personalization features on SNSs . \\n'],\n",
              " [' Given a surveillance video of a moving person we present a novel method of estimating layout of a cluttered indoor scene . We propose an idea that trajectories of a moving person can be used to generate features to segment an indoor scene into different areas of interest . We assume a static uncalibrated camera . Using pixel level color and perspective cues of the scene each pixel is assigned to a particular class either a sitting place the ground floor or the static background areas like walls and ceiling . The pixel level cues are locally integrated along global topological order of classes such as sitting objects and background areas are above ground floor into a conditional random field by an ordering constraint . The proposed method yields very accurate segmentation results on challenging real world scenes . We focus on videos with people walking in the scene and show the effectiveness of our approach through quantitative and qualitative results . The proposed estimation method shows better estimation results as compared to the state of the art scene layout estimation methods . We are able to correctly segment 90.3 of background 89.4 of sitting areas and 74.7 of the ground floor . \\n'],\n",
              " [' Some human detection or tracking algorithms output a low dimensional representation of the human body such as a bounding box . Even though this representation is enough for some tasks a more accurate and detailed point wise representation of the human body is more useful for pose estimation and action recognition . The refinement process can produce a point wise mask of the human body from its low dimensional representation . In this paper we tackle the problem of refining low dimensional human shapes using RGB D data with a novel and accurate method for this purpose . This algorithm combines low level cues such as shape and color and high level observations such as the estimated ground plane in a multi layer graph cut framework . In our algorithm shape prior information is learned by training a classifier . Unlike some existing work our method does not utilize or carry features from the internal steps of the methods which provide the bounding box so our method can work on the outputs of any similar shape providers . Extensive experiments demonstrate that the proposed technique significantly outperforms other suitable methods . Moreover a previously published refinement method is extended by incorporating more generic cues to serve this purpose . \\n'],\n",
              " [' Research in natural language processing has increasingly focused on normalizing Twitter messages . Currently while different well defined approaches have been proposed for the English language the problem remains far from being solved for other languages such as Malay . Thus in this paper we propose an approach to normalize the Malay Twitter messages based on corpus driven analysis . An architecture for Malay Tweet normalization is presented which comprises seven main modules enhanced tokenization In Vocabulary detection specialized dictionary query repeated letter elimination abbreviation adjusting English word translation and de tokenization . A parallel Tweet dataset consisting of 9000 Malay Tweets is used in the development and testing stages . To measure the performance of the system an evaluation is carried out . The result is promising whereby we score 0.83 in BLEU against the baseline BLEU which scores 0.46 . To compare the accuracy of the architecture with other statistical approaches an SMT like normalization system is implemented trained and evaluated with an identical parallel dataset . The experimental results demonstrate that we achieve higher accuracy by the normalization system which is designed based on the features of Malay Tweets compared to the SMT like system . \\n'],\n",
              " [' This paper describes the use of Wikipedia as a rich knowledge source for a question answering system . We suggest multiple answer matching modules based on different types of semi structured knowledge sources of Wikipedia including article content infoboxes article structure category structure and definitions . These semi structured knowledge sources each have their unique strengths in finding answers for specific question types such as infoboxes for factoid questions category structure for list questions and definitions for descriptive questions . The answers extracted from multiple modules are merged using an answer merging strategy that reflects the specialized nature of the answer matching modules . Through an experiment our system showed promising results with a precision of 87.1 a recall of 52.7 and an F measure of 65.6 all of which are much higher than the results of a simple text analysis based system . \\n'],\n",
              " ['Nowadays with so many surveillance cameras having been installed the market demand for intelligent violence detection is continuously growing while it is still a challenging topic in research area. Therefore we attempt to make some improvements of existing violence detectors. The primary contributions of this paper are two fold. Firstly a novel feature extraction method named Oriented VIolent Flows OViF which takes full advantage of the motion magnitude change information in statistical motion orientations is proposed for practical violence detection in videos. The comparison of OViF and baseline approaches on two public databases demonstrates the efficiency of the proposed method. Secondly feature combination and multi classifier combination strategies are adopted and excellent results are obtained. Experimental results show that using combined features with AdaBoost Linear SVM achieves improved performance over the state of the art on the Violent Flows benchmark. \\n'],\n",
              " [' Extensible Markup Language documents are associated with time in two ways XML documents evolve over time and XML documents contain temporal information . The efficient management of the temporal and multi versioned XML documents requires optimized use of storage and efficient processing of complex historical queries . This paper provides a comparative analysis of the various schemes available to efficiently store and query the temporal and multi versioned XML documents based on temporal change management versioning and querying support . Firstly the paper studies the multi versioning control schemes to detect manage and query change in dynamic XML documents . Secondly it describes the storage structures used to efficiently store and retrieve XML documents . Thirdly it provides a comparative analysis of the various commercial tools based on change management versioning collaborative editing and validation support . Finally the paper presents some future research and development directions for the multi versioned XML documents . \\n'],\n",
              " [' Collaborative information retrieval involves retrieval settings in which a group of users collaborates to satisfy the same underlying need . One core issue of collaborative IR models involves either supporting collaboration with adapted tools or developing IR models for a multiple user context and providing a ranked list of documents adapted for each collaborator . In this paper we introduce the first document ranking model supporting collaboration between two users characterized by roles relying on different domain expertise levels . Specifically we propose a two step ranking model we first compute a document relevance score taking into consideration domain expertise based roles . We introduce specificity and novelty factors into language model smoothing and then we assign via an Expectation Maximization algorithm documents to the best suited collaborator . Our experiments employ a simulation based framework of collaborative information retrieval and show the significant effectiveness of our model at different search levels . \\n'],\n",
              " ['This paper introduces an adaptive visual tracking method that combines the adaptive appearance model and the optimization capability of the Markov decision process. Most tracking algorithms are limited due to variations in object appearance from changes in illumination viewing angle object scale and object shape. This paper is motivated by the fact that tracking performance degradation is caused not only by changes in object appearance but also by the inflexible controls of tracker parameters. To the best of our knowledge optimization of tracker parameters has not been thoroughly investigated even though it critically influences tracking performance. The challenge is to equip an adaptive tracking algorithm with an optimization capability for a more flexible and robust appearance model. In this paper the Markov decision process which has been applied successfully in many dynamic systems is employed to optimize an adaptive appearance model based tracking algorithm. The adaptive visual tracking is formulated as a Markov decision process based dynamic parameter optimization problem with uncertain and incomplete information. The high computation requirements of the Markov decision process formulation are solved by the proposed prioritized Q learning approach. We carried out extensive experiments using realistic video sets and achieved very encouraging and competitive results. \\n'],\n",
              " [' Image classification is to assign a category of an image and image annotation is to describe individual components of an image by using some annotation terms . These two learning tasks are strongly related . The main contribution of this paper is to propose a new discriminative and sparse topic model for image classification and annotation by combining visual annotation and label information from a set of training images . The essential features of DSTM different from existing approaches are that the label information is enforced in the generation of both visual words and annotation terms such that each generative latent topic corresponds to a category the zero mean Laplace distribution is employed to give a sparse representation of images in visual words and annotation terms such that relevant words and terms are associated with latent topics . Experimental results demonstrate that the proposed method provides the discrimination ability in classification and annotation and its performance is better than the other testing methods for LabelMe UIUC NUS WIDE and PascalVOC07 images . \\n'],\n",
              " [' With ever increasing information being available to the end users search engines have become the most powerful tools for obtaining useful information scattered on the Web . However it is very common that even most renowned search engines return result sets with not so useful pages to the user . Research on semantic search aims to improve traditional information search and retrieval methods where the basic relevance criteria rely primarily on the presence of query keywords within the returned pages . This work is an attempt to explore different relevancy ranking approaches based on semantics which are considered appropriate for the retrieval of relevant information . In this paper various pilot projects and their corresponding outcomes have been investigated based on methodologies adopted and their most distinctive characteristics towards ranking . An overview of selected approaches and their comparison by means of the classification criteria has been presented . With the help of this comparison some common concepts and outstanding features have been identified . \\n'],\n",
              " [' This article describes in depth research on machine learning methods for sentiment analysis of Czech social media . Whereas in English Chinese or Spanish this field has a long history and evaluation datasets for various domains are widely available in the case of the Czech language no systematic research has yet been conducted . We tackle this issue and establish a common ground for further research by providing a large human annotated Czech social media corpus . Furthermore we evaluate state of the art supervised machine learning methods for sentiment analysis . We explore different pre processing techniques and employ various features and classifiers . We also experiment with five different feature selection algorithms and investigate the influence of named entity recognition and preprocessing on sentiment classification performance . Moreover in addition to our newly created social media dataset we also report results for other popular domains such as movie and product reviews . We believe that this article will not only extend the current sentiment analysis research to another family of languages but will also encourage competition potentially leading to the production of high end commercial solutions . \\n'],\n",
              " [' In this paper we present an accelerated system for segmenting flower images based on graph cut technique which formulates the segmentation problem as an energy function minimization. The contribution of this paper consists to propose an improvement of the classical used energy function which is composed of a data consistent term and a boundary term. For this we integrate an additional data consistent term based on the spatial prior and we add gradient information in the boundary term. Then we propose an automated coarse to fine segmentation method composed mainly of two levels coarse segmentation and fine segmentation. First the coarse segmentation level is based on minimizing the proposed energy function. Then the fine segmentation is done by optimizing the energy function through the standard graph cut technique. Experiments were performed on a subset of Oxford flower database and the obtained results are compared to the reimplemented method of Nilsback et al. 1 . The evaluation shows that our method consumes less CPU time and it has a satisfactory accuracy compared with the mentioned method above 1 . \\n'],\n",
              " ['Semi supervised sparse feature selection which can exploit the large number unlabeled data and small number labeled data simultaneously has placed an important role in web image annotation. However most of the semi supervised sparse feature selection methods are developed for single view data and these methods cannot naturally deal with the multi view data though it has shown that leveraging information contained in multiple views can dramatically improve the feature selection performance. Recently multi view learning has obtained much research attention because it can reveal and leverage the correlated and complementary information between different views. So in this paper we apply multi view learning into semi supervised sparse feature selection and propose a semi supervised sparse feature selection method based on multi view Laplacian regularization namely multi view Laplacian sparse feature selection MLSFS .1 MLSFS utilizes multi view Laplacian regularization to boost semi supervised sparse feature selection performance. A simple iterative method is proposed to solve the objective function of MLSFS. We apply MLSFS algorithm into image annotation task and conduct experiments on two web image datasets. The experimental results show that the proposed MLSFS outperforms the state of art single view sparse feature selection methods. \\n'],\n",
              " [' We present PubSearch a hybrid heuristic scheme for re ranking academic papers retrieved from standard digital libraries such as the ACM Portal . The scheme is based on the hierarchical combination of a custom implementation of the term frequency heuristic a time depreciated citation score and a graph theoretic computed score that relates the paper s index terms with each other . We designed and developed a meta search engine that submits user queries to standard digital repositories of academic publications and re ranks the repository results using the hierarchical heuristic scheme . We evaluate our proposed re ranking scheme via user feedback against the results of ACM Portal on a total of 58 different user queries specified from 15 different users . The results show that our proposed scheme significantly outperforms ACM Portal in terms of retrieval precision as measured by most common metrics in Information Retrieval including Normalized Discounted Cumulative Gain Expected Reciprocal Rank as well as a newly introduced lexicographic rule of ranking search results . In particular PubSearch outperforms ACM Portal by more than 77 in terms of ERR by more than 11 in terms of NDCG and by more than 907.5 in terms of LEX . We also re rank the top 10 results of a subset of the original 58 user queries produced by Google Scholar Microsoft Academic Search and ArnetMiner the results show that PubSearch compares very well against these search engines as well . The proposed scheme can be easily plugged in any existing search engine for retrieval of academic publications . \\n'],\n",
              " ['Sudden illumination changes are a fundamental problem in background modeling applications. Most strategies to solve it are based on determining the particular form of the color transformation which the pixels undergo when an illumination change occurs. Here we present an approach which does not assume any specific form of the color transformation. It is based on a quantitative assessment of the smoothness of the local color transformation from one frame to the background model. In addition to this an assessment of the obtained illumination states of the pixels is carried out with the help of fuzzy logic. Experimental results are presented which demonstrate the performance of our approach in a range of situations. \\n'],\n",
              " [' Practical classification problems often involve some kind of trade off between the decisions a classifier may take . Indeed it may be the case that decisions are not equally good or costly therefore it is important for the classifier to be able to predict the risk associated with each classification decision . Bayesian decision theory is a fundamental statistical approach to the problem of pattern classification . The objective is to quantify the trade off between various classification decisions using probability and the costs that accompany such decisions . Within this framework a loss function measures the rates of the costs and the risk in taking one decision over another . In this paper we give a formal justification for a decision function under the Bayesian decision framework that comprises the minimisation of Bayesian risk and an empirical decision function found by Domingos and Pazzani . This new decision function has a very intuitive geometrical interpretation that can be explored on a Cartesian plane . We use this graphical interpretation to analyse different approaches to find the best decision on four different Na ve Bayes classifiers Gaussian Bernoulli Multinomial and Poisson on different standard collections . We show that the graphical interpretation significantly improves the understanding of the models and opens new perspectives for new research studies . \\n'],\n",
              " [' In this paper a novel method is proposed for real world pose invariant face recognition from only a single image in a gallery . A 3D Facial Expression Generic Elastic Model is proposed to reconstruct a 3D model of each human face using only a single 2D frontal image . Then for each person in the database a Sparse Dictionary Matrix is created from all face poses by rotating the 3D reconstructed models and extracting features in the rotated face . Each SDM is subsequently rendered based on triplet angles of face poses . Before matching to SDM an initial estimate of triplet angles of face poses is obtained in the probe face image using an automatic head pose estimation approach . Then an array of the SDM is selected based on the estimated triplet angles for each subject . Finally the selected arrays from SDMs are compared with the probe image by sparse representation classification . Convincing results were acquired to handle pose changes on the FERET CMU PIE LFW and video face databases based on the proposed method compared to several state of the art in pose invariant face recognition . \\n'],\n",
              " [' In this work we elaborate on the meaning of metadata quality by surveying efforts and experiences matured in the digital library domain . In particular an overview of the frameworks developed to characterize such a multi faceted concept is presented . Moreover the most common quality related problems affecting metadata both during the creation and the aggregation phase are discussed together with the approaches technologies and tools developed to mitigate them . This survey on digital library developments is expected to contribute to the ongoing discussion on data and metadata quality occurring in the emerging yet more general framework of data infrastructures . \\n'],\n",
              " [' The basic goal of scene understanding is to organize the video into sets of events and to find the associated temporal dependencies . Such systems aim to automatically interpret activities in the scene as well as detect unusual events that could be of particular interest such as traffic violations and unauthorized entry . The objective of this work therefore is to learn behaviors of multi agent actions and interactions in a semi supervised manner . Using tracked object trajectories we organize similar motion trajectories into clusters using the spectral clustering technique . This set of clusters depicts the different paths routes i.e . the distinct events taking place at various locations in the scene . A temporal mining algorithm is used to mine interval based frequent temporal patterns occurring in the scene . A temporal pattern indicates a set of events that are linked based on their relationship with other events in the set and we use Allen s interval based temporal logic to describe these relations . The resulting frequent patterns are used to generate temporal association rules which convey the semantic information contained in the scene . Our overall aim is to generate rules that govern the dynamics of the scene and perform anomaly detection . We apply the proposed approach on two publicly available complex traffic datasets and demonstrate considerable improvements over the existing techniques . \\n'],\n",
              " [' Gender and ethnicity are both key demographic attributes of human beings and they play a very fundamental and important role in automatic machine based face analysis therefore there has been increasing attention for face based gender and ethnicity classification in recent years . In this paper we present an effective and efficient approach on this issue by combining both boosted local texture and shape features extracted from 3D face models in contrast to the existing ones that only depend on either 2D texture or 3D shape of faces . In order to comprehensively represent the difference between different genders or ethnicities we propose a novel local descriptor namely local circular patterns . LCP improves the widely utilized local binary patterns and its variants by replacing the binary quantization with a clustering based one resulting in higher discriminative power as well as better robustness to noise . Meanwhile the following Adaboost based feature selection finds the most discriminative gender and race related features and assigns them with different weights to highlight their importance in classification which not only further raises the performance but reduces the time and memory cost as well . Experimental results achieved on the FRGC v2.0 and BU 3DFE datasets clearly demonstrate the advantages of the proposed method . \\n'],\n",
              " [' Multimedia objects can be retrieved using their context that can be for instance the text surrounding them in documents . This text may be either near or far from the searched objects . Our goal in this paper is to study the impact in term of effectiveness of text position relatively to searched objects . The multimedia objects we consider are described in structured documents such as XML ones . The document structure is therefore exploited to provide this text position in documents . Although structural information has been shown to be an effective source of evidence in textual information retrieval only a few works investigated its interest in multimedia retrieval . More precisely the task we are interested in this paper is to retrieve multimedia fragments . Our general approach is built on two steps we first retrieve XML elements containing multimedia objects and we then explore the surrounding information to retrieve relevant multimedia fragments . In both cases we study the impact of the surrounding information using the documents structure . Our work is carried out on images but it can be extended to any other media since the physical content of multimedia objects is not used . We conducted several experiments in the context of the Multimedia track of the INEX evaluation campaign . Results showed that structural evidences are of high interest to tune the importance of textual context for multimedia retrieval . Moreover the proposed approach outperforms state of the art approaches . \\n'],\n",
              " ['This paper proposes a novel approach to recognize object and scene categories in depth images. We introduce a Bag of Words BoW representation in 3D the Selective 3D Spatial Pyramid Matching Kernel 3DSPMK . It starts quantizing 3D local descriptors computed from point clouds to build a vocabulary of 3D visual words. This codebook is used to build the 3DSPMK which starts partitioning a working volume into fine sub volumes and computing a hierarchical weighted sum of histogram intersections of visual words at each level of the 3D pyramid structure. With the aim of increasing both the classification accuracy and the computational efficiency of the kernel we propose two selective hierarchical volume decomposition strategies based on representative and discriminative sub volume selection processes which drastically reduce the pyramid to consider. Results on different RGBD datasets show that our approaches obtain state of the art results for both object recognition and scene categorization. \\n'],\n",
              " [' Researchers have recently been performing region of interest detection in such applications as object recognition object segmentation and adaptive coding . In this paper a novel region of interest detection model that is based on visually salient regions is introduced by utilizing the frequency and space domain features in very high resolution remote sensing images . First the frequency domain features that are based on a multi scale spectrum residual algorithm are extracted to yield intensity features . Next we extract the color and orientation features by generating space dynamic pyramids . Then spectral features are obtained by analyzing spectral information content . In addition a multi scale feature fusion method is proposed to generate a saliency map . Finally the detected visual saliency regions are described using adaptive threshold segmentation . Compared with existing models our model eliminates the background information effectively and highlights the salient detected results with well defined boundaries and shapes . Moreover an experimental evaluation indicates promising results from our model with respect to the accuracy of detection results . \\n'],\n",
              " [' We propose a method to produce near laser scan quality 3 D face models of a freely moving user with a low cost low resolution range sensor in real time . Our approach does not require any prior knowledge about the geometry of a face and can produce faithful geometric models of any star shaped object . We use a cylindrical representation which enables us to efficiently process the 3 D mesh by applying 2 D filters . We use the first frame as a reference and incrementally build the model by registering each subsequent cloud of 3 D points to the reference using the ICP algorithm implemented on a GPU . The registered point clouds are merged into a single image through a cylindrical representation . The noise from the sensor and from the pose estimation error is removed with a temporal integration and a spatial smoothing of the successively incremented model . To validate our approach we quantitatively compare our model to laser scans and show comparable accuracy . This paper extends the method presented in . \\n'],\n",
              " [' Recently sentiment classification has received considerable attention within the natural language processing research community . However since most recent works regarding sentiment classification have been done in the English language there are accordingly not enough sentiment resources in other languages . Manual construction of reliable sentiment resources is a very difficult and time consuming task . Cross lingual sentiment classification aims to utilize annotated sentiment resources in one language for sentiment classification of text documents in another language . Most existing research works rely on automatic machine translation services to directly project information from one language to another . However different term distribution between original and translated text documents and translation errors are two main problems faced in the case of using only machine translation . To overcome these problems we propose a novel learning model based on active learning and semi supervised co training to incorporate unlabelled data from the target language into the learning process in a bi view framework . This model attempts to enrich training data by adding the most confident automatically labelled examples as well as a few of the most informative manually labelled examples from unlabelled data in an iterative process . Further in this model we consider the density of unlabelled data so as to select more representative unlabelled examples in order to avoid outlier selection in active learning . The proposed model was applied to book review datasets in three different languages . Experiments showed that our model can effectively improve the cross lingual sentiment classification performance and reduce labelling efforts in comparison with some baseline methods . \\n'],\n",
              " [' The volume of entity centric structured data grows rapidly on the Web . The description of an entity composed of property value pairs has become very large in many applications . To avoid information overload efforts have been made to automatically select a limited number of features to be shown to the user based on certain criteria which is called automatic entity summarization . However to the best of our knowledge there is a lack of extensive studies on how humans rank and select features in practice which can provide empirical support and inspire future research . In this article we present a large scale statistical analysis of the descriptions of entities provided by DBpedia and the abstracts of their corresponding Wikipedia articles to empirically study along several different dimensions which kinds of features are preferable when humans summarize . Implications for automatic entity summarization are drawn from the findings . \\n'],\n",
              " ['In this paper we propose a novel contactless palmprint authentication system where the system uses a CCD camera to capture the user s hand at a distance without any restrictions and touching the device. Furthermore a novel and high performance region of interest ROI extraction method which makes use of nonlinear regression and palm model to extract the ROIs with high success is proposed. Comparative results indicate that the proposed ROI extraction method gives superior performance as compared to the previously proposed point based approaches. To show the performance of the proposed system a novel contactless database has also been created. This database includes images captured from the users who present their hands with various hand positions and orientations in cluttered backgrounds. Furthermore experiments show that the proposed system has achieved a recognition rate of 99.488 and equal error rate of 0.277 on the contactless database of 145 people containing 1752 hand images. \\n'],\n",
              " [' Uncertainty is an important idea in information retrieval research but the concept has yet to be fully elaborated and explored . Common assumptions about uncertainty are that it is a negative state and that it will be reduced through information search and retrieval . Research in the domain of uncertainty in illness however has demonstrated that uncertainty is a complex phenomenon that shares a complicated relationship with information . Past research on people living with HIV and individuals who have tested positive for genetic risk for different illnesses has revealed that information and the reduction of uncertainty can in fact produce anxiety and that maintaining uncertainty can be associated with optimism and hope . We review the theory of communication and uncertainty management and offer nine principles based on that theoretical work that can be used to influence IR system design . The principles reflect a view of uncertainty as a multi faceted and dynamic experience one subject to ongoing appraisal and management efforts that include interaction with and use of information in a variety of forms . \\n'],\n",
              " [' In many wide area surveillance applications tracking objects is usually accomplished by using network of cameras . A common approach to any multi objects tracking algorithm in a network of cameras comprises of two main steps . First the movement trajectory of each object within the field of view of a camera is extracted and is called object tracklet . Then the set of tracklets are used to determine the persistent trace of each object . In this paper we assume that the tracklets are extracted by a conventional tracking algorithm . The occurrence of occlusion between objects within the viewing scene leads to various types of errors on the extracted tracklets . If these erroneous tracklets are used in a multi object tracking algorithm and ignoring the correction phase then the errors are propagated and affect the results of tracking algorithm . Therefore the true tracklets have to be estimated from the erroneous tracklets . In this paper we propose a variational model for estimating the true tracklets . The variational principle proposed in this model is established by first introducing a variational energy function . Then the erroneous tracklets are used to estimate the true tracklets through optimizing the energy function . The proposed method is evaluated on two well known datasets and a synthetic dataset which is particularly developed to demonstrate the performance of our algorithm under challenging scenarios . The 10 common metrics which are used in other multi objects tracking applications are used for quantitative evaluations . Our experimental results illustrate that our proposed model estimates the true tracklets which improves the overall association performances . \\n'],\n",
              " [' Finger vein identification is a new biometric identification technology . While many existing works approach the problem by using shape matching which is the generative method in this paper we introduce a joint discriminative and generative algorithm for the task . Our method considers both the discriminative appearance of local image patches as well as their generative spatial layout . The method is based on the popular vocabulary tree model where we utilize the hidden leaf node layer to calculate a generative confidence to weight the discriminative vote from the leaf node . The training process remains the same as building a conventional vocabulary tree while the prediction process utilizes a proposed point set matching method to support non parametric patch layout matching . In this way the entire model retains the efficiency of the vocabulary tree model which is much lighter than other similar models such as the constellation model . The overall estimation follows the Bayesian theory . Experimental results show that our proposed joint model outperformed the purely generative or discriminative counterpart and can offer competitive performance than existing methods for both the vein authentication and recognition tasks . \\n'],\n",
              " [' In this paper we propose an optimization framework to retrieve an optimal group of experts to perform a multi aspect task . While a diverse set of skills are needed to perform a multi aspect task the group of assigned experts should be able to collectively cover all these required skills . We consider three types of multi aspect expert group formation problems and propose a unified framework to solve these problems accurately and efficiently . The first problem is concerned with finding the top k experts for a given task while the required skills of the task are implicitly described . In the second problem the required skills of the tasks are explicitly described using some keywords but each expert has a limited capacity to perform these tasks and therefore should be assigned to a limited number of them . Finally the third problem is the combination of the first and the second problems . Our proposed optimization framework is based on the Facility Location Analysis which is a well known branch of the Operation Research . In our experiments we compare the accuracy and efficiency of the proposed framework with the state of the art approaches for the group formation problems . The experiment results show the effectiveness of our proposed methods in comparison with state of the art approaches . \\n'],\n",
              " [' The study explores the relationship between value attribution and information source use of 17 Chinese business managers during their knowledge management strategic decision making . During semi structured interviews the Chinese business managers half in the telecommunications sector and half in the manufacturing sector were asked to rate 16 information sources on five point Likert Scales . The 16 information sources were grouped into internal external and personal impersonal types . The participants rated the information sources according to five value criteria relevancy comprehensiveness reliability time effort and accessibility . Open ended questions were also asked to get at how and why value attribution affected the participants use of one information source over another during decision making . Findings show that the participants preferred internal personal type of information sources over external impersonal information sources . The differences in value ratings per information source were striking Telecommunications managers rated customers newspapers magazines and conferences trips much lower than the manufacturing managers but they rated corporate library intranet and databases much higher than manufacturing managers . The type of industrial sector therefore highly influenced information source use for decision making by the study s Chinese business managers . Based on this conclusion we added organizational and environmental categories to revise the De Alwis Majid and Chaudhry s typology of factors affecting Chinese managers information source preferences during decision making . \\n'],\n",
              " [' This paper presents yet another algorithm for finding polygonal approximations of digital planar curves however with a significant distinction the vertices of an approximating polygon need not lie on the contour itself . This approach gives us more flexibility to reduce the approximation error of the polygon compared to the conventional way where the vertices of the polygon are restricted to lie on the contour . To compute the approximation efficiently we adaptively define a local neighborhood of each point on the contour . The vertices of the polygonal approximation are allowed to move around in the neighborhoods . In addition we demonstrate a general approach where the error measure of an already computed polygonal approximation can possibly be reduced further by vertex relocation without increasing the number of dominant points . Moreover the proposed method is non parametric requiring no parameter to set for any particular application . Suitability of the proposed algorithm is validated by testing on several databases and comparing with existing methods . \\n'],\n",
              " [' We propose a new methodology for facial landmark detection . Similar to other state of the art methods we rely on the use of cascaded regression to perform inference and we use a feature representation that results from concatenating 66 HOG descriptors one per landmark . However we propose a novel regression method that substitutes the commonly used Least Squares regressor . This new method makes use of the L 2 1 norm and it is designed to increase the robustness of the regressor to poor initialisations or partial occlusions . Furthermore we propose to use multiple initialisations consisting of both spatial translation and 4 head poses corresponding to different pan rotations . These estimates are aggregated into a single prediction in a robust manner . Both strategies are designed to improve the convergence behaviour of the algorithm so that it can cope with the challenges of in the wild data . We further detail some important experimental details and show extensive performance comparisons highlighting the performance improvement attained by the method proposed here . \\n'],\n",
              " ['Recent research trends in Content based Video Retrieval have shown topic models as an effective tool to deal with the semantic gap challenge. In this scenario this paper has a dual target 1 it is aimed at studying how the use of different topic models pLSA LDA and FSTM affects video retrieval performance 2 a novel incremental topic model IpLSA is presented in order to cope with incremental scenarios in an effective and efficient way. A comprehensive comparison among these four topic models using two different retrieval systems and two reference benchmarking video databases is provided. Experiments revealed that pLSA is the best model in sparse conditions LDA tend to outperform the rest of the models in a dense space and IpLSA is able to work properly in both cases. \\n'],\n",
              " ['Great variances in visual features often present significant challenges in human action recognitions. To address this common problem this paper proposes a statistical adaptive metric learning SAML method by exploring various selections and combinations of multiple statistics in a unified metric learning framework. Most statistics have certain advantages in specific controlled environments and systematic selections and combinations can adapt them to more realistic in the wild scenarios. In the proposed method multiple statistics include means covariance matrices and Gaussian distributions are explicitly mapped or generated in the Riemannian manifolds. Typically d dimensional mean vectors in Rd are mapped to a Rd d space of symmetric positive definite SPD matrices . Subsequently by embedding the heterogeneous manifolds in their tangent Hilbert space subspace combination with minimal deviation is selected from multiple statistics. Then Mahalanobis metrics are introduced to map them back into the Euclidean space. Unified optimizations are finally performed based on the Euclidean distances. In the proposed method subspaces with smaller deviations are selected before metric learning. Therefore by exploring different metric combinations the final learning is more representative and effective than exhaustively learning from all the hybrid metrics. Experimental evaluations are conducted on human action recognitions in both static and dynamic scenarios. Promising results demonstrate that the proposed method performs effectively for human action recognitions in the wild. \\n'],\n",
              " [' Texture classification is one of the most important tasks in computer vision field and it has been extensively investigated in the last several decades . Previous texture classification methods mainly used the template matching based methods such as Support Vector Machine and k Nearest Neighbour for classification . Given enough training images the state of the art texture classification methods could achieve very high classification accuracies on some benchmark databases . However when the number of training images is limited which usually happens in real world applications because of the high cost of obtaining labelled data the classification accuracies of those state of the art methods would deteriorate due to the overfitting effect . In this paper we aim to develop a novel framework that could correctly classify textural images with only a small number of training images . By taking into account the repetition and sparsity property of textures we propose a sparse representation based multi manifold analysis framework for texture classification from few training images . A set of new training samples are generated from each training image by a scale and spatial pyramid and then the training samples belonging to each class are modelled by a manifold based on sparse representation . We learn a dictionary of sparse representation and a projection matrix for each class and classify the test images based on the projected reconstruction errors . The framework provides a more compact model than the template matching based texture classification methods and mitigates the overfitting effect . Experimental results show that the proposed method could achieve reasonably high generalization capability even with as few as 3 training images and significantly outperforms the state of the art texture classification approaches on three benchmark datasets . \\n'],\n",
              " [' An algorithm for fitting multiple models that characterize the projective relationships between point matches in pairs of images is proposed herein . Specifically the problem of estimating multiple algebraic varieties that relate the projections of 3 dimensional points in one or more views is predominantly turned into a problem of inference over a Markov random field using labels that include outliers and a set of candidate models estimated from subsets of the point matches . Thus not only the MRF can trivially incorporate the errors of fit in singleton factors but the sheer benefit of this approach is the ability to consider the interactions between data points . The proposed method refines the outlier posterior over the course of consecutive inference sweeps until the process settles at a local minimum . The inference engine employed is a Markov Chain Monte Carlo method which samples new labels from clusters of data points . The advantage of this technique pertains to the fact that cluster formation can be manipulated to favor common label assignments between points related to each other by image based criteria . Moreover although CSAMMFIT uses a Potts like pairwise factor the inference algorithm allows for arbitrary prior formulations thereby accommodating the needs for more elaborate feature based constraints . \\n'],\n",
              " ['This paper proposes a novel robust texture descriptor based on Gaussian Markov random fields GMRFs . A spatially localized parameter estimation technique using local linear regression is performed and the distributions of local parameter estimates are constructed to formulate the texture features. The inconsistencies arising in localized parameter estimation are addressed by applying generalized inverse regularization and an estimation window size selection criterion. The texture descriptors are named as local parameter histograms LPHs and are used in texture segmentation with the k means clustering algorithm. The segmentation results on general texture datasets demonstrate that LPH descriptors significantly improve the performance of classical GMRF features and achieve better results compared to the state of the art texture descriptors based on local feature distributions. Impressive natural image segmentation results are also achieved and comparisons to the other standard natural image segmentation algorithms are also presented. LPH descriptors produce promising texture features that integrate both statistical and structural information about a texture. The region boundary localization can be further improved by integrating colour information and using advanced segmentation algorithms. \\n'],\n",
              " [' Although most of the queries submitted to search engines are composed of a few keywords and have a length that ranges from three to six words more than 15 of the total volume of the queries are verbose introduce ambiguity and cause topic drifts . We consider verbosity a different property of queries from length since a verbose query is not necessarily long it might be succinct and a short query might be verbose . This paper proposes a methodology to automatically detect verbose queries and conditionally modify queries . The methodology proposed in this paper exploits state of the art classification algorithms combines concepts from a large linguistic database and uses a topic gisting algorithm we designed for verbose query modification purposes . Our experimental results have been obtained using the TREC Robust track collection thirty topics classified by difficulty degree four queries per topic classified by verbosity and length and human assessment of query verbosity . Our results suggest that the methodology for query modification conditioned to query verbosity detection and topic gisting is significantly effective and that query modification should be refined when topic difficulty and query verbosity are considered since these two properties interact and query verbosity is not straightforwardly related to query length . \\n'],\n",
              " [' Extracting local keypoints and keypoint descriptions from images is a primary step for many computer vision and image retrieval applications . In the literature many researchers have proposed methods for representing local texture around keypoints with varying levels of robustness to photometric and geometric transformations . Gradient based descriptors such as the Scale Invariant Feature Transform are among the most consistent and robust descriptors . The SIFT descriptor a 128 element vector consisting of multiple gradient histograms computed from local image patches around a keypoint is widely considered as the gold standard keypoint descriptor . However SIFT descriptors require at least 128bytes of storage per descriptor . Since images are typically described by thousands of keypoints it may require more space to store the SIFT descriptors for an image than the original image itself . This may be prohibitive in extremely large scale applications and applications on memory constrained devices such as tablets and smartphones . In this paper with the goal of reducing the memory requirements of keypoint descriptors such as SIFT without affecting their performance we propose BIG OH a simple yet extremely effective method for binary quantization of any descriptor based on gradient orientation histograms . BIG OH s memory requirements are very small when it uses SIFT s default parameters for the construction of the gradient orientation histograms it only requires 16bytes per descriptor . BIG OH quantizes gradient orientation histograms by computing a bit vector representing the relative magnitudes of local gradients associated with neighboring orientation bins . In a series of experiments on keypoint matching with different types of keypoint detectors under various photometric and geometric transformations we find that the quantized descriptor has performance comparable to or better than other descriptors including BRISK CARD BRIEF D BRIEF SQ and PCA SIFT . Our experiments also show that BIG OH is extremely effective for image retrieval with modestly better performance than SIFT . BIG OH s drastic reduction in memory requirements obtained while preserving or improving the image matching and image retrieval performance of SIFT makes it an excellent descriptor for large image databases and applications running on memory constrained devices . \\n'],\n",
              " [' Various visual tracking approaches have been proposed for robust target tracking among which using sparse representation of the tracking target yields promising performance . Some earlier works in this line used a fixed subset of features to compress the target s appearance which has limited modeling capacity between the target and the background and could not accommodate their appearance change over long period of time . In this paper we propose a visual tracking method by modeling targets with online learned sparse features . We first extract high dimensional Haar like features as an over completed basis set and then solve the feature selection problem in an efficient L 1 regularized sparse coding process . The selected low dimensional representation best discriminates the target from its neighboring background . Next we use a naive Bayesian classifier to select the most likely target candidate by a binary classification process . The online feature selection process happens when there are significant appearance changes identified by a thresholding strategy . In this way our proposed method could work for long tracking tasks . At the same time our comprehensive experimental evaluation has shown that the proposed methods achieve excellent running speed and higher accuracy over many state of the art approaches . \\n'],\n",
              " [' In our paper we present an experimental study which investigated the possibility to project the need for information specialists serving knowledge workers in knowledge industries on the basis of an average university library serving their counterparts at a university . Information management functions i.e . functions and processes related to information evaluation acquisition metadata creation etc . performed in an average university library are the starting point of this investigation . The fundamental assumption is that these functions do not only occur in libraries but also in other contexts like for instance in knowledge industries . As a consequence we try to estimate the need for information professionals in knowledge industries by means of quantitative methods from library and information science and economics . Our study confirms the validity of our assumption . Accordingly the number of information specialists projected on the basis of university libraries is consistent with their actual number reported in national statistics . However in order to attain a close fit we had to revise the original research model by dismissing the split up of information specialists into reader services and technical services staff . \\n'],\n",
              " [' Multi document summarization techniques aim to reduce documents into a small set of words or paragraphs that convey the main meaning of the original document . Many approaches to multi document summarization have used probability based methods and machine learning techniques to simultaneously summarize multiple documents sharing a common topic . However these techniques fail to semantically analyze proper nouns and newly coined words because most depend on an out of date dictionary or thesaurus . To overcome these drawbacks we propose a novel multi document summarization system called FoDoSu or Folksonomy based Multi Document Summarization that employs the tag clusters used by Flickr a Folksonomy system for detecting key sentences from multiple documents . We first create a word frequency table for analyzing the semantics and contributions of words using the HITS algorithm . Then by exploiting tag clusters we analyze the semantic relationships between words in the word frequency table . Finally we create a summary of multiple documents by analyzing the importance of each word and its semantic relatedness to others . Experimental results from the TAC 2008 and 2009 data sets demonstrate the improvement of our proposed framework over existing summarization systems . \\n'],\n",
              " [' Pedestrian detection is an important image understanding problem with many potential applications . There has been little success in creating an algorithm which exhibits a high detection rate while keeping the false alarm in a relatively low rate . This paper presents a method designed to resolve this problem . The proposed method uses the Kinect or any similar type of sensors which facilitate the extraction of a distinct foreground . Then potential regions which are candidates for the presence of human are detected by employing the widely used Histogram of Oriented Gradients technique which performs well in terms of good detection rates but suffers from significantly high false alarm rates . Our method applies a sequence of operations to eliminate the false alarms produced by the HOG detector based on investigating the fine details of local shape information . Local shape information can be identified by efficient utilization of the edge points which in this work are used to formulate the so called Shape Context model . The proposed detection framework is divided in four sequential stages with each stage aiming at refining the detection results of the previous stage . In addition our approach employs a pre evaluation stage to pre screen and restrict further detection results . Extensive experimental results on the dataset created by the authors involves 673 images collected from 11 different scenes demonstrate that the proposed method eliminates a large percentage of the false alarms produced by the HOG pedestrian detector . \\n'],\n",
              " [' This work deals with the challenging task of activity recognition in unconstrained videos. Standard methods are based on video encoding of low level features using Fisher Vectors or Bag of Features. However these approaches model every sequence into a single vector with fixed dimensionality that lacks any long term temporal information which may be important for recognition especially of complex activities. This work proposes a novel framework with two main technical novelties First a video encoding method that maintains the temporal structure of sequences and second a Time Flexible Kernel that allows comparison of sequences of different lengths and random alignment. Results on challenging benchmarks and comparison to previous work demonstrate the applicability and value of our framework. \\n'],\n",
              " [' We describe an Eikonal based algorithm for computing dense oversegmentation of an image often called superpixels . This oversegmentation respects local image boundaries while limiting undersegmentation . The proposed algorithm relies on a region growing scheme where the potential map used is not fixed and evolves during the diffusion . Refinement steps are also proposed to enhance at low cost the first oversegmentation . Quantitative comparisons on the Berkeley dataset show good performance on traditional metrics over current state of the art superpixel methods . \\n'],\n",
              " [' The rotation scaling and translation invariant property of image moments has a high significance in image recognition . Legendre moments as a classical orthogonal moment have been widely used in image analysis and recognition . Since Legendre moments are defined in Cartesian coordinate the rotation invariance is difficult to achieve . In this paper we first derive two types of transformed Legendre polynomial substituted and weighted radial shifted Legendre polynomials . Based on these two types of polynomials two radial orthogonal moments named substituted radial shifted Legendre moments and weighted radial shifted Legendre moments are proposed . The proposed moments are orthogonal in polar coordinate domain and can be thought as generalized and orthogonalized complex moments . They have better image reconstruction performance lower information redundancy and higher noise robustness than the existing radial orthogonal moments . At last a mathematical framework for obtaining the rotation scaling and translation invariants of these two types of radial shifted Legendre moments is provided . Theoretical and experimental results show the superiority of the proposed methods in terms of image reconstruction capability and invariant recognition accuracy under both noisy and noise free conditions . \\n'],\n",
              " [' This paper deals with the problem of estimating the human upper body orientation . We propose a framework which integrates estimation of the human upper body orientation and the human movements . Our human orientation estimator utilizes a novel approach which hierarchically employs partial least squares based models of the gradient and texture features coupled with the random forest classifier . The movement predictions are done by projecting detected persons into 3D coordinates and running an Unscented Kalman Filter based tracker . The body orientation results are then fused with the movement predictions to build a more robust estimation of the human upper body orientation . We carry out comprehensive experiments and provide comparison results to show the advantages of our system over the other existing methods . \\n'],\n",
              " [' Recently sparse representation has been applied to object tracking where each candidate target is approximately represented as a sparse linear combination of target templates. In this paper we present a new tracking algorithm which is faster and more robust than other tracking algorithms based on sparse representation. First with an analysis of many typical tracking examples with various degrees of corruption we model the corruption as a Laplacian distribution. Then a LAD Lasso optimisation model is proposed based on Bayesian Maximum A Posteriori MAP estimation theory. Compared with L1 Tracker and APG L1 Tracker the number of optimisation variables is reduced greatly it is equal to the number of target templates regardless of the dimensions of the feature. Finally we use the Alternating Direction Method of Multipliers ADMM to solve the proposed optimisation problem. Experiments on some challenging sequences demonstrate that our proposed method performs better than the state of the art methods in terms of accuracy and robustness. \\n'],\n",
              " [' This paper describes how to generate optimal projection patterns to supplement general stereo camera systems . In contrast to structured light the active stereo systems utilize the projected patterns only as auxiliary information in correspondence search whereas the structured light systems have to detect the patterns and decode them to compute depth . The concept of non recurring De Bruijn sequences is introduced and a few algorithms based on the non recurring De Bruijn sequence are designed to build optimized projection patterns for several stereo parameters . When only the search window size of a stereo system is given we show that a non recurring De Bruijn sequence with corresponding parameters makes the longest functional pattern and presents experimental results using real scenes to show the effectiveness of the proposed projection patterns . Additionally if the pattern length is given in the form of maximum disparity search range the algorithm using branch and bound search scheme to find an optimal sub sequence of a non recurring De Bruijn sequence is proposed . \\n'],\n",
              " [' This paper investigates the effects of adding texture to images with poorly textured regions on optical flow performance namely the accuracy of foreground boundary detection and computation time . Despite significant improvements in optical flow computations poor texture still remains a challenge to even the most accurate methods . Accordingly we explored the effects of simple modification of images rather than the algorithms . To localize and add texture to poorly textured regions in the background which induce the propagation of foreground optical flow we first perform a texture segmentation using Laws masks and generate a texture map . Next using a binary frame difference we constrain the poorly textured regions to those with negligible motion . Finally we calculate the optical flow for the modified images with added texture using the best optical flow methods available . It is shown that if the threshold used for binarizing the frame difference is in a specific range determined empirically variations in the final foreground detection will be insignificant . Employing the texture addition in conjunction with leading optical flow methods on multiple real and animation sequences with different texture distributions revealed considerable advantages including improvement in the accuracy of foreground boundary preservation prevention of object merging and reduction in the computation time . The F measure and the Boundary Displacement Error metrics were used to evaluate the similarity between detected and ground truth foreground masks . Furthermore preventing foreground optical flow propagation and reduction in the computation time are discussed using analysis of optical flow convergence . \\n'],\n",
              " [' We propose to represent a time of flight camera by the map of internal radial distances associating an intrinsic distance to each pixel as an alternative for the classic pinhole model . This representation is more general than the perspective model and appears to be a natural concept for 3D reconstruction and other applications of TOF cameras . In this new framework calibrating a ToF camera comes down to the determination of this IRD map . We show how this can be accomplished by images of flat surfaces without performing any feature detection . We prove deterministic calibration formulas using one or more plane images . We also offer a numerical optimization method that in principle needs only one image of a flat surface . This paper has been recommended for acceptance by Peter Sturm . \\n'],\n",
              " [' Archives are an extremely valuable part of our cultural heritage since they represent the trace of the activities of a physical or juridical person in the course of their business . Despite their importance the models and technologies that have been developed over the past two decades in the Digital Library field have not been specifically tailored to archives . This is especially true when it comes to formal and foundational frameworks as the Streams Structures Spaces Scenarios Societies model is . Therefore we propose an innovative formal model called NEsted SeTs for Object hieRarchies for archives explicitly built around the concepts of context and hierarchy which play a central role in the archival realm . NESTOR is composed of two set based data models the Nested Sets Model and the Inverse Nested Sets Model that express the hierarchical relationships between objects through the inclusion property between sets . We formally study the properties of these models and prove their equivalence with the notion of hierarchy entailed by archives . We then use NESTOR to extend the 5S model in order to take into account the specific features of archives and to tailor the notion of digital library accordingly . This offers the possibility of opening up the full wealth of DL methods and technologies to archives . We demonstrate the impact of NESTOR on this problem through three example use cases . \\n'],\n",
              " [' This paper presents an unsupervised deep learning framework that derives spatio temporal features for human robot interaction . The respective models extract high level features from low level ones through a hierarchical network viz . the Hierarchical Temporal Memory providing at the same time a solution to the curse of dimensionality in shallow techniques . The presented work incorporates the tensor based framework within the operation of the nodes and thus enhances the feature derivation procedure . This is due to the fact that tensors allow the preservation of the initial data format and their respective correlation and moreover attain more compact representations . The computational nodes form spatial and temporal groups by exploiting the multilinear algebra and subsequently express the samples according to those groups in terms of proximity . This generic framework may be applied in a diverse of visual data while it has been examined on sequences of color and depth images exhibiting remarkable performance . \\n'],\n",
              " [' In this article we propose a novel formalism to model and analyse gene regulatory networks using a well established formal verification technique . We model the possible behaviours of networks by logical formulae in linear temporal logic . By checking the satisfiability of LTL it is possible to check whether some or all behaviours satisfy a given biological property which is difficult in quantitative analyses such as the ordinary differential equation approach . Owing to the complexity of LTL satisfiability checking analysis of large networks is generally intractable in this method . To mitigate this computational difficulty we developed two methods . One is a modular checking method where we divide a network into subnetworks check them individually and then integrate them . The other is an approximate analysis method in which we specify behaviours in simpler formulae which compress or expand the possible behaviours of networks . In the approximate method we focused on network motifs and presented approximate specifications for them . We confirmed by experiments that both methods improved the analysis of large networks . \\n'],\n",
              " [' The ability of most existing approaches to classify abandoned and removed objects in images is affected by external environmental conditions such as illumination and traffic volume because the approaches use several pre defined threshold values and generate many falsely classified static regions . To reduce these effects we propose an accurate ARO classification method using a hierarchical finite state machine that consists of pixel layer region layer and event layer FSMs where the result of the lower layer FSM is used as the input of the higher layer FSM . Each FSM is defined by a Mealy state machine with three states and several state transitions where a support vector machine determines the state transition based on the current state and input features such as area intensity motion shape time duration color and edge . Because it uses the hierarchical FSM structure with features that are optimally trained by SVM classifiers the proposed ARO classification method does not require threshold values and guarantees better classification accuracy under severe environmental changes . In experiments the proposed ARO classification method provided much higher classification accuracy and lower false alarm rate than the state of the art methods in both public databases and a commercial database . The proposed ARO classification method can be applied to many practical applications such as detection of littering illegal parking theft and camouflaged soldiers . \\n'],\n",
              " [' When a videometric system operates over a long period temperature variations in the camera and its environment will affect the measurement results which can not be ignored . How to eliminate or compensate for the effects of such variations in temperature is an emergent problem . Starting with the image drift phenomenon this paper presents an image drift model that analyzes the relationship between variations in the camera parameters and drift in the coordinates of the image . A simplified model is then introduced by analyzing the coupling relationships among the variations in the camera parameters . Furthermore a model of the relationship between the camera parameters and temperature variations is established with the system identification method . Finally several compensation experiments on image drift are carried out using the parameter temperature relationship model calibrated with one arbitrary data set to compensate the others . The analyses and experiments demonstrate the feasibility and efficiency of the proposed method . \\n'],\n",
              " [' In recent years monitoring the compliance of business processes with relevant regulations constraints and rules during runtime has evolved as major concern in literature and practice . Monitoring not only refers to continuously observing possible compliance violations but also includes the ability to provide fine grained feedback and to predict possible compliance violations in the future . The body of literature on business process compliance is large and approaches specifically addressing process monitoring are hard to identify . Moreover proper means for the systematic comparison of these approaches are missing . Hence it is unclear which approaches are suitable for particular scenarios . The goal of this paper is to define a framework for Compliance Monitoring Functionalities that enables the systematic comparison of existing and new approaches for monitoring compliance rules over business processes during runtime . To define the scope of the framework at first related areas are identified and discussed . The CMFs are harvested based on a systematic literature review and five selected case studies . The appropriateness of the selection of CMFs is demonstrated in two ways a systematic comparison with pattern based compliance approaches and a classification of existing compliance monitoring approaches using the CMFs . Moreover the application of the CMFs is showcased using three existing tools that are applied to two realistic data sets . Overall the CMF framework provides powerful means to position existing and future compliance monitoring approaches . \\n'],\n",
              " [' In accordance with Basel Capital Accords the Capital Requirements for market risk exposure of banks is a nonlinear function of Value at Risk . Importantly the CR is calculated based on a bank s actual portfolio i.e . the portfolio represented by its current holdings . To tackle mean VaR portfolio optimization within the actual portfolio framework we propose a novel mean VaR optimization method where VaR is estimated using a univariate Generalized AutoRegressive Conditional Heteroscedasticity volatility model . The optimization was performed by employing a Nondominated Sorting Genetic Algorithm . On a sample of 40 large US stocks our procedure provided superior mean VaR trade offs compared to those obtained from applying more customary mean multivariate GARCH and historical VaR models . The results hold true in both low and high volatility samples . \\n'],\n",
              " [' Knowledge acquisition and bilingual terminology extraction from multilingual corpora are challenging tasks for cross language information retrieval . In this study we propose a novel method for mining high quality translation knowledge from our constructed Persian English comparable corpus University of Tehran Persian English Comparable Corpus . We extract translation knowledge based on Term Association Network constructed from term co occurrences in same language as well as term associations in different languages . We further propose a post processing step to do term translation validity check by detecting the mistranslated terms as outliers . Evaluation results on two different data sets show that translating queries using UTPECC and using the proposed methods significantly outperform simple dictionary based methods . Moreover the experimental results show that our methods are especially effective in translating Out Of Vocabulary terms and also expanding query words based on their associated terms . \\n'],\n",
              " [' Expertise seeking is the activity of selecting people as sources for consultation about an information need . This review of 72 expertise seeking papers shows that across a range of tasks and contexts people in particular work group colleagues and other strong ties are among the most frequently used sources . Studies repeatedly show the influence of the social network of friendships and personal dislikes on the expertise seeking network of organisations . In addition people are no less prominent than documentary sources in work contexts as well as daily life contexts . The relative influence of source quality and source accessibility on source selection varies across studies . Overall expertise seekers appear to aim for sufficient quality composed of reliability and relevance while also attending to accessibility composed of access to the source and access to the source information . Earlier claims that seekers disregard quality to minimise effort receive little support . Source selection is also affected by task related seeker related and contextual factors . For example task complexity has been found to increase the use of information sources whereas task importance has been found to amplify the influence of quality on source selection . Finally the reviewed studies identify a number of barriers to expertise seeking . \\n'],\n",
              " [' In this paper we propose a robust dense stereo reconstruction algorithm using a random walk with restart . The pixel wise matching costs are aggregated into superpixels and the modified random walk with restart algorithm updates the matching cost for all possible disparities between the superpixels . In comparison to the majority of existing stereo methods using the graph cut belief propagation or semi global matching our proposed method computes the final reconstruction through the determination of the best disparity at each pixel in the matching cost update . In addition our method also considers occlusion and depth discontinuities through the visibility and fidelity terms . These terms assist in the cost update procedure in the calculation of the standard smoothness constraint . The method results in minimal computational costs while achieving high accuracy in the reconstruction . We test our method on standard benchmark datasets and challenging real world sequences . We also show that the processing time increases linearly in relation to an increase in the disparity search range . \\n'],\n",
              " [' On the onset of the second decade of research in eye movement biometrics the already demonstrated results strongly support the promising perspectives of the field . This paper presents a description of the research conducted in eye movement biometrics based on an extended analysis of the characteristics and results of the BioEye 2015 Competition on Biometrics via Eye Movements . This extended presentation can contribute to the understanding of the current level of research in eye movement biometrics covering areas such as the previous work in the field the procedures for the creation of a database of eye movement recordings and the different approaches that can be used for the analysis of eye movements . Also the presented results from BioEye 2015 competition can demonstrate the potential identification accuracy that can be achieved under easier and more difficult scenarios . Based on the provided presentation we discuss topics related to the current status in eye movement biometrics and suggest possible directions for the future research in the field . \\n'],\n",
              " [' Human action recognition from still image has recently drawn increasing attention in human behavior analysis and also poses great challenges due to the huge inter ambiguity and intra variability . Vector of locally aggregated descriptors has achieved state of the art performance in many image classification tasks based on local features . The great success of VLAD is largely due to its high descriptive ability and computational efficiency . In this paper towards optimal VLAD representations for human action recognition from still images we improve VLAD by tackling three important issues including empty cavity ambiguity and pooling strategies . The empty cavity limits the performance of VLAD and has long been overlooked . We investigate the empty cavity and provide an effective solution to deal with it which improves the performance of VLAD we enhance the codewords with middle level of assignments which are more reliable and can provide more useful information for realistic activity we propose incorporating the generalized max pooling to replace sum pooling in VLAD which is more reliable for the final representation . We have conducted extensive experiments on four widely used benchmarks to validate the proposed method for human action recognition from still images . Our method produces competitive performance with state of the art algorithms . \\n'],\n",
              " [' This paper proposes a unified multi lateral filter to efficiently increase the spatial resolution of low resolution and noisy depth maps in real time . Time of Flight cameras have become a very promising alternative to stereo based range sensing systems as they provide depth measurements at a high frame rate . However there are actually two main drawbacks that restrict their use in a wide range of applications namely their fairly low spatial resolution as well as the amount of noise within the depth estimation . In order to address these drawbacks we propose a new approach based on sensor fusion . That is we couple a ToF camera of low resolution with a 2 D camera of higher resolution to which the low resolution depth map will be efficiently upsampled . In this paper we first review the existing depth map enhancement approaches based on sensor fusion and discuss their limitations . We then propose a unified multi lateral filter that accounts for the inaccuracy of depth edges position due to the low resolution ToF depth maps . By doing so unwanted artefacts such as texture copying and edge blurring are almost entirely eliminated . Moreover the proposed filter is configurable to behave as most of the alternative depth enhancement approaches . Using a convolution based formulation and data quantization and downsampling the described filter has been effectively and efficiently implemented for dynamic scenes in real time applications . The experimental results show a sensitive qualitative as well as quantitative improvement on raw depth maps outperforming state of the art multi lateral filters . \\n'],\n",
              " ['We propose a holistic approach to the problem of re identification in an environment of distributed smart cameras. We model the re identification process in a distributed camera network as a distributed multi class classifier composed of spatially distributed binary classifiers. We treat the problem of re identification as an open world problem and address novelty detection and forgetting. As there are many tradeoffs in design and operation of such a system we propose a set of evaluation measures to be used in addition to the recognition performance. The proposed concept is illustrated and evaluated on a new many camera surveillance dataset and SAIVT SoftBio dataset. \\n'],\n",
              " [' This paper presents a novel stereo disparity estimation method which combines three different cost metrics defined using RGB information the CENSUS transform as well as Scale Invariant Feature Transform coefficients . The selected cost metrics are aggregated based on an adaptive weight approach in order to calculate their corresponding cost volumes . The resulting cost volumes are then merged into a combined one following a novel two phase strategy which is further refined by exploiting scanline optimization . A mean shift segmentation driven approach is exploited to deal with outliers in the disparity maps . Additionally low textured areas are handled using disparity histogram analysis which allows for reliable disparity plane fitting on these areas . Finally an efficient two step approach is introduced to refine disparity discontinuities . Experiments performed on the four images of the Middlebury benchmark demonstrate the accuracy of this methodology which currently ranks first among published methods . Moreover this algorithm is tested on 27 additional Middlebury stereo pairs for evaluating thoroughly its performance . The extended comparison verifies the efficiency of this work . \\n'],\n",
              " [' Automatic lip reading is a challenging task because the visual speech signal is known to be missing some important information such as voicing . We propose an approach to ALR that acknowledges that this information is missing but assumes that it is substituted or deleted in a systematic way that can be modelled . We describe a system that learns such a model and then incorporates it into decoding which is realised as a cascade of weighted finite state transducers . Our results show a small but statistically significant improvement in recognition accuracy . We also investigate the issue of suitable visual units for ALR and show that visemes are sub optimal not but because they introduce lexical ambiguity but because the reduction in modelling units entailed by their use reduces accuracy . \\n'],\n",
              " [' This paper proposes a globally rotation invariant multi scale co occurrence local binary pattern feature for texture relevant tasks . In MCLBP we arrange all co occurrence patterns into groups according to properties of the co patterns and design three encoding functions to extract features from each group . The MCLBP can effectively capture the correlation information between different scales and is also globally rotation invariant . The MCLBP is substantially different from most existing LBP variants including the LBP the CLBP and the MSJ LBP that achieves rotation invariance by locally rotation invariant encoding . We fully evaluate the properties of the MCLBP and compare it with some powerful features on five challenging databases . Extensive experiments demonstrate the effectiveness of the MCLBP compared to the state of the art LBP variants including the CLBP and the LBPHF . Meanwhile the dimension and computational cost of the MCLBP is also lower than that of the CLBP S M C and LBPHF S M . \\n'],\n",
              " [' This paper proposed a new method based on spatial filter banks and discrete wavelet transform for invariant texture classification . The method used a multi resolution analysis method like DWT and applied the proposed filter bank on different resolutions . Then a simple fusion of features on different resolutions was used for invariant texture analysis . A comprehensive study was done to examine the effectiveness of the proposed method . Different datasets with different properties were used in this paper such as Brodatz Outex and KTH TIPS for the evaluation . Local binary pattern methods have been one of the powerful methods in recent years for invariant texture classification . A comparative study was performed with some state of the art LBP methods . This comparison indicated promising results for the proposed approach as compared with the LBP methods . \\n'],\n",
              " [' Learning based hashing methods are becoming the mainstream for approximate scalable multimedia retrieval . They consist of two main components hash codes learning for training data and hash functions learning for new data points . Tremendous efforts have been devoted to designing novel methods for these two components i.e . supervised and unsupervised methods for learning hash codes and different models for inferring hashing functions . However there is little work integrating supervised and unsupervised hash codes learning into a single framework . Moreover the hash function learning component is usually based on hand crafted visual features extracted from the training images . The performance of a content based image retrieval system crucially depends on the feature representation and such hand crafted visual features may degrade the accuracy of the hash functions . In this paper we propose a semi supervised deep learning hashing method for fast multimedia retrieval . More specifically in the first component we utilize both visual and label information to learn an optimal similarity graph that can more precisely encode the relationship among training data and then generate the hash codes based on the graph . In the second stage we apply a deep convolutional network to simultaneously learn a good multimedia representation and a set of hash functions . Extensive experiments on five popular datasets demonstrate the superiority of our DLH over both supervised and unsupervised hashing methods . \\n'],\n",
              " [' We digitized three years of Dutch election manifestos annotated by the Dutch political scientist Isaac Lipschits . We used these data to train a classifier that can automatically label new unseen election manifestos with themes . Having the manifestos in a uniform XML format with all paragraphs annotated with their themes has advantages for both electronic publishing of the data and diachronic comparative data analysis . The data that we created will be disclosed to the public through a search interface . This means that it will be possible to query the data and filter them on themes and parties . We optimized the Lipschits classifier on the task of classifying election manifestos using models trained on earlier years . We built a classifier that is suited for classifying election manifestos from 2002 onwards using the data from the 1980s and 1990s . We evaluated the results by having a domain expert manually assess a sample of the classified data . We found that our automatic classifier obtains the same precision as a human classifier on unseen data . Its recall could be improved by extending the set of themes with newly emerged themes . Thus when using old political texts to classify new texts work is needed to link and expand the set of themes to newer topics . \\n'],\n",
              " ['In this paper we present a new algorithm for the computation of the focus of expansion in a video sequence. Although several algorithms have been proposed in the literature for its computation almost all of them are based on the optical flow vectors between a pair of consecutive frames so being very sensitive to noise optical flow errors and camera vibrations. Our algorithm is based on the computation of the vanishing point of point trajectories thus integrating information for more than two consecutive frames. It can improve performance in the presence of erroneous correspondences and occlusions in the field of view of the camera. The algorithm has been tested with virtual sequences generated with Blender as well as some real sequences from both the public KITTI benchmark and a number of challenging video sequences also proposed in this paper. For comparison purposes some algorithms from the literature have also been implemented. The results show that the algorithm has proven to be very robust outperforming the compared algorithms specially in outdoor scenes where the lack of texture can make optical flow algorithms yield inaccurate results. Timing evaluation proves that the proposed algorithm can reach up to 15fps showing its suitability for real time applications. \\n'],\n",
              " ['Visual speech information plays an important role in automatic speech recognition ASR especially when audio is corrupted or even inaccessible. Despite the success of audio based ASR the problem of visual speech decoding remains widely open. This paper provides a detailed review of recent advances in this research area. In comparison with the previous survey 97 which covers the whole ASR system that uses visual speech information we focus on the important questions asked by researchers and summarize the recent studies that attempt to answer them. In particular there are three questions related to the extraction of visual features concerning speaker dependency pose variation and temporal information respectively. Another question is about audio visual speech fusion considering the dynamic changes of modality reliabilities encountered in practice. In addition the state of the art on facial landmark localization is briefly introduced in this paper. Those advanced techniques can be used to improve the region of interest detection but have been largely ignored when building a visual based ASR system. We also provide details of audio visual speech databases. Finally we discuss the remaining challenges and offer our insights into the future research on visual speech decoding. \\n'],\n",
              " [' A trustworthy protocol is essential to evaluate a text detection algorithm in order to first measure its efficiency and adjust its parameters and second to compare its performances with those of other algorithms . However current protocols do not give precise enough evaluations because they use coarse evaluation metrics and deal with inconsistent matchings between the output of detection algorithms and the ground truth both often limited to rectangular shapes . In this paper we propose a new evaluation protocol named EvaLTex that solves some of the current problems associated with classical metrics and matching strategies . Our system deals with different kinds of annotations and detection shapes . It also considers different kinds of granularity between detections and ground truth objects and hence provides more realistic and accurate evaluation measures . We use this protocol to evaluate text detection algorithms and highlight some key examples that show that the provided scores are more relevant than those of currently used evaluation protocols . \\n'],\n",
              " [' With the increasing number of videos all over the Internet and the increasing number of cameras looking at people around the world one of the most interesting applications would be human activity recognition in videos . Many researches have been conducted in the literature for this purpose . But still recognizing activities in a video with unrestricted conditions is a challenging problem . Moreover finding the spatio temporal location of the activity in the video is another issue . In this paper we present a method based on a non negative matrix completion framework that learns to label videos with activity classes and localizes the activity of interest spatio temporally throughout the video . This approach has a multi label weakly supervised setting for activity detection with a convex optimization procedure . The experimental results show that the proposed approach is competitive with the state of the art methods . \\n'],\n",
              " [' The possibility of sharing multimedia contents in easy and ubiquitous way has brought to the creation of multiuser photo albums . Pictures and video sequences taken by different people attending common social events are gathered together into huge sets of heterogeneous multimedia data . These databases require effective compression strategies that exploit the common visual information related to the scene but compensate effectively the differences depending on the acquiring viewpoints camera models and acquisition time instants . The paper presents a predictive coding strategy for multi user photo gallery which initially localizes each picture in terms of viewpoint orientation time and acquired elements . This information permits ordering all the images in a prediction tree and associates to each of them a reference picture . From this structure it is possible to build a predictive coding strategy that exploits the redundant elements between the image to be coded and its reference . Experimental results show an average bit rate reduction up to 75 with respect to HEVC Intra low complexity coding . \\n'],\n",
              " [' Recent studies witness the success of Bag of Features frameworks for video based human action recognition . The detection and description of local interest regions are two fundamental problems in BoF framework . In this paper we propose a motion boundary based sampling strategy and spatial temporal co occurrence descriptors for action video representation and recognition . Our sampling strategy is partly inspired by the recent success of dense trajectory based features for action recognition . Compared with DT we densely sample spatial temporal cuboids along a motion boundary which can greatly reduce the number of valid trajectories and preserve the discriminative power . Moreover we develop a set of 3D co occurrence descriptors which take account of the spatial temporal context within local cuboids and deliver rich information for recognition . Furthermore we decompose each 3D co occurrence descriptor at pixel level and bin level and integrate the decomposed components with a multi channel framework which can improve the performance significantly . To evaluate the proposed methods we conduct extensive experiments on three benchmarks including KTH YouTube and HMDB51 . The results show that our sampling strategy significantly reduces the computational cost of point tracking without degrading performance . Meanwhile we achieve superior performance than the state of the art methods . We report 95.6 on KTH 87.6 on YouTube and 51.8 on HMDB51 . \\n'],\n",
              " [' This study aims to compare representations of Japanese personal and corporate name authority data in Japan South Korea China and the Library of Congress in order to identify differences and to bring to light issues affecting name authority data sharing projects such as the Virtual International Authority File . For this purpose actual data manuals formats and case reports of organizations as research objects were collected . Supplemental e mail and face to face interviews were also conducted . Subsequently five check points considered to be important in creating Japanese name authority data were set and the data of each organization were compared from these five perspectives . Before the comparison an overview of authority control in Chinese Japanese Korean speaking countries was also provided . The findings of the study are as follows the databases of China and South Korea have mixed headings in Kanji and other Chinese characters few organizations display the correspondence between Kanji and their yomi romanization is not mandatory in some organizations and is different among organizations some organizations adopt representations in their local language and some names in hiragana are not linked with their local forms and might elude a search . \\n'],\n",
              " ['This article focuses on the usability evaluation of biometric recognition systems in mobile devices. In particular a behavioural modality has been used the dynamic handwritten signature. Testing usability in behavioural modalities involves a big challenge due to the number of degrees of freedom that users have in interacting with sensors as well as the variety of capture devices to be used. In this context we propose a usability evaluation that allows users to interact freely with the system while minimizing errors at the same time. The participants signed in a smartphone with a stylus through the different phases in the use of a biometric system training enrolment and verification. In addition a profound study on the automation of the evaluation processes has been done so as to reduce the resources employed. The influence of the users stress has also been studied to obtain conclusions on its impact on both the usability systems in scenarios where the user may suffer a certain level of stress such as in courts banks or even shopping. In brief the results shown in this paper prove not only that a dynamic handwritten signature is a trustable solution for a large number of applications in the real world but also that the evaluation of the usability of biometric systems can be carried out at lower costs and shorter duration. \\n'],\n",
              " [' Enabling process changes constitutes a major challenge for any process aware information system . This not only holds for processes running within a single enterprise but also for collaborative scenarios involving distributed and autonomous partners . In particular if one partner adapts its private process the change might affect the processes of the other partners as well . Accordingly it might have to be propagated to concerned partners in a transitive way . A fundamental challenge in this context is to find ways of propagating the changes in a decentralized manner . Existing approaches are limited with respect to the change operations considered as well as their dependency on a particular process specification language . This paper presents a generic change propagation approach that is based on the Refined Process Structure Tree i.e . the approach is independent of a specific process specification language . Further it considers a comprehensive set of change patterns . For all these change patterns it is shown that the provided change propagation algorithms preserve consistency and compatibility of the process choreography . Finally a proof of concept prototype of a change propagation framework for process choreographies is presented . Overall comprehensive change support in process choreographies will foster the implementation and operational support of agile collaborative process scenarios . \\n'],\n",
              " [' 3D face recognition and emotion analysis play important roles in many fields of communication and edutainment . An effective facial descriptor with higher discriminating capability for face recognition and higher descriptiveness for facial emotion analysis is a challenging issue . However in the practical applications the descriptiveness and discrimination are independent and contradictory to each other . 3D facial data provide a promising way to balance these two aspects . In this paper a robust regional bounding spherical descriptor is proposed to facilitate 3D face recognition and emotion analysis . In our framework we first segment a group of regions on each 3D facial point cloud by shape index and spherical bands on the human face . Then the corresponding facial areas are projected to regional bounding spheres to obtain our regional descriptor . Finally a regional and global regression mapping technique is employed to the weighted regional descriptor for boosting the classification accuracy . Three largest available databases FRGC v2 CASIA and BU 3DFE are contributed to the performance comparison and the experimental results show a consistently better performance for 3D face recognition and emotion analysis . \\n'],\n",
              " [' In this paper we study the problem of Face Recognition when using Single Sensor Multi Wavelength imaging systems that operate in the Short Wave Infrared band . The contributions of our work are four fold First a SWIR database is collected when using our developed SSMW system under the following scenarios i.e . Multi Wavelength multi pose images were captured when the camera was focused at either 1150 1350 or 1550nm . Second an automated quality based score level fusion scheme is proposed for the classification of input MW images . Third a weighted quality based score level fusion scheme is proposed for the automated classification of full frontal vs. nonfrontal face images . Fourth a set of experiments is performed indicating that our proposed algorithms for the classification of multiwavelength images and FF vs. NFF face images are beneficial when designing different steps of multi spectral face recognition systems including face detection eye detection and face recognition . We also determined that when our SWIR based system is focused at 1350nm the identification performance increases compared to focusing the camera at any of the other SWIR wavelengths available . This outcome is particularly important for unconstrained FR scenarios where imaging at 1550nm at long distances and when operating at night time environments is preferable over different SWIR wavelengths . \\n'],\n",
              " [' Cross Lingual Link Discovery is a new problem in Information Retrieval . The aim is to automatically identify meaningful and relevant hypertext links between documents in different languages . This is particularly helpful in knowledge discovery if a multi lingual knowledge base is sparse in one language or another or the topical coverage in each language is different such is the case with Wikipedia . Techniques for identifying new and topically relevant cross lingual links are a current topic of interest at NTCIR where the CrossLink task has been running since the 2011 NTCIR 9 . This paper presents the evaluation framework for benchmarking algorithms for cross lingual link discovery evaluated in the context of NTCIR 9 . This framework includes topics document collections assessments metrics and a toolkit for pooling assessment and evaluation . The assessments are further divided into two separate sets manual assessments performed by human assessors and automatic assessments based on links extracted from Wikipedia itself . Using this framework we show that manual assessment is more robust than automatic assessment in the context of cross lingual link discovery . \\n'],\n",
              " [' Current semantic video analysis systems are usually hierarchical and consist of some levels to overcome semantic gaps between low level features and high level concepts . In these systems some features descriptors objects or concepts are extracted in each level and therefore total computational complexity of such systems is huge . In this paper we present a new general framework to impose attention control on a video analysis system using Q learning . Thus our proposed framework restructures a given system dynamically to direct attention to the blocks extracting the most informative features concepts and reduces computational complexity of the system . In other words the proposed framework directs flow of processing actively using a learning attention control method . The proposed framework is evaluated for event detection in broadcast soccer videos using limited numbers of training samples . Experiments show that the proposed framework is able to learn how to direct attention to informative features concepts and restructure the initial structure of the system dynamically to reach the final goal with less computational complexity . \\n'],\n",
              " [' This paper presents an accurate and efficient eye detection method using the discriminatory Haar features and a new efficient support vector machine . The DHFs are extracted by applying a discriminating feature extraction method to the 2D Haar wavelet transform . The DFE method is capable of extracting multiple discriminatory features for two class problems based on two novel measure vectors and a new criterion in the whitened principal component analysis space . The eSVM significantly improves the computational efficiency upon the conventional SVM for eye detection without sacrificing the generalization performance . Experiments on the Face Recognition Grand Challenge database and the BioID face database show that the DHFs exhibit promising classification capability for eye detection problem the eSVM runs much faster than the conventional SVM and the proposed eye detection method achieves near real time eye detection speed and better eye detection performance than some state of the art eye detection methods . \\n'],\n",
              " [' In daily life humans demonstrate an amazing ability to remember images they see on magazines commercials TV web pages etc . but automatic prediction of intrinsic memorability of images using computer vision and machine learning techniques has only been investigated very recently . Our goal in this article is to explore the role of visual attention and image semantics in understanding image memorability . In particular we present an attention driven spatial pooling strategy and show that considering image features from the salient parts of images improves the results of the previous models . We also investigate different semantic properties of images by carrying out an analysis of a diverse set of recently proposed semantic features which encode meta level object categories scene attributes and invoked feelings . We show that these features which are automatically extracted from images provide memorability predictions as nearly accurate as those derived from human annotations . Moreover our combined model yields results superior to those of state of the art fully automatic models . \\n'],\n",
              " [' We consider the problem of searching posts in microblog environments . We frame this microblog post search problem as a late data fusion problem . Previous work on data fusion has mainly focused on aggregating document lists based on retrieval status values or ranks of documents without fully utilizing temporal features of the set of documents being fused . Additionally previous work on data fusion has often worked on the assumption that only documents that are highly ranked in many of the lists are likely to be of relevance . We propose BurstFuseX a fusion model that not only utilizes a microblog post s ranking information but also exploits its publication time . BurstFuseX builds on an existing fusion method and rewards posts that are published in or near a burst of posts that are highly ranked in many of the lists being aggregated . We experimentally verify the effectiveness of the proposed late data fusion algorithm and demonstrate that in terms of mean average precision it significantly outperforms the standard state of the art fusion approaches as well as burst or time sensitive retrieval methods . \\n'],\n",
              " [' Retrieval systems with non deterministic output are widely used in information retrieval . Common examples include sampling approximation algorithms or interactive user input . The effectiveness of such systems differs not just for different topics but also for different instances of the system . The inherent variance presents a dilemma What is the best way to measure the effectiveness of a non deterministic IR system Existing approaches to IR evaluation do not consider this problem or the potential impact on statistical significance . In this paper we explore how such variance can affect system comparisons and propose an evaluation framework and methodologies capable of doing this comparison . Using the context of distributed information retrieval as a case study for our investigation we show that the approaches provide a consistent and reliable methodology to compare the effectiveness of a non deterministic system with a deterministic or another non deterministic system . In addition we present a statistical best practice that can be used to safely show how a non deterministic IR system has equivalent effectiveness to another IR system and how to avoid the common pitfall of misusing a lack of significance as a proof that two systems have equivalent effectiveness . \\n'],\n",
              " [' Social media websites such as YouTube and Flicker are currently gaining in popularity . A large volume of information is generated by online users and how to appropriately provide personalized content is becoming more challenging . Traditional recommendation models are overly dependent on preference ratings and often suffer from the problem of data sparsity . Recent research has attempted to integrate sentiment analysis results of online affective texts into recommendation models however these studies are still limited . The one class collaborative filtering method is more applicable in the social media scenario yet it is insufficient for item recommendation . In this study we develop a novel sentiment aware social media recommendation framework referred to as SA OCCF in order to tackle the above challenges . We leverage inferred sentiment feedback information and OCCF models to improve recommendation performance . We conduct comprehensive experiments on a real social media web site to verify the effectiveness of the proposed framework and methods . The results show that the proposed methods are effective in improving the performance of the baseline OCCF methods . \\n'],\n",
              " [' Bibliographic collections in traditional libraries often compile records from distributed sources where variable criteria have been applied to the normalization of the data . Furthermore the source records often follow classical standards such as MARC21 where a strict normalization of author names is not enforced . The identification of equivalent records in large catalogues is therefore required for example when migrating the data to new repositories which apply modern specifications for cataloguing such as the FRBR and RDA standards . An open source tool has been implemented to assist authority control in bibliographic catalogues when external features are not available for the disambiguation of creator names . This tool is based on similarity measures between the variants of author names combined with a parser which interprets the dates and periods associated with the creator . An efficient data structure has been used to accelerate the identification of variants . The algorithms employed and the attribute grammar are described in detail and their implementation is distributed as an open source resource to allow for an easier uptake . \\n'],\n",
              " [' In the web environment most of the queries issued by users are implicit by nature . Inferring the different temporal intents of this type of query enhances the overall temporal part of the web search results . Previous works tackling this problem usually focused on news queries where the retrieval of the most recent results related to the query are usually sufficient to meet the user s information needs . However few works have studied the importance of time in queries such as Philip Seymour Hoffman where the results may require no recency at all . In this work we focus on this type of queries named time sensitive queries where the results are preferably from a diversified time span not necessarily the most recent one . Unlike related work we follow a content based approach to identify the most important time periods of the query and integrate time into a re ranking model to boost the retrieval of documents whose contents match the query time period . For that purpose we define a linear combination of topical and temporal scores which reflects the relevance of any web document both in the topical and temporal dimensions thus contributing to improve the effectiveness of the ranked results across different types of queries . Our approach relies on a novel temporal similarity measure that is capable of determining the most important dates for a query while filtering out the non relevant ones . Through extensive experimental evaluation over web corpora we show that our model offers promising results compared to baseline approaches . As a result of our investigation we publicly provide a set of web services and a web search interface so that the system can be graphically explored by the research community . \\n'],\n",
              " [' One of the major reasons why people find music so enjoyable is its emotional impact . Creating emotion based playlists is a natural way of organizing music . The usability of online music streaming services could be greatly improved by developing emotion based access methods and automatic music emotion recognition is the most quick and feasible way of achieving it . When resorting to music for emotional regulation purposes users are interested in the MER method to predict their induced or felt emotion . The progress of MER in this area is impeded by the absence of publicly accessible ground truth data on musically induced emotion . Also there is no consensus on the question which emotional model best fits the demands of the users and can provide an unambiguous linguistic framework to describe musical emotions . In this paper we address these problems by creating a sizeable publicly available dataset of 400 musical excerpts from four genres annotated with induced emotion . We collected the data using an online game with a purpose Emotify which attracted a big and varied sample of participants . We employed a nine item domain specific emotional model GEMS . In this paper we analyze the collected data and report agreement of participants on different categories of GEMS . We also analyze influence of extra musical factors on induced emotion . We suggest that modifications in GEMS model are necessary . \\n'],\n",
              " [' Social media is playing a growing role in elections world wide . Thus automatically analyzing electoral tweets has applications in understanding how public sentiment is shaped tracking public sentiment and polarization with respect to candidates and issues understanding the impact of tweets from various entities etc . Here for the first time we automatically annotate a set of 2012 US presidential election tweets for a number of attributes pertaining to sentiment emotion purpose and style by crowdsourcing . Overall more than 100 000 crowdsourced responses were obtained for 13 questions on emotions style and purpose . Additionally we show through an analysis of these annotations that purpose even though correlated with emotions is significantly different . Finally we describe how we developed automatic classifiers using features from state of the art sentiment analysis systems to predict emotion and purpose labels respectively in new unseen tweets . These experiments establish baseline results for automatic systems on this new data . \\n'],\n",
              " [' Information filtering has been a major task of study in the field of information retrieval for a long time focusing on filtering well formed documents such as news articles . Recently more interest was directed towards applying filtering tasks to user generated content such as microblogs . Several earlier studies investigated microblog filtering for focused topics . Another vital filtering scenario in microblogs targets the detection of posts that are relevant to long standing broad and dynamic topics i.e . topics spanning several subtopics that change over time . This type of filtering in microblogs is essential for many applications such as social studies on large events and news tracking of temporal topics . In this paper we introduce an adaptive microblog filtering task that focuses on tracking topics of broad and dynamic nature . We propose an entirely unsupervised approach that adapts to new aspects of the topic to retrieve relevant microblogs . We evaluated our filtering approach using 6 broad topics each tested on 4 different time periods over 4 months . Experimental results showed that on average our approach achieved 84 increase in recall relative to the baseline approach while maintaining an acceptable precision that showed a drop of about 8 . Our filtering method is currently implemented on TweetMogaz a news portal generated from tweets . The website compiles the stream of Arabic tweets and detects the relevant tweets to different regions in the Middle East to be presented in the form of comprehensive reports that include top stories and news in each region . \\n'],\n",
              " [' Recommender systems are filters which suggest items or information that might be interesting to users . These systems analyze the past behavior of a user build her profile that stores information about her interests and exploit that profile to find potentially interesting items . The main limitation of this approach is that it may provide accurate but likely obvious suggestions since recommended items are similar to those the user already knows . In this paper we investigate this issue known as overspecialization or serendipity problem by proposing a strategy that fosters the suggestion of surprisingly interesting items the user might not have otherwise discovered . The proposed strategy enriches a graph based recommendation algorithm with background knowledge that allows the system to deeply understand the items it deals with . The hypothesis is that the infused knowledge could help to discover hidden correlations among items that go beyond simple feature similarity and therefore promote non obvious suggestions . Two evaluations are performed to validate this hypothesis an in vitro experiment on a subset of the hetrec2011 movielens 2k dataset and a preliminary user study . Those evaluations show that the proposed strategy actually promotes non obvious suggestions by narrowing the accuracy loss . \\n'],\n",
              " [' In this paper we develop a supply contract for a two echelon manufacturer retailer supply chain with a bidirectional option which may be exercised as either a call option or a put option . Under the bidirectional option contract we derive closed form expressions for the retailer s optimal order strategies including the initial order strategy and the option purchasing strategy with a general demand distribution . We also analytically examine the feedback effects of the bidirectional option on the retailer s initial order strategy . In addition taking a chain wide perspective we explore how the bidirectional option contract should be set to attain supply chain coordination . \\n'],\n",
              " [' Nowadays a large number of opinion reviews are posted on the Web . Such reviews are a very important source of information for customers and companies . The former rely more than ever on online reviews to make their purchase decisions and the latter to respond promptly to their clients expectations . Unfortunately due to the business that is behind there is an increasing number of deceptive opinions that is fictitious opinions that have been deliberately written to sound authentic in order to deceive the consumers promoting a low quality product or criticizing a potentially good quality one . In this paper we focus on the detection of both types of deceptive opinions positive and negative . Due to the scarcity of examples of deceptive opinions we propose to approach the problem of the detection of deceptive opinions employing PU learning . PU learning is a semi supervised technique for building a binary classifier on the basis of positive and unlabeled examples only . Concretely we propose a novel method that with respect to its original version is much more conservative at the moment of selecting the negative examples from the unlabeled ones . The obtained results show that the proposed PU learning method consistently outperformed the original PU learning approach . In particular results show an average improvement of 8.2 and 1.6 over the original approach in the detection of positive and negative deceptive opinions respectively . \\n'],\n",
              " [' Transductive classification is a useful way to classify texts when labeled training examples are insufficient . Several algorithms to perform transductive classification considering text collections represented in a vector space model have been proposed . However the use of these algorithms is unfeasible in practical applications due to the independence assumption among instances or terms and the drawbacks of these algorithms . Network based algorithms come up to avoid the drawbacks of the algorithms based on vector space model and to improve transductive classification . Networks are mostly used for label propagation in which some labeled objects propagate their labels to other objects through the network connections . Bipartite networks are useful to represent text collections as networks and perform label propagation . The generation of this type of network avoids requirements such as collections with hyperlinks or citations computation of similarities among all texts in the collection as well as the setup of a number of parameters . In a bipartite heterogeneous network objects correspond to documents and terms and the connections are given by the occurrences of terms in documents . The label propagation is performed from documents to terms and then from terms to documents iteratively . Nevertheless instead of using terms just as means of label propagation in this article we propose the use of the bipartite network structure to define the relevance scores of terms for classes through an optimization process and then propagate these relevance scores to define labels for unlabeled documents . The new document labels are used to redefine the relevance scores of terms which consequently redefine the labels of unlabeled documents in an iterative process . We demonstrated that the proposed approach surpasses the algorithms for transductive classification based on vector space model or networks . Moreover we demonstrated that the proposed algorithm effectively makes use of unlabeled documents to improve classification and it is faster than other transductive algorithms . \\n'],\n",
              " [' We consider a consignment contract with consumer non defective returns behavior . In our model an upstream vendor contracts with a downstream retailer . The vendor decides his consignment price charged to the retailer for each unit sold and his refund price for each returned item and then the retailer sets her retail price for selling the product . The vendor gets paid based on net sold units and salvages unsold units as well as returned items in a secondary market . Under the framework we study and compare two different consignment arrangements the retailer vendor manages consignment inventory programs . To study the impact of return policy we discuss a consignment contract without return policy as a benchmark . We show that whether or not the vendor offers a return policy it is always beneficial for the channel to delegate the inventory decision to the vendor . We find that the vendor s return policy depends crucially on the salvage value of returns . If the product has no salvage value the vendor s optimal decision is not to offer a return policy otherwise the vendor can gain more profit by offering a return policy when the salvage value turns out to be positive . \\n'],\n",
              " [' In the Prize Collecting Steiner Tree Problem we are given a set of customers with potential revenues and a set of possible links connecting these customers with fixed installation costs . The goal is to decide which customers to connect into a tree structure so that the sum of the link costs plus the revenues of the customers that are left out is minimized . The problem as well as some of its variants is used to model a wide range of applications in telecommunications gas distribution networks protein protein interaction networks or image segmentation . In many applications it is unrealistic to assume that the revenues or the installation costs are known in advance . In this paper we consider the well known Bertsimas and Sim robust optimization approach in which the input parameters are subject to interval uncertainty and the level of robustness is controlled by introducing a control parameter which represents the perception of the decision maker regarding the number of uncertain elements that will present an adverse behavior . We propose branch and cut approaches to solve the robust counterparts of the PCStT and the Budget Constraint variant and provide an extensive computational study on a set of benchmark instances that are adapted from the deterministic PCStT inputs . We show how the Price of Robustness influences the cost of the solutions and the algorithmic performance . Finally we adapt our recent theoretical results regarding algorithms for a general class of B S robust optimization problems for the robust PCStT and its budget and quota constrained variants . \\n'],\n",
              " [' Document filtering is a popular task in information retrieval . A stream of documents arriving over time is filtered for documents relevant to a set of topics . The distinguishing feature of document filtering is the temporal aspect introduced by the stream of documents . Document filtering systems up to now have been evaluated in terms of traditional metrics like precision recall MAP nDCG F1 and utility . We argue that these metrics do not capture all relevant aspects of the systems being evaluated . In particular they lack support for the temporal dimension of the task . We propose a time sensitive way of measuring performance of document filtering systems over time by employing trend estimation . In short the performance is calculated for batches a trend line is fitted to the results and the estimated performance of systems at the end of the evaluation period is used to compare systems . We detail the application of our proposed trend estimation framework and examine the assumptions that need to hold for valid significance testing . Additionally we analyze the requirements a document filtering metric has to meet and show that traditional macro averaged true positive based metrics like precision recall and utility fail to capture essential information when applied in a batch setting . In particular false positives returned in a batch for topics that are absent from the ground truth in that batch go unnoticed . This is a serious flaw as over generation of a system might be overlooked this way . We propose a new metric aptness that does capture false positives . We incorporate this metric in an overall score and show that this new score does meet all requirements . To demonstrate the results of our proposed evaluation methodology we analyze the runs submitted to the two most recent editions of a document filtering evaluation campaign . We re evaluate the runs submitted to the Cumulative Citation Recommendation task of the 2012 and 2013 editions of the TREC Knowledge Base Acceleration track and show that important new insights emerge . \\n'],\n",
              " [' This study proposes a new 4D framework for citation distribution analysis . The importance and differences in the breadth and depth of citation distribution are analyzed . Easily computable indices X Y and XY are proposed which provide estimates of the breadth and depth of citation distribution . A knowledge unit can be an article author institution journal or a set of something . Index X which represents the breadth of citation distribution is the number of different knowledge units that cite special knowledge units . Index Y which represents the depth of citation distribution is the maximum number of citations among several knowledge units that refer to specific knowledge units . Index XY which synthetically represents Indices X and Y the feature and focus impacts of a knowledge unit is index X divided by index Y . We analyze empirically the citation and reference distributions of 84 journals from the Information science and library science category of the Journal Citation Reports at the journal to journal level . Indices X Y and XY reflect the actual breadth and depth of citation distribution . Differences exist among Indices X Y and XY . Differences also exist between these indices and other bibliometric indicators . These indices can not be replaced by existing bibliometric indicators . Specifically the absolute values of indices X and Y are good supplements to existing bibliometric indicators . However index XY and the relative values of Indices X and Y represent new aspects of bibliometric indicators . \\n'],\n",
              " [' The absence of diacritics in text documents or search queries is a serious problem for Turkish information retrieval because it creates homographic ambiguity . Thus the inappropriate handling of diacritics reduces the retrieval performance in search engines . A straightforward solution to this problem is to normalize tokens by replacing diacritic characters with their American Standard Code for Information Interchange counterparts . However this so called ASCIIfication produces either synthetic words that are not legitimate Turkish words or legitimate words with meanings that are completely different from those of the original words . These non valid synthetic words can not be processed by morphological analysis components which expect the input to be valid Turkish words . By contrast synthetic words are not a problem when no stemmer or a simple first n characters stemmer is used in the text analysis pipeline . This difference emphasizes the notion of the diacritic sensitivity of stemmers . In this study we propose and evaluate an alternative solution based on the application of deASCIIfication which restores accented letters in query terms or text documents . Our risk sensitive evaluation results showed that the diacritics restoration approach yielded more effective and robust results compared with normalizing tokens to remove diacritics . \\n'],\n",
              " [' The International Classification of Diseases is a type of meta data found in many Electronic Patient Records . Research to explore the utility of these codes in medical Information Retrieval applications is new and many areas of investigation remain including the question of how reliable the assignment of the codes has been . This paper proposes two uses of the ICD codes in two different contexts of search Pseudo Relevance Judgments and Pseudo Relevance Feedback . We find that our approach to evaluate the TREC challenge runs using simulated relevance judgments has a positive correlation with the TREC official results and our proposed technique for performing PRF based on the ICD codes significantly outperforms a traditional PRF approach . The results are found to be consistent over the two years of queries from the TREC medical test collection . \\n'],\n",
              " [' We use extractive multi document summarization techniques to perform complex question answering and formulate it as a reinforcement learning problem . Given a set of complex questions a list of relevant documents per question and the corresponding human generated summaries as training data the reinforcement learning module iteratively learns a number of feature weights in order to facilitate the automatic generation of summaries i.e . answers to previously unseen complex questions . A reward function is used to measure the similarities between the candidate summary sentences and the abstract summaries . In the training stage the learner iteratively selects the important document sentences to be included in the candidate summary analyzes the reward function and updates the related feature weights accordingly . The final weights are used to generate summaries as answers to unseen complex questions in the testing stage . Evaluation results show the effectiveness of our system . We also incorporate user interaction into the reinforcement learner to guide the candidate summary sentence selection process . Experiments reveal the positive impact of the user interaction component on the reinforcement learning framework . \\n'],\n",
              " [' We present a novel generic programming implementation of a column generation algorithm for the generalized staff rostering problem . The problem is represented as a generalized set partitioning model which is able to capture commonly occurring problem characteristics given in the literature . Columns of the set partitioning problem are generated dynamically by solving a pricing subproblem and constraint branching in a branch and bound framework is used to enforce integrality . The pricing problem is formulated as a novel three stage nested shortest path problem with resource constraints that exploits the inherent problem structure . A very efficient implementation of this pricing problem is achieved by using generic programming principles in which careful use of the C pre processor allows the generator to be customized for the target problem at compile time . As well as decreasing run times this new approach creates a more flexible modeling framework that is well suited to handling the variety of problems found in staff rostering . Comparison with a more standard run time customization approach shows that speedups of around a factor of 20 are achieved using our new approach . The adaption to a new problem is simple and the implementation is automatically adjusted internally according to the new definition . We present results for three practical rostering problems . The approach captures all features of each problem and is able to provide high quality solutions in less than 15minutes . In two of the three instances the optimal solution is found within this time frame . \\n'],\n",
              " [' Participatory budgets are becoming increasingly popular in many municipalities all around the world . The underlying idea is to allow citizens to participate in the allocation of a municipal budget . Many advantages have been suggested for such experiences including legitimization and more informed and transparent decisions . There are many conceivable variants of such processes . However in most cases both its design and implementation are carried out in an informal way . In this paper we propose a methodology to design a participatory budget process based on a multicriteria decision making model . \\n'],\n",
              " [' We present IntoNews a system to match online news articles with spoken news from a television newscasts represented by closed captions . We formalize the news matching problem as two independent tasks closed captions segmentation and news retrieval . The system segments closed captions by using a windowing scheme sliding or tumbling window . Next it uses each segment to build a query by extracting representative terms . The query is used to retrieve previously indexed news articles from a search engine . To detect when a new article should be surfaced the system compares the set of retrieved articles with the previously retrieved one . The intuition is that if the difference between these sets is large enough it is likely that the topic of the newscast currently on air has changed and a new article should be displayed to the user . In order to evaluate IntoNews we build a test collection using data coming from a second screen application and a major online news aggregator . The dataset is manually segmented and annotated by expert assessors and used as our ground truth . It is freely available for download through the Webscope program . http webscope.sandbox.yahoo.com . Our evaluation is based on a set of novel time relevance metrics that take into account three different aspects of the problem at hand precision timeliness and coverage . We compare our algorithms against the best method previously proposed in literature for this problem . Experiments show the trade offs involved among precision timeliness and coverage of the airing news . Our best method is four times more accurate than the baseline . \\n'],\n",
              " [' Every item produced transported used and discarded within a Supply Chain generates costs and creates an impact on the environment . The increase of forward flows as effects of market globalization and reverse flows due to legislation warranty recycling and disposal activities affect the ability of a modern SC to be economically and environmentally sustainable . In this context the study considers an innovative sustainable closed loop SC problem . It first introduces a linear programming model that aims to minimize the total SC costs . Environmental sustainability is guaranteed by the complete reprocessing of an end of life product the re use of components the disposal of unusable parts sent directly from the manufacturers with a closed loop transportation system that maximizes transportation efficiency . Secondly the authors consider the problem by means of a parametrical study by analyzing the economical sustainability of the proposed CLSC model versus the classical Forward Supply Chain model from two perspectives Case 1 the traditional company perspective where the SC ends at the customers and the disposal costs are not included in the SC and Case 2 the social responsibility company perspective where the disposal costs are considered within the SC . The relative impact of the different variables in the SC structure and the applicability of the proposed model in terms of total costs SC structure and social responsibility are investigated thoroughly and the results are reported at the conclusion of the paper . \\n'],\n",
              " [' In this paper we present an efficient spectral clustering method for large scale data sets given a set of pairwise constraints . Our contribution is threefold clustering accuracy is increased by injecting prior knowledge of the data points constraints to a small affinity submatrix connected components are identified automatically based on the data points pairwise constraints generating thus isolated islands of points furthermore local neighborhoods of points of the same connected component are adapted dynamically and constraints propagation is performed so as to further increase the clustering accuracy finally the complexity is preserved low by following a sparse coding strategy of a landmark spectral clustering . In our experiments with three benchmark shape face and handwritten digit image data sets we show that the proposed method outperforms competitive spectral clustering methods that either follow semi supervised or scalable strategies . \\n'],\n",
              " [' Under circumstances of increasing environmental pressures from markets and regulators focal companies in supply chains have recognized the importance of greening their supply chain through green supplier development programs . Various studies have started to explore the inter relationships between green supply chain management and supplier performance . Much of this performance can be achieved only with suppliers involvement in green supplier development programs . But the literature focusing on green supplier development programs and supplier involvement propensity is very limited . In addition formal tools and models for focal companies to evaluate these inter relationships especially considering propensity of suppliers involvement are even rarer . To help address this gap in the literature we introduce a grey analytical network process based model to identify green supplier development programs that will effectively improve suppliers performance . We further comprehensively evaluate green supplier development programs with explicit consideration of suppliers involvement propensity levels . A real world example is introduced to demonstrate the effectiveness of the model . We end with a discussion of managerial implications and present some directions for further research . \\n'],\n",
              " [' General graph random walk has been successfully applied in multi document summarization but it has some limitations to process documents by this way . In this paper we propose a novel hypergraph based vertex reinforced random walk framework for multi document summarization . The framework first exploits the Hierarchical Dirichlet Process topic model to learn a word topic probability distribution in sentences . Then the hypergraph is used to capture both cluster relationship based on the word topic probability distribution and pairwise similarity among sentences . Finally a time variant random walk algorithm for hypergraphs is developed to rank sentences which ensures sentence diversity by vertex reinforcement in summaries . Experimental results on the public available dataset demonstrate the effectiveness of our framework . \\n'],\n",
              " [' Query suggestion is generally an integrated part of web search engines . In this study we first redefine and reduce the query suggestion problem as comparison of queries . We then propose a general modular framework for query suggestion algorithm development . We also develop new query suggestion algorithms which are used in our proposed framework exploiting query session and user features . As a case study we use query logs of a real educational search engine that targets K 12 students in Turkey . We also exploit educational features in our query suggestion algorithms . We test our framework and algorithms over a set of queries by an experiment and demonstrate a 66 90 statistically significant increase in relevance of query suggestions compared to a baseline method . \\n'],\n",
              " [' Applying text mining techniques to legal issues has been an emerging research topic in recent years . Although a few previous studies focused on assisting professionals in the retrieval of related legal documents to our knowledge no previous studies could provide relevant statutes to the general public using problem statements . In this work we design a text mining based method the three phase prediction algorithm which allows the general public to use everyday vocabulary to describe their problems and find pertinent statutes for their cases . The experimental results indicate that our approach can help the general public who are not familiar with professional legal terms to acquire relevant statutes more accurately and effectively . \\n'],\n",
              " [' Summary writing is a process for creating a short version of a source text . It can be used as a measure of understanding . As grading students summaries is a very time consuming task computer assisted assessment can help teachers perform the grading more effectively . Several techniques such as BLEU ROUGE N gram co occurrence Latent Semantic Analysis LSA Ngram and LSA ERB have been proposed to support the automatic assessment of students summaries . Since these techniques are more suitable for long texts their performance is not satisfactory for the evaluation of short summaries . This paper proposes a specialized method that works well in assessing short summaries . Our proposed method integrates the semantic relations between words and their syntactic composition . As a result the proposed method is able to obtain high accuracy and improve the performance compared with the current techniques . Experiments have displayed that it is to be preferred over the existing techniques . A summary evaluation system based on the proposed method has also been developed . \\n'],\n",
              " [' Nowadays opinion mining systems play a strategic role in different areas such as Marketing Decision Support Systems or Policy Support . Since the arrival of the Web 2.0 more and more textual documents containing information that express opinions or comments in different languages are available . Given the proven importance of such documents the use of effective multilingual opinion mining systems has become of high importance to different fields . This paper presents the experiments carried out with the objective to develop a multilingual sentiment analysis system . We present initial evaluations of methods and resources performed in two international evaluation campaigns for English and for Spanish . After our participation in both competitions additional experiments were carried out with the aim of improving the performance of both Spanish and English systems by using multilingual machine translated data . Based on our evaluations we show that the use of hybrid features and multilingual machine translated data can help to better distinguish relevant features for sentiment classification and thus increase the precision of sentiment analysis systems . \\n'],\n",
              " [' Background Our methodology describes a human activity recognition framework based on feature extraction and feature selection techniques where a set of time statistical and frequency domain features taken from 3 dimensional accelerometer sensors are extracted . This framework specifically focuses on activity recognition using on body accelerometer sensors . We present a novel interactive knowledge discovery tool for accelerometry in human activity recognition and study the sensitivity to the feature extraction parametrization . Results The implemented framework achieved encouraging results in human activity recognition . We have implemented a new set of features extracted from wearable sensors that are ambitious from a computational point of view and able to ensure high classification results comparable with the state of the art wearable systems . A feature selection framework is developed in order to improve the clustering accuracy and reduce computational complexity . The software OpenSignals was used for signal acquisition and signal processing algorithms were developed in Python Programming Language and Orange Software . Several clustering methods such as K Means Affinity Propagation Mean Shift and Spectral Clustering were applied . The K means methodology presented promising accuracy results for person dependent and independent cases with 99.29 and 88.57 respectively . Conclusions The presented study performs two different tests in intra and inter subject context and a set of 180 features is implemented which are easily selected to classify different activities . The implemented algorithm does not stipulate a priori any value for time window or its overlap percentage of the signal but performs a search to find the best parameters that define the specific data . A clustering metric based on the construction of the data confusion matrix is also proposed . The main contribution of this work is the design of a novel gesture recognition system based solely on data from a single 3 dimensional accelerometer . \\n'],\n",
              " [' We propose two novel language models to improve the performance of sentence retrieval in Question Answering class based language model and trained trigger language model . As the search in sentence retrieval is conducted over smaller segments of text than in document retrieval the problems of data sparsity and exact matching become more critical . Different techniques such as the translation model are also proposed to overcome the word mismatch problem . Our class based and trained trigger language models however use different approaches to this aim and are shown to outperform the exiting models . The class model uses word clustering algorithm to capture term relationships . In this model we assume a relation between the terms that belong to the same clusters as a result they can be substituted when searching for relevant sentences . The trigger model captures pairs of trigger and target words while training on a large corpus . The model considers a relation between a question and a sentence if a trigger word appears in the question and the sentence contains the corresponding target word . For both proposed models we introduce different notions of co occurrence to find word relations . In addition we study the impact of corpus size and domain on the models . Our experiments on TREC QA collection verify that the proposed model significantly improves the sentence retrieval performance compared to the state of the art translation model . While the translation model based on mutual information has 0.3927 Mean Average Precision the class model achieves 0.4174 MAP and the trigger model enhances the performance to 0.4381 . \\n'],\n",
              " [' In this paper a new homomorphic image watermarking method implementing the Singular Value Decomposition algorithm is presented . The idea of the proposed method is based on embedding the watermark with the SVD algorithm in the reflectance component after applying the homomorphic transform . The reflectance component contains most of the image features but with low energy and hence watermarks embedded in this component will be invisible . A block by block implementation of the proposed method is also introduced . The watermark embedding on a block by block basis makes the watermark more robust to attacks . A comparison study between the proposed method and the traditional SVD watermarking method is presented in the presence of attacks . The proposed method is more robust to various attacks . The embedding of chaotic encrypted watermarks is also investigated in this paper to increase the level of security . \\n'],\n",
              " [' We study a problem of tactical planning in a divergent supply chain . It involves decisions regarding production inventory internal transportation sales and distribution to customers . The problem is motivated by the context of a company in the speciality oils industry . The overall objective at tactical level is to maximize contribution and in order to achieve this the planning has been divided into two separate problems . The first problem concerns sales where the final sales and distribution planning is decentralized to individual sellers . The second problem concerns production transportation and inventory planning through refineries hubs and depots and is managed centrally with the aim of minimizing costs . Due to this decoupling the solution of the two problems needs to be coordinated in order to achieve the overall objective . In the company this is pursued through an internal price system aiming at giving the sellers the incentives needed to align their decisions with the overall objective . We propose and discuss linear programming models for the decoupled and integrated planning problems . We present numerical examples to illustrate potential effects of integration and coordination and discuss the advantages and disadvantages of the integrated over the decoupled approach . While the total contribution is higher in the integrated approach it has also been found that the sellers contribution can be considerably lower . Therefore we also suggest contribution sharing rules to achieve a solution where both the company and the sellers attain a better outcome under the integrated planning . \\n'],\n",
              " [' Question Answering systems are developed to answer human questions . In this paper we have proposed a framework for answering definitional and factoid questions enriched by machine learning and evolutionary methods and integrated in a web based QA system . Our main purpose is to build new features by combining state of the art features with arithmetic operators . To accomplish this goal we have presented a Genetic Programming based approach . The exact GP duty is to find the most promising formulas made by a set of features and operators which can accurately rank paragraphs sentences and words . We have also developed a QA system in order to test the new features . The input of our system is texts of documents retrieved by a search engine . To answer definitional questions our system performs paragraph ranking and returns the most related paragraph . Moreover in order to answer factoid questions the system evaluates sentences of the filtered paragraphs ranked by the previous module of our framework . After this phase the system extracts one or more words from the ranked sentences based on a set of hand made patterns and ranks them to find the final answer . We have used Text Retrieval Conference QA track questions web data and AQUAINT and AQUAINT 2 datasets for training and testing our system . Results show that the learned features can perform a better ranking in comparison with other evaluation formulas . \\n'],\n",
              " [' XML is a pervasive technology for representing and accessing semi structured data . XPath is the standard language for navigational queries on XML documents and there is a growing demand for its efficient processing . In order to increase the efficiency in executing four navigational XML query primitives namely descendants ancestors children and parent we introduce a new paradigm where traditional approaches based on the efficient traversing of nodes and edges to reconstruct the requested subtrees are replaced by a brand new one based on basic set operations which allow us to directly return the desired subtree avoiding to create it passing through nodes and edges . Our solution stems from the NEsted SeTs for Object hieRarchies formal model which makes use of set inclusion relations for representing and providing access to hierarchical data . We define in memory efficient data structures to implement NESTOR we develop algorithms to perform the descendants ancestors children and parent query primitives and we study their computational complexity . We conduct an extensive experimental evaluation by using several datasets digital archives INEX 2009 Wikipedia collection and two widely used synthetic datasets . We show that NESTOR based data structures and query primitives consistently outperform state of the art solutions for XPath processing at execution time and they are competitive in terms of both memory occupation and pre processing time . \\n'],\n",
              " [' In recent years there has been a rapid growth of user generated data in collaborative tagging systems due to the prevailing of Web 2.0 communities . To effectively assist users to find their desired resources it is critical to understand user behaviors and preferences . Tag based profile techniques which model users and resources by a vector of relevant tags are widely employed in folksonomy based systems . This is mainly because that personalized search and recommendations can be facilitated by measuring relevance between user profiles and resource profiles . However conventional measurements neglect the sentiment aspect of user generated tags . In fact tags can be very emotional and subjective as users usually express their perceptions and feelings about the resources by tags . Therefore it is necessary to take sentiment relevance into account into measurements . In this paper we present a novel generic framework SenticRank to incorporate various sentiment information to various sentiment based information for personalized search by user profiles and resource profiles . In this framework content based sentiment ranking and collaborative sentiment ranking methods are proposed to obtain sentiment based personalized ranking . To the best of our knowledge this is the first work of integrating sentiment information to address the problem of the personalized tag based search in collaborative tagging systems . Moreover we compare the proposed sentiment based personalized search with baselines in the experiments the results of which have verified the effectiveness of the proposed framework . In addition we study the influences by popular sentiment dictionaries and SenticNet is the most prominent knowledge base to boost the performance of personalized search in folksonomy . \\n'],\n",
              " [' This paper surveys the academic OR analytics literature describing research into the laws and rules of sports and sporting competitions . The literature is divided into post hoc analyses and proposals for future changes and is also divided into laws rules of sports themselves and rules organisation of tournaments or competitions . The survey outlines a large number of studies covering 21 sports in many parts of the world . The analytical approaches most commonly used are found to be various forms of regression analysis and simulation . Issues highlighted by this survey include the different views of what constitutes fairness and the frequency with which changes produce unintended consequences . \\n'],\n",
              " [' As demand for mobile broadband services continues to explode mobile wireless networks must expand greatly their capacities . This paper describes and quantifies the economic and technical challenges associated with deepening wireless networks to meet this growing demand . Methods of capacity expansion divide into three general categories the deployment of more radio spectrum more intensive geographic reuse of spectrum and increasing the throughput capacity of each MHz of spectrum within a given geographic area . The paper describes these several basic methods to deepen mobile wireless capacity . It goes on to measure the contribution of each of these methods to historical capacity growth within U.S. networks . The paper then describes the capabilities of 4G LTE wireless technology and further innovations off of it to further improve network capacity . These capacity expansion capabilities of LTE Advanced along with traditional spectrum reuse are quantified and compared to forecasts of future demand to evaluate the ability of U.S. networks to match future demand . Without significantly increasing current spectrum allocations by 560MHz over the 2014 2022 period the presented model suggests that U.S. wireless capacity expansion will be inadequate to accommodate expected demand growth . This conclusion is in contrast to claims that the U.S. faces no spectrum shortage . \\n'],\n",
              " [' Applying natural language processing for mining and intelligent information access to tweets is a challenging emerging research area . Unlike carefully authored news text and other longer content tweets pose a number of new challenges due to their short noisy context dependent and dynamic nature . Information extraction from tweets is typically performed in a pipeline comprising consecutive stages of language identification tokenisation part of speech tagging named entity recognition and entity disambiguation . In this work we describe a new Twitter entity disambiguation dataset and conduct an empirical analysis of named entity recognition and disambiguation investigating how robust a number of state of the art systems are on such noisy texts what the main sources of error are and which problems should be further investigated to improve the state of the art . \\n'],\n",
              " [' Production ramp up is an important phase in the lifecycle of a manufacturing system which still has significant potential for improvement and thereby reducing the time to market of new and updated products . Production systems today are mostly one of a kind complex engineered to order systems . Their ramp up is a complex order of physical and logical adjustments which are characterised by try and error decision making resulting in frequent reiterations and unnecessary repetitions . Studies have shown that clear goal setting and feedback can significantly improve the effectiveness of decision making in predominantly human decision processes such as ramp up . However few measurement driven decision aides have been reported which focus on ramp up improvement and no systematic approach for ramp up time reduction has yet been defined . In this paper a framework for measuring the performance during ramp up is proposed in order to support decision making by providing clear metrics based on the measurable and observable status of the technical system . This work proposes a systematic framework for data preparation ramp up formalisation and performance measurement . A model for defining the ramp up state of a system has been developed in order to formalise and capture its condition . Functionality quality and performance based metrics have been identified to formalise a clear ramp up index as a measurement to guide and support the human decision making . For the validation of the proposed framework two ramp up processes of an assembly station were emulated and their comparison was used to evaluate this work . \\n'],\n",
              " [' This paper considers the uncapacitated lot sizing problem with batch delivery focusing on the general case of time dependent batch sizes . We study the complexity of the problem depending on the other cost parameters namely the setup cost the fixed cost per batch the unit procurement cost and the unit holding cost . We establish that if any one of the cost parameters is allowed to be time dependent the problem is NP hard . On the contrary if all the cost parameters are stationary and assuming no unit holding cost we show that the problem is polynomially solvable in time O where T denotes the number of periods of the horizon . We also show that in the case of divisible batch sizes the problem with time varying setup costs a stationary fixed cost per batch and no unit procurement nor holding cost can be solved in time O . \\n'],\n",
              " [' To avoid a sarcastic message being understood in its unintended literal meaning in microtexts such as messages on Twitter.com sarcasm is often explicitly marked with a hashtag such as sarcasm . We collected a training corpus of about 406 thousand Dutch tweets with hashtag synonyms denoting sarcasm . Assuming that the human labeling is correct we train a machine learning classifier on the harvested examples and apply it to a sample of a day s stream of 2.25 million Dutch tweets . Of the 353 explicitly marked tweets on this day we detect 309 with the hashtag removed . We annotate the top of the ranked list of tweets most likely to be sarcastic that do not have the explicit hashtag . 35 of the top 250 ranked tweets are indeed sarcastic . Analysis indicates that the use of hashtags reduces the further use of linguistic markers for signaling sarcasm such as exclamations and intensifiers . We hypothesize that explicit markers such as hashtags are the digital extralinguistic equivalent of non verbal expressions that people employ in live interaction when conveying sarcasm . Checking the consistency of our finding in a language from another language family we observe that in French the hashtag sarcasme has a similar polarity switching function be it to a lesser extent . \\n'],\n",
              " [' In this paper we study on effective and efficient processing of keyword based queries over graph databases . To produce more relevant answers to a query than the previous approaches we suggest a new answer tree structure which has no constraint on the number of keyword nodes chosen for each keyword in the query . For efficient search of answer trees on the large graph databases we design an inverted list index to pre compute and store connectivity and relevance information of nodes to keyword terms in the graph . We propose a query processing algorithm which aggregates from the pre constructed inverted lists the best keyword nodes and root nodes to find top k answer trees most relevant to the given query . We also enhance the method by extending the structure of the inverted list and adopting a relevance lookup table which enables more accurate estimation of the relevance scores of candidate root nodes and efficient search of top k answer trees . Performance evaluation by experiments with real graph datasets shows that the proposed method can find more effective top k answers than the previous approaches and provides acceptable and scalable execution performance for various types of keyword queries on large graph databases . \\n'],\n",
              " [' Teams are ranked to show their authority over each other . Existing methods rank the cricket teams using an ad hoc points system entirely based on the winning and losing of matches and ignores number of runs or wickets from which a team wins . In this paper adoptions of h index and PageRank are proposed for ranking teams to overcome the weakness of existing methods . Each team is represented by a node in the graph with two teams creates a weighted directed edge between each other by playing a match and the losing team points to the winning team . The intuition is to get more points for a team winning from a stronger team than winning from a weaker team by considering the number of runs or wickets also in addition to just winning and losing matches . The results show that proposed ranking methods provide quite promising insights of one day and test team rankings . The effect of damping factor d is also studied on the performance of PageRank based methods on both ODI and test matches teams ranking and interesting trends are found . \\n'],\n",
              " [' In this work we develop new journal classification methods based on the h index . The introduction of the h index for research evaluation has attracted much attention in the bibliometric study and research quality evaluation . The main purpose of using an h index is to compare the index for different research units to differentiate their research performance . However the h index is defined by only comparing citations counts of one s own publications it is doubtful that the h index alone should be used for reliable comparisons among different research units like researchers or journals . In this paper we propose a new global h index where the publications in the core are selected in comparison with all the publications of the units to be evaluated . Furthermore we introduce some variants of the Gh index to address the issue of discrimination power . We show that together with the original h index they can be used to evaluate and classify academic journals with some distinct advantages in particular that they can produce an automatic classification into a number of categories without arbitrary cut off points . We then carry out an empirical study for classification of operations research and management science journals using this index and compare it with other well known journal ranking results such as the Association of Business Schools Journal Quality Guide and the Committee of Professors in OR ranking lists . \\n'],\n",
              " [' This study addresses the impact of domain expertise on the performance and query strategies used by users while searching for information . Twenty four experts and 24 non experts had to search for psychology information from the Universalis website in order to perform six information problems of varying complexity two simple problems two more difficult problems and two impossible problems . The results showed that participants with prior knowledge in the domain performed better than non experts . This difference was stronger as the complexity of the problems increased . This study also showed that experts and non experts displayed different query strategies . Experts reformulated the impossible problems more often than non experts because they produced new queries with psychology related keywords . The participants rarely used thematic category tool and when they did so this did not enhance their performance . \\n'],\n",
              " [' Products that are not recycled at the end of their life increasingly damage the environment . In a collection remanufacturing scheme these end of life products can generate new profits . Designed on the personal computers industry this study defines an analytical model used to explore the implications of recycling on the reverse supply chain from an efficiency perspective for all participants in the process . The cases considered for analysis are the two and three echelon supply chains where we first look at the decentralized reverse setting followed by the coordinated setting through implementation of revenue sharing contract . We define customer willingness to return obsolete units as a function of the discount offered by the retailer in exchange for recycling devices with a remanufacturing value . The results show that performance measures and total supply chain profits improve through coordination with revenue sharing contracts on both two and three echelon reverse supply chains . \\n'],\n",
              " [' In contrast with their monolingual counterparts little attention has been paid to the effects that misspelled queries have on the performance of Cross Language Information Retrieval systems . The present work makes a first attempt to fill this gap by extending our previous work on monolingual retrieval in order to study the impact that the progressive addition of misspellings to input queries has this time on the output of CLIR systems . Two approaches for dealing with this problem are analyzed in this paper . Firstly the use of automatic spelling correction techniques for which in turn we consider two algorithms the first one for the correction of isolated words and the second one for a correction based on the linguistic context of the misspelled word . The second approach to be studied is the use of character n grams both as index terms and translation units seeking to take advantage of their inherent robustness and language independence . All these approaches have been tested on a from Spanish to English CLIR system that is Spanish queries on English documents . Real user generated spelling errors have been used under a methodology that allows us to study the effectiveness of the different approaches to be tested and their behavior when confronted with different error rates . The results obtained show the great sensitiveness of classic word based approaches to misspelled queries although spelling correction techniques can mitigate such negative effects . On the other hand the use of character n grams provides great robustness against misspellings . \\n'],\n",
              " [' This paper examines the research patterns and trends of Recommendation System in China during the period of 2004 2013 . Data was collected from the China Academic Journal Network Publishing Database and the China Science Periodical Database . A co word analysis was conducted to measure correlation among the extracted keywords . The cluster analysis and social network analysis revealed 12 theme clusters network characteristics of the clusters the strategic diagram and the correlation network . The study results show that there are several important themes with a high correlation in Chinese RecSys research which is considered to be relatively focused mature and well developed overall . Some research themes have developed on a considerable scale while others remain isolated and undeveloped . This study also identified a few emerging themes with great potential for development . It was also determined that studies overall on the applications of RecSys are increasing . \\n'],\n",
              " [' Learning to rank is an increasingly important scientific field that comprises the use of machine learning for the ranking task . New learning to rank methods are generally evaluated on benchmark test collections . However comparison of learning to rank methods based on evaluation results is hindered by the absence of a standard set of evaluation benchmark collections . In this paper we propose a way to compare learning to rank methods based on a sparse set of evaluation results on a set of benchmark datasets . Our comparison methodology consists of two components Normalized Winning Number which gives insight in the ranking accuracy of the learning to rank method and Ideal Winning Number which gives insight in the degree of certainty concerning its ranking accuracy . Evaluation results of 87 learning to rank methods on 20 well known benchmark datasets are collected through a structured literature search . ListNet SmoothRank FenchelRank FSMRank LRUF and LARF are Pareto optimal learning to rank methods in the Normalized Winning Number and Ideal Winning Number dimensions listed in increasing order of Normalized Winning Number and decreasing order of Ideal Winning Number . \\n'],\n",
              " [' The worldwide propagation of mobile phone and the rapid development of location technologies have provided the chance to monitor freeway traffic conditions without requiring extra infrastructure investment . Over the past decade a number of research studies and operational tests have attempted to investigate the methods to estimate traffic measures using information from mobile phone . However most of these works ignored the fact that each vehicle has more than one phone due to the rapid popularity of mobile phone . This paper considered the circumstance of multi phones and proposed a relatively simplistic clustering technique to identify whether phones travel in the same vehicle . By using this technique mobile phone data can be used to determine not only speed but also vehicle counts by type and therefore density . A complex simulation covering different traffic condition and location accuracy of mobile phone has been developed to evaluate the proposed approach . Simulation results indicate that location accuracy of mobile phone is a crucial factor to estimate accurate traffic measures in case of a given location frequency and the number of continuous location data . In addition traffic demand and clustering method have a certain effect on the accuracy of traffic measures . \\n'],\n",
              " [' This paper presents a new relaxation technique to globally optimize mixed integer polynomial programming problems that arise in many engineering and management contexts . Using a bilinear term as the basic building block the underlying idea involves the discretization of one of the variables up to a chosen accuracy level . Multiparametric disaggregation technique for global optimization of polynomial programming problems . J. Glob . Optim . 55 227 251 by means of a radix based numeric representation system coupled with a residual variable to effectively make its domain continuous . Binary variables are added to the formulation to choose the appropriate digit for each position together with new sets of continuous variables and constraints leading to the transformation of the original mixed integer non linear problem into a larger one of the mixed integer linear programming type . The new underestimation approach can be made as tight as desired and is shown capable of providing considerably better lower bounds than a widely used global optimization solver for a specific class of design problems involving bilinear terms . \\n'],\n",
              " [' We analyze the transitions from external search searching on web search engines to internal search searching on websites . We categorize 295 571 search episodes composed of a query submitted to web search engines and the subsequent queries submitted to a single website search by the same users . There are a total of 1 136 390 queries from all searches of which 295 571 are external search queries and 840 819 are internal search queries . We algorithmically classify queries into states and then use n grams to categorize search patterns . We cluster the searching episodes into major patterns and identify the most commonly occurring which are Explorers with a broad external search query and then broad internal search queries Navigators with an external search query containing a URL component and then specific internal search queries and Shifters with a different seemingly unrelated query types when transitioning from external to internal search . The implications of this research are that external search and internal search sessions are part of a single search episode and that online businesses can leverage these search episodes to more effectively target potential customers . \\n'],\n",
              " [' The Question Answering task aims to provide precise and quick answers to user questions from a collection of documents or a database . This kind of IR system is sorely needed with the dramatic growth of digital information . In this paper we address the problem of QA in the medical domain where several specific conditions are met . We propose a semantic approach to QA based on Natural Language Processing techniques which allow a deep analysis of medical questions and documents and semantic Web technologies at both representation and interrogation levels . We present our Semantic Question Answering System called MEANS and our proposed method for Answer Search based on semantic search and query relaxation . We evaluate the overall system performance on real questions and answers extracted from MEDLINE articles . Our experiments show promising results and suggest that a query relaxation strategy can further improve the overall performance . \\n'],\n",
              " [' The equilibrium and socially optimal balking strategies are investigated for unobservable and observable single server classical retrial queues . There is no waiting space in front of the server . If an arriving customer finds the server idle he occupies the server immediately and leaves the system after service . Otherwise if the server is found busy the customer decides whether or not to enter a retrial pool with infinite capacity and becomes a repeated customer based on observation of the system and the reward cost structure imposed on the system . Accordingly two cases with respect to different levels of information are studied and the corresponding Nash equilibrium and social optimization balking strategies for all customers are derived . Finally we compare the equilibrium and optimal behavior regarding these two information levels through numerical examples . \\n'],\n",
              " [' We propose and apply a novel approach for modeling special day effects to predict electricity demand in Korea . Notably we model special day effects on an hourly rather than a daily basis . Hourly specified predictor variables are implemented in the regression model with a seasonal autoregressive moving average type error structure in order to efficiently reflect the special day effects . The interaction terms between the hour of day effects and the hourly based special day effects are also included to capture the unique intraday patterns of special days more accurately . The multiplicative SARMA mechanism is employed in order to identify the double seasonal cycles namely the intraday effect and the intraweek effect . The forecast results of the suggested model are evaluated by comparing them with those of various benchmark models for the following year . The empirical results indicate that the suggested model outperforms the benchmark models for both special and non special day predictions . \\n'],\n",
              " [' Analyzing and modeling users online search behaviors when conducting exploratory search tasks could be instrumental in discovering search behavior patterns that can then be leveraged to assist users in reaching their search task goals . We propose a framework for evaluating exploratory search based on implicit features and user search action sequences extracted from the transactional log data to model different aspects of exploratory search namely uncertainty creativity exploration and knowledge discovery . We show the effectiveness of the proposed framework by demonstrating how it can be used to understand and evaluate user search performance and thereby make meaningful recommendations to improve the overall search performance of users . We used data collected from a user study consisting of 18 users conducting an exploratory search task for two sessions with two different topics in the experimental analysis . With this analysis we show that we can effectively model their behavior using implicit features to predict the user s future performance level with above 70 accuracy in most cases . Further using simulations we demonstrate that our search process based recommendations improve the search performance of low performing users over time and validate these findings using both qualitative and quantitative approaches . \\n'],\n",
              " [' Emission trading schemes such as the European Union Emissions Trading System attempt to reconcile economic efficiency with ecological efficiency by creating financial incentives for companies to invest in climate friendly innovations . Using real options methodology we demonstrate that under uncertainty economic and ecological efficiency continue to be mutually exclusive . This problem is even worse if a climate friendly project depends on investing in of a whole supply chain . We model a sequential bargaining game in a supply chain where the parties negotiate over implementation of a carbon dioxide saving investment project . We show that the outcome of their bargaining is not economically efficient and even less ecologically efficient . Furthermore we show that a supply chain becomes less economically efficient and less ecologically efficient with every additional chain link . Finally we make recommendations for how managers or politicians can improve the situation and thereby increase economic as well as ecological efficiency and thus also the eco efficiency of supply chains . \\n'],\n",
              " [' Cross language plagiarism detection aims to detect plagiarised fragments of text among documents in different languages . In this paper we perform a systematic examination of Cross language Knowledge Graph Analysis an approach that represents text fragments using knowledge graphs as a language independent content model . We analyse the contributions to cross language plagiarism detection of the different aspects covered by knowledge graphs word sense disambiguation vocabulary expansion and representation by similarities with a collection of concepts . In addition we study both the relevance of concepts and their relations when detecting plagiarism . Finally as a key component of the knowledge graph construction we present a new weighting scheme of relations between concepts based on distributed representations of concepts . Experimental results in Spanish English and German English plagiarism detection show state of the art performance and provide interesting insights on the use of knowledge graphs . \\n'],\n",
              " [' Aspect level sentiment analysis is important for numerous opinion mining and market analysis applications . In this paper we study the problem of identifying and rating review aspects which is the fundamental task in aspect level sentiment analysis . Previous review aspect analysis methods seldom consider entity or rating but only 2 tuples i.e . head and modifier pair e.g . in the phrase nice room room is the head and nice is the modifier . To solve this problem we novelly present a Quad tuple Probability Latent Semantic Analysis which incorporates entity and its rating together with the 2 tuples into the PLSA model . Specifically QPLSA not only generates fine granularity aspects but also captures the correlations between words and ratings . We also develop two novel prediction approaches the Quad tuple Prediction and the Expectation Prediction . For evaluation systematic experiments show that Quad tuple PLSA outperforms 2 tuple PLSA significantly on both aspect identification and aspect rating prediction for publication datasets . Moreover for aspect rating prediction QPLSA shows significant superiority over state of the art baseline methods . Besides the Quad tuple Prediction and the Expectation Prediction also show their strong ability in aspect rating on different datasets . \\n'],\n",
              " [' Word sense ambiguity has been identified as a cause of poor precision in information retrieval systems . Word sense disambiguation and discrimination methods have been defined to help systems choose which documents should be retrieved in relation to an ambiguous query . However the only approaches that show a genuine benefit for word sense discrimination or disambiguation in IR are generally supervised ones . In this paper we propose a new unsupervised method that uses word sense discrimination in IR . The method we develop is based on spectral clustering and reorders an initially retrieved document list by boosting documents that are semantically similar to the target query . For several TREC ad hoc collections we show that our method is useful in the case of queries which contain ambiguous terms . We are interested in improving the level of precision after 5 10 and 30 retrieved documents respectively . We show that precision can be improved by 8 above current state of the art baselines . We also focus on poor performing queries . \\n'],\n",
              " [' Video summarization aims at producing a compact version of a full length video while preserving the significant content of the original video . Movie summarization condenses a full length movie into a summary that still retains the most significant and interesting content of the original movie . In the past several movie summarization systems have been proposed to generate a movie summary based on low level video features such as color motion texture etc . However a generic summary which is common to everyone and is produced based only on low level video features will not satisfy every user . As users preferences for the summary differ vastly for the same movie there is a need for a personalized movie summarization system nowadays . To address this demand this paper proposes a novel system to generate semantically meaningful video summaries for the same movie which are tailored to the preferences and interests of a user . For a given movie shots and scenes are automatically detected and their high level features are semi automatically annotated . Preferences over high level movie features are explicitly collected from the user using a query interface . The user preferences are generated by means of a stored query . Movie summaries are generated at shot level and scene level where shots or scenes are selected for summary skim based on the similarity measured between shots and scenes and the user s preferences . The proposed movie summarization system is evaluated subjectively using a sample of 20 subjects with eight movies in the English language . The quality of the generated summaries is assessed by informativeness enjoyability relevance and acceptance metrics and Quality of Perception measures . Further the usability of the proposed summarization system is subjectively evaluated by conducting a questionnaire survey . The experimental results on the performance of the proposed movie summarization approach show the potential of the proposed system . \\n'],\n",
              " [' Considering the inherent connection between supplier selection and inventory management in supply chain networks this article presents a multi period inventory lot sizing model for a single product in a serial supply chain where raw materials are purchased from multiple suppliers at the first stage and external demand occurs at the last stage . The demand is known and may change from period to period . The stages of this production distribution serial structure correspond to inventory locations . The first two stages stand for storage areas for raw materials and finished products in a manufacturing facility and the remaining stages symbolize distribution centers or warehouses that take the product closer to customers . The problem is modeled as a time expanded transshipment network which is defined by the nodes and arcs that can be reached by feasible material flows . A mixed integer nonlinear programming model is developed to determine an optimal inventory policy that coordinates the transfer of materials between consecutive stages of the supply chain from period to period while properly placing purchasing orders to selected suppliers and satisfying customer demand on time . The proposed model minimizes the total variable cost including purchasing production inventory and transportation costs . The model can be linearized for certain types of cost structures . In addition two continuous and concave approximations of the transportation cost function are provided to simplify the model and reduce its computational time . \\n'],\n",
              " [' In social networks identifying influential nodes is essential to control the social networks . Identifying influential nodes has been among one of the most intensively studies of analyzing the structure of networks . There are a multitude of evaluation indicators of node importance in social networks such as degree betweenness and cumulative nomination and so on . But most of the indicators only reveal one characteristic of the node . In fact in social networks node importance is not affected by a single factor but is affected by a number of factors . Therefore the paper puts forward a relatively comprehensive and effective method of evaluation node importance in social networks by using the multi objective decision method . Firstly we select several different representative indicators given a certain weight . We regard each node as a solution and different indicators of each node as the solution properties . Then through calculating the closeness degree of each node to the ideal solution we obtain evaluation indicator of node importance in social networks . Finally we verify the effectiveness of the proposed method experimentally on a few actual social networks . \\n'],\n",
              " [' In order to successfully apply opinion mining to the large amounts of user generated content produced every day we need robust models that can handle the noisy input well yet can easily be adapted to a new domain or language . We here focus on opinion mining for YouTube by modeling classifiers that predict the type of a comment and its polarity while distinguishing whether the polarity is directed towards the product or video proposing a robust shallow syntactic structure that adapts well when tested across domains and evaluating the effectiveness on the proposed structure on two languages English and Italian . We rely on tree kernels to automatically extract and learn features with better generalization power than traditionally used bag of word models . Our extensive empirical evaluation shows that STRUCT outperforms the bag of words model both within the same domain it is particularly useful when tested across domains especially when little training data is available and the proposed structure is also effective in a lower resource language scenario where only less accurate linguistic processing tools are available . \\n'],\n",
              " [' This paper highlights the subject of integrated projects planning in contemporary IS departments and presents a multi period multi project selection and assignment approach to assist the departments in handling continuous project based IS requests . The MPPA features a model to optimize the selection and assignment of IS projects . In the scope of multi project multi period planning the model innovatively considers the losses due to the accumulated postponement of a previously unselected IS request and the expected delay of ongoing projects when inserting a new project request . The MPPA also features an event based decisional process for cumulative selection and assignment on a multi period basis . Due to the complex and contextual nature of data in this paper a computerized system is implemented for aiding the execution of the model and the process . The paper reports on an industrial case for a demonstration of the proposed work . Finally the paper compares the MPPA with related work to summarize the value and role it may play in the IPP context . \\n'],\n",
              " [' Apart from the well known weaknesses of the standard Malmquist productivity index related to infeasibility and not accounting for slacks already addressed in the literature we identify a new and significant drawback of the Malmquist Luenberger index decomposition that questions its validity as an empirical tool for environmental productivity measurement associated with the production of bad outputs . In particular we show that the usual interpretation of the technical change component in terms of production frontier shifts can be inconsistent with its numerical value thereby resulting in an erroneous interpretation of this component that passes on to the index itself . We illustrate this issue with a simple numerical example . Finally we propose a solution for this inconsistency issue based on incorporating a new postulate for the technology related to the production of bad outputs . \\n'],\n",
              " [' This paper presents a Web intelligence portal that captures and aggregates news and social media coverage about Game of Thrones an American drama television series created for the HBO television network based on George R.R . Martin s series of fantasy novels . The system collects content from the Web sites of Anglo American news media as well as from four social media platforms Twitter Facebook Google and YouTube . An interactive dashboard with trend charts and synchronized visual analytics components not only shows how often Game of Thrones events and characters are being mentioned by journalists and viewers but also provides a real time account of concepts that are being associated with the unfolding storyline and each new episode . Positive or negative sentiment is computed automatically which sheds light on the perception of actors and new plot elements . \\n'],\n",
              " [' Complex applications such as big data analytics involve different forms of coupling relationships that reflect interactions between factors related to technical business and environmental aspects . There are diverse forms of couplings embedded in poor structured and ill structured data . Such couplings are ubiquitous implicit and or explicit objective and or subjective heterogeneous and or homogeneous presenting complexities to existing learning systems in statistics mathematics and computer sciences such as typical dependency association and correlation relationships . Modeling and learning such couplings thus is fundamental but challenging . This paper discusses the concept of coupling learning focusing on the involvement of coupling relationships in learning systems . Coupling learning has great potential for building a deep understanding of the essence of business problems and handling challenges that have not been addressed well by existing learning theories and tools . This argument is verified by several case studies on coupling learning including handling coupling in recommender systems incorporating couplings into coupled clustering coupling document clustering coupled recommender algorithms and coupled behavior analysis for groups . \\n'],\n",
              " [' Cloud computing combines established computing technologies and outsourcing advantages into a new ICT paradigm that is generally expected to foster productivity and economic growth . However despite a series of studies on the drivers of cloud adoption evidence of its economic effects is lacking possibly because many of the datasets on cloud computing are of insufficient size and often lack a time dimension as well as precise definitions of cloud computing thus making them unsuitable for rigorous quantitative analysis . To overcome these limitations we propose a proxy variable for cloud computing usage cloud adaptiveness based on survey panel data from European firms . Observations based on a descriptive analysis suggest three important aspects for further research . First cloud studies should be conducted at the industry level as cloud computing adaptiveness differs widely across industry sectors . Second it is important to know what firms do with cloud computing to understand the economic mechanisms and effects triggered by this innovation . And third cloud adaptiveness is potentially correlated to a firm s position in the supply chain and thus the type of output it produces as well as the market in which it operates . Our indicator can be employed to further analyze the effects of cloud computing in the context of firm heterogeneity . \\n'],\n",
              " [' The attributes of vehicle routing problems are additional characteristics or constraints that aim to better take into account the specificities of real applications . The variants thus formed are supported by a well developed literature including a large variety of heuristics . This article first reviews the main classes of attributes providing a survey of heuristics and meta heuristics for Multi Attribute Vehicle Routing Problems . It then takes a closer look at the concepts of 64 remarkable meta heuristics selected objectively for their outstanding performance on 15 classic MAVRP with different attributes . This cross analysis leads to the identification of winning strategies in designing effective heuristics for MAVRP . This is an important step in the development of general and efficient solution methods for dealing with the large range of vehicle routing variants . \\n'],\n",
              " [' The rapid growth of information in the digital world especially on the web calls for automated methods of organizing the digital information for convenient access and efficient information retrieval . Topic modeling is a branch of machine learning and probabilistic graphical modeling that helps in arranging the web pages according to their topical structure . The topic distribution over a set of documents and the affinity of a document toward a specific topic can be revealed using topic modeling . Topic modeling algorithms are typically computationally expensive due to their iterative nature . Recent research efforts have attempted to parallelize specific topic models and are successful in their attempts . These parallel algorithms however have tightly coupled parallel processes which require frequent synchronization and are also tightly coupled with the underlying topic model which is used for inferring the topic hierarchy . In this paper we propose a parallel algorithm to infer topic hierarchies from a large scale document corpus . A key feature of the proposed algorithm is that it exploits coarse grained parallelism and the components running in parallel need not synchronize after every iteration thus the algorithm lends itself to be implemented on a geographically dispersed set of processing elements interconnected through a network . The parallel algorithm realizes a speed up of 53.5 on a 32 node cluster of dual core workstations and at the same time achieving approximately the same likelihood or predictive accuracy as that of the sequential algorithm with respect to the performance of Information Retrieval tasks . \\n'],\n",
              " [' We live in the Information Age where most of the personal business and administrative data are collected and managed electronically . However poor data quality may affect the effectiveness of knowledge discovery processes thus making the development of the data improvement steps a significant concern . In this paper we propose the Multidimensional Robust Data Quality Analysis a domain independent technique aimed to improve data quality by evaluating the effectiveness of a black box cleansing function . Here the proposed approach has been realized through model checking techniques and then applied on a weakly structured dataset describing the working careers of millions of people . Our experimental outcomes show the effectiveness of our model based approach for data quality as they provide a fine grained analysis of both the source dataset and the cleansing procedures enabling domain experts to identify the most relevant quality issues as well as the action points for improving the cleansing activities . Finally an anonymized version of the dataset and the analysis results have been made publicly available to the community . \\n'],\n",
              " [' In this paper we focus on applying sentiment analysis to resources from online art collections by exploiting as information source tags intended as textual traces that visitors leave to comment artworks on social platforms . We present a framework where methods and tools from a set of disciplines ranging from Semantic and Social Web to Natural Language Processing provide us the building blocks for creating a semantic social space to organize artworks according to an ontology of emotions . The ontology is inspired by the Plutchik s circumplex model a well founded psychological model of human emotions . Users can be involved in the creation of the emotional space through a graphical interactive interface . The development of such semantic space enables new ways of accessing and exploring art collections . The affective categorization model and the emotion detection output are encoded into W3C ontology languages . This gives us the twofold advantage to enable tractable reasoning on detected emotions and related artworks and to foster the interoperability and integration of tools developed in the Semantic Web and Linked Data community . The proposal has been evaluated against a real word case study a dataset of tagged multimedia artworks from the ArsMeteo Italian online collection and validated through a user study . \\n'],\n",
              " [' The emerging research area of opinion mining deals with computational methods in order to find extract and systematically analyze people s opinions attitudes and emotions towards certain topics . While providing interesting market research information the user generated content existing on the Web 2.0 presents numerous challenges regarding systematic analysis the differences and unique characteristics of the various social media channels being one of them . This article reports on the determination of such particularities and deduces their impact on text preprocessing and opinion mining algorithms . The effectiveness of different algorithms is evaluated in order to determine their applicability to the various social media channels . Our research shows that text preprocessing algorithms are mandatory for mining opinions on the Web 2.0 and that part of these algorithms are sensitive to errors and mistakes contained in the user generated content . \\n'],\n",
              " [' This article introduces and solves a new rich routing problem integrated with practical operational constraints . The problem examined calls for the determination of the optimal routes for a vehicle fleet to satisfy a mix of two different request types . Firstly vehicles must transport three dimensional rectangular and stackable boxes from a depot to a set of predetermined customers . In addition vehicles must also transfer products between pairs of pick up and delivery locations . Service of both request types is subject to hard time window constraints . In addition feasible palletization patterns must be identified for the transported products . A practical application of the problem arises in the transportation systems of chain stores where vehicles replenish the retail points by delivering products stored at a central depot while they are also responsible for transferring stock between pairs of the retailer network . To solve this very complex combinatorial optimization problem our major objective was to develop an efficient methodology whose required computational effort is kept within reasonable limits . To this end we propose a local search based framework for optimizing vehicle routes in which feasible loading arrangements are identified via a simple structured packing heuristic . The algorithmic framework is enhanced with various memory components which store and retrieve useful information gathered through the search process in order to avoid any duplicate unnecessary calculations . The proposed solution approach is assessed on newly introduced benchmark instances . \\n'],\n",
              " [' Electronic word of mouth communication is an important force in building a digital marketplace . The study of eWOM has implications for how to build an online community through social media design web communication and knowledge exchange . Innovative use of eWOM has significant benefits especially for start up firms . We focus on how users on the web communicate value related to online products . It is the premise of this paper that generating emotional value in social media and networking sites is critical for the survival of new e service ventures . Hence by introducing a formal value theory as a coding scheme we report a study on E value in SMNS by analyzing how a Swedish start up industrial design company attempted to build a global presence by creating followers on the web . The aim of the study was to investigate how the company s website design and communication can affect eWOM over time . This was done by capturing a series of emoticon and value expressions generated by community members from three different e communication campaigns with changing website content hence giving different stimuli to viewers . Those members who expressed emotional value often incorporating emoticons displayed both shorter verbal expressions and reaction time . These value expressions we suggest are important aspects of eWOM and need to be actively taken into account . The study has implications for information management strategies through using eWOM . \\n'],\n",
              " [' This paper presents a genetic algorithm for solving the resource constrained project scheduling problem . The innovative component of the algorithm is the use of a magnet based crossover operator that can preserve up to two contiguous parts from the receiver and one contiguous part from the donator genotype . For this purpose a number of genes in the receiver genotype absorb one another to have the same order and contiguity they have in the donator genotype . The ability of maintaining up to three contiguous parts from two parents distinguishes this crossover operator from the powerful and famous two point crossover operator which can maintain only two contiguous parts both from the same parent . Comparing the performance of the new procedure with that of other procedures indicates its effectiveness and competence . \\n'],\n",
              " [' In this paper we propose a linguistically motivated query expansion framework that recognizes and encodes significant query constituents characterizing query intent in order to improve retrieval performance . Concepts of Interest are recognized as the core concepts that represent the gist of the search goal whilst the remaining query constituents which serve to specify the search goal and complete the query structure are classified as descriptive relational or structural . Acknowledging the need to form semantically associated base pairs for the purpose of extracting related potential expansion concepts an algorithm which capitalizes on syntactical dependencies to capture relationships between adjacent and non adjacent query concepts is proposed . Lastly a robust weighting scheme that duly emphasizes the importance of query constituents based on their linguistic role within the expanded query is presented . We demonstrate improvements in retrieval effectiveness in terms of increased mean average precision garnered by the proposed linguistic based query expansion framework through experimentation on the TREC ad hoc test collections . \\n'],\n",
              " [' Automatic document summarization using citations is based on summarizing what others explicitly say about the document by extracting a summary from text around the citations . While this technique works quite well for summarizing the impact of scientific articles other genres of documents as well as other types of summaries require different approaches . In this paper we introduce a new family of methods that we developed for legal documents summarization to generate catchphrases for legal cases . Our methods use both incoming and outgoing citations and we show how citances can be combined with other elements of cited and citing documents including the full text of the target document and catchphrases of cited and citing cases . On a legal summarization corpus our methods outperform competitive baselines . The combination of full text sentences and catchphrases from cited and citing cases is particularly successful . We also apply and evaluate the methods on scientific paper summarization where they perform at the level of state of the art techniques . Our family of citation based summarization methods is powerful and flexible enough to target successfully a range of different domains and summarization tasks . \\n'],\n",
              " [' Urban legends are a genre of modern folklore consisting of stories about rare and exceptional events just plausible enough to be believed which tend to propagate inexorably across communities . In our view while urban legends represent a form of sticky deceptive text they are marked by a tension between the credible and incredible . They should be credible like a news article and incredible like a fairy tale to go viral . In particular we will focus on the idea that urban legends should mimic the details of news to be credible while they should be emotional and readable like a fairy tale to be catchy and memorable . Using NLP tools we will provide a quantitative analysis of these prototypical characteristics . We also lay out some machine learning experiments showing that it is possible to recognize an urban legend using just these simple features . \\n'],\n",
              " [' Online review mining has been used to help manufacturers and service providers improve their products and services and to provide valuable support for consumer decision making . Product aspect extraction is fundamental to online review mining . This research is aimed to improve the performance of aspect extraction from online consumer reviews . To this end we augment a frequency based extraction method with PMI IR which utilizes web search in measuring the semantic similarity between aspect candidates and target entities . In addition we extend RCut an algorithm originally developed for text classification to learn the threshold for selecting candidate aspects . Experiment results with Chinese online reviews show that our proposed method not only outperforms the state of the art frequency based method for aspect extraction but also generalizes across different product domains and various data sizes . \\n'],\n",
              " [' This paper presents a generalized weighted vertex p center model that represents uncertain nodal weights and edge lengths using prescribed intervals or ranges . The objective of the robust WVPC model is to locate p facilities on a given set of candidate sites so as to minimize worst case deviation in maximum weighted distance from the optimal solution . The RWVPC model is well suited for locating urgent relief distribution centers in an emergency logistics system responding to quick onset natural disasters in which precise estimates of relief demands from affected areas and travel times between URDCs and affected areas are not available . To reduce the computational complexity of solving the model this work proposes a theorem that facilitates identification of the worst case scenario for a given set of facility locations . Since the problem is NP hard a heuristic framework is developed to efficiently obtain robust solutions . Then a specific implementation of the framework based on simulated annealing is developed to conduct numerical experiments . Experimental results show that the proposed heuristic is effective and efficient in obtaining robust solutions . We also examine the impact of the degree of data uncertainty on the selected performance measures and the tradeoff between solution quality and robustness . Additionally this work applies the proposed RWVPC model to a real world instance based on a massive earthquake that hit central Taiwan on September 21 1999 . \\n'],\n",
              " [' Asset allocation among diverse financial markets is essential for investors especially under situations such as the financial crisis of 2008 . Portfolio optimization is the most developed method to examine the optimal decision for asset allocation . We employ the hidden Markov model to identify regimes in varied financial markets a regime switching model gives multiple distributions and this information can convert the static mean variance model into an optimization problem under uncertainty which is the case for unobservable market regimes . We construct a stochastic program to optimize portfolios under the regime switching framework and use scenario generation to mathematically formulate the optimization problem . In addition we build a simple example for a pension fund and examine the behavior of the optimal solution over time by using a rolling horizon simulation . We conclude that the regime information helps portfolios avoid risk during left tail events . \\n'],\n",
              " [' Questionnaires are commonly used to measure attitudes toward systems and perceptions of search experiences . Whilst the face validity of such measures has been established through repeated use in information retrieval research their reliability and wider validity are not typically examined this threatens internal validity . The evaluation of self report questionnaires is important not only for the internal validity of studies and by extension increased confidence in the results but also for examining constructs of interest over time and across different domains and systems . In this paper we look at a specific questionnaire the User Engagement Scale for its robustness as a measure . We describe three empirical studies conducted in the online news domain and investigate the reliability and validity of the UES . Our results demonstrate good reliability of the UES sub scales however we argue that a four factor structure may be more appropriate than the original six factor structure proposed in earlier work . In addition we found evidence to suggest that the UES can differentiate between systems and experimental conditions . \\n'],\n",
              " [' This article describes in depth research on machine learning methods for sentiment analysis of Czech social media . Whereas in English Chinese or Spanish this field has a long history and evaluation datasets for various domains are widely available in the case of the Czech language no systematic research has yet been conducted . We tackle this issue and establish a common ground for further research by providing a large human annotated Czech social media corpus . Furthermore we evaluate state of the art supervised machine learning methods for sentiment analysis . We explore different pre processing techniques and employ various features and classifiers . We also experiment with five different feature selection algorithms and investigate the influence of named entity recognition and preprocessing on sentiment classification performance . Moreover in addition to our newly created social media dataset we also report results for other popular domains such as movie and product reviews . We believe that this article will not only extend the current sentiment analysis research to another family of languages but will also encourage competition potentially leading to the production of high end commercial solutions . \\n'],\n",
              " [' The widespread availability of the Internet and the variety of Internet based applications have resulted in a significant increase in the amount of web pages . Determining the behaviors of search engine users has become a critical step in enhancing search engine performance . Search engine user behaviors can be determined by content based or content ignorant algorithms . Although many content ignorant studies have been performed to automatically identify new topics previous results have demonstrated that spelling errors can cause significant errors in topic shift estimates . In this study we focused on minimizing the number of wrong estimates that were based on spelling errors . We developed a new hybrid algorithm combining character n gram and neural network methodologies and compared the experimental results with results from previous studies . For the FAST and Excite datasets the proposed algorithm improved topic shift estimates by 6.987 and 2.639 respectively . Moreover we analyzed the performance of the character n gram method in different aspects including the comparison with Levenshtein edit distance method . The experimental results demonstrated that the character n gram method outperformed to the Levensthein edit distance method in terms of topic identification . \\n'],\n",
              " [' Because individual interpretations of the analytic hierarchy process linguistic scale vary for each user this study proposes a novel framework that AHP decision makers can use to generate numerical scales individually based on the 2 tuple linguistic modeling of AHP scale problems . By using the concept of transitive calibration individual characteristics in understanding the AHP linguistic scale are first defined . An algorithm is then proposed for detecting the individual characteristics from the linguistic pairwise comparison data that is associated with each of the AHP individual decision makers . Finally a nonlinear programming model is proposed to generate individual numerical scales that optimally match the obtained individual characteristics . Two well known numerical examples are re examined using the proposed framework to demonstrate its validity . \\n'],\n",
              " [' This paper proposes a novel query expansion method to improve accuracy of text retrieval systems . Our method makes use of a minimal relevance feedback to expand the initial query with a structured representation composed of weighted pairs of words . Such a structure is obtained from the relevance feedback through a method for pairs of words selection based on the Probabilistic Topic Model . We compared our method with other baseline query expansion schemes and methods . Evaluations performed on TREC 8 demonstrated the effectiveness of the proposed method with respect to the baseline . \\n'],\n",
              " [' This investigation deals with the problem of language identification of noisy texts which could represent the primary step of many natural language processing or information retrieval tasks . Language identification is the task of automatically identifying the language of a given text . Although there exists several methods in the literature their performances are not so convincing in practice . In this contribution we propose two statistical approaches the high frequency approach and the nearest prototype approach . In the first one 5 algorithms of language identification are proposed and implemented namely character based identification word based identification special characters based identification sequential hybrid algorithm and parallel hybrid algorithm . In the second one we use 11 similarity measures combined with several types of character N Grams . For the evaluation task the proposed methods are tested on forum datasets containing 32 different languages . Furthermore an experimental comparison is made between the proposed approaches and some referential language identification tools such as LIGA NTC Google translate and Microsoft Word . Results show that the proposed approaches are interesting and outperform the baseline methods of language identification on forum texts . \\n'],\n",
              " [' Probabilistic topic models are unsupervised generative models which model document content as a two step generation process that is documents are observed as mixtures of latent concepts or topics while topics are probability distributions over vocabulary words . Recently a significant research effort has been invested into transferring the probabilistic topic modeling concept from monolingual to multilingual settings . Novel topic models have been designed to work with parallel and comparable texts . We define multilingual probabilistic topic modeling and present the first full overview of the current research methodology advantages and limitations in MuPTM . As a representative example we choose a natural extension of the omnipresent LDA model to multilingual settings called bilingual LDA . We provide a thorough overview of this representative multilingual model from its high level modeling assumptions down to its mathematical foundations . We demonstrate how to use the data representation by means of output sets of per topic word distributions and per document topic distributions coming from a multilingual probabilistic topic model in various real life cross lingual tasks involving different languages without any external language pair dependent translation resource cross lingual event centered news clustering cross lingual document classification cross lingual semantic similarity and cross lingual information retrieval . We also briefly review several other applications present in the relevant literature and introduce and illustrate two related modeling concepts topic smoothing and topic pruning . In summary this article encompasses the current research in multilingual probabilistic topic modeling . By presenting a series of potential applications we reveal the importance of the language independent and language pair independent data representations by means of MuPTM . We provide clear directions for future research in the field by providing a systematic overview of how to link and transfer aspect knowledge across corpora written in different languages via the shared space of latent cross lingual topics that is how to effectively employ learned per topic word distributions and per document topic distributions of any multilingual probabilistic topic model in various cross lingual applications . \\n'],\n",
              " [' The uncertainty children experience when searching for information influences their information seeking behavior by stimulating curiosity or hindering their search efforts . This study explored the interactions and the usability of various search interfaces and the enjoyment or uncertainty experienced by children when using them . Structural Equation Modeling was used to determine whether children feel uncertainty or a sense of control when using virtual game like interfaces to search for information associated with entertainment or as a means to satisfy an assigned learning task . We then analyzed the weight relationships among three latent variables using statistical analysis . Our results indicate that children prefer using a retrieval interface with situated affordance to satisfy entertainment related information needs as opposed to searching for information to solve specific problems . Furthermore their perceptions of text and graphic icons determined the degree to which they experienced a sense of uncertainty or control . When searching for entertainment related information they were better able to deal with uncertainty and sought greater control in their search interface compared to when they were searching for information related to assigned tasks . According to their information needs children may regard a game like interface as a toy or a tool for learning . The results of this study can serve as reference for the future development of information search interfaces aimed at arousing the interest of children . The use of virtual game like interfaces to guide the IS behavior of children warrants further study . \\n'],\n",
              " [' In this paper we investigate the impact of emotions on author profiling concretely identifying age and gender . Firstly we propose the EmoGraph method for modelling the way people use the language to express themselves on the basis of an emotion labelled graph . We apply this representation model for identifying gender and age in the Spanish partition of the PAN AP 13 corpus obtaining comparable results to the best performing systems of the PAN Lab of CLEF . \\n'],\n",
              " [' Majority of parallel machine scheduling studies consider machine as the only resource . However in most real life manufacturing environments jobs may require additional resources such as automated guided vehicles machine operators tools pallets dies and industrial robots for their handling and processing . This paper presents a review and discussion of studies on the parallel machine scheduling problems with additional resources . Papers are surveyed in five main categories machine environment additional resource objective functions complexity results and solution methods and other important issues . The strengths and weaknesses of the literature together with open areas for future studies are also emphasized . Finally extensions of integer programming models for two main classes of related problems are given and conclusions are drawn based on computational studies . number of jobs set of jobs number of machines set of machines indices of jobs i h 1 the set of possible modes belonging to job i index of resource modes processing time of job i when processed in mode k Ki processing time of job i on machine j when processed in mode k Ki the size of the single additional resource type the earliest time at which job i can start its processing i.e . release time due date of job i weight of job i denoting the importance of job i relative to other jobs completion time of job i maximum completion time of all jobs in the system i.e . makespan tardiness of job i equals to 1 if job i is tardy 0 otherwise maximum lateness \\n'],\n",
              " [' Several major environmental issues like biodiversity loss and climate change currently concern the international community . These topics that are related to the development of human societies have become increasingly important since the United Nations Conference on Environment and Development or Earth Summit in Rio de Janeiro in 1992 . In this article we are interested in the first issue . We present here many examples of the help that using mathematical programming can provide to decision makers in the protection of biodiversity . The examples we have chosen concern the selection of nature reserves the control of adverse effects caused by landscape fragmentation including the creation or restoration of biological corridors the ecological exploitation of forests the control of invasive species and the maintenance of genetic diversity . Most of the presented models are or can be approximated with linear quadratic or fractional integer formulations and emphasize spatial aspects of conservation planning . Many of them represent decisions taken in a static context but temporal dimension is also considered . The problems presented are generally difficult combinatorial optimization problems some are well solved and others less well . Research is still needed to progress in solving them in order to deal with real instances satisfactorily . Moreover relations between researchers and practitioners have to be strengthened . Furthermore many recent achievements in the field of robust optimization could probably be successfully used for biodiversity protection a domain in which many data are uncertain . \\n'],\n",
              " [' The intention gap between users and queries results in ambiguous and broad queries . To solve these problems subtopic mining has been studied which returns a ranked list of possible subtopics according to their relevance popularity and diversity . This paper proposes a novel method to mine subtopics using simple patterns and a hierarchical structure of subtopic candidates . First relevant and various phrases are extracted as subtopic candidates using simple patterns based on noun phrases and alternative partial queries . Second a hierarchical structure of the subtopic candidates is constructed using sets of relevant documents from a web document collection . Finally the subtopic candidates are ranked considering a balance between popularity and diversity using this structure . In experiments our proposed methods outperformed the baselines and even an external resource based method at high ranked subtopics which shows that our methods can be effective and useful in various search scenarios like result diversification . \\n'],\n",
              " [' Sentiment analysis on Twitter has attracted much attention recently due to its wide applications in both commercial and public sectors . In this paper we present SentiCircles a lexicon based approach for sentiment analysis on Twitter . Different from typical lexicon based approaches which offer a fixed and static prior sentiment polarities of words regardless of their context SentiCircles takes into account the co occurrence patterns of words in different contexts in tweets to capture their semantics and update their pre assigned strength and polarity in sentiment lexicons accordingly . Our approach allows for the detection of sentiment at both entity level and tweet level . We evaluate our proposed approach on three Twitter datasets using three different sentiment lexicons to derive word prior sentiments . Results show that our approach significantly outperforms the baselines in accuracy and F measure for entity level subjectivity and polarity detections . For tweet level sentiment detection our approach performs better than the state of the art SentiStrength by 4 5 in accuracy in two datasets but falls marginally behind by 1 in F measure in the third dataset . \\n'],\n",
              " [' Time is an important aspect of text documents . While some documents are atemporal many have strong temporal characteristics and contain contents related to time . Such documents can be mapped to their corresponding time periods . In this paper we propose estimating the focus time of documents which is defined as the time period to which document s content refers and which is considered complementary dimension to the document s creation time . We propose several estimators of focus time by utilizing statistical knowledge from external resources such as news article collections . The advantage of our approach is that document focus time can be estimated even for documents that do not contain any temporal expressions or contain only few of them . We evaluate the effectiveness of our methods on the diverse datasets of documents about historical events related to 5 countries . Our approach achieves average error of less than 21years on collections of Wikipedia pages extracts from history related books and web pages while using the total time frame of 113years . We also demonstrate an example classification method to distinguish temporal from atemporal documents . \\n'],\n",
              " [' The task of finding groups or teams has recently received increased attention as a natural and challenging extension of search tasks aimed at retrieving individual entities . We introduce a new group finding task given a query topic we try to find knowledgeable groups that have expertise on that topic . We present five general strategies for this group finding task given a heterogenous document repository . The models are formalized using generative language models . Two of the models aggregate expertise scores of the experts in the same group for the task one locates documents associated with experts in the group and then determines how closely the documents are associated with the topic whilst the remaining two models directly estimate the degree to which a group is a knowledgeable group for a given topic . For evaluation purposes we construct a test collection based on the TREC 2005 and 2006 Enterprise collections and define three types of ground truth for our task . Experimental results show that our five knowledgeable group finding models achieve high absolute scores . We also find significant differences between different ways of estimating the association between a topic and a group . \\n'],\n",
              " [' Research into unsupervised ways of stemming has resulted in the past few years in the development of methods that are reliable and perform well . Our approach further shifts the boundaries of the state of the art by providing more accurate stemming results . The idea of the approach consists in building a stemmer in two stages . In the first stage a stemming algorithm based upon clustering which exploits the lexical and semantic information of words is used to prepare large scale training data for the second stage algorithm . The second stage algorithm uses a maximum entropy classifier . The stemming specific features help the classifier decide when and how to stem a particular word . In our research we have pursued the goal of creating a multi purpose stemming tool . Its design opens up possibilities of solving non traditional tasks such as approximating lemmas or improving language modeling . However we still aim at very good results in the traditional task of information retrieval . The conducted tests reveal exceptional performance in all the above mentioned tasks . Our stemming method is compared with three state of the art statistical algorithms and one rule based algorithm . We used corpora in the Czech Slovak Polish Hungarian Spanish and English languages . In the tests our algorithm excels in stemming previously unseen words . Moreover it was discovered that our approach demands very little text data for training when compared with competing unsupervised algorithms . \\n'],\n",
              " [' In the paper we consider three quadratic optimization problems which are frequently applied in portfolio theory i.e . the Markowitz mean variance problem as well as the problems based on the mean variance utility function and the quadratic utility . Conditions are derived under which the solutions of these three optimization procedures coincide and are lying on the efficient frontier the set of mean variance optimal portfolios . It is shown that the solutions of the Markowitz optimization problem and the quadratic utility problem are not always mean variance efficient . The conditions for the mean variance efficiency of the solutions depend on the unknown parameters of the asset returns . We deal with the problem of parameter uncertainty in detail and derive the probabilities that the estimated solutions of the Markowitz problem and the quadratic utility problem are mean variance efficient . Because these probabilities deviate from one the above mentioned quadratic optimization problems are not stochastically equivalent . The obtained results are illustrated by an empirical study . \\n'],\n",
              " [' In reputation management knowing what impact a tweet has on the reputation of a brand or company is crucial . The reputation polarity of a tweet is a measure of how the tweet influences the reputation of a brand or company . We consider the task of automatically determining the reputation polarity of a tweet . For this classification task we propose a feature based model based on three dimensions the source of the tweet the contents of the tweet and the reception of the tweet i.e . how the tweet is being perceived . For evaluation purposes we make use of the RepLab 2012 and 2013 datasets . We study and contrast three training scenarios . The first is independent of the entity whose reputation is being managed the second depends on the entity at stake but has over 90 fewer training samples per model on average . The third is dependent on the domain of the entities . We find that reputation polarity is different from sentiment and that having less but entity dependent training data is significantly more effective for predicting the reputation polarity of a tweet than an entity independent training scenario . Features related to the reception of a tweet perform significantly better than most other features . \\n'],\n",
              " [' This paper focuses on detecting nuclear weapons on cargo containers using port security screening methods where the nuclear weapons would presumably be used to attack a target within the United States . This paper provides a linear programming model that simultaneously identifies optimal primary and secondary screening policies in a prescreening based paradigm where incoming cargo containers are classified according to their perceived risk . The proposed linear programming model determines how to utilize primary and secondary screening resources in a cargo container screening system given a screening budget prescreening classifications and different device costs . Structural properties of the model are examined to shed light on the optimal screening policies . The model is illustrated with a computational example . Sensitivity analysis is performed on the ability of the prescreening in correctly identifying prescreening classifications and secondary screening costs . Results reveal that there are fewer practical differences between the screening policies of the prescreening groups when prescreening is inaccurate . Moreover devices that can better detect shielded nuclear material have the potential to substantially improve the system s detection capabilities . \\n'],\n",
              " [' This paper clarifies the relation between decisions of a risk averse decision maker based on expected utility theory on the one hand and spectral risk measures on the other . We first demonstrate that recent approaches to this problem generally do not provide strongly consistent results i.e . they fail to induce identical preference orders simultaneously with both concepts . Then we detail the relation between risk averse decisions under the dual theory of choice and spectral risk measures . This relation is identified as the fundamental reason why it is not in general possible to establish a simple one to one mapping between expected utility theory and spectral risk measures . We are nonetheless able to use spectral risk measures to model decisions obtained using expected utility theory . Interestingly this implies that a given utility function corresponds to a whole family of risk spectra . \\n'],\n",
              " [' The explosion of online user generated content and the development of big data analysis provide a new opportunity and challenge to understand and respond to public opinions in the G2C e government context . To better understand semantic searching of public comments on an online platform for citizens opinions about urban affairs issues this paper proposed an approach based on the latent Dirichlet allocation a probabilistic topic modeling method and designed a practical system to provide users municipal administrators of B city with satisfying searching results and the longitudinal changing curves of related topics . The system is developed to respond to actual demand from B city s local government and the user evaluation experiment results show that a system based on the LDA method could provide information that is more helpful to relevant staff members . Municipal administrators could better understand citizens online comments based on the proposed semantic search approach and could improve their decision making process by considering public opinions . \\n'],\n",
              " [' We propose TripBuilder an unsupervised framework for planning personalized sightseeing tours in cities . We collect categorized Points of Interests from Wikipedia and albums of geo referenced photos from Flickr . By considering the photos as traces revealing the behaviors of tourists during their sightseeing tours we extract from photo albums spatio temporal information about the itineraries made by tourists and we match these itineraries to the Points of Interest of the city . The task of recommending a personalized sightseeing tour is modeled as an instance of the Generalized Maximum Coverage problem where a measure of personal interest for the user given her preferences and visiting time budget is maximized . The set of actual trajectories resulting from the GMC solution is scheduled on the tourist s agenda by exploiting a particular instance of the Traveling Salesman Problem . Experimental results on three different cities show that our approach is effective efficient and outperforms competitive baselines . \\n'],\n",
              " [' Data envelopment analysis allows us to evaluate the relative efficiency of each of a set of decision making units . However the methodology does not permit us to identify specific sources of inefficiency because DEA views the DMU as a black box that consumes a mix of inputs and produces a mix of outputs . Thus DEA does not provide a DMU manager with insight regarding the internal source of the organization s inefficiency . Recent methodological developments have extended the basic DEA methodology to allow the analyst to look inside the DMU and model the network of production processes that comprise the organization . In such models sub DMUs consume inputs from outside the DMU and intermediate products from other sub DMUs to produce outputs that flow out of the DMU and intermediate products that flow into other sub DMUs . In this paper we present an unoriented two stage DEA model to measure efficiency in situations in which analysts seek to simultaneously reduce input quantities and increase output quantities . The methodology extends previous work in which the model must be either input oriented or output oriented . The key to the methodology is an iterative algorithm that alternates between an input oriented push backward step and an output oriented push forward step that is characterized by damped oscillations in the intermediate products . We apply the methodology to Major League Baseball teams during the 2009 season to demonstrate how this approach provides a deeper understanding of each team s operations . \\n'],\n",
              " [' Researchers have shown that a weighted linear combination in data fusion can produce better results than an unweighted combination . Many techniques have been used to determine the linear combination weights . In this work we have used the Genetic Algorithm for the same purpose . The GA is not new and it has been used earlier in several other applications . But to the best of our knowledge the GA has not been used for fusion of runs in information retrieval . First we use GA to learn the optimum fusion weights using the entire set of relevance assessment . Next we learn the weights from the relevance assessments of the top retrieved documents only . Finally we also learn the weights by a twofold training and testing on the queries . We test our method on the runs submitted in TREC . We see that our weight learning scheme using both full and partial sets of relevance assessment produces significant improvements over the best candidate run CombSUM CombMNZ Z Score linear combination method with performance level performance level square weighting scheme multiple linear regression based weight learning scheme mixture model result merging scheme LambdaMerge ClustFuseCombSUM and ClustFuseCombMNZ . Furthermore we study how the correlation among the scores in the runs can be used to eliminate redundant runs in a set of runs to be fused . We observe that similar runs have similar contributions in fusion . So eliminating the redundant runs in a group of similar runs does not hurt fusion performance in any significant way . \\n'],\n",
              " [' A main challenge in Cross Language Information Retrieval is to estimate a proper translation model from available translation resources since translation quality directly affects the retrieval performance . Among different translation resources we focus on obtaining translation models from comparable corpora because they provide appropriate translations for both languages and domains with limited linguistic resources . In this paper we employ a two step approach to build an effective translation model from comparable corpora without requiring any additional linguistic resources for the CLIR task . In the first step translations are extracted by deriving correlations between source target word pairs . These correlations are used to estimate word translation probabilities in the second step . We propose a language modeling approach for the first step where modeling based on probability distribution provides two key advantages . First our approach can be tuned easier in comparison with heuristically adjusted previous work . Second it provides a principled basis for integrating additional lexical and translational relations to improve the accuracy of translations from comparable corpora . As an indication we integrate monolingual relations of word co occurrences into the process of translation extraction which helps to extract more reliable translations for low frequency words in a comparable corpus . Experimental results on an English Persian comparable corpus show that our method outperforms the previous approaches in terms of both translation quality and the performance of CLIR . Indeed the proposed method is naturally applicable to any comparable corpus regardless of its languages . In addition we demonstrate the significant impact of word translation probabilities estimated in the second step of our approach on the performance of CLIR . \\n'],\n",
              " [' A popular assumption in the current literature on remanufacturing is that the whole new product is produced by an integrated manufacturer which is inconsistent with most industries . In this paper we model a decentralised closed loop supply chain consisting of a key component supplier and a non integrated manufacturer and demonstrate that the interaction between these players significantly impacts the economic and environmental implications of remanufacturing . In our model the non integrated manufacturer can purchase new components from the supplier to produce new products and remanufacture used components to produce remanufactured products . Thus the non integrated manufacturer is not only a buyer but also a rival to the supplier . In a steady state period we analyse the performances of an integrated manufacturer and the decentralised supply chain . We find that although the integrated manufacturer always benefits from remanufacturing the remanufacturing opportunity may constitute a lose lose situation to the supplier and the non integrated manufacturer making their profits be lower than in an identical supply chain without remanufacturing . In addition the non integrated manufacturer may be worse off with a lower remanufacturing cost or a larger return rate of used products due to the interaction with the supplier . We further demonstrate that the government subsidised remanufacturing in the non integrated manufacturer is detrimental to the environment . \\n'],\n",
              " [' This practical study aims to enrich the current literature by providing new practical evidence of the positive and negative influence factors of the Internet on generations Y and Z in Australia and Portugal . The Internet has become a powerful force among these Gens in relation to communication cooperation collaboration and connection but numerous problems from cognitive social and physical developments perspective are developed throughout the Internet usage . A quantitative approach was used to collect new practical evidence from 180 Australian and 85 Portuguese respondents with a total of 265 respondents completing an online survey . This study identifies new positive factors to the Internet usage as problem solving skills proactive study information gathering and awareness globally and locally communication and collaboration with their peers and family were improved and enhanced . Alternatively this study identifies new negative factors as physical contact and physical activities were prevented thinking concentrating and memory skills were reduced depressed and isolated laziness having increased nevertheless the Internet encourages Gens Y and Z to play physical and virtual games . Finally this study concluded that the Internet is becoming an essential part of the everyday routines and practices of Gens Y and Z . \\n'],\n",
              " [' We consider the optimal ship navigation problem wherein the goal is to find the shortest path between two given coordinates in the presence of obstacles subject to safety distance and turn radius constraints . These obstacles can be debris rock formations small islands ice blocks other ships or even an entire coastline . We present a graph theoretic solution on an appropriately weighted directed graph representation of the navigation area obtained via 8 adjacency integer lattice discretization and utilization of the A algorithm . We explicitly account for the following three conditions as part of the turn radius constraints the ship s left and right turn radii are different ship s speed reduces while turning and the ship needs to navigate a certain minimum number of lattice edges along a straight line before making any turns . The last constraint ensures that the navigation area can be discretized at any desired resolution . Once the optimal path is determined we smoothen it to emulate the actual navigation of the ship . We illustrate our methodology on an ice navigation example involving a 100 000 DWT merchant ship and present a proof of concept by simulating the ship s path in a full mission ship handling simulator . \\n'],\n",
              " [' Competitive location problems can be characterized by the fact that the decisions made by others will affect our own payoffs . In this paper we address a discrete competitive location game in which two decision makers have to decide simultaneously where to locate their services without knowing the decisions of one another . This problem arises in a franchising environment in which the decision makers are the franchisees and the franchiser defines the potential sites for locating services and the rules of the game . At most one service can be located at each site and one of the franchisees has preferential rights over the other . This means that if both franchisees are interested in opening the service in the same site only the one that has preferential rights will open it . We consider that both franchisees have budget constraints but the franchisee without preferential rights is allowed to show interest in more sites than the ones she can afford . We are interested in studying the influence of the existence of preferential rights and overbidding on the outcomes for both franchisees and franchiser . A model is presented and an algorithmic approach is developed for the calculation of Nash equilibria . Several computational experiments are defined and their results are analysed showing that preferential rights give its holder a relative advantage over the other competitor . The possibility of overbidding seems to be advantageous for the franchiser as well as the inclusion of some level of asymmetry between the two decision makers . \\n'],\n",
              " [' Robust optimization problems which have uncertain data are considered . We prove surrogate duality theorems for robust quasiconvex optimization problems and surrogate min max duality theorems for robust convex optimization problems . We give necessary and sufficient constraint qualifications for surrogate duality and surrogate min max duality and show some examples at which such duality results are used effectively . Moreover we obtain a surrogate duality theorem and a surrogate min max duality theorem for semi definite optimization problems in the face of data uncertainty . \\n'],\n",
              " [' A series of events generates multiple types of time series data such as numeric and text data over time and the variations of the data types capture the events from different angles . This paper aims to integrate the analyses on such numerical and text time series data influenced by common events with a single model to better understand the events . Specifically we present a topic model called an associative topic model which finds the soft cluster of time series text data guided by time series numerical value . The identified clusters are represented as word distributions per clusters and these word distributions indicate what the corresponding events were . We applied ATM to financial indexes and president approval rates . First ATM identifies topics associated with the characteristics of time series data from the multiple types of data . Second ATM predicts numerical time series data with a higher level of accuracy than does the iterative model which is supported by lower mean squared errors . \\n'],\n",
              " [' Robust portfolios reduce the uncertainty in portfolio performance . In particular the worst case optimization approach is based on the Markowitz model and form portfolios that are more robust compared to mean variance portfolios . However since the robust formulation finds a different portfolio from the optimal mean variance portfolio the two portfolios may have dissimilar levels of factor exposure . In most cases investors need a portfolio that is not only robust but also has a desired level of dependency on factor movement for managing the total portfolio risk . Therefore we introduce new robust formulations that allow investors to control the factor exposure of portfolios . Empirical analysis shows that the robust portfolios from the proposed formulations are more robust than the classical mean variance approach with comparable levels of exposure on fundamental factors . \\n'],\n",
              " [' Natural earthquake disasters are unprecedented incidents which take many lives as a consequence and cause major damages to lifeline infrastructures . Various agencies in a country are responsible for reducing such adverse impacts within specific budgets . These responsibilities range from before to after the incident targeting one of the main phases of disaster management . Use of OR in disaster management and coordination of its phases has been mostly ignored and highly recommended in former reviews . This paper presents a formulation to coordinate three main agencies and proposes a heuristic approach to solve the different introduced sub problems . The results show an improvement of 7.5 24 when the agencies are coordinated . entire population of sub region r in region l vulnerable population ratio of sub region r in region l vulnerable population of sub region r in region l improvement ratio of sub region r in region l budget for investment in the building renovation sector existing emergency supplies in region k inventory cost of one unit of humanitarian goods additional level of humanitarian goods level of relief supplies sent from sub region kto sub region rl budget for investment in the emergency response sector travel time from sub region k to sub region rl survival function which describes the efficiency of goods mobility full cost of retrofitting link failure probability of link retrofitting ratio of link budget for investment in the transportation sector \\n'],\n",
              " [' Temporal aspects have been receiving a great deal of interest in Information Retrieval and related fields . Although previous studies have proposed designed and implemented temporal aware systems and solutions understanding of people s temporal information searching behaviour is still limited . This paper reports the findings of a user study that explored temporal information searching behaviour and strategies in a laboratory setting . Information needs were grouped into three temporal classes to systematically study their characteristics . The main findings of our experiment are as follows . It is intuitive for people to augment topical keywords with temporal expressions such as history recent or future as a tactic of temporal search . However such queries produce mixed results and the success of query reformulations appears to depend on topics to a large extent . Search engine interfaces should detect temporal information needs to trigger the display of temporal search options . Finding a relevant Wikipedia page or similar summary page is a popular starting point of past information needs . Current search engines do a good job for information needs related to recent events but more work is needed for past and future tasks . Participants found it most difficult to find future information . Searching for domain experts was a key tactic in Future search and file types of relevant documents are different from other temporal classes . Overall the comparison of search across temporal classes indicated that Future search was the most difficult and the least successful followed by the search for the Past and then for Recency information . This paper discusses the implications of these findings on the design of future temporal IR systems . \\n'],\n",
              " [' The paper reports on some of the results of a research project into how changes in digital behaviour and services impacts on concepts of trust and authority held by researchers in the sciences and social sciences in the UK and the USA . Interviews were used in conjunction with a group of focus groups to establish the form and topic of questions put to a larger international sample in an online questionnaire . The results of these 87 interviews were analysed to determine whether or not attitudes have indeed changed in terms of sources of information used citation behaviour in choosing references and in dissemination practices . It was found that there was marked continuity in attitudes though an increased emphasis on personal judgement over established and new metrics . Journals were more highly respected than other sources and still the vehicle for formal scholarly communication . The interviews confirmed that though an open access model did not in most cases lead to mistrust of a journal a substantial number of researchers were worried about the approaches from what are called predatory OA journals . Established researchers did not on the whole use social media in their professional lives but a question about outreach revealed that it was recognised as effective in reaching a wider audience . There was a remarkable similarity in practice across research attitudes in all the disciplines covered and in both the countries where interviews were held . \\n'],\n",
              " [' Statistical process control and maintenance planning have long been treated as two separate problems . The interdependence between these two activities has not been adequately addressed in the literature despite their apparent connections . Information obtained in the course of statistical process control signals the need for possible maintenance actions and thus affects the preventive maintenance schedules . Preventive maintenance actions can prevent a production process from further deterioration and improve product quality in conjunction with statistical process control . This paper presents an integrated model for the joint optimization of statistical process control and preventive maintenance . The proposed model is developed for a production process that deteriorates according to a discrete time Markov chain . It is assumed that preventive maintenance is imperfect and both preventive and corrective maintenance are instantaneous . The formulation of the deterioration process with maintenance interventions formulated as a Markov chain provides a breakthrough in designing an efficient solution algorithm and obtaining analytical results . A numerical example is used to illustrate the proposed integrated statistical process control and preventive maintenance policies . Sensitivity analysis is conducted to analyze the impact of model parameters on optimal policies . Sensitivity analysis further indicates the interrelationship between statistical process control and maintenance actions . Numerical results indicate that potential cost savings can be achieved from the proposed integrated policies . \\n'],\n",
              " [' Search task difficulty has been attracting much research attention in recent years mostly regarding its relationship with searchers behaviors and the prediction of task difficulty from search behaviors . However it remains unknown what makes searchers feel the difficulty . A study consisting of 48 undergraduate students was conducted to explore this question . Each participant was given 4 search tasks that were carefully designed following a task classification scheme . Questionnaires were used to elicit participants ratings on task difficulty and why they gave those ratings . Based on the collected difficulty reasons a coding scheme was developed which covered various aspects of task user and user task interaction . Difficulty reasons were then categorized following this scheme . Results showed that searchers reported some common reasons leading to task difficulty in different tasks but most of the difficulty reasons varied across tasks . In addition task difficulty had some common reasons between searchers with low and high levels of topic knowledge although there were also differences in top task difficulty reasons between high and low knowledge users . These findings further our understanding of search task difficulty the relationship between task difficulty and task type and that between task difficulty and knowledge level . The findings can also be helpful with designing tasks for information search experiments and have implications on search system design both in general and for personalization based on task type and searchers knowledge . \\n'],\n",
              " [' Vendors of Business Intelligence software have recently started extending their systems by features from social software . The generated reports may include profiles of report authors and later be supplemented by information about users who accessed the report user evaluations of the report or other social cues . With these features users can support each other in discovering and filtering valuable information in the context of BI . Users who consider reusing an existing report that was not designed by or for them can now not only peruse the report content but also take the social cues into consideration . We analyze which report features influence their perception of report usefulness . Our analysis is based on the elaboration likelihood model which assumes that information recipients are either influenced by the quality of information or peripheral cues . We conduct an experiment with knowledge workers from different companies . The results confirm most hypotheses derived from ELM in the context of BI reports but we also find a deviation from the basic ELM expectations . We find that even people who are able and motivated to scrutinize the report content use community cues to decide on report usefulness in addition to report quality considerations . \\n'],\n",
              " [' Query auto completion models recommend possible queries to web search users when they start typing a query prefix . Most of today s QAC models rank candidate queries by popularity and in doing so they tend to follow a strict query matching policy when counting the queries . That is they ignore the contributions from so called homologous queries queries with the same terms but ordered differently or queries that expand the original query . Importantly homologous queries often express a remarkably similar search intent . Moreover today s QAC approaches often ignore semantically related terms . We argue that users are prone to combine semantically related terms when generating queries . We propose a learning to rank based QAC approach where for the first time features derived from homologous queries and semantically related terms are introduced . In particular we consider the observed and predicted popularity of homologous queries for a query candidate and the semantic relatedness of pairs of terms inside a query and pairs of queries inside a session . We quantify the improvement of the proposed new features using two large scale real world query logs and show that the mean reciprocal rank and the success rate can be improved by up to 9 over state of the art QAC models . \\n'],\n",
              " [' With the rise of Web 2.0 platforms personal opinions such as reviews ratings recommendations and other forms of user generated content have fueled interest in sentiment classification in both academia and industry . In order to enhance the performance of sentiment classification ensemble methods have been investigated by previous research and proven to be effective theoretically and empirically . We advance this line of research by proposing an enhanced Random Subspace method POS RS for sentiment classification based on part of speech analysis . Unlike existing Random Subspace methods using a single subspace rate to control the diversity of base learners POS RS employs two important parameters i.e . content lexicon subspace rate and function lexicon subspace rate to control the balance between the accuracy and diversity of base learners . Ten publicly available sentiment datasets were investigated to verify the effectiveness of proposed method . Empirical results reveal that POS RS achieves the best performance through reducing bias and variance simultaneously compared to the base learner i.e . Support Vector Machine . These results illustrate that POS RS can be used as a viable method for sentiment classification and has the potential of being successfully applied to other text classification problems . \\n'],\n",
              " [' Clusterwise regression consists of finding a number of regression functions each approximating a subset of the data . In this paper a new approach for solving the clusterwise linear regression problems is proposed based on a nonsmooth nonconvex formulation . We present an algorithm for minimizing this nonsmooth nonconvex function . This algorithm incrementally divides the whole data set into groups which can be easily approximated by one linear regression function . A special procedure is introduced to generate a good starting point for solving global optimization problems at each iteration of the incremental algorithm . Such an approach allows one to find global or near global solution to the problem when the data sets are sufficiently dense . The algorithm is compared with the multistart Sp th algorithm on several publicly available data sets for regression analysis . \\n'],\n",
              " [' In this paper we consider a latent Markov process governing the intensity rate of a Poisson process model for software failures . The latent process enables us to infer performance of the debugging operations over time and allows us to deal with the imperfect debugging scenario . We develop the Bayesian inference for the model and also introduce a method to infer the unknown dimension of the Markov process . We illustrate the implementation of our model and the Bayesian approach by using actual software failure data . \\n'],\n",
              " [' This work presents a content based semantics and image retrieval system for semantically categorized hierarchical image databases . Each module is designed with an aim to develop a system that works closer to human perception . Images are mapped to a multidimensional feature space where images belonging a semantic are clustered and indexed to acquire its efficient representation . This helps in handling the existing variability or heterogeneity within this semantic . Adaptive combinations of the obtained depictions are utilized by the branch selection and pruning algorithms to identify some closer semantics and select only a part of the large hierarchical search space for actual search . So obtained search space is finally used to retrieve desired semantics and similar images corresponding to them . The system is evaluated in terms of accuracy of the retrieved semantics and precision recall curves . Experiments show promising semantics and image retrieval results on hierarchical image databases . The results reported with non hierarchical but categorized image databases further prove the efficacy of the proposed system . \\n'],\n",
              " [' Websites can learn what their users do on their pages to provide better content and services to those users . A website can easily find out where a user has been but in order to find out what content is consumed and how it was consumed at a sub page level prior work has proposed client side tracking to record cursor activity which is useful for computing the relevance for search results or determining user attention on a page . While recording cursor interactions can be done without disturbing the user the overhead of recording the cursor trail and transmitting this data over the network can be substantial . In our work we investigate methods to compress cursor data taking advantage of the fact that not every cursor coordinate has equal value to the website developer . We evaluate 5 lossless and 5 lossy compression algorithms over two datasets reporting results about client side performance space savings and how well a lossy algorithm can replicate the original cursor trail . The results show that different compression techniques may be suitable for different goals LZW offers reasonable lossless compression but lossy algorithms such as piecewise linear interpolation and distance thresholding offer better client side performance and bandwidth reduction . \\n'],\n",
              " [' Community question answering services that enable users to ask and answer questions have become popular on the internet . However lots of new questions usually can not be resolved by appropriate answerers effectively . To address this question routing task in this paper we treat it as a ranking problem and rank the potential answerers by the probability that they are able to solve the given new question . We utilize tensor model and topic model simultaneously to extract latent semantic relations among asker question and answerer . Then we propose a learning procedure based on the above models to get optimal ranking of answerers for new questions by optimizing the multi class AUC . Experimental results on two real world CQA datasets show that the proposed method is able to predict appropriate answerers for new questions and outperforms other state of the art approaches . \\n'],\n",
              " [' Media sharing applications such as Flickr and Panoramio contain a large amount of pictures related to real life events . For this reason the development of effective methods to retrieve these pictures is important but still a challenging task . Recognizing this importance and to improve the retrieval effectiveness of tag based event retrieval systems we propose a new method to extract a set of geographical tag features from raw geo spatial profiles of user tags . The main idea is to use these features to select the best expansion terms in a machine learning based query expansion approach . Specifically we apply rigorous statistical exploratory analysis of spatial point patterns to extract the geo spatial features . We use the features both to summarize the spatial characteristics of the spatial distribution of a single term and to determine the similarity between the spatial profiles of two terms i.e . term to term spatial similarity . To further improve our approach we investigate the effect of combining our geo spatial features with temporal features on choosing the expansion terms . To evaluate our method we perform several experiments including well known feature analyzes . Such analyzes show how much our proposed geo spatial features contribute to improve the overall retrieval performance . The results from our experiments demonstrate the effectiveness and viability of our method . \\n'],\n",
              " [' With constant growth in size of analyzable data ranking of academic entities is becoming an attention grabbing task . For ranking of authors this study considers the author s own contribution as well as the impact of mutual influence of the co authors along with exclusivity in their received citations . The ranking of researchers is influenced by the ranking of their co authors more so if co authors are seniors . Tracking the citations received by an author is also an important factor to measure standing of an author . This study proposes Mutual Influence and Citation Exclusivity Author Rank algorithm . We performed a sequence of experiments to calculate the MuICE Rank . First we calculated Mutual Influence considering three different factors the number of papers the number of citations and the author s appearance as first author . Secondly we computed MuICE incorporating all three factors of MuInf along with the exclusivity in citations received by an author . Empirically it is shown that the proposed methods generate substantial results . \\n'],\n",
              " [' Nowadays the increasing demand for group recommendations can be observed . In this paper we address the problem of recommendation performance for groups of users . We focus on the performance of very Top N recommendations which are important when recommending the long lasting items . To improve existing group recommenders we propose a mixed hybrid recommender for groups combining content based and collaborative strategies . The principle of proposed group recommender is to generate content and collaborative recommendations for each user apply an aggregation strategy to solve the group conflict preferences for the content and collaborative sets separately and finally reorder the collaborative candidates based on the content based ones . It is based on an idea that candidates recommended by both recommendation strategies at the same time are presumably more appropriate for the group than the candidates recommended by individual strategies . The evaluation is performed by several experiments in the multimedia domain . Both online and offline experiments were performed in order to compare real users satisfaction to the standard group recommenders and also to compare performance of proposed approach to the state of the art recommenders based on the MovieLens dataset . Finally we experimented with the proposed hybrid recommender to generate the recommendation for a group of size one . Obtained results support our hypothesis that proposed mixed hybrid approach improves the precision of the recommendation for groups of users and for the single user recommendation respectively on very Top N recommended items . \\n'],\n",
              " [' To reduce labor intensive and costly order picking activities many distribution centers are subdivided into a forward area and a reserve area . The former is a small area where most popular stock keeping units can conveniently be picked and the latter is applied for replenishing the forward area and storing SKUs that are not assigned to the forward area at all . Clearly reducing SKUs stored in forward area enables a more compact forward area but requires a more frequent replenishment . To tackle this basic trade off different versions of forward reserve problems determine the SKUs to be stored in forward area the space allocated to each SKU and the overall size of the forward area . As previous research mainly focuses on simplified problem versions where the forward area can continuously be subdivided we investigate discrete forward reserve problems . Important subproblems are defined and computation complexity is investigated . Furthermore we experimentally analyze the model gaps between the different fluid models and their discrete counterparts . \\n'],\n",
              " [' This study proposes an efficient exact algorithm for the precedence constrained single machine scheduling problem to minimize total job completion cost where machine idle time is forbidden . The proposed algorithm is based on the SSDP method and is an extension of the authors previous algorithms for the problem without precedence constraints . In this method a lower bound is computed by solving a Lagrangian relaxation of the original problem via dynamic programming and then it is improved successively by adding constraints to the relaxation until the gap between the lower and upper bounds vanishes . Numerical experiments will show that the algorithm can solve all instances with up to 50 jobs of the precedence constrained total weighted tardiness and total weighted earliness tardiness problems and most instances with 100 jobs of the former problem . \\n'],\n",
              " [' Semantic similarity assessment between concepts is an important task in many language related applications . In the past several approaches to assess similarity by evaluating the knowledge modeled in an ontology have been proposed . However there are some limitations such as the facts of relying on predefined ontologies and fitting non dynamic domains in the existing measures . Wikipedia provides a very large domain independent encyclopedic repository and semantic network for computing semantic similarity of concepts with more coverage than usual ontologies . In this paper we propose some novel feature based similarity assessment methods that are fully dependent on Wikipedia and can avoid most of the limitations and drawbacks introduced above . To implement similarity assessment based on feature by making use of Wikipedia firstly a formal representation of Wikipedia concepts is presented . We then give a framework for feature based similarity based on the formal representation of Wikipedia concepts . Lastly we investigate several feature based approaches to semantic similarity measures resulting from instantiations of the framework . The evaluation based on several widely used benchmarks and a benchmark developed in ourselves sustains the intuitions with respect to human judgements . Overall several methods proposed in this paper have good human correlation and constitute some effective ways of determining similarity between Wikipedia concepts . \\n'],\n",
              " [' This paper presents a novel solution heuristic to the General Lotsizing and Scheduling Problem for Parallel production Lines . The GLSPPL addresses the problem of simultaneously deciding about the sizes and schedules of production lots on parallel heterogeneous production lines with respect to scarce capacity sequence dependent setup times and deterministic dynamic demand of multiple products . Its objective is to minimize inventory holding sequence dependent setup and production costs . The new heuristic iteratively decomposes the multi line problem into a series of single line problems which are easier to solve . Different approaches for decomposition and for the iteration between a modified multi line master problem and the single line subproblems are proposed . They are compared with an existing solution method for the GLSPPL by means of medium sized and large practical problem instances from different types of industries . The new methods prove to be superior with respect to both solution quality and computation time . \\n'],\n",
              " [' Daily deals have emerged in the last three years as a successful form of online advertising . The downside of this success is that users are increasingly overloaded by the many thousands of deals offered each day by dozens of deal providers and aggregators . The challenge is thus offering the right deals to the right users i.e . the relevance ranking of deals . This is the problem we address in our paper . Exploiting the characteristics of deals data we propose a combination of a term and a concept based retrieval model that closes the semantic gap between queries and documents expanding both of them with category information . The method consistently outperforms state of the art methods based on term matching alone and existing approaches for ad classification and ranking . \\n'],\n",
              " [' This paper proposes a test for whether data are over represented in a given production zone i.e . a subset of a production possibility set which has been estimated using the non parametric Data Envelopment Analysis approach . A binomial test is used that relates the number of observations inside such a zone to a discrete probability weighted relative volume of that zone . A Monte Carlo simulation illustrates the performance of the proposed test statistic and provides good estimation of both facet probabilities and the assumed common inefficiency distribution in a three dimensional input space . Potential applications include tests for whether benchmark units dominate more observations than expected . \\n'],\n",
              " [' In the area of Information Retrieval the task of automatic text summarization usually assumes a static underlying collection of documents disregarding the temporal dimension of each document . However in real world settings collections and individual documents rarely stay unchanged over time . The World Wide Web is a prime example of a collection where information changes both frequently and significantly over time with documents being added modified or just deleted at different times . In this context previous work addressing the summarization of web documents has simply discarded the dynamic nature of the web considering only the latest published version of each individual document . This paper proposes and addresses a new challenge the automatic summarization of changes in dynamic text collections . In standard text summarization retrieval techniques present a summary to the user by capturing the major points expressed in the most recent version of an entire document in a condensed form . In this new task the goal is to obtain a summary that describes the most significant changes made to a document during a given period . In other words the idea is to have a summary of the revisions made to a document over a specific period of time . This paper proposes different approaches to generate summaries using extractive summarization techniques . First individual terms are scored and then this information is used to rank and select sentences to produce the final summary . A system based on Latent Dirichlet Allocation model is used to find the hidden topic structures of changes . The purpose of using the LDA model is to identify separate topics where the changed terms from each topic are likely to carry at least one significant change . The different approaches are then compared with the previous work in this area . A collection of articles from Wikipedia including their revision history is used to evaluate the proposed system . For each article a temporal interval and a reference summary from the article s content are selected manually . The articles and intervals in which a significant event occurred are carefully selected . The summaries produced by each of the approaches are evaluated comparatively to the manual summaries using ROUGE metrics . It is observed that the approach using the LDA model outperforms all the other approaches . Statistical tests reveal that the differences in ROUGE scores for the LDA based approach is statistically significant at 99 over baseline . \\n'],\n",
              " [' This paper investigates the contributions of digital infrastructure policies of provincial governments in Canada to the development of broadband networks . Using measurements of broadband network speeds between 2007 and 2011 the paper analyzes potential causes for observed differences in network performance growth across the provinces including geography Internet use intensity platform competition and provincial broadband policies . The analysis suggests provincial policies that employed public sector procurement power to open access to essential facilities and channeled public investments in Internet backbone infrastructure were associated with the emergence of relatively high quality broadband networks . However a weak essential facilities regime and regulatory barriers to entry at the national level limit the scope for decentralized policy solutions . \\n'],\n",
              " [' The Sequential Probability Ratio Test control chart is a powerful tool for monitoring manufacturing processes . It is highly suitable for the applications where testing is destructive or very expensive such as the automobile airbags test . This article studies the effect of the Average Sample Number on the chart s performance . A design algorithm is proposed to develop the optimal SPRT chart for monitoring the fraction nonconforming p of Bernoulli processes . By optimizing the ASN and other charting parameters the average detection speed of the SPRT chart is almost doubled . It is also found that the optimal SPRT chart significantly outperforms the optimal np and binomial CUSUM charts in terms of Average Number of Defectives under different combinations of the design specifications . It is observed that the SPRT chart using a relatively smaller ASN and a shorter sampling interval has a higher overall detection effectiveness . \\n'],\n",
              " [' Cluster analysis using multiple representations of data is known as multi view clustering and has attracted much attention in recent years . The major drawback of existing multi view algorithms is that their clustering performance depends heavily on hyperparameters which are difficult to set . In this paper we propose the Multi View Normalized Cuts approach a two step algorithm for multi view clustering . In the first step an initial partitioning is performed using a spectral technique . In the second step a local search procedure is used to refine the initial clustering . MVNC has been evaluated and compared to state of the art multi view clustering approaches using three real world datasets . Experimental results have shown that MVNC significantly outperforms existing algorithms in terms of clustering quality and computational efficiency . In addition to its superior performance MVNC is parameter free which makes it easy to use . \\n'],\n",
              " [' This note proposes an alternative procedure for identifying violated subtour elimination constraints in branch and cut algorithms for elementary shortest path problems . The procedure is also applicable to other routing problems such as variants of travelling salesman or shortest Hamiltonian path problems on directed graphs . The proposed procedure is based on computing the strong components of the support graph . The procedure possesses a better worst case time complexity than the standard way of separating SECs which uses maximum flow algorithms and is easier to implement . \\n'],\n",
              " [' Many problems in data mining involve datasets with multiple views where the feature space consists of multiple feature groups . Previous studies employed view weighting method to find a shared cluster structure underneath different views . However most of these studies applied gradient optimization method to optimize the cluster centroids and feature weights iteratively and made the final partition local optimal . In this work we proposed a novel bi level weighted multi view clustering method with emphasizing fuzzy weighting on both view and feature . Furthermore an efficient global search strategy that combines particle swarm optimization and gradient optimization was proposed to solve the induced non convex loss function . In the experimental analysis the performance of the proposed method was compared with five state of the art weighted clustering algorithms on three real world high dimensional multi view datasets . \\n'],\n",
              " [' In this paper we propose and evaluate the Block Max WAND with Candidate Selection and Preserving Top K Results algorithm or BMW CSP . It is an extension of BMW CS a method previously proposed by us . Although very efficient BMW CS does not guarantee preserving the top k results for a given query . Algorithms that do not preserve the top results may reduce the quality of ranking results in search systems . BMW CSP extends BMW CS to ensure that the top k results will have their rankings preserved . In the experiments we performed for computing the top 10 results the final average time required for processing queries with BMW CSP was lesser than the ones required by the baselines adopted . For instance when computing top 10 results the average time achieved by MBMW the best multi tier baseline we found in the literature was 36.29 ms per query while the average time achieved by BMW CSP was 19.64 ms per query . The price paid by BMW CSP is an extra memory required to store partial scores of documents . As we show in the experiments this price is not prohibitive and in cases where it is acceptable BMW CSP may constitute an excellent alternative query processing method . \\n'],\n",
              " [' Object matching is an important task for finding the correspondence between objects in different domains such as documents in different languages and users in different databases . In this paper we propose probabilistic latent variable models that offer many to many matching without correspondence information or similarity measures between different domains . The proposed model assumes that there is an infinite number of latent vectors that are shared by all domains and that each object is generated from one of the latent vectors and a domain specific projection . By inferring the latent vector used for generating each object objects in different domains are clustered according to the vectors that they share . Thus we can realize matching between groups of objects in different domains in an unsupervised manner . We give learning procedures of the proposed model based on a stochastic EM algorithm . We also derive learning procedures in a semi supervised setting where correspondence information for some objects are given . The effectiveness of the proposed models is demonstrated by experiments on synthetic and real data sets . \\n'],\n",
              " [' The Team Orienteering Problem is a particular vehicle routing problem in which the aim is to maximize the profit gained from visiting customers without exceeding a travel cost time limit . This paper proposes a new and fast evaluation process for TOP based on an interval graph model and a Particle Swarm Optimization inspired Algorithm to solve the problem . Experiments conducted on the standard benchmark of TOP clearly show that our algorithm outperforms the existing solving methods . PSOiA reached a relative error of 0.0005 whereas the best known relative error in the literature is 0.0394 . Our algorithm detects all but one of the best known solutions . Moreover a strict improvement was found for one instance of the benchmark and a new set of larger instances was introduced . \\n'],\n",
              " [' Lateral transshipments are an effective strategy to pool inventories . We present a Semi Markov decision problem formulation for proactive and reactive transshipments in a multi location continuous review distribution inventory system with Poisson demand and one for one replenishment policy . For a two location model we state the monotonicity of an optimal policy . In a numerical study we compare the benefits of proactive and different reactive transshipment rules . The benefits of proactive transshipments are the largest for networks with intermediate opportunities of demand pooling and the difference between alternative reactive transshipment rules is negligible . \\n'],\n",
              " [' Topic time reflects the temporal feature of topics in Web news pages which can be used to establish and analyze topic models for many time sensitive text mining tasks . However there are two critical challenges in discovering topic time from Web news pages . The first issue is how to normalize different kinds of temporal expressions within a Web news page e.g . explicit and implicit temporal expressions into a unified representation framework . The second issue is how to determine the right topic time for topics in Web news . Aiming at solving these two problems we propose a systematic framework for discovering topic time from Web news . In particular for the first issue we propose a new approach that can effectively determine the appropriate referential time for implicit temporal expressions and further present an effective defuzzification algorithm to find the right explanation for a fuzzy temporal expression . For the second issue we propose a relation model to describe the relationship between news topics and topic time . Based on this model we design a new algorithm to extract topic time from Web news . We build a prototype system called Topic Time Parser and conduct extensive experiments to measure the effectiveness of our proposal . The results suggest that our proposal is effective in both temporal expression normalization and topic time extraction . \\n'],\n",
              " [' Interactive approaches employing cone contraction for multi criteria mixed integer optimization are introduced . In each iteration the decision maker is asked to give a reference point . The subsequent Pareto optimal point is the reference point projected on the set of admissible objective vectors using a suitable scalarizing function . Thereby the procedures solve a sequence of optimization problems with integer variables . In such a process the DM provides additional preference information via pair wise comparisons of Pareto optimal points identified . Using such preference information and assuming a quasiconcave and non decreasing value function of the DM we restrict the set of admissible objective vectors by excluding subsets which can not improve over the solutions already found . The procedures terminate if all Pareto optimal solutions have been either generated or excluded . In this case the best Pareto point found is an optimal solution . Such convergence is expected in the special case of pure integer optimization indeed numerical simulation tests with multi criteria facility location models and knapsack problems indicate reasonably fast convergence in particular under a linear value function . We also propose a procedure to test whether or not a solution is a supported Pareto point . \\n'],\n",
              " [' One of the problems of opinion mining is the domain adaptation of the sentiment classifiers . There are several approaches to tackling this problem . One of these is the integration of a list of opinion bearing words for the specific domain . This paper presents the generation of several resources for domain adaptation to polarity detection . On the other hand the lack of resources in languages different from English has orientated our work towards developing sentiment lexicons for polarity classifiers in Spanish . The results show the validity of the new sentiment lexicons which can be used as part of a polarity classifier . \\n'],\n",
              " [' In this article we present a new exact algorithm for solving the simple assembly line balancing problem given a determined cycle time . The algorithm is a station oriented bidirectional branch and bound procedure based on a new enumeration strategy that explores the feasible solutions tree in a non decreasing idle time order . The procedure uses several well known lower bounds dominance rules and a new logical test based on the assimilation of the feasibility problem for a given cycle time and number of stations to a maximum flow problem . The algorithm has been tested with a series of computational experiments on a well known set of problem instances . The tests show that the proposed algorithm outperforms the best existing exact algorithm for solving SALBP 1 verifying optimality for 264 of the 269 benchmark instances . \\n'],\n",
              " [' We consider single item and inventory systems with integer valued demand processes . While most of the inventory literature studies continuous approximations of these models and establishes joint convexity properties of the policy parameters in the continuous space we show that these properties no longer hold in the discrete space in the sense of linear interpolation extension and L convexity . This nonconvexity can lead to failure of optimization techniques based on local optimality to obtain the optimal inventory policies . It can also make certain comparative properties established previously using continuous variables invalid . We revise these properties in the discrete space . \\n'],\n",
              " [' The purpose of this article is to validate through two empirical studies a new method for automatic evaluation of written texts called Inbuilt Rubric based on the Latent Semantic Analysis technique which constitutes an innovative and distinct turn with respect to LSA application so far . In the first empirical study evidence of the validity of the method to identify and evaluate the conceptual axes of a text in a sample of 78 summaries by secondary school students is sought . Results show that the proposed method has a significantly higher degree of reliability than classic LSA methods of text evaluation and displays very high sensitivity to identify which conceptual axes are included or not in each summary . A second study evaluates the method s capacity to interact and provide feedback about quality in a real online system on a sample of 924 discursive texts written by university students . Results show that students improved the quality of their written texts using this system and also rated the experience very highly . The final conclusion is that this new method opens a very interesting way regarding the role of automatic assessors in the identification of presence absence and quality of elaboration of relevant conceptual information in texts written by students with lower time costs than the usual LSA based methods . \\n'],\n",
              " [' Trade credit arises when a buyer delays payment for purchased goods or services . Its nature has predominantly been an area of inquiry for researchers from the disciplines of finance marketing and economics but it has received relatively little attention in other domains . In our article we provide an integrative review of the existing literature and discuss conflicting study outcomes . We organize the relevant literature into seven areas of inquiry and analyze four in detail trade credit motives order quantity decisions credit term decisions and settlement period decisions . Additionally we derive a detailed agenda for future research in these areas . \\n'],\n",
              " [' Social media represents an emerging challenging sector where the natural language expressions of people can be easily reported through blogs and short text messages . This is rapidly creating unique contents of massive dimensions that need to be efficiently and effectively analyzed to create actionable knowledge for decision making processes . A key information that can be grasped from social environments relates to the polarity of text messages . To better capture the sentiment orientation of the messages several valuable expressive forms could be taken into account . In this paper three expressive signals typically used in microblogs have been explored adjectives emoticon emphatic and onomatopoeic expressions and expressive lengthening . Once a text message has been normalized to better conform social media posts to a canonical language the considered expressive signals have been used to enrich the feature space and train several baseline and ensemble classifiers aimed at polarity classification . The experimental results show that adjectives are more discriminative and impacting than the other considered expressive signals . \\n'],\n",
              " [' We develop a dynamic continuous time theory of the competitive firm under multiple correlated uncertainties . In doing so we completely generalize and extend the previous comparative statics results . Particularly we relax the assumption of the statistical independence between the risks and the restrictions on the coefficient of absolute relative risk aversion . Furthermore we generally show the impact of one risk on the aversion to another . Moreover we show the role of the factor of correlation between risks on the decisions of the firm . \\n'],\n",
              " [' Autonomous word of mouth as a channel of social influence that is out of firms direct control has acquired particular importance with the development of the Internet . Depending on whether a given product or service is a good or a bad deal this can significantly contribute to commercial success or failure . Yet the existing dynamic models of sales in marketing still assume that the influence of word of mouth on sales is at best advertising dependent . This omission can produce ineffective management and therefore misleading marketing policies . This paper seeks to bridge the gap by introducing a contagion sales model of a monopolist firm s product where sales are affected by advertising dependent as well as autonomous word of mouth . We assume that the firm s attraction rate of new customers is determined by the degree at which the current sales price is advantageous or not compared with the current customers reservation price . A primary goal of the paper is to determine the optimal sales price and advertising effort . We show that despite costly price adjustments the interactions between sales price advertising dependent and autonomous word of mouth can result in complex dynamic pricing policies involving history dependence or limit cycling consisting of alternating attraction of new customers and attrition of current customers . \\n'],\n",
              " [' The existence of positive and negative externalities ought to be considered in a productivity analysis in order to obtain unbiased measures of efficiency . In this research we present an additive style data envelopment analysis model that considers the production of both negative and positive externalities and permits a limited increase in input utilisation where relevant . The directional economic environmental distance function is a unified approach based on a linear program that evaluates the relative inefficiency of the units under examination with respect to a unique reference technology . We discuss the impact of disposability assumptions in depth and demonstrate how different versions of the DEED model improve on models presented in the literature to date . \\n'],\n",
              " [' Faced with a short turn around request to characterise several hand held mine detection systems the authors developed and applied an analytical methodology that was sufficiently robust and pragmatic to satisfy the needs of the various military stakeholders involved yet it was appropriately rigorous and transparent to bear external scrutiny . The methodology can be applied in situations where data collection and analysis must be done quickly while preserving scientific veracity . For mine detection systems considerable uncertainties existed that needed to be characterised including application location operational situation and involvement of human operators . Constraints on the time and expertise available implied there would be difficulties ensuring a sufficient number of trials could be conducted to levels of statistical confidence that would assure appropriate credibility across all of the parameters . This problem was effectively rectified through experimental design and by heavily involving the sponsor stakeholders and subject matter experts throughout the study thus boosting the credibility and acceptance of its results . The process followed involved liaison with the sponsor identification of critical issues measurements in field environments reporting mechanisms and discussion on implementation and further development . The critical focus was operational capability rather than specific equipment characteristics . A robust data presentation technique was developed to deal with the complexities associated with different needs of multiple stakeholders . This technique enabled the results to be reviewed from different stakeholders perspectives the formation of a common understanding and the results to be reusable in future analyses . \\n'],\n",
              " [' We analyze a time fenced planning system where both expediting and canceling are allowed inside the time fence but only with a penalty . Previous research has allowed only for the case of expediting inside the time fence and has overlooked the opportunity for additional improvement by also allowing for cancelations . Some researchers also have found that for traditional time fenced models the choice of the more complex stochastic linear programming approach versus the simpler deterministic approach is not justified . We formulate both the deterministic and stochastic problems as dynamic programs and develop analytic bounds that limit the search space of the stochastic approach . We run extensive simulations and numerical experiments to understand better the benefit of adding cancelation and to compare the performance of the stochastic model with the more common deterministic model when they are employed as heuristics in a rolling horizon setting . Across all experiments we find that allowing expediting lowered costs by 11.3 using the deterministic approach but costs were reduced by 27.8 if both expediting and canceling are allowed . We find that the benefit of using the stochastic model versus the deterministic model varies widely across demand distributions and levels of recourse the ratio of stochastic average costs to deterministic average costs ranged from 43.3 to 78.5 . \\n'],\n",
              " [' In this paper the resource constrained project scheduling problem with general temporal constraints is extended by the concept of break calendars in order to incorporate the possible absence of renewable resources . Three binary linear model formulations are presented that use either start based or changeover based or execution based binary decision variables . In addition a priority rule method as well as three different versions of a scatter search procedure are proposed in order to solve the problem heuristically . All exact and heuristic solution procedures use a new and powerful time planning method which identifies all time and calendar feasible start times for activities as well as all corresponding absolute time lags between activities . In a comprehensive performance analysis small and medium scale instances are solved with CPLEX 12.6 . Furthermore large scale instances of the problem are tackled with scatter search where the results of the three versions are compared to each other and to the priority rule method . \\n'],\n",
              " [' This note takes up a shortcoming of Coelli et al . s popular environmental efficiency measure and its extension to economic environmental trade off analysis namely that they do not reward emission reductions by pollution control . A new environmental efficiency measure that overcomes this issue and similar to Coelli et al . s efficiency measure is in line with the materials balance principle is proposed and further decomposed into technical environmental efficiency and material and nonmaterial allocative environmental efficiencies . The new efficiency measure collapses into Coelli et al . s efficiency measure if none of the considered Decision Making Units control pollutants . A numerical example using Data Envelopment Analysis is provided to further explore the properties of the new efficiency measure . \\n'],\n",
              " [' We introduce and solve the Vehicle Routing Problem with Simultaneous Pick ups and Deliveries and Two Dimensional Loading Constraints . The 2L SPD model covers cases where customers raise delivery and pick up requests for transporting non stackable rectangular items . 2L SPD belongs to the class of composite routing packing optimization problems . However it is the first such problem to consider bi directional material flows dictated in practice by reverse logistics policies . The aspect of simultaneously satisfying deliveries and pick ups has a major impact on the underlying loading constraints feasible loading patterns must be identified for every arc traveled in the routing plan . This implies that 2L SPD generalizes previous routing problem variants with two dimensional loading constraints which call for one feasible loading per route . From a managerial perspective the simultaneous service of deliveries and pick ups may bring substantial cost savings but the generalized loading constraints are very hard to tackle in reasonable computational times . To this end we propose an optimization framework which employs memorization techniques designed for the 2L SPD model to accelerate the solution methodology . To assess the performance of our routing and packing algorithmic components we have solved the Vehicle Routing Problem with Simultaneous Pick ups and Deliveries and the Vehicle Routing Problem with Two Dimensional Constraints . Computational results are also reported on newly constructed 2L SPD benchmark problems . Apart from the basic 2L SPD version we introduce the 2L SPD with LIFO constraints which prohibit item rearrangements along the routes . Computational experiments are conducted to understand the impact of the LIFO constraints on the routing plans obtained . \\n'],\n",
              " [' The characterization of a technology from an economic point of view often uses the first derivatives of either the transformation or the production function . In a parametric setting these quantities are readily available as they can be easily deduced from the first derivatives of the specified function . In the standard framework of data envelopment analysis models these quantities are not so easily obtained . The difficulty resides in the fact that marginal changes of inputs and outputs might affect the position of the frontier itself while the calculation of first derivatives for economic purposes assumes that the frontier is held constant . We develop here a procedure to recover first derivatives of transformation functions in DEA models and we show how we can evacuate the problem of the shift of the frontier . We show how the knowledge of the first derivatives of the frontier estimated by DEA can be used to deduce and compute marginal products marginal rates of substitution and returns to scale for each decision making unit in the sample . \\n'],\n",
              " [' A characteristic aspect of risks in a complex modern society is the nature and degree of the public response sometimes significantly at variance with objective assessments of risk . A large part of the risk management task involves anticipating explaining and reacting to this response . One of the main approaches we have for analysing the emergent public response the social amplification of risk framework has been the subject of little modelling . The purpose of this paper is to explore how social risk amplification can be represented and simulated . The importance of heterogeneity among risk perceivers and the role of their social networks in shaping risk perceptions makes it natural to take an agent based approach . We look in particular at how to model some central aspects of many risk events the way actors come to observe other actors more than external events in forming their risk perceptions the way in which behaviour both follows risk perception and shapes it and the way risk communications are fashioned in the light of responses to previous communications . We show how such aspects can be represented by availability cascades but also how this creates further problems of how to represent the contrasting effects of informational and reputational elements and the differentiation of private and public risk beliefs . Simulation of the resulting model shows how certain qualitative aspects of risk response time series found empirically such as endogenously produced peaks in risk concern can be explained by this model . \\n'],\n",
              " [' We consider robust one way trading with limited information on price fluctuations . Our analysis finds the best guarantee of difference from the optimal offline performance . We provide closed form solution and reveal for the first time all possible worst case scenarios . Numerical experiments show that our policy is more tolerant of information inaccuracy than Bayesian policies and can earn higher average revenue than other robust policies while keeping a lower standard deviation . \\n'],\n",
              " [' Discontinuities are common in the pricing of financial derivatives and have a tremendous impact on the accuracy of quasi Monte Carlo method . While if the discontinuities are parallel to the axes good efficiency of the QMC method can still be expected . By realigning the discontinuities to be axes parallel succeeded in recovering the high efficiency of the QMC method for a special class of functions . Motivated by this work we propose an auto realignment method to deal with more general discontinuous functions . The k means clustering algorithm a classical algorithm of machine learning is used to select the most representative normal vectors of the discontinuity surface . By applying this new method the discontinuities of the resulting function are realigned to be friendly for the QMC method . Numerical experiments demonstrate that the proposed method significantly improves the performance of the QMC method . \\n'],\n",
              " [' We propose a Hybrid Scenario Cluster Decomposition heuristic for solving a large scale multi stage stochastic mixed integer programming model corresponding to a supply chain tactical planning problem . The HSCD algorithm decomposes the original scenario tree into smaller sub trees that share a certain number of predecessor nodes . Then the MS MIP model is decomposed into smaller scenario cluster multi stage stochastic sub models coordinated by Lagrangian terms in their objective functions in order to compensate the lack of non anticipativity corresponding to common ancestor nodes of sub trees . The sub gradient algorithm is then implemented in order to guide the scenario cluster sub models into an implementable solution . Moreover a Variable Fixing Heuristic is embedded into the sub gradient algorithm in order to accelerate its convergence rate . Along with the possibility of parallelization the HSCD algorithm provides the possibility of embedding various heuristics for solving scenario cluster sub models . The algorithm is specialized to lumber supply chain tactical planning under demand and supply uncertainty . An ad hoc heuristic based on Lagrangian Relaxation is proposed to solve each scenario cluster sub model . Our experimental results on a set of realistic scale test cases reveal the efficiency of HSCD in terms of solution quality and computation time . set of harvesting blocks set of manufacturing mills set of raw materials set of available raw material scenarios set of time periods corresponding to node n in the scenario tree the demand scenario tree set of immediate predecessor of each node n in scenario tree the total harvesting capacity in period t the total transportation capacity in period t unit cost to harvest block bl during period t unit cost to transport raw material rm from block bl to mill m during period t unit cost to store raw material rm in block bl during period t stumpage fee for raw material rm in block bl during period t inventory holding cost of raw material rm at mill m inventory capacity at mill m during period t supply capacity of block bl in period t maximum number of periods over which harvesting can occur in block bl lead time of procuring raw material rm from block bl unit purchase cost of raw material rm from block m in period t maximum number of blocks in which harvesting can occur during period t probability of node n in scenario tree probability of supply scenario sc minimum contract purchase quantity from block bl safety stock of raw material rm at mill m volume of available raw material rm in block bl for supply scenario sc forecasted demand of raw material rm for mill m in period t at node n of the scenario tree binary variable that takes 1 if harvesting occurs in block bl during time period t at node n of the scenario tree and 0 otherwise inventory of raw material rm in block bl at the end of period t for supply scenario sc at. \\n'],\n",
              " [' Demand based pricing is often used to moderate demand fluctuations so as to level resource utilization and increase profitability . However such pricing policies may not be effective when customers purchase decisions are influenced by social interactions . This paper investigates the demand dynamics under a demand based pricing policy of a frequently purchased service when social interactions are at work . Customers are heterogeneous and adaptively forward looking . Existing customers re purchase decisions are based on adaptively formed price expectations and reservation prices . Potential customers are attracted through social interactions with existing customers . The demand process is characterized by a two dimensional dynamical system . It is shown that the equilibrium demand can be unstable . For a given reservation price distribution we first analyze the stability of the equilibrium demand under various scenarios of social interactions and customers adaptively forward looking behavior and then characterize their dynamics using the bifurcation plots Lyapunov exponents and return maps . The results indicate that the demand process can be stable periodic or chaotic . The study shows that the intended effect of a demand based pricing policy may be offset by customers adaptively forward looking behavior under the influence of social interactions . In fact the interplay of these factors may even lead to chaotic demand dynamics . The result highlights the complex dynamics produced by a simple demand price mechanism under social interactions . For a demand based pricing strategy to be effective companies must take social interactions into account . \\n'],\n",
              " [' We consider storage loading problems where items with uncertain weights have to be loaded into a storage area taking into account stacking and payload constraints . Following the robust optimization paradigm we propose strict and adjustable optimization models for finite and interval based uncertainties . To solve these problems exact decomposition and heuristic solution algorithms are developed . For strict robustness we also propose a compact formulation based on a characterization of worst case scenarios . Computational results for randomly generated data with up to 300 items are presented showing that the robustness concepts have different potential depending on the type of data being used . \\n'],\n",
              " [' We consider a Cournot duopoly under general demand and cost functions where an incumbent patentee has a cost reducing technology that it can license to its rival by using combinations of royalties and upfront fees . We show that for drastic technologies licensing occurs and both firms stay active if the cost function is superadditive and licensing does not occur and the patentee monopolizes the market if the cost function is additive or subadditive . For non drastic technologies licensing takes place provided the average efficiency gain from the cost reducing technology is higher than the marginal gain computed at the licensee s reservation output . Optimal licensing policies have both royalties and fees for significantly superior technologies if the cost function is superadditive . By contrast for additive and certain subadditive cost functions optimal licensing policies have only royalties and no fees . \\n'],\n",
              " [' This study addresses the optimal pipe sizing problem of a tree shaped gas distribution network with a single supply source . An algorithm was developed with the aim of minimizing the investment for constructing a gas distribution network with a tree shaped layout in which demands are fixed . The construction cost is known to depend on the pipe diameters used for each arc in the network . However under the assumption that pipe diameters are continuous we prove that it is possible to obtain the minimum construction cost directly and analytically by an iterating procedure that converts the original tree into a single equivalent arc . In addition we also show that expanding the converted single arc inversely to the original tree computes the optimal continuous pipe diameter for each arc . Following this we present an additional heuristic to convert optimal continuous pipe diameters into approximate discrete pipe diameters . The algorithms were evaluated by applying them to sample networks . The numerical results obtained by comparing the approximate discrete diameters with the optimal discrete diameters confirm the efficiency of our algorithms thereby demonstrating their suitability for designing real gas distribution networks . \\n'],\n",
              " [' Given the evolution in the agricultural sector and the new challenges it faces managing agricultural supply chains efficiently has become an attractive topic for researchers and practitioners . Against this background the integration of uncertain aspects has continuously gained importance for managerial decision making since it can lead to an increase in efficiency responsiveness business integration and ultimately in market competitiveness . In order to capture appropriately the uncertain conjuncture of most agricultural real life applications an increasing amount of research effort is especially dedicated to treating uncertainty . In particular quantitative modeling approaches have found extensive use in agricultural supply chain management . This paper provides an overview of the latest advances and developments in the application of operations research methodologies to handling uncertainty occurring in the agricultural supply chain management problems . It seeks to offer a representative overview of the predominant research topics highlight the most pertinent and widely used frameworks and discuss the emergence of new operations research advances in the agricultural sector . The broad spectrum of reviewed contributions is classified and presented with respect to three most relevant discerned features uncertainty modeling types programming approaches and functional application areas . Ultimately main review findings are pointed out and future research directions which emerge are suggested . \\n'],\n",
              " [' We study the regulation of one way station based vehicle sharing systems through parking reservation policies . We measure the performance of these systems in terms of the total excess travel time of all users caused as a result of vehicle or parking space shortages . We devise mathematical programming based bounds on the total excess travel time of vehicle sharing systems under any passive regulation and in particular under any parking space reservation policy . These bounds are compared to the performance of several partial parking reservation policies a parking space overbooking policy and to the complete parking reservation and no reservation policies introduced in a previous paper . A detailed user behavior model for each policy is presented and a discrete event simulation is used to evaluate the performance of the system under various settings . The analysis of two case studies of real world systems shows the following a significant improvement of what can theoretically be achieved is obtained via the CPR policy the performances of the proposed partial reservation policies monotonically improve as more reservations are required and parking space overbooking is not likely to be beneficial . In conclusion our results reinforce the effectiveness of the CPR policy and suggest that parking space reservations should be used in practice even if only a small share of users are required to place reservations . \\n'],\n",
              " [' In this paper we present an approach to generate cyclic production schemes for multiple products with stochastic demand to be produced on a single production unit . This scheme specifies a fixed and periodic production sequence called a cycle where each product may occur more than once in the sequence . In order to stabilize the cycle length alternative control strategies are introduced . These strategies keep the cycle length close to a target length by means of adding idle time to the cycle cutting production short or overproduction . On basis of the selected control strategy as well as a base stock policy target inventory levels are determined for each production run in the cycle . Demand can be backlogged but a certain service level is required . Setup times are sequence dependent and the storage is capacitated . Employing the developed strategies we investigate the tradeoff between stability of cycle length and total cost induced by the production schedule in a computational study based on both real world data and random test instances . \\n'],\n",
              " [' This paper addresses an integrated framework for deciding about the supplier selection in the processed food industry under uncertainty . The relevance of including tactical production and distribution planning in this procurement decision is assessed . The contribution of this paper is three fold . Firstly we propose a new two stage stochastic mixed integer programming model for the supplier selection in the process food industry that maximizes profit and minimizes risk of low customer service . Secondly we reiterate the importance of considering main complexities of food supply chain management such as perishability of both raw materials and final products uncertainty at both downstream and upstream parameters and age dependent demand . Thirdly we develop a solution method based on a multi cut Benders decomposition and generalized disjunctive programming . Results indicate that sourcing and branding actions vary significantly between using an integrated and a decoupled approach . The proposed multi cut Benders decomposition algorithm improved the solutions of the larger instances of this problem when compared with a classical Benders decomposition algorithm and with the solution of the monolithic model . \\n'],\n",
              " [' In this paper we consider the Asymmetric Quadratic Traveling Salesman Problem . Given a directed graph and a function that maps every pair of consecutive arcs to a cost the problem consists in finding a cycle that visits every vertex exactly once and such that the sum of the costs is minimal . We propose an extended Linear Programming formulation that has a variable for each cycle in the graph . Since the number of cycles is exponential in the graph size we propose a column generation approach . Moreover we apply a particular reformulation linearization technique on a compact representation of the problem and compute lower bounds based on Lagrangian relaxation . We compare our new bounds with those obtained by some linearization models proposed in the literature . Computational results on some set of benchmarks used in the literature show that our lower bounding procedures are very promising . \\n'],\n",
              " [' We propose a novel meta approach to support collaborative multi objective supplier selection and order allocation decisions by integrating multi criteria decision analysis and linear programming . The proposed model accounts for suppliers performance synergy effects within a hierarchical decision structure . It incorporates both heterogeneous objective data and subjective judgments of the decision makers representing various groups with different voting powers . We maximize the total value of purchasing by optimizing order quantity assignment to suppliers and taking into consideration their synergies encountered in different time horizons . We apply the proposed model to a contractor selection and order quantity assignment problem in an agricultural commodity trading company . We maximize the strategic effectiveness of both the customers and the suppliers minimize risks increase the degree of cooperation between trading partners on all levels of supply chain integration enhance transparent knowledge sharing and aggregation and support collaborative decision making . \\n'],\n",
              " [' The linguistic term set is an applicable and flexible technique in qualitative decision making . To further develop the linguistic term set this paper proposes a generalized asymmetric linguistic term set based on the asymmetric sigmoid semantics which belongs to an asymmetric and non uniform linguistic term set and can be used to address the QDM problems involving risk appetites of the decision maker . Then a value at risk fitting approach is designed for obtaining the risk appetite parameters of the GALTS and six desirable properties of the GALTS are analyzed i.e . asymmetry non uniformity generality variability range consistency and diminishing utility . Based on the above approaches and the generalized asymmetric linguistic preference relations a QDM process involving risk appetites of the DM is designed . Because the GALPRs consist of subjective information provided by the DM the process is not perfectly consistent and is usually difficult to change or repeat . Thus a transitivity improvement approach is investigated and the corresponding calculation steps are provided . Finally an example dealing with the problem of investment decision making is provided and the results fully demonstrate the validity of the proposed methods for QDM involving risk appetites . \\n'],\n",
              " [' Today s power systems are experiencing a transition from primarily fossil fuel based generation toward greater shares of renewable energy sources . It becomes increasingly costly to manage the resulting uncertainty and variability in power system operations solely through flexible generation assets . Incorporating demand side flexibility through appropriately designed incentive structures can add an additional lever to balance demand and supply . Based on a supply model using empirical wind generation data and a discrete model of flexible demand with temporal constraints we design and evaluate a local online market mechanism for matching flexible load and uncertain supply . Under this mechanism truthful reporting of flexibility is a dominant strategy for consumers reducing payments and increasing the likelihood of allocation . Suppliers during periods of scarce supply benefit from elevated critical value payments as a result of flexibility induced competition on the demand side . We find that for a wide range of the key parameters the cost of ensuring incentive compatibility in a smart grid market relative to the welfare optimal matching is relatively small . This suggests that local matching of demand and supply can be organized in a decentral manner in the presence of a sufficiently flexible demand side . Extending the stylized demand model to include complementary demand structures we demonstrate that decentral matching induces only minor efficiency losses if demand is sufficiently flexible . Furthermore by accounting for physical grid limitations we show that flexibility and grid capacity exhibit complementary characteristics . \\n'],\n",
              " [' This manuscript reviews recent advances in deterministic global optimization for Mixed Integer Nonlinear Programming as well as Constrained Derivative Free Optimization . This work provides a comprehensive and detailed literature review in terms of significant theoretical contributions algorithmic developments software implementations and applications for both MINLP and CDFO . Both research areas have experienced rapid growth with a common aim to solve a wide range of real world problems . We show their individual prerequisites formulations and applicability but also point out possible points of interaction in problems which contain hybrid characteristics . Finally an inclusive and complete test suite is provided for both MINLP and CDFO algorithms which is useful for future benchmarking . \\n'],\n",
              " [' Provocative messages targeting childhood obesity are a central means to increase problem awareness . But what happens when different online media platforms take up the campaign comment re contextualize and evaluate it Relating to preliminary findings of persuasion research we postulate that source credibility perceptions vary across types of online media platforms and contextualization of the message . Individual characteristics in particular weight related factors are assumed to influence message effects . A 3 2 experimental design with students aged between 13 and 18 years was conducted . Results show an interaction between media type and argumentation for affective self perceptions of weight . Self relevance varies based on different source credibility perceptions . Overall campaign re contextualization of provocative messages may result in negative persuasion effects and needs to be considered in campaign development . Childhood anti obesity campaigns are often designed in a highly provocative way in order to increase attention . Social media platforms are a prominent online environment to spread such health related public service announcements raise awareness and motivate discussion . However when applying this strategy public healthcare runs the risk of losing control over the effects of its campaigns . Follow up communication in different media platforms such as blogs online news sites or social networking sites takes up the campaign comments re contextualizes and evaluates it . On the one hand this strengthens the attention focused on the campaign on the other hand the question emerges how different context formats affect campaign perceptions . Even if the core messages of the campaigns are still equal variations in the argumentative contextualization and the media platform change the representation of the message or in other words the frame of the message . In terms of how campaigns are embedded two aspects stand out . First the contextualization of the content could either reinforce the argumentation of the campaign or impair its original message . The arguments of the original message are either supported or rejected which resembles value framing in political communication . Second follow up communication regarding the campaigns could be placed on different online media platforms reaching from more traditional journalistic contexts such as online newspapers to social media environments such as Facebook or blogs . Both re contextualization factors address the credibility and trustworthiness of the context the message and of the communicator . However research has failed to analyze the comparative effects of social campaigns in these different online media environments . A highly relevant example in regard to such concerns is the perception of obesity related health messages especially when these campaigns address children and adolescents since such campaigns are increasingly at risk of offending and stigmatizing children affected by obesity . However there is little research on the effects of campaigns against childhood obesity and the fact that they convey negative or even shocking messages in online media . Considering the lack of research on this subject our study aims to close the gap and investigate the effects of different. \\n'],\n",
              " [' Language is being increasingly harnessed to not only create natural human machine interfaces but also to infer social behaviors and interactions . In the same vein we investigate a novel spoken language task of inferring social relationships in two party conversations whether the two parties are related as family strangers or are involved in business transactions . For our study we created a corpus of all incoming and outgoing calls from a few homes over the span of a year . On this unique naturalistic corpus of everyday telephone conversations which is unlike Switchboard or any other public domain corpora we demonstrate that standard natural language processing techniques can achieve accuracies of about 88 82 74 and 80 in differentiating business from personal calls family from non family calls familiar from unfamiliar calls and family from other personal calls respectively . Through a series of experiments with our classifiers we characterize the properties of telephone conversations and find that 30 words of openings are sufficient to predict business from personal calls which could potentially be exploited in designing context sensitive interfaces in smart phones our corpus based analysis does not support Schegloff and Sack s manual analysis of exemplars in which they conclude that pre closings differ significantly between business and personal calls closing fared no better than a random segment and the distribution of different types of calls are stable over durations as short as 1 2 months . In summary our results show that social relationships can be inferred automatically in two party conversations with sufficient accuracy to support practical applications . \\n'],\n",
              " [' In recent years the use of rhythm based features in speech processing systems has received growing interest . This approach uses a wide array of rhythm metrics that have been developed to capture speech timing differences between and within languages . However the reliability of rhythm metrics is being increasingly called into question . In this paper we propose two modifications to this approach . First we describe a model that is based on auditory cues that simulate the external middle and inner parts of the ear . We evaluate this model by performing experiments to discriminate between native and non native Arabic speech . Data are from the West Point Arabic Speech Corpus testing is done on standard classifiers based on Gaussian Mixture Models Support Vector Machines and a hybrid GMM SVM . Results show that the auditory based model consistently outperforms a traditional rhythm metric approach that includes both duration and intensity based metrics . Second we propose a framework that combines the rhythm metrics and the auditory based cues in the context of a Logistic Regression method that can optimize feature combination . Further results show that the proposed LR based method improves performance over the standard classifiers in the discrimination between the native and non native Arabic speech . \\n'],\n",
              " [' Sentiment analysis is the natural language processing task dealing with sentiment detection and classification from texts . In recent years due to the growth in the quantity and fast spreading of user generated contents online and the impact such information has on events people and companies worldwide this task has been approached in an important body of research in the field . Despite different methods having been proposed for distinct types of text the research community has concentrated less on developing methods for languages other than English . In the above mentioned context the present work studies the possibility to employ machine translation systems and supervised methods to build models able to detect and classify sentiment in languages for which less no resources are available for this task when compared to English stressing upon the impact of translation quality on the sentiment classification performance . Our extensive evaluation scenarios show that machine translation systems are approaching a good level of maturity and that they can in combination to appropriate machine learning algorithms and carefully chosen features be used to build sentiment analysis systems that can obtain comparable performances to the one obtained for English . \\n'],\n",
              " [' Phonological studies suggest that the typical subword units such as phones or phonemes used in automatic speech recognition systems can be decomposed into a set of features based on the articulators used to produce the sound . Most of the current approaches to integrate articulatory feature representations into an automatic speech recognition system are based on a deterministic knowledge based phoneme to AF relationship . In this paper we propose a novel two stage approach in the framework of probabilistic lexical modeling to integrate AF representations into an ASR system . In the first stage the relationship between acoustic feature observations and various AFs is modeled . In the second stage a probabilistic relationship between subword units and AFs is learned using transcribed speech data . Our studies on a continuous speech recognition task show that the proposed approach effectively integrates AFs into an ASR system . Furthermore the studies show that either phonemes or graphemes can be used as subword units . Analysis of the probabilistic relationship captured by the parameters has shown that the approach is capable of adapting the knowledge based phoneme to AF representations using speech data and allows different AFs to evolve asynchronously . \\n'],\n",
              " [' In this work we present a comprehensive study on the use of deep neural networks for automatic language identification . Motivated by the recent success of using DNNs in acoustic modeling for speech recognition we adapt DNNs to the problem of identifying the language in a given utterance from its short term acoustic features . We propose two different DNN based approaches . In the first one the DNN acts as an end to end LID classifier receiving as input the speech features and providing as output the estimated probabilities of the target languages . In the second approach the DNN is used to extract bottleneck features that are then used as inputs for a state of the art i vector system . Experiments are conducted in two different scenarios the complete NIST Language Recognition Evaluation dataset 2009 and a subset of the Voice of America data from LRE 09 in which all languages have the same amount of training data . Results for both datasets demonstrate that the DNN based systems significantly outperform a state of art i vector system when dealing with short duration utterances . Furthermore the combination of the DNN based and the classical i vector system leads to additional performance improvements . \\n'],\n",
              " [' In this paper we present a survey on the application of recurrent neural networks to the task of statistical language modeling . Although it has been shown that these models obtain good performance on this task often superior to other state of the art techniques they suffer from some important drawbacks including a very long training time and limitations on the number of context words that can be taken into account in practice . Recent extensions to recurrent neural network models have been developed in an attempt to address these drawbacks . This paper gives an overview of the most important extensions . Each technique is described and its performance on statistical language modeling as described in the existing literature is discussed . Our structured overview makes it possible to detect the most promising techniques in the field of recurrent neural networks applied to language modeling but it also highlights the techniques for which further research is required . \\n'],\n",
              " [' In this paper we suggest a list of high level features and study their applicability in detection of cyberpedophiles . We used a corpus of chats downloaded from http www.perverted justice.com and two negative datasets of different nature cybersex logs available online and the NPS chat corpus . The classification results show that the NPS data and the pedophiles conversations can be accurately discriminated from each other with character n grams while in the more complicated case of cybersex logs there is need for high level features to reach good accuracy levels . In this latter setting our results show that features that model behaviour and emotion significantly outperform the low level ones and achieve a 97 accuracy . \\n'],\n",
              " [' Discriminative confidence based on multi layer perceptrons and multiple features has shown significant advantage compared to the widely used lattice based confidence in spoken term detection . Although the MLP based framework can handle any features derived from a multitude of sources choosing all possible features may lead to over complex models and hence less generality . In this paper we design an extensive set of features and analyze their contribution to STD individually and as a group . The main goal is to choose a small set of features that are sufficiently informative while keeping the model simple and generalizable . We employ two established models to conduct the analysis one is linear regression which targets for the most relevant features and the other is logistic linear regression which targets for the most discriminative features . We find the most informative features are comprised of those derived from diverse sources and the two models deliver highly consistent feature ranks . STD experiments on both English and Spanish data demonstrate significant performance gains with the proposed feature sets . \\n'],\n",
              " [' Globalization has dramatically increased the need of translating information from one language to another . Frequently such translation needs should be satisfied under very tight time constraints . Machine translation techniques can constitute a solution to this overly complex problem . However the documents to be translated in real scenarios are often limited to a specific domain such as a particular type of medical or legal text . This situation seriously hinders the applicability of MT since it is usually expensive to build a reliable translation system no matter what technology is used due to the linguistic resources that are required to build them such as dictionaries translation memories or parallel texts . In order to solve this problem we propose the application of automatic post editing in an online learning framework . Our proposed technique allows the human expert to translate in a specific domain by using a base translation system designed to work in a general domain whose output is corrected by means of an automatic post editing module . This automatic post editing module learns to make its corrections from user feedback in real time by means of online learning techniques . We have validated our system using different translation technologies to implement the base translation system as well as several texts involving different domains and languages . In most cases our results show significant improvements in terms of BLEU with respect to the baseline systems . The proposed technique works effectively when the n grams of the document to be translated presents a certain rate of repetition situation which is common according to the document internal repetition property . \\n'],\n",
              " [' Natural languages are known for their expressive richness . Many sentences can be used to represent the same underlying meaning . Only modelling the observed surface word sequence can result in poor context coverage and generalization for example when using n gram language models . This paper proposes a novel form of language model the paraphrastic LM that addresses these issues . A phrase level paraphrase model statistically learned from standard text data with no semantic annotation is used to generate multiple paraphrase variants . LM probabilities are then estimated by maximizing their marginal probability . Multi level language models estimated at both the word level and the phrase level are combined . An efficient weighted finite state transducer based paraphrase generation approach is also presented . Significant error rate reductions of 0.5 0.6 absolute were obtained over the baseline n gram LMs on two state of the art recognition tasks for English conversational telephone speech and Mandarin Chinese broadcast speech using a paraphrastic multi level LM modelling both word and phrase sequences . When it is further combined with word and phrase level feed forward neural network LMs a significant error rate reduction of 0.9 absolute and 0.5 absolute were obtained over the baseline n gram and neural network LMs respectively . \\n'],\n",
              " [' SAMAR is a system for subjectivity and sentiment analysis for Arabic social media genres . Arabic is a morphologically rich language which presents significant complexities for standard approaches to building SSA systems designed for the English language . Apart from the difficulties presented by the social media genres processing the Arabic language inherently has a high number of variable word forms leading to data sparsity . In this context we address the following 4 pertinent issues how to best represent lexical information whether standard features used for English are useful for Arabic how to handle Arabic dialects and whether genre specific features have a measurable impact on performance . Our results show that using either lemma or lexeme information is helpful as well as using the two part of speech tagsets . However the results show that we need individualized solutions for each genre and task but that lemmatization and the ERTS POS tagset are present in a majority of the settings . \\n'],\n",
              " [' Non verbal communication involves encoding transmission and decoding of non lexical cues and is realized using vocal or visual channels during conversation . These cues perform the function of maintaining conversational flow expressing emotions and marking personality and interpersonal attitude . In particular non verbal cues in speech such as paralanguage and non verbal vocal events are used to nuance meaning and convey emotions mood and attitude . For instance laughters are associated with affective expressions while fillers are used to hold floor during a conversation . In this paper we present an automatic non verbal vocal events detection system focusing on the detect of laughter and fillers . We extend our system presented during Interspeech 2013 Social Signals Sub challenge for frame wise event detection and test several schemes for incorporating local context during detection . Specifically we incorporate context at two separate levels in our system the raw frame wise features and the output decisions . Furthermore our system processes the output probabilities based on a few heuristic rules in order to reduce erroneous frame based predictions . Our overall system achieves an Area Under the Receiver Operating Characteristics curve of 95.3 for detecting laughters and 90.4 for fillers on the test set drawn from the data specifications of the Interspeech 2013 Social Signals Sub challenge . We perform further analysis to understand the interrelation between the features and obtained results . Specifically we conduct a feature sensitivity analysis and correlate it with each feature s stand alone performance . The observations suggest that the trained system is more sensitive to a feature carrying higher discriminability with implications towards a better system design . \\n'],\n",
              " [' Due to the increasing aging population in modern society and to the proliferation of smart devices there is a need to enhance speech recognition among smart devices in order to make information easily accessible to the elderly as it is to the younger population . In general speech recognition systems are optimized to an average adult s voice and tend to exhibit a lower accuracy rate when recognizing an elderly person s voice due to the effects of speech articulation and speaking style . Additional costs are bound to be incurred when adding modifications to current speech recognitions systems for better speech recognition among elderly users . Thus using a preprocessing application on a smart device can not only deliver better speech recognition but also substantially reduce any added costs . Audio samples of 50 words uttered by 80 elderly and young adults were collected and comparatively analyzed . The speech patterns of the elderly have a slower speech rate with longer inter syllabic silence length and slightly lower speech intelligibility . The speech recognition rate for elderly adults could be improved by means of increasing the speech rate adding a 1.5 increase in accuracy eliminating silence periods adding another 4.2 increase in accuracy and boosting the energy of the formant frequency bands for a 6 boost in accuracy . After all the preprocessing a 12 increase in the accuracy of elderly speech recognition was achieved . Through this study we show that speech recognition of elderly voices can be improved through modifying specific aspects of differences in speech articulation and speaking style . In the future we will conduct studies on methods that can precisely measure and adjust speech rate and find additional factors that impact intelligibility . \\n'],\n",
              " [' This work presents a conceptually simple experiment consisting of a cantilever beam with a nonlinear spring at the tip . The configuration allows manipulation of the relative spacing between the modal frequencies of the underlying linear structure and this permits the deliberate introduction of internal resonance . A 3 1 resonance is studied in detail the response around the first mode shows a classic stiffening response with the addition of more complex dynamic behaviour and an isola region . Quasiperiodic responses are also observed but in this work the focus remains on periodic responses . Predictions using Normal Form analysis and continuation methods show good agreement with experimental observations . The experiment provides valuable insight into frequency responses of nonlinear modal structures and the implications of nonlinearity for vibration tests . Young s modulus vector of Fourier components of the forcing signal nth modal variable first modal response amplitude taken at drive frequency second modal response amplitude taken at third harmonic of drive frequency vector of Fourier components of the voltage signal sent to the shaker amplifier resonant component of the nth modal variable response amplitude of u axial coordinate lateral coordinate linear modal damping ratio phase Poisson s ratio mass density nth mode shape function shorthand for linear natural angular frequency of nth mode resonant response frequency of nth modal variable \\n'],\n",
              " [' For acquiring new skills or knowledge contemporary learners frequently rely on the help of educational technologies supplementing human teachers as a learning aid . In the interaction with such systems speech based communication between the human user and the technical system has increasingly gained importance . Since spoken computer output can take on a variety of forms depending on the method of speech generation and the employment of prosodic modulations the effects of such auditory variations on the user s learning achievement require systematic investigation . The experiment reported here examined the specific effects of speech generation method and prosody of spoken system feedback in a computer supported learning environment and may serve as validational tool for future investigations of spoken computer feedback effects on learning . Learning performance in a basic cognitive task was compared between users receiving pre recorded naturally spoken system feedback with neutral prosody pre recorded feedback with motivating prosody or computer synthesized feedback . The observed results provide empirical evidence that users of technical tutoring systems benefit from pre recorded naturally spoken feedback and do even more so from feedback with motivational prosodic modulations matching their performance success . Theoretical implications and considerations for future implementations of spoken feedback in computer based educational systems are discussed . \\n'],\n",
              " [' There has been much debate surrounding the potential benefits and costs of online interaction . The present research argues that engagement with online discussion forums can have underappreciated benefits for users well being and engagement in offline civic action and that identification with other online forum users plays a key role in this regard . Users of a variety of online discussion forums participated in this study . We hypothesized and found that participants who felt their expectations had been exceeded by the forum reported higher levels of forum identification . Identification in turn predicted their satisfaction with life and involvement in offline civic activities . Formal analyses confirmed that identification served as a mediator for both of these outcomes . Importantly whether the forum concerned a stigmatized topic moderated certain of these relationships . Findings are discussed in the context of theoretical and applied implications . \\n'],\n",
              " [' On Facebook users are exposed to posts from both strong and weak ties . Even though several studies have examined the emotional consequences of using Facebook less attention has been paid to the role of tie strength . This paper aims to explore the emotional outcomes of reading a post on Facebook and examine the role of tie strength in predicting happiness and envy . Two studies one correlational based on a sample of 207 American participants and the other experimental based on a sample of 194 German participants were conducted in 2014 . In Study 2 envy was further distinguished into benign and malicious envy . Based on a multi method approach the results showed that positive emotions are more prevalent than negative emotions while browsing Facebook . Moreover tie strength is positively associated with the feeling of happiness and benign envy whereas malicious envy is independent of tie strength after reading a post on Facebook . New communication technologies such as social media have made social news more pervasive . Facebook continuously keeps users updated with a variety of posts and passive consumption of news updates is the main Facebook activity that people engage in . The majority of these updates are positive . There is evidence for emotional contagion showing happiness can spread through the news updates on online social networks . However recent studies also indicate that exposure to positive posts on Facebook may induce envy and lead to depression and reduced well being over time . Given that Facebook has over 1.35 billion active users and there are on average 1500 potential stories for users to check per visit we are eager to understand how Facebook affects users emotions and identify relevant factors that will determine emotional reactions . We argue that tie strength between the user and the poster is one important factor that should affect emotional outcomes . The use of social media such as Facebook can cause both positive and negative feelings and the results of prior studies on the psychological effects of social media usage are quite mixed . From a long term perspective the use of social media offers benefits such as the possibility of developing and maintaining social capital and social connectedness Nevertheless it may also lead to negative outcomes such as social overload an over optimistic perception towards others lives and a decrease in life satisfaction . From a short term perspective the use of Facebook can evoke a feeling of flow which is characterized by high positive valence and high arousal and joyful and fun are the most common positive feelings reported by users while using Facebook . Nonetheless the consumption of social news on Facebook can also trigger invidious emotions such as jealousy and envy . Faced with mixed results from prior research on the psychological effects of Facebook usage it is important to differentiate between interactive and non interactive social media behavior . Previous research has shown a consistent relation between using FB for interpersonal interaction. \\n'],\n",
              " [' This paper regards social question and answer collections such as Yahoo Answers as knowledge repositories and investigates techniques to mine knowledge from them to improve sentence based complex question answering systems . Specifically we present a question type specific method that extracts question type dependent cue expressions from social Q A pairs in which the question types are the same as the submitted questions . We compare our approach with the question specific and monolingual translation based methods presented in previous works . The question specific method extracts question dependent answer words from social Q A pairs in which the questions resemble the submitted question . The monolingual translation based method learns word to word translation probabilities from all of the social Q A pairs without considering the question or its type . Experiments on the extension of the NTCIR 2008 Chinese test data set demonstrate that our models that exploit social Q A collections are significantly more effective than baseline methods such as LexRank . The performance ranking of these methods is QTSM QSM MTM . The largest F3 improvements in our proposed QTSM over QSM and MTM reach 6.0 and 5.8 respectively . \\n'],\n",
              " [' The increasing convergence of the gambling and gaming industries has raised questions about the extent to which social casino game play may influence gambling . This study aimed to examine the relationship between social casino gaming and gambling through an online survey of 521 adults who played social casino games in the previous 12 months . Most social casino game users reported that these games had no impact on how much they gambled . However 9.6 reported that their gambling overall had increased and 19.4 reported that they had gambled for money as a direct result of these games . Gambling as a direct result of social casino games was more common among males younger users those with higher levels of problem gambling severity and more involved social casino game users in terms of game play frequency and in game payments . The most commonly reported reason for gambling as a result of playing social casino games was to win real money . As social casino games increased gambling for some users this suggests that simulated gambling may influence actual gambling expenditure particularly amongst those already vulnerable to or affected by gambling problems . \\n'],\n",
              " [' Pathological speech usually refers to the condition of speech distortion resulting from atypicalities in voice and or in the articulatory mechanisms owing to disease illness or other physical or biological insult to the production system . Although automatic evaluation of speech intelligibility and quality could come in handy in these scenarios to assist experts in diagnosis and treatment design the many sources and types of variability often make it a very challenging computational processing problem . In this work we propose novel sentence level features to capture abnormal variation in the prosodic voice quality and pronunciation aspects in pathological speech . In addition we propose a post classification posterior smoothing scheme which refines the posterior of a test sample based on the posteriors of other test samples . Finally we perform feature level fusions and subsystem decision fusion for arriving at a final intelligibility decision . The performances are tested on two pathological speech datasets the NKI CCRT Speech Corpus and the TORGO database by evaluating classification accuracy without overlapping subjects data among training and test partitions . Results show that the feature sets of each of the voice quality subsystem prosodic subsystem and pronunciation subsystem offer significant discriminating power for binary intelligibility classification . We observe that the proposed posterior smoothing in the acoustic space can further reduce classification errors . The smoothed posterior score fusion of subsystems shows the best classification performance . \\n'],\n",
              " [' This paper describes a noisy channel approach for the normalization of informal text such as that found in emails chat rooms and SMS messages . In particular we introduce two character level methods for the abbreviation modeling aspect of the noisy channel model a statistical classifier using language based features to decide whether a character is likely to be removed from a word and a character level machine translation model . A two phase approach is used in the first stage the possible candidates are generated using the selected abbreviation model and in the second stage we choose the best candidate by decoding using a language model . Overall we find that this approach works well and is on par with current research in the field . \\n'],\n",
              " [' Traditional concept retrieval is based on usual word definition dictionaries with simple performance they just map words to their definitions . This approach is mostly helpful for readers and language students but writers sometimes need to find a word that encompasses a set of ideas that they have in mind . For this task inverse dictionaries are ready to help however in some cases a sought word does not correspond to a single definition but to a composite meaning of several concepts . A language producer then tends to require a concept search that starts with a group of words or a series of related terms looking for a target word . This paper aims to assist on this task by presenting a new approach for concept blending through the development of a search by concept method based on vector space representation using semantic analysis and statistical natural language processing techniques . Words are represented as numeric vectors based on different semantic similarity measures and probabilistic measures the semantic properties of a word are captured in the vector elements determined by a given linguistic context . Three different sources are used as context for word vector construction WordNet a distributional thesaurus and the Latent Dirichlet Allocation algorithm each source is used for building a different semantic vector space . The concept blender input is then conformed by a set of n nouns . All input members are read and substituted by their corresponding vectors . Then a semantic space analysis including a filtering and ranking process is carried out to deploy a list of target words . A test set of 50 concepts was created in order to evaluate the system s performance . A group of 30 evaluators found our integrated concept blending model to provide better results for finding an adequate word for the provided set of concepts . \\n'],\n",
              " [' Living Labs are innovation infrastructures where software companies and research organizations collaborate with lead users to design and develop new products and services . There is not any reference model related to the processes or practices to manage a living lab . This article presents a reference model to manage effectively the synergies of software companies with the other stakeholders participating in a living lab . The article describes the approach used to create the reference model through the analysis of a multiple case study considering six living labs and discusses the lessons learned during the creation of the process reference model . \\n'],\n",
              " [' The development of connected mobile applications is a complex task due to device diversity . Therefore device independent approaches are aimed at hiding the differences among the distinct mobile devices in the market . This work proposes DIMAG a software framework to generate connected mobile applications for multiple software platforms following a declarative approach . DIMAG provides transparent data and state synchronization between the server and the client side applications . The proposed platform has been designed making use of existing standards extending them when a required functionality is not provided . \\n'],\n",
              " [' Queuing network models provide powerful notations and tools for modeling and analyzing the performance of many different kinds of systems . Although several powerful tools currently exist for solving QNMs some of these tools define their own model representations have been developed in platform specific ways and are normally difficult to extend for coping with new system properties probability distributions or system behaviors . This paper shows how Domain Specific Languages when used in conjunction with Model driven engineering techniques provide a high level and very flexible approach for the specification and analysis of QNMs . We build on top of an existing metamodel for QNMs to define a DSL and its associated tools able to provide a high level notation for the specification of different kinds of QNMs and easy to extend for dealing with other probability distributions or system properties such as system reliability . \\n'],\n",
              " [' Flexibility maintainability and evolvability are very desirable properties for modern automation control systems . In order to achieve these characteristics modularity is regarded as an important concept in several scientific domains . The reuse of modules facilitates the reproduction of functionality or extensions of existing systems in similar environments . However it is often necessary to prepare such an environment to be able to reuse the intended programmed functionality . In an IEC 61131 3 environment cross vendor reuse of modules is problematic due to dependencies in proprietary programming environments and existing configurations . In this paper we aim to enable cross vendor reuse of modules by controlling these dependencies . Our approach is based on the Normalized Systems Theory from which we derived three guidelines for the design of reusable modules in an IEC 61131 3 environment for automation control projects . These guidelines are intended to support programmers in controlling dependencies regardless of the commercial programming environment they work with . \\n'],\n",
              " [' Food consumption data are collected and used in several fields of science . The data are often combined from various sources and interchanged between different systems . There is however no harmonized and widely used data interchange format . In addition food consumption data are often combined with other data such as food composition data . In the field of food composition successful harmonization has recently been achieved by the European Food Information Resource Network which is now the basis of a standard draft by the European Committee for Standardization . We present an XML based data interchange format for food consumption based on work and experiences related to food composition . The aim is that the data interchange format will provide a basis for wider harmonization in the future . \\n'],\n",
              " [' A variety of vision ailments are indicated by anomalies in the choroid layer of the posterior visual section . Consequently choroidal thickness and volume measurements usually performed by experts based on optical coherence tomography images have assumed diagnostic significance . Now to save precious expert time it has become imperative to develop automated methods . To this end one requires choroid outer boundary detection as a crucial step where difficulty arises as the COB divides the choroidal granularity and the scleral uniformity only notionally without marked brightness variation . In this backdrop we measure the structural dissimilarity between choroid and sclera by structural similarity index and hence estimate the COB by thresholding . Subsequently smooth COB estimates mimicking manual delineation are obtained using tensor voting . On five datasets each consisting of 97 adult OCT B scans automated and manual segmentation results agree visually . We also demonstrate close statistical match between choroidal thickness distributions obtained algorithmically and manually . Further quantitative superiority of our method is established over existing results by respective factors of 27.67 and 76.04 in two quotient measures defined relative to observer repeatability . Finally automated choroidal volume estimation being attempted for the first time also yields results in close agreement with that of manual methods . \\n'],\n",
              " [' Conventional in room cone beam computed tomography lacks explicit representation of patient respiratory motion and usually has poor image quality and inaccurate CT numbers for target delineation and or adaptive treatment planning . In room four dimensional CBCT image acquisition is still time consuming and suffers the same issue of poor image quality . To overcome this limitation we developed a computational framework to digitally synthesize high quality daily 4D CBCT images using the prior knowledge of motion and appearance learned from the planning 4D CT dataset . A patient specific respiratory motion model was first constructed from the planning 4D CT images using principal component analysis of displacement vector fields across different respiratory phases . Subsequently the respiratory motion model as well as the image content of the planning CT was spatially mapped onto the daily CBCT using deformable image registration . The synthesized 4D images possess explicit patient motion while maintaining the geometric accuracy of patient s anatomy at the time of treatment . We validated our model by quantitatively comparing the synthesized 4D CBCT against the 4D CT dataset acquired in the same day from protocol patients undergoing daily in room CBCT setup and weekly 4D CT for treatment evaluation . Our preliminary results have demonstrated good agreement of contours in different motion phases between the synthesized and acquired scans . Various imaging artifacts were also suppressed and soft tissue visibility was enhanced . \\n'],\n",
              " [' Volume visualization is a very important work in medical imaging and surgery plan . However determining an ideal transfer function is still a challenging task because of the lack of measurable metrics for quality of volume visualization . In the paper we presented the voxel vibility model as a quality metric to design the desired visibility for voxels instead of designing transfer functions directly . Transfer functions are obtained by minimizing the distance between the desired visibility distribution and the actual visibility distribution . The voxel model is a mapping function from the feature attributes of voxels to the visibility of voxels . To consider between class information and with class information simultaneously the voxel visibility model is described as a Gaussian mixture model . To highlight the important features the matched result can be obtained by changing the parameters in the voxel visibility model through a simple and effective interface . Simultaneously we also proposed an algorithm for transfer functions optimization . The effectiveness of this method is demonstrated through experimental results on several volumetric data sets . \\n'],\n",
              " [' We propose a novel 3D 2D registration approach for micro computed tomography and histology constructed for dental implant biopsies that finds the position and normal vector of the oblique slice from CT that corresponds to HI . During image pre processing the implants and the bone tissue are segmented using a combination of thresholding morphological filters and component labeling . After this chamfer matching is employed to register the implant edges and fine registration of the bone tissues is achieved using simulated annealing . The method was tested on n 10 biopsies obtained at 20 weeks after non submerged healing in the canine mandible . The specimens were scanned with CT 100 and processed for hard tissue sectioning . After registration we assessed the agreement of bone to implant contact using automated and manual measurements . Statistical analysis was conducted to test the agreement of the BIC measurements in the registered samples . Registration was successful for all specimens and agreement of the respective binary images was high . Direct comparison of BIC yielded that automated and manual measures from CT were significant positively correlated with HI between CT and HI groups . The results show that this method yields promising results and that CT may become a valid alternative to assess osseointegration in three dimensions . \\n'],\n",
              " [' Prostate cancer is one of the major causes of cancer death for men . Magnetic resonance imaging is being increasingly used as an important modality to localize prostate cancer . Therefore localizing prostate cancer in MRI with automated detection methods has become an active area of research . Many methods have been proposed for this task . However most of previous methods focused on identifying cancer only in the peripheral zone or classifying suspicious cancer ROIs into benign tissue and cancer tissue . Few works have been done on developing a fully automatic method for cancer localization in the entire prostate region including central gland and transition zone . In this paper we propose a novel learning based multi source integration framework to directly localize prostate cancer regions from in vivo MRI . We employ random forests to effectively integrate features from multi source images together for cancer localization . Here multi source images include initially the multi parametric MRIs and later also the iteratively estimated and refined tissue probability map of prostate cancer . Experimental results on 26 real patient data show that our method can accurately localize cancerous sections . The higher section based evaluation combined with the ROC analysis result of individual patients shows that the proposed method is promising for in vivo MRI based prostate cancer localization which can be used for guiding prostate biopsy targeting the tumor in focal therapy planning triage and follow up of patients with active surveillance as well as the decision making in treatment selection . The common ROC analysis with the AUC value of 0.832 and also the ROI based ROC analysis with the AUC value of 0.883 both illustrate the effectiveness of our proposed method . \\n'],\n",
              " [' Diabetes increases the risk of developing any deterioration in the blood vessels that supply the retina an ailment known as Diabetic Retinopathy . Since this disease is asymptomatic it can only be diagnosed by an ophthalmologist . However the growth of the number of ophthalmologists is lower than the growth of the population with diabetes so that preventive and early diagnosis is difficult due to the lack of opportunity in terms of time and cost . Preliminary affordable and accessible ophthalmological diagnosis will give the opportunity to perform routine preventive examinations indicating the need to consult an ophthalmologist during a stage of non proliferation . During this stage there is a lesion on the retina known as microaneurysm which is one of the first clinically observable lesions that indicate the disease . In recent years different image processing algorithms which allow the detection of the DR have been developed however the issue is still open since acceptable levels of sensitivity and specificity have not yet been reached preventing its use as a pre diagnostic tool . Consequently this work proposes a new approach for MA detection based on reduction of non uniform illumination normalization of image grayscale content to improve dependence of images from different contexts application of the bottom hat transform to leave reddish regions intact while suppressing bright objects binarization of the image of interest with the result that objects corresponding to MAs blood vessels and other reddish objects are completely separated from the background application of the hit or miss Transformation on the binary image to remove blood vessels from the ROIs two features are extracted from a candidate to distinguish real MAs from FPs where one feature discriminates round shaped candidates from elongated shaped ones through application of Principal Component Analysis the second feature is a count of the number of times that the radon transform of the candidate ROI evaluated at the set of discrete angle values 0 1 2 180 is characterized by a valley between two peaks . The proposed approach is tested on the public databases DiaretDB1 and Retinopathy Online Challenge competition . The proposed MA detection method achieves sensitivity specificity and precision of 92.32 93.87 and 95.93 for the diaretDB1 database and 88.06 97.47 and 92.19 for the ROC database . Theory results challenges and performance related to the proposed MA detecting method are presented . \\n'],\n",
              " [' The automatization of the analysis of Indirect Immunofluorescence images is of paramount importance for the diagnosis of autoimmune diseases . This paper proposes a solution to one of the most challenging steps of this process the segmentation of HEp 2 cells through an adaptive marker controlled watershed approach . Our algorithm automatically conforms the marker selection pipeline to the peculiar characteristics of the input image hence it is able to cope with different fluorescent intensities and staining patterns without any a priori knowledge . Furthermore it shows a reduced sensitivity to over segmentation errors and uneven illumination that are typical issues of IIF imaging . \\n'],\n",
              " [' A computerized framework for monitoring four dimensional dose distributions during stereotactic body radiation therapy based on a portal dose image based 2D 3D registration approach has been proposed in this study . Using the PDI based registration approach simulated 4D treatment CT images were derived from the deformation of 3D planning CT images so that a 2D planning PDI could be similar to a 2D dynamic clinical PDI at a breathing phase . The planning PDI was calculated by applying a dose calculation algorithm to the geometry of the planning CT image and a virtual water equivalent phantom . The dynamic clinical PDIs were estimated from electronic portal imaging device dynamic images including breathing phase data obtained during a treatment . The parameters of the affine transformation matrix were optimized based on an objective function and a gamma pass rate using a Levenberg Marquardt algorithm . The proposed framework was applied to the EPID dynamic images of ten lung cancer patients which included 183 frames . The 4D dose distributions during the treatment time were successfully obtained by applying the dose calculation algorithm to the simulated 4D treatment CT images . The mean standard deviation of the percentage errors between the prescribed dose and the estimated dose at an isocenter for all cases was 3.25 4.43 . The maximum error for the ten cases was 14.67 and the minimum error was 0.00 . The proposed framework could be feasible for monitoring the 4D dose distribution and dose errors within a patient s body during treatment . \\n'],\n",
              " [' This paper proposes a QoS active queue management mechanism for multi QoS classes named as M GREEN which includes the consideration of QoS parameters and provides service differentiation among different flows classes . M GREEN extends the concept of Random and Early Detection in RED to Global Random and Early Estimation respectively . Furthermore M GREEN extends the linear concept of RED to an exponential one to enhance the efficiency of AQM . For performance evaluation extensive numerical cases are employed to compare M GREEN with some popular AQM schemes and to show the superior performance and characteristics of M GREEN . Consequently M GREEN is a possible way to provide the future multimedia Internet with differential services for different traffic classes of diverse QoS requirements . \\n'],\n",
              " [' Shape based 3D surface reconstructing methods for liver vessels have difficulties to tackle with limited contrast of medical images and the intrinsic complexity of multi furcation parts . In this paper we propose an effective and robust technique called Gap Border Pairing to reconstruct surface of liver vessels with complicated multi furcations . The proposed method starts from a tree like skeleton which is extracted from segmented liver vessel volumes and preprocessed as a number of simplified smooth branching lines . Secondly for each center point of any branching line an optimized elliptic cross section ring is generated by optimizedly fitting its actual cross section outline based on its tangent vector . Thirdly a tubular surface mesh is generated for each branching line by weaving all of its adjacent rings . Then for every multi furcation part a transitional regular mesh is effectively and regularly reconstructed by using GBP . An initial model is generated after reconstructing all multi furcation parts . Finally the model is refined by using just one time subdivision and its topologies can be re maintained by grouping its facets according to the skeleton providing high level editability . Our method can be automatically implemented in parallel if the segmented vessel volume and corresponding skeletons are provided . The experimental results show that GBP model is accurate enough in terms of the boundary deviations between segmented volume and the model . \\n'],\n",
              " [' Many organizations are implementing process improvement models seeking to increase their organizational maturity for software development . However implementing traditional maturity models involves a large investment which is beyond the reach of vast majority of small organizations . This paper presents the use and adaptation of some ISO models in the creation of an organizational maturity model for the Spanish software industry . This model was used satisfactorily to improve the software processes of several Spanish small firms and obtain an organizational maturity certification for software development granted by the Spanish Association for Standardization and Certification . \\n'],\n",
              " [' Colorectal cancer usually appears in polyps developed from the mucosa . Carcinoma is frequently found in those polyps larger than 10mm and therefore only this kind of polyps is sent for pathology examination . In consequence accurate estimation of a polyp size determines the surveillance interval after polypectomy . The follow up consists in a periodic colonoscopy whose frequency depends on the estimation of the size polyp . Typically this polyp measure is achieved by examining the lesion with a calibrated endoscopy tool . However measurement is very challenging because it must be performed during a procedure subjected to a complex mix of noise sources namely anatomical variability drastic illumination changes and abrupt camera movements . This work introduces a semi automatic method that estimates a polyp size by propagating an initial manual delineation in a single frame to the whole video sequence using a spatio temporal characterization of the lesion during a routine endoscopic examination . The proposed approach achieved a Dice Score of 0.7 in real endoscopy video sequences when comparing with an expert . In addition the method obtained a root mean square error of 0.87mm in videos artificially captured in a cylindric structure with spheres of known size that simulated the polyps . Finally in real endoscopy sequences the diameter estimation was compared with measures obtained by a group of four experts with similar experience obtaining a RMSE of 4.7mm for a set of polyps measuring from 5 to 20mm . An ANOVA test performed for the five groups of measurements showed no significant differences . \\n'],\n",
              " [' Mitral valve diseases are among the most common types of heart diseases while heart diseases are the most common cause of death worldwide . MV repair surgery is connected to higher survival rates and fewer complications than the total replacement of the MV but MV repair requires extensive patient specific therapy planning . The simulation of MV repair with a patient specific model could help to optimize surgery results and make MV repair available to more patients . However current patient specific simulations are difficult to transfer to clinical application because of time constraints or prohibitive requirements on the resolution of the image data . As one possible solution to the problem of patient specific MV modeling we present a mass spring MV model based on 3D transesophageal echocardiographic images already routinely acquired for MV repair therapy planning . Our novel approach to the rest length estimation of springs allows us to model the global support of the MV leaflets through the chordae tendinae without the need for high resolution image data . The model is used to simulate MV annuloplasty for five patients undergoing MV repair and the simulated results are compared to post surgical TEE images . The comparison shows that our model is able to provide a qualitative estimate of annuloplasty surgery . In addition the data suggests that the model might also be applied to simulating the implantation of artificial chordae . \\n'],\n",
              " [' The present research was directed to effective image restoration with the extraction of ischemic edema signs . Computerized support of hyperacute stroke diagnosis based on routinely used computerized tomography scans was optimized to visualize the infarct extent more precisely . In particular a beneficial support of time limited appropriate decision of whether to treat the patient by thrombolysis is expected . Because of a limited accuracy in determining the area of core infarction particularly in the early hours of symptoms onset a variational approach to sensed data recovery was applied . Proposed methodology adjusts fidelity norms and regularization priors integrated with simulated sensing procedures in a compressed sensing framework . Experimental study confirmed almost perfect recognition of ischemic stroke in a test set of over 500 CT scans . \\n'],\n",
              " [' Comparison of human brain MR images is often challenged by large inter subject structural variability . To determine correspondences between MR brain images most existing methods typically perform a local neighborhood search based on certain morphological features . They are limited in two aspects pre defined morphological features often have limited power in characterizing brain structures thus leading to inaccurate correspondence detection and correspondence matching is often restricted within local small neighborhoods and fails to cater to images with large anatomical difference . To address these limitations we propose a novel method to detect distinctive landmarks for effective correspondence matching . Specifically we first annotate a group of landmarks in a large set of training MR brain images . Then we use regression forest to simultaneously learn the optimal sets of features to best characterize each landmark and the non linear mappings from the local patch appearances of image points to their 3D displacements towards each landmark . The learned regression forests are used as landmark detectors to predict the locations of these landmarks in new images . Because each detector is learned based on features that best distinguish the landmark from other points and also landmark detection is performed in the entire image domain our method can address the limitations in conventional methods . The deformation field estimated based on the alignment of these detected landmarks can then be used as initialization for image registration . Experimental results show that our method is capable of providing good initialization even for the images with large deformation difference thus improving registration accuracy . \\n'],\n",
              " [' An interactive loop between motion recognition and motion generation is a fundamental mechanism for humans and humanoid robots . We have been developing an intelligent framework for motion recognition and generation based on symbolizing motion primitives . The motion primitives are encoded into Hidden Markov Models which we call motion symbols . However to determine the motion primitives to use as training data for the HMMs this framework requires a manual segmentation of human motions . Essentially a humanoid robot is expected to participate in daily life and must learn many motion symbols to adapt to various situations . For this use manual segmentation is cumbersome and impractical for humanoid robots . In this study we propose a novel approach to segmentation the Real time Unsupervised Segmentation method which comprises three phases . In the first phase short human movements are encoded into feature HMMs . Seamless human motion can be converted to a sequence of these feature HMMs . In the second phase the causality between the feature HMMs is extracted . The causality data make it possible to predict movement from observation . In the third phase movements having a large prediction uncertainty are designated as the boundaries of motion primitives . In this way human whole body motion can be segmented into a sequence of motion primitives . This paper also describes an application of RUS to AUtonomous Symbolization of motion primitives . Each derived motion primitive is classified into an HMM for a motion symbol and parameters of the HMMs are optimized by using the motion primitives as training data in competitive learning . The HMMs are gradually optimized in such a way that the HMMs can abstract similar motion primitives . We tested the RUS and AUS frameworks on captured human whole body motions and demonstrated the validity of the proposed framework . \\n'],\n",
              " [' In this study we compare three commonly used methods for hyperspectral image classification namely Support Vector Machines Gaussian Processes and the Spectral Angle Mapper . We assess their performance in combination with different kernels . The assessment is done in two experiments under ideal conditions in the laboratory and separately in the field using natural light . For both experiments independent training and test sets are used . Results show that GPs generally outperform the SVMs irrespective of the kernel used . Furthermore angle based methods including the Spectral Angle Mapper outperform GPs and SVMs when using distance based kernels in the field experiment . A new GP method using an angle based kernel the Observation Angle Dependent covariance function outperforms SAM and SVMs in both experiments using only a small number of training spectra . These findings show that distance based kernels are more affected by changes in illumination between the training and test set than are angular based methods kernels . Taken together this study shows that independent training data can be used for classification of hyperspectral data in the field such as in open pit mines by using Bayesian machine learning methods and non stationary kernels such as GPs and the OAD kernel . This provides a necessary component for automated classifications such as autonomous mining where many images have to be classified without user interaction . \\n'],\n",
              " [' The development of distributed testing frameworks is more complex where the implementation process must consider the mechanisms and functions required to support interaction as long as the communication and the coordination between distributed testing components . The typical reactions of such systems are the generation of errors set time outs locks observability controllability and synchronization problems . In other side the distributed testing process must not only check if the output events have been observed but also the dates when these events have been occurred . In this paper we show how to cope with these problems by using a distributed testing method including timing constraints . Afterwards a multi agent architecture is proposed in the design process to describe the behavior of testing a distributed chat group application on high level of abstraction . \\n'],\n",
              " [' The rise of Internet of Things has been improving the so called mobile Online Social Networks in sense of more ubiquitous inter communication and information sharing . Meanwhile location sharing service is known as the key cornerstone of mOSNs . Unfortunately location sharing has also caused similarly serious concerns on the potential privacy leakage . We propose BMobishare a security enhanced privacy preserving location sharing mechanism . It employs the Bloom Filter to mask sensitive data exchanges such that exchange of both sides can not obtain unauthorized privacy information . Analyses and evaluations show that BMobishare s enhanced location sharing procedure achieves significantly better performance when compared to existing approaches . \\n'],\n",
              " [' The driving force behind software development of the Electronic Medical Record has been gradually changing . Heterogeneous software requirements have emerged so how to correctly carry out development project has become a complex task . This paper adopts the knowledge engineering and management mechanism i.e . CommonKADS and software quality engineering to improve existing strategic information management plan as a design methodology to help software implementation for medical institutes . We evaluate the adopting performance by a real case that examines the maturity level of the architecture alignment between the target solution in the proposed SIM plan and the built medical system . \\n'],\n",
              " [' Due to the advancements of robotic systems they are able to be employed in more unstructured outdoor environments . In such environments the robot terrain interaction becomes a highly non linear function . Several methods were proposed to estimate the robot terrain interaction machine learning methods iterative geometric methods quasi static and fully dynamic physics simulations . However to the best of our knowledge there has been no systematic evaluation comparing those methods . In this paper we present a newly developed iterative contact point estimation method for static stability estimation of actively reconfigurable robots . This new method is systematically compared to a physics simulation in a comprehensive evaluation . Both interaction models determine the contact points between robot and terrain and facilitate a subsequent static stability prediction . Hence they can be used in our state space global planner for rough terrain to evaluate the robot s pose and stability . The analysis also compares deterministic versions of both methods to stochastic versions which account for uncertainty in the robot configuration and the terrain model . The results of this analysis show that the new iterative method is a valid and fast approximate method . It is significantly faster compared to a physics simulation while providing good results in realistic robotic scenarios . \\n'],\n",
              " [' Controller Area Network is very popular in networked embedded systems . On the other hand intranets are now ubiquitous in office home and factory environments . Namely the Internet Protocol is the glue that permits any kind of information to be exchanged between devices in heterogeneous systems . In this paper a network architecture to seamlessly integrate CAN busses in intranets is described . Flexibility and scalability were the key design requirements to conceive a comprehensive solution that suits both inexpensive and very complex applications . A prototype implementation has been tested to confirm architecture feasibility and assess the performance it can achieve . \\n'],\n",
              " [' A significant number of recommender systems utilize the k nearest neighbor algorithm as the collaborative filtering core . This algorithm is simple it utilizes updated data and facilitates the explanations of recommendations . Its greatest inconveniences are the amount of execution time that is required and the non scalable nature of the algorithm . The algorithm is based on the repetitive execution of the selected similarity metric . In this paper an innovative similarity metric is presented HwSimilarity . This metric attains high quality recommendations that are similar to those provided by the best existing metrics and can be processed by employing low cost hardware circuits . This paper examines the key design concepts and recommendation quality results of the metric . The hardware design cost of implementation and improvements achieved during execution are also explored . \\n'],\n",
              " [' The Future Internet is expected to be composed of a mesh of interoperable web services accessed from all over the Web . This approach has been supported by many software providers who have provided a wide range of mash up tools for creating composite applications based on components prepared by the respective provider . These tools aim to achieve the end user development of rich internet applications however most having failed to meet the needs of end users without programming knowledge have been unsuccessful . Thus many studies have investigated success factors in order to propose scales of success factor objectives and assess the adequacy of mashup tools for their purpose . After reviewing much of the available literature this paper proposes a new success factor scale based on human factors human computer interaction factors and the specialization functionality relationship . It brings together all these factors offering a general conception of EUD success factors . The proposed scale was applied in an empirical study on current EUD tools which found that today s EUD tools have many shortcomings . In order to achieve an acceptable success rate among end users we then designed a mashup tool architecture called FAST Wirecloud which was built taking into account the proposed EUD success factor scale . The results of a new empirical study carried out using this tool have demonstrated that users are better able to successfully develop their composite applications and that FAST Wirecloud has scored higher than all the other tools under study on all scales of measurement and particularly on the scale proposed in this paper . \\n'],\n",
              " [' Objective Machine learning techniques can be used to extract predictive models for diseases from electronic medical records . However the nature of EMRs makes it difficult to apply off the shelf machine learning techniques while still exploiting the rich content of the EMRs . In this paper we explore the usage of a range of natural language processing techniques to extract valuable predictors from uncoded consultation notes and study whether they can help to improve predictive performance . Methods We study a number of existing techniques for the extraction of predictors from the consultation notes namely a bag of words based approach and topic modeling . In addition we develop a dedicated technique to match the uncoded consultation notes with a medical ontology . We apply these techniques as an extension to an existing pipeline to extract predictors from EMRs . We evaluate them in the context of predictive modeling for colorectal cancer a disease known to be difficult to diagnose before performing an endoscopy . Results Our results show that we are able to extract useful information from the consultation notes . The predictive performance of the ontology based extraction method moves significantly beyond the benchmark of age and gender alone of 0.870 versus 0.831 . We also observe more accurate predictive models by adding features derived from processing the consultation notes compared to solely using coded data although the difference is not significant . The extracted features from the notes are shown be equally predictive compared to the coded data of the consultations . Conclusion It is possible to extract useful predictors from uncoded consultation notes that improve predictive performance . Techniques linking text to concepts in medical ontologies to derive these predictors are shown to perform best for predicting CRC in our EMR dataset . \\n'],\n",
              " [' Objective Radiotherapy treatment planning aims at delivering a sufficient radiation dose to cancerous tumour cells while sparing healthy organs in the tumour surrounding area . It is a time consuming trial and error process that requires the expertise of a group of medical experts including oncologists and medical physicists and can take from 2 to 3h to a few days . Our objective is to improve the performance of our previously built case based reasoning system for brain tumour radiotherapy treatment planning . In this system a treatment plan for a new patient is retrieved from a case base containing patient cases treated in the past and their treatment plans . However this system does not perform any adaptation which is needed to account for any difference between the new and retrieved cases . Generally the adaptation phase is considered to be intrinsically knowledge intensive and domain dependent . Therefore an adaptation often requires a large amount of domain specific knowledge which can be difficult to acquire and often is not readily available . In this study we investigate approaches to adaptation that do not require much domain knowledge referred to as knowledge light adaptation . Methodology We developed two adaptation approaches adaptation based on machine learning tools and adaptation guided retrieval . They were used to adapt the beam number and beam angles suggested in the retrieved case . Two machine learning tools neural networks and naive Bayes classifier were used in the adaptation to learn how the difference in attribute values between the retrieved and new cases affects the output of these two cases . The adaptation guided retrieval takes into consideration not only the similarity between the new and retrieved cases but also how to adapt the retrieved case . Results The research was carried out in collaboration with medical physicists at the Nottingham University Hospitals NHS Trust City Hospital Campus UK . All experiments were performed using real world brain cancer patient cases treated with three dimensional conformal radiotherapy . Neural networks based adaptation improved the success rate of the CBR system with no adaptation by 12 . However naive Bayes classifier did not improve the current retrieval results as it did not consider the interplay among attributes . The adaptation guided retrieval of the case for beam number improved the success rate of the CBR system by 29 . However it did not demonstrate good performance for the beam angle adaptation . Its success rate was 29 versus 39 when no adaptation was performed . Conclusions The obtained empirical results demonstrate that the proposed adaptation methods improve the performance of the existing CBR system in recommending the number of beams to use . However we also conclude that to be effective the proposed adaptation of beam angles requires a large number of relevant cases in the case base . \\n'],\n",
              " [' Hidden and exposed terminal problems are known to negatively impact wireless communications degrading potential computing services on top . These effects are more significant in Wireless Mesh Sensor Networks and particularly in those based on the IEEE 802.15.5 Low Rate Wireless Personal Area Network standard a promising solution for enabling low power WMSNs . The first contribution of this paper is a quantitative evaluation of these problems under the IEEE 802.15.5 Asynchronous Energy Saving mode which is intended for asynchronous data collection applications . The results obtained show a sharp deterioration of the network performance . Therefore this paper also reviews the most relevant approaches that cope with these problems and are compatible with ASES . Finally a set of these proposals is assessed to find out those more suitable for their potential integration with ASES which constitutes the second major contribution of the paper . \\n'],\n",
              " [' Enterprise Resource Planning packages are information systems that automate the business processes of organizations thereby improving their operational efficiency substantially . ERP projects that involve customization are often affected by inaccurate estimation of efforts . Size of the software forms the basis for effort estimation . Methods used for effort estimation either employ function points or lines of code to measure the size of customized ERP packages . Literature review reveals that the existing software size methods which are meant for custom built software products may not be suitable for COTS products such as customized ERP packages . Hence the effort estimation using conventional methods for customized ERP packages may not be accurate . This paper proposes a new approach to estimate the size of customized ERP packages using Package Points . The proposed approach was validated with data collected from 14 ERP projects delivered by the same company . A positive correlation was observed between Package Points and the efforts of these projects . This result indicates the feasibility of our proposed approach as well as the positive climate for its utility by the project managers of future ERP projects . Lastly we examine the implication of these results for practice and future research scope . \\n'],\n",
              " [' In this study we propose a novel solution for collecting smart meter data by merging Vehicular Ad Hoc Networks and smart grid communication technologies . In our proposed mechanism Vehicular Ad Hoc Networks are utilized for collecting data from smart meters eliminating the need for manpower . To the best of our knowledge this is the first study proposing the utilization of public transportation vehicles for collecting data from smart meters . With this work the use of the IEEE 802.11p protocol has been proposed for the first time for use in smart grid applications . In our scheme data flows first from smart meters to a bus through infrastructure to vehicle communication and then from the bus to a bus stop through vehicle to infrastructure communication . The performance of our proposed mechanism has been investigated in detail in terms of end to end delay and delivery ratio by using Network Simulator 2 and with different routing protocols . \\n'],\n",
              " [' Objective Recurrence of cancer after treatment increases the risk of death . The ability to predict the treatment outcome can help to design the treatment planning and can thus be beneficial to the patient . We aim to select predictive features from clinical and PET based features in order to provide doctors with informative factors so as to anticipate the outcome of the patient treatment . Methods In order to overcome the small sample size problem of datasets usually met in the medical domain we propose a novel wrapper feature selection algorithm named HFS which searches forward in a hierarchical feature subset space . Feature subsets are iteratively evaluated with the prediction performance using SVM . All feature subsets performing better than those at the preceding iteration are retained . Moreover as SUV based features have been recognized as significant predictive factors for a patient outcome we propose to incorporate this prior knowledge into the selection procedure to improve its robustness and reduce its computational cost . Results Two real world datasets from cancer patients are included in the evaluation . We extract dozens of clinical and PET based features to characterize the patient s state including SUV parameters and texture features . We use leave one out cross validation to evaluate the prediction performance in terms of prediction accuracy and robustness . Using SVM as the classifier our HFS method produces accuracy values of 100 and 94 on the two datasets respectively and robustness values of 89 and 96 . Without accuracy loss the prior based version improves the robustness up to 100 and 98 on the two datasets respectively . Conclusions Compared with other feature selection methods the proposed HFS and pHFS provide the most promising results . For our HFS method we have empirically shown that the addition of prior knowledge improves the robustness and accelerates the convergence . \\n'],\n",
              " [' Background Evidence based medicine practice requires practitioners to obtain the best available medical evidence and appraise the quality of the evidence when making clinical decisions . Primarily due to the plethora of electronically available data from the medical literature the manual appraisal of the quality of evidence is a time consuming process . We present a fully automatic approach for predicting the quality of medical evidence in order to aid practitioners at point of care . Methods Our approach extracts relevant information from medical article abstracts and utilises data from a specialised corpus to apply supervised machine learning for the prediction of the quality grades . Following an in depth analysis of the usefulness of features they are extracted from the text via rule based approaches and from the meta data associated with the articles and then applied in the supervised classification model . We propose the use of a highly scalable and portable approach using a sequence of high precision classifiers and introduce a simple evaluation metric called average error distance that simplifies the comparison of systems . We also perform elaborate human evaluations to compare the performance of our system against human judgments . Results We test and evaluate our approaches on a publicly available specialised annotated corpus containing 1132 evidence based recommendations . Our rule based approach performs exceptionally well at the automatic extraction of publication types of articles with F scores of up to 0.99 for high quality publication types . For evidence quality classification our approach obtains an accuracy of 63.84 and an AED of 0.271 . The human evaluations show that the performance of our system in terms of AED and accuracy is comparable to the performance of humans on the same data . Conclusions The experiments suggest that our structured text classification framework achieves evaluation results comparable to those of human performance . Our overall classification approach and evaluation technique are also highly portable and can be used for various evidence grading scales . \\n'],\n",
              " [' Objective Proteins are considered to be the most important individual components of biological systems and they combine to form physical protein complexes which are responsible for certain molecular functions . Despite the large availability of protein protein interaction information not much information is available about protein complexes . Experimental methods are limited in terms of time efficiency cost and performance constraints . Existing computational methods have provided encouraging preliminary results but they phase certain disadvantages as they require parameter tuning some of them can not handle weighted PPI data and others do not allow a protein to participate in more than one protein complex . In the present paper we propose a new fully unsupervised methodology for predicting protein complexes from weighted PPI graphs . Methods and materials The proposed methodology is called evolutionary enhanced Markov clustering and it is a hybrid combination of an adaptive evolutionary algorithm and a state of the art clustering algorithm named enhanced Markov clustering . EE MC was compared with state of the art methodologies when applied to datasets from the human and the yeast Saccharomyces cerevisiae organisms . Results Using public available datasets EE MC outperformed existing methodologies . Moreover when applied to new human datasets its performance was encouraging in the prediction of protein complexes which consist of proteins with high functional similarity . In specific 5737 protein complexes were predicted and 72.58 of them are enriched for at least one gene ontology function term . Conclusions EE MC is by design able to overcome intrinsic limitations of existing methodologies such as their inability to handle weighted PPI networks their constraint to assign every protein in exactly one cluster and the difficulties they face concerning the parameter tuning . This fact was experimentally validated and moreover new potentially true human protein complexes were suggested as candidates for further validation using experimental techniques . \\n'],\n",
              " [' Introduction The length of stay of critically ill patients in the intensive care unit is an indication of patient ICU resource usage and varies considerably . Planning of postoperative ICU admissions is important as ICUs often have no nonoccupied beds available . Problem statement Estimation of the ICU bed availability for the next coming days is entirely based on clinical judgement by intensivists and therefore too inaccurate . For this reason predictive models have much potential for improving planning for ICU patient admission . Objective Our goal is to develop and optimize models for patient survival and ICU length of stay based on monitored ICU patient data . Furthermore these models are compared on their use of sequential organ failure scores as well as underlying raw data as input features . Methodology Different machine learning techniques are trained using a 14 480 patient dataset both on SOFA scores as well as their underlying raw data values from the first five days after admission in order to predict the patient LOS and the patient mortality . Furthermore to help physicians in assessing the prediction credibility a probabilistic model is tailored to the output of our best performing model assigning a belief to each patient status prediction . A two by two grid is built using the classification outputs of the mortality and prolonged stay predictors to improve the patient LOS regression models . Results For predicting patient mortality and a prolonged stay the best performing model is a support vector machine with G A D 65.9 of 0.77 and G S L 73.2 . In terms of LOS regression the best performing model is support vector regression achieving a mean absolute error of 1.79 days and a median absolute error of 1.22 days for those patients surviving a nonprolonged stay . Conclusion Using a classification grid based on the predicted patient mortality and prolonged stay allows more accurate modeling of the patient LOS . The detailed models allow to support the decisions made by physicians in an ICU setting . \\n'],\n",
              " [' Context awareness enables the personalization of computer systems according to the users needs and their particular situation at a given time . The personalization capabilities are usually implemented by programmers due to the complex processes that are involved . However an important trend in software development is that more and more software systems are being implemented not only by programmers but also by people with expertise in other domains . Since most of the existing context aware development toolkits are designed for programmers non technical users can not develop these kinds of systems . The design of tools to create context aware systems by users that do not have programming skills but are experts in the domain where the system is going to be deployed will contribute to speed up the adoption of these kinds of services by the society . This paper presents a cloud based platform to ease the development of context aware mobile applications by people without programming skills . The platform has been designed to be used in a tourism domain . This way tourism experts can send tourist information to mobile users according to their context data . An energy efficient mobile app has been developed in order to obtain context data from the user s device and to receive personalized information in real time based on these data . The architecture and implementation details of the system are presented and the evaluation of the platform by tourism domain experts is discussed . \\n'],\n",
              " [' Objectives To develop a rigorous and repeatable method for building effective Bayesian network models for medical decision support from complex unstructured and incomplete patient questionnaires and interviews that inevitably contain examples of repetitive redundant and contradictory responses To exploit expert knowledge in the BN development since further data acquisition is usually not possible To ensure the BN model can be used for interventional analysis To demonstrate why using data alone to learn the model structure and parameters is often unsatisfactory even when extensive data is available . Method The method is based on applying a range of recent BN developments targeted at helping experts build BNs given limited data . While most of the components of the method are based on established work its novelty is that it provides a rigorous consolidated and generalised framework that addresses the whole life cycle of BN model development . The method is based on two original and recent validated BN models in forensic psychiatry known as DSVM MSS and DSVM P. Results When employed with the same datasets the DSVM MSS demonstrated competitive to superior predictive performance against the state of the art and the DSVM P demonstrated superior predictive performance against the state of the art . More importantly the resulting models go beyond improving predictive accuracy and into usefulness for risk management purposes through intervention and enhanced decision support in terms of answering complex clinical questions that are based on unobserved evidence . Conclusions This development process is applicable to any application domain which involves large scale decision analysis based on such complex information rather than based on data with hard facts and in conjunction with the incorporation of expert knowledge for decision support via intervention . The novelty extends to challenging the decision scientists to reason about building models based on what information is really required for inference rather than based on what data is available and hence forces decision scientists to use available data in a much smarter way . \\n'],\n",
              " [' Indexing the Web is becoming a laborious task for search engines as the Web exponentially grows in size and distribution . Presently the most effective known approach to overcome this problem is the use of focused crawlers . A focused crawler employs a significant and unique algorithm in order to detect the pages on the Web that relate to its topic of interest . For this purpose we proposed a custom method that uses specific HTML elements of a page to predict the topical focus of all the pages that have an unvisited link within the current page . These recognized on topic pages have to be sorted later based on their relevance to the main topic of the crawler for further actual downloads . In the Treasure Crawler we use a hierarchical structure called T Graph which is an exemplary guide to assign appropriate priority score to each unvisited link . These URLs will later be downloaded based on this priority . This paper embodies the implementation test results and performance evaluation of the Treasure Crawler system . The Treasure Crawler is evaluated in terms of specific information retrieval criteria such as recall and precision both with values close to 50 . Gaining such outcome asserts the significance of the proposed approach . \\n'],\n",
              " [' Accurate and reliable prediction of diarrhoea outpatient visits is necessary for the health authorities to ensure the appropriate action for the control of the outbreak . In this study a novel method based on time series decomposition and multi local predictor fusion has been proposed to predict the diarrhoea outpatient visits . For time series decomposition the Ensemble Empirical Mode Decomposition with Adaptive Noise is used to decompose diarrhoea outpatient visits time series into a finite set of Intrinsic Mode Function components and a residue . The IMF components and residue are modeled and predicted respectively by means of Generalized Regression Neural Network as local predictor . Then the prediction results of all components are fusioned using another independent GRNN as fusion predictor to obtain final prediction results . This is the first study on using a EEMDAN and GRNN to constructing an prediction model for diarrhoea outpatient visits prediction problems . The pre procession and post processing techniques are used to take into account the seasonal and trend effects in the datasets for improving the prediction precision of proposed model . The performance of the proposed EEMDAN GRNN model has been compared with Seasonal Auto Regressive Moving Average Single GRNN Wavelet GRNN and also with EEMD GRNN by applying them to predict four real world diarrhoea outpatient visits . The results indicate that the proposed EEMDAN GRNN model provides more accurate prediction results compared to the other traditional techniques . Thus EEMDAN GRNN can be an alternate tool to facilitate the prediction of diarrhoea outpatient visits . \\n'],\n",
              " [' In the simulation of a chain of manufacturing processes several finite element packages can be employed and for each process or package a different mesh density or element type may be the most suitable . Therefore there is a need for transferring finite element analysis data among packages and mapping it between meshes . This paper presents efficient algorithms for mapping FEA data between meshes with different densities and element types . An in core spatial index is created on the mesh from which FEA data is transferred . The index is represented by a dynamic grid partitioning the underlying space from which nodes and elements are drawn into equal sized cells . Buckets containing references to the nodes indexed are associated with the cells in a many to one correspondence . Such an index makes nearest neighbour searches of nodes and elements much faster than sequential scans . An experimental evaluation of the mapping techniques using the index is conducted . The algorithms have been implemented in the open source finite element data exchange system FEDES . \\n'],\n",
              " [' A new hybrid mixed stress finite element model for the static and dynamic non linear analysis of concrete structures is presented and discussed in this paper . The main feature of this model is the simultaneous and independent approximation of the stress the strain and the displacement fields in the domain of each element . The displacements along the static boundary which is considered to include inter element boundaries are also directly approximated . To define the approximation bases in the space domain complete sets of orthonormal Legendre polynomials are used . The adoption of these functions enables the use of analytical closed form solutions for the computation of all linear structural operators and leads to the development of very effective p refinement procedures . To represent the material quasi brittle behaviour a physically non linear model is considered by using damage mechanics . A simple isotropic damage model is adopted and to control strain localisation problems a non local integral formulation is considered . To solve the dynamic non linear governing system a time integration procedure based on the use of the HHT method is used . For each time step the solution of the non linear governing system is achieved using an iterative algorithm based on a secant method . The model being discussed is applied to the solution of two dimensional structures . To validate the model to illustrate its potential and to assess its accuracy and numerical efficiency several numerical examples are discussed and comparisons are made with solutions provided by experimental tests and with other numerical results obtained using conventional finite elements . \\n'],\n",
              " [' This paper proposes a new variant of particle swarm optimization namely multiple learning PSO with space transformation perturbation to improve the performance of PSO . The proposed MLPSO STP uses a novel learning strategy and STP . The novel learning strategy allows each particle to learn from the average information on the personal historical best position of all particles and from the information on multiple best positions that are randomly chosen from the top 100p of pbest . This learning strategy enables the preservation of swarm diversity to prevent premature convergence . Meanwhile STP increases the chance to find optimal solutions . The performance of MLPSO STP is comprehensively evaluated in 21 unimodal and multimodal benchmark functions with or without rotation . Compared with eight popular PSO variants and seven state of the art metaheuristic search algorithms MLPSO STP performs more competitively on the majority of the benchmark functions . Finally MLPSO STP shows satisfactory performance in optimizing the operating conditions of an ethylene cracking furnace to improve the yields of ethylene and propylene . \\n'],\n",
              " [' Evidence theory employs a much more general and flexible framework to quantify the epistemic uncertainty and thereby it is adopted to conduct reliability analysis for engineering structures recently . However the large computational cost caused by its discrete property significantly influences the practicability of evidence theory . This paper proposes an efficient response surface method to evaluate the reliability for structures using evidence theory and hence improves its applicability in engineering problems . A new design of experiments technique is developed whose key issue is the search of the important control points . These points are the intersections of the limit state surface and the uncertainty domain thus they have a significant contribution to the accuracy of the subsequent established RS . Based on them a high precise radial basis functions RS to the actual limit state surface is established . With the RS the reliability interval can be efficiently computed for the structure . Four numerical examples are investigated to demonstrate the effectiveness of the proposed method . \\n'],\n",
              " [' The induction of decision tree searches for relevant characteristics in the data which would allow it to precisely model a certain concept but it also worries about the comprehensibility of the generated model helping human specialists to discover new knowledge something very important in the medical and biological areas . On the other hand such inducers present some instability . The main problem handled here refers to the behavior of those inducers when it comes to high dimensional data more specifically to gene expression data irrelevant attributes may harm the learning process and many models with similar performance may be generated . In order to treat those problems we have explored and revised windowing pruning of the trees generated during intermediary steps of the algorithm the use of the estimated error instead of the training error the use of the error weighted according to the size of the current window and the use of the classification confidence as the window update criterion . The results show that the proposed algorithm outperform the classical one especially considering measures of complexity and comprehensibility of the induced models . \\n'],\n",
              " [' The integration of multiple features is important for action categorization and object recognition in videos because single feature based representation hardly captures imaging variations and individual attributes . In this paper a novel formulation named Multivariate video Information Bottleneck is defined . It is an extensional type of multivariate information bottleneck and can discover categories from a collection of unlabeled videos automatically . Differing from the original multivariate information bottleneck the novel approach extracts the video categories from multiple features simultaneously such as local static and dynamic feature each type of feature is treated as a relevant variable . Specifically by preserving the relevant information with respect to these feature variables maximally the MvIB method is able to integrate various aspects of semantic information into the final video partitioning results and thus captures the complementary information resided in multiple feature variables . Extensive experimental results on five challenging video data sets show that the proposed approach can consistently and significantly outperform other state of the art unsupervised learning methods . \\n'],\n",
              " [' A parallel computing region growing algorithm for surface reconstruction from unorganized point clouds is proposed in this research . The traditional region growing algorithm belongs to sequential process and needs to update the topology information continuously to maintain the boundaries of the growing region . This constraint becomes a bottleneck for efficiency improvement . The proposed GPU based region growing algorithm is to decompose the traditional sequence and re plan specific framework for the purpose of utilizing parallel computation . Then a graphics card with multi processing units will be used to build triangles in the parallel computing mode . In our GPU based reconstruction process each sampling point is regarded as an independent seed and expands simultaneously until all surrounding patches overlap each other . Following this the overlapping patches are removed and holes are filled by the GPU based calculation . Finally a complete model is created . In order to validate the algorithm proposed the unorganized point cloud was obtained by a 3D scanner and then reconstructed using the parallel computing region growing algorithm . According to the results obtained the algorithm proposed here shows 10 times better performance when compared to the traditional region growing method . \\n'],\n",
              " [' In multi agent systems stereotypical trust models are widely used to bootstrap a priori trust in case historical trust evidences are unavailable . These models can work well if and only if malicious agents share some common features in their profiles and these features can be detected . However this condition may not hold for all the adversarial scenarios . Smart attackers can show different trustworthiness to different agents and services . In this paper we propose CAST a novel Context Aware Stereotypical Trust deep learning framework . CAST coins a comprehensive set of seven context aware stereotypes each of which can capture a unique type of context correlated attacks as well as a deep learning architecture to keep the trust stereotyping robust . The basic idea is to construct a multi layer perceptive structure to learn the latent correlations between context aware stereotypes and the trustworthiness and thus can estimate the new trust by taking into account the context information . We have evaluated CAST using a rich set of experiments over a simulated multi agent system . The experimental results have successfully confirmed that our CAST can achieve approximately tens of times higher trust inference accuracy in average than the competing algorithms in the presence of context correlated attacks and more importantly can maintain a much better trust inference robustness against stereotyping errors . \\n'],\n",
              " [' The paper is focussed on the robustness of parallel computation in the case of buckling and post buckling analyses . In the nonlinear context domain decomposition methods are mainly used as a solver for the tangent problem solved at each iteration of a Newton Raphson algorithm . In case of strongly nonlinear and heterogeneous problems as those encountered in buckling and post buckling this procedure may lead to severe difficulties regarding convergence and efficiency . The problem of convergence is regarded as the most critical issue at the industrial level . Indeed if a method which can show efficiency for some problems is not robust with respect to convergence the method will not be implemented by industrial end users . Therefore two paths are explored to gain robustness when making use of domain decomposition methods a nonlinear localization strategy which may also improve the robustness by treating the nonlinearity at the subdomain level and a mixed framework allowing to circumvent the problem of local divergence . It is to be noted that those two ingredients may also be used to improve the numerical efficiency of the method but this is not the main focus of the paper . Simple structures are first considered to illustrate the method performances . Results obtained in the case of a boxed structure and of a stiffened panel are then discussed . \\n'],\n",
              " [' Accurate interval forecasting of agricultural commodity futures prices over future horizons is challenging and of great interests to governments and investors by providing a range of values rather than a point estimate . Following the well established linear and nonlinear modeling framework this study extends it to forecast interval valued agricultural commodity futures prices with vector error correction model and multi output support vector regression which is capable of capturing the linear and nonlinear patterns exhibited in agricultural commodity futures prices . Two agricultural commodity futures prices from Chinese futures market are used to justify the performance of the proposed VECM MSVR method against selected competitors . The quantitative and comprehensive assessments are performed and the results indicate that the proposed VECM MSVR method is a promising alternative for forecasting interval valued agricultural commodity futures prices . \\n'],\n",
              " [' We study the problem of clustering uncertain objects whose locations are uncertain and described by probability density functions . We analyze existing pruning algorithms and experimentally show that there exists a new bottleneck in the performance due to the overhead of pruning candidate clusters for assignment of each uncertain object in each iteration . In this article we will show that by considering squared Euclidean distance UK means is reduced to K means and performs much faster than pruning algorithms however with some discrepancies in the clustering results due to using different distance functions . Thus we propose Approximate UK means to heuristically identify objects of boundary cases and re assign them to better clusters . Three models for the representation of cluster representative are proposed to calculate expected squared Euclidean distance between objects and cluster representatives in this paper . Our experimental results show that on average the execution time of Approximate UK means is only 25 more than K means and our approach reduces the discrepancies of K means clustering results by up to 70 . \\n'],\n",
              " [' Covering rough sets are a generalization of Pawlak rough sets in which a partition of the universal set induced by an equivalence relation is replaced by a covering . In this paper covering rough sets are transformed into generalized rough sets induced by binary relations . The paper discusses three theoretical topics . First we consider a special type of covering in which the neighborhoods form a reduction of the covering and we obtain necessary and sufficient conditions for neighborhoods in a covering form a reduction of the covering . Second we study another special type of covering and give conditions for the covering lower and upper approximations to be dual to each other . Finally we give an axiomatic system that characterizes the lower and upper approximations of rough sets based on a partial order . \\n'],\n",
              " [' This study proposes a new robust multi objective maintenance planning approach of the deteriorating bridges against uncertainty in performance degradation model . The main focus is to guarantee the performance requirements of the bridge by the scheduled maintenance interventions even in the presence of uncertainty in time dependent performance degradation model . The uncertainties are modeled as the perturbation of the system parameters . These are simulated by a sampling method and incorporated into the GA based multi objective optimization framework which produces a set of optimal preventive maintenance scenarios . In order to focus the searching on the most preferable region the performance models of the bridge components are all integrated into single overall performance measure by using the preference based objective space reduction method . Numerical example of a typical prestressed concrete girder bridge is provided to demonstrate the new robust maintenance scheduling approach . For comparison purpose non robust multi objective maintenance planning without considering uncertainty of the bridge performance is also provided . It is verified that the proposed approach can produce successfully performing maintenance scenarios under the perturbation of bridge condition grades while maintaining well balanced maintenance strategy both in terms of bridge performance and maintenance cost . \\n'],\n",
              " [' This work describes a technique for generating two dimensional triangular meshes using distributed memory parallel computers based on a master slaves model . This technique uses a coarse quadtree to decompose the domain and a serial advancing front technique to generate the mesh in each subdomain concurrently . In order to advance the front to a neighboring subdomain each subdomain suffers a shift to a Cartesian direction and the same advancing front approach is performed on the shifted subdomain . This shift and remesh procedure is repeatedly applied until no more mesh can be generated shifting the subdomains to different directions each turn . A finer quadtree is also employed in this work to help estimate the processing load associated with each subdomain . This load estimation technique produces results that accurately represent the number of elements to be generated in each subdomain leading to proper runtime prediction and to a well balanced algorithm . The meshes generated with the parallel technique have the same quality as those generated serially within acceptable limits . Although the presented approach is two dimensional the idea can be easily extended to three dimensions . \\n'],\n",
              " [' This paper aims to develop and compare several elicitation criterions for decision making of incomplete soft sets which are generated by restricted intersection . One time elicitation process is divided into two steps . Using the greedy idea four criterions for elicitation of objects are built based on maximax maximin minimax regret and combination of expected choice values and elicitation times . Then these initial unknown values which produce incomplete values together with known information are in priority . Fast methods for computing possibly and necessarily optimal solutions before or in the elicitation process are invented . As far as the sizes of soft sets used in the simulation experiments it is found statistically that we should choose the criterion based on the combination of expected choice value and expected elicitation times in the first step of one time elicitation . The developed methods can be used for decision making of incomplete 0 1 information systems which are generated by the conjunction of two experts incomplete 0 1 evaluation results . Whenever the available information is not enough for choosing a necessarily optimal solution the elicitation algorithms can help elicitate as few unknown values as possible until an optimal result is found . An elicitation system is made to show that our elicitation methods can potentially be embedded in recommender or decision support systems . The elicitation problems are proposed for decision making of operation generated soft sets by extracting from some practical problems . The concept of expected elicitation times of objects is defined and used for developing one type of elicitation strategy . \\n'],\n",
              " [' Ultra large scale systems are a new generation of distributed software system that are composed of various changing inconsistent or even conflicting components that are distributed in a wide domain . Some important characteristics of these systems include their very large size global geographical distribution operational and managerial independence of their member systems . The main function of these systems arises from the interoperability between their components . Nowadays one of the most important challenges facing ultra large scale systems is the interoperability of their component systems . Interoperability is the ability by which system elements can exchange and understand the information required with each other . This paper aims to solve the mentioned challenge which is divided into two main parts . In the first part this paper presents a maturity model for the interoperability of ultra large scale systems by using the interoperability level of the component system of one ultra large scale system its maturity level can be determined . In the second part by proposing a framework we try to increase the interoperability of the component systems in ultra large scale systems based on the interoperability maturity levels determined in the first part . Consequently their interoperability is improved . \\n'],\n",
              " [' We mainly study the low rank image recovery problem by proposing a bilinear low rank coding framework called Tensor Low Rank Representation . For enhanced low rank recovery and error correction our method constructs a low rank tensor subspace to reconstruct given images along row and column directions simultaneously by computing two low rank matrices alternately from a nuclear norm minimization problem so both column and row information of data can be effectively preserved . Our bilinear approach seamlessly integrates the low rank coding and dictionary learning into a unified framework . Thus our formulation can be treated as enhanced Inductive Robust Principal Component Analysis with noise removed by low rank representation and can also be considered as the enhanced low rank representation with a clean informative dictionary via low rank embedding . To enable our method to include outside images the out of sample extension is also presented by regularizing the model to correlate image features with the low rank recovery of the images . Comparison with other criteria shows that our model exhibits stronger robustness and enhanced performance . We also use the outputted bilinear low rank codes for feature learning . Two unsupervised local and global low rank subspace learning methods are proposed for extracting image features for classification . Simulations verified the validity of our techniques for image recovery representation and classification . \\n'],\n",
              " [' In this paper an effective bi population estimation of distribution algorithm is presented to solve the no idle permutation flow shop scheduling problem with the total tardiness criterion . To enhance the search efficiency and maintain the diversity of the whole population two sub populations are used in the BEDA . The two sub populations are generated by sampling the probability models that are updated differently for the global exploration and the local exploitation respectively . Meanwhile the two sub populations collaborate with each other to share search information for adjusting the models . To well adjust the models for generating promising solutions the global probability model is updated during the evolution with the superior population and the local probability model is updated with the best solution that has been explored . To further enhance exploitation in the promising region the insertion operator is used iteratively as the local search procedure . To investigate the influence of parameter setting numerical study based on the Taguchi method of design of experiment is carried out . The effectiveness of the bi population strategy and local search procedure is shown by numerical comparisons and the comparisons with the recently published algorithms by using the benchmarking instances also demonstrate the effectiveness of the proposed BEDA . \\n'],\n",
              " [' Noise components are a major cause of poor performance in document analysis . To reduce undesired components most recent research works have applied an image processing technique . However the effectiveness of these techniques is suitable only for a Latin script document but not a non Latin script document . The characteristics of the non Latin script document such as Thai are considerably more complicated than the Latin script document and include many levels of character alignment no word or sentence separator and variability in a character s size . When applying an image processing technique to a Thai document we usually remove the characters that are relatively close to noise . Hence in this paper we propose a novel noise reduction method by applying a machine learning technique to classify and reduce noise in document images . The proposed method uses a semi supervised cluster and label approach with an improved labeling method namely feature selected sub cluster labeling . Feature selected sub cluster labeling focuses on the clusters that are incorrectly labeled by conventional labeling methods . These clusters are re clustered into small groups with a new feature set that is selected according to class labels . The experimental results show that this method can significantly improve the accuracy of labeling examples and the performance of classification . We compared the performance of noise reduction and character preservation between the proposed method and two related noise reduction approaches i.e . a two phased stroke like pattern noise removal and a commercial noise reduction software called ScanFix Xpress 6.0 . The results show that semi supervised noise reduction is significantly better than the compared methods of which an F measure of character and noise is 86.01 and 97.82 respectively . \\n'],\n",
              " [' This paper is devoted to two issues involved in the one class support vector machine i.e . the optimization algorithm and the kernel parameter selection . For appropriate choices of parameters the primal maximum margin problem of OCSVM is equivalent to a nearest point problem . A generalized Gilbert algorithm is proposed to solve the nearest point problem . Compared with the algebraic algorithms developed for OCSVM such as the well known sequential minimal optimization algorithm the GG algorithm is a novel geometric algorithm that has an intuitive and explicit optimization target at each iteration . Moreover an improved MIES is developed for the Gaussian kernel parameter selection . IMIES is implemented by constraining the geometric locations of edge and interior sample mappings relative to OCSVM separating hyper planes . The experimental results on 2 D artificial datasets and benchmark datasets show that IMIES is able to select suitable kernel parameters and the GG algorithm is computationally more efficient while achieving comparable accuracies to the SMO algorithm . \\n'],\n",
              " [' Knowledge reduction is a basic issue in knowledge representation and data mining . Although various methods have been developed to reduce the size of classical formal contexts the reduction of formal fuzzy contexts based on fuzzy lattices remains a difficult problem owing to its complicated derivation operators . To address this problem we propose a general method of knowledge reduction by reducing attributes and objects in formal fuzzy contexts based on the variable threshold concept lattices . Employing the proposed approaches we remove attributes and objects which are non essential to the structure of a variable threshold concept lattice i.e . with a given threshold level the concept lattice constructed from a reduced formal context is made identical to that constructed from the original formal context . Discernibility matrices and Boolean functions are respectively employed to compute the attribute reducts and object reducts of the formal fuzzy contexts by which all the attribute reducts and object reducts of the formal fuzzy contexts are determined without changing the structure of the lattice . \\n'],\n",
              " [' This study analyzed the Cost efficiency and Revenue efficiency of 207 certified public accountant firms in Taiwan by using the additive efficiency decomposition DEA approach . Furthermore this study applied the Tobit regression to explore the relationship between CPA firms and intellectual capital . The study found that the Big 4 and CPA firms that practiced auditing in China were relatively efficient in both cost and revenue . In addition this research discovered that CPA firms relying mainly on auditing are more efficient in creating revenue and utilizing costs . Furthermore the Tobit regression was employed to evaluate whether IC affected CPA firms cost efficiency and revenue efficiency . This study found that IC played an important role in performance representation both in cost efficiency and revenue efficiency . Therefore this study suggests that CPA firms should manage IC efficiently to enhance CPA firms competitive abilities . \\n'],\n",
              " [' From traditional clusters to cloud systems job scheduling is one of the most critical factors for achieving high performance in any distributed environment . In this paper we propose an adaptive algorithm for scheduling modular non linear parallel jobs in meteorological Cloud which has a unique parallelism that can only be configured at the very beginning of the execution . Different from existing work our algorithm takes into account four characteristics of the jobs at the same time including the average execution time the deadlines of jobs the number of assigned resources and the overall system loads . We demonstrate the effectiveness and efficiency of our scheduling algorithm through simulations using WRF that which is widely used in scientific computing . Our evaluation results show that the proposed algorithm has multiple advantages compared with previous methods including more than 10 reduction in terms of execution time a higher completion ratio in terms of meeting soft deadlines and a much smaller standard deviation of the average weighted execution time . Moreover we show that the proposed algorithm can tolerate inaccuracy in system load estimation . \\n'],\n",
              " [' In multiple attribute decision making different attribute weights may generate different solutions which means that attribute weights significantly influence solutions . When there is a lack of sufficient data knowledge and experience for a decision maker to generate attribute weights the decision maker may expect to find the most satisfactory solution based on unknown attribute weights called a robust solution in this study . To generate such a solution this paper proposes a robust evidential reasoning approach to compare alternatives by measuring their robustness with respect to attribute weights in the ER context . Alternatives that can become the best with the support of one or more sets of attribute weights are firstly identified . The measurement of robustness of each identified alternative from two perspectives i.e . the optimal situation of the alternative and the insensitivity of the alternative to a variation in attribute weights is then presented . The procedure of the proposed approach is described based on the combination of such identification of alternatives and the measurement of their robustness . A problem of car performance assessment is investigated to show that the proposed approach can effectively produce a robust solution to a MADM problem with unknown attribute weights . \\n'],\n",
              " [' In order to follow modern trends in contemporary building architecture which is moving off the limits of current fire design models assumption of homogeneous temperature conditions used for structural fire analysis needs to be validated . In this paper it is described how temperature distribution in a medium size fire compartment has been investigated experimentally by conducting fire test in two storey experimental building in September 2011 in the Czech Republic . In the upper floor a scenario of travelling fire was prepared . It has been observed that as flames were spreading across the compartment considerable temperature gradients appeared . Numerical simulation of the travelling fire test conducted using FDS has been compared with simulation of compartment fire under uniform temperature conditions to highlight the potential impact of the gas temperature heterogeneity on structural behaviour . The temperature measurements from the fire test have been used for validation of the numerical simulation of travelling fire . The fire test has provided important data for design model of travelling fire and shown that its impact on structural behaviour is not in agreement with the assumption of homogenous temperature conditions . \\n'],\n",
              " [' Smoke is a leading cause of death in fire . To minimize the potential harm from the smoke hazards in the course of a fire a rational virtual reality based fire training simulator taking full account of the various aspects of smoke hazards has been developed and is described herein . In this simulator a visualization technique based on volume rendering and fire dynamics data has been especially designed to create a realistic and accurate smoke environment for the purposes of effective virtual training which allows the trainees to experience a realistic and yet non threatening fire scenario . In addition an integrated assessment model of smoke hazards is also established in order to assess the safety of different paths for evacuation or rescue in virtual training which allows the trainees to learn to identify the safest path . Two case studies of a subway station and a primary school demonstrated a high level of accuracy and smooth interactive performance of the proposed simulator which is thus shown to be valuable for the training of both people who might become trapped in fire and firefighters engaged in learning the proper rescue procedures . \\n'],\n",
              " [' This paper presents an open and integrated framework that performs the structural design optimization by associating the improved sequential approximation optimization algorithm with the CAD CAE integration technique . In the improved SAO algorithm a new estimate of the width of Gaussian kernel functions is proposed to enhance the surrogate models for SAO . Based on the improved surrogate models an adaptive sampling strategy is developed to balance the exploration exploitation in the sampling process which better balances between the competence to locate the global optimum and the computation efficiency in the optimization process . Fewer function evaluations are required to seek the optimum which is of great significance for computation intensive structural optimization problems . Moreover based on scripting program languages and Application Programming Interfaces integration between commercial CAD and CAE software packages is implemented to expand the applications of the SAO algorithm in mechanical practices . Two benchmark tests from simple to complex from low dimension to moderate dimension were performed to validate the efficacy of the proposed framework . Results show that the proposed approach facilitates the structural optimization process and reduces the computing cost immensely compared to other approaches . \\n'],\n",
              " [' The main goal of this paper was to develop an integrated simulation design of experiments model to optimize a petrol station queuing system and sales rate . Initially the petrol station operating system was simulated using Witness 2014 simulation software . Then the responses of simulation were deployed as the input of DOE . Two level full factorial experiments with center points were performed where the simulated model parameter studied were number of pump number of cashier and inter arrival times . The response variables analyzed were queue length and sales rate . The obtained model from experimental design revealed that number of cashier and inter arrival time were significant in determining the queue length while all the factors and their interaction were significantly affecting the sales rate . \\n'],\n",
              " [' The present article proposes an advanced methodology for numerically simulating complex noise problems . More precisely we consider the so called multi stage acoustic hybrid approach which principle is to couple sound generation and acoustic propagation stages . Under that approach we propose an advanced hybrid method which acoustic propagation stage relies on Computational AeroAcoustics techniques . To this end first an innovative weak coupling technique is developed which allows an implicit forcing of the CAA stage with a given source signal coming from an a priori evaluation whether the latter evaluation is of analytical or computational nature . Then thanks to additional innovative solutions the resulting CAA based hybrid approach is optimized so that it can be applied to realistic and complex acoustic problems in an easier and safer way . All these innovative features are then validated on the basis of an academic test case before the resulting advanced CAA based hybrid methodology is applied to two problems of flow induced noise radiation . This demonstrates the ability of the here proposed method to address realistic problems by offering to handle at the same time both acoustic generation and propagation phenomena despite their intrinsic multiscale character . \\n'],\n",
              " [' In reinforced concrete structural experiments the development of concrete surface cracks is an important factor of concern to experts . One conventional crack observation method is to suspend a test at a few selected testing steps and send inspectors to mark pen strokes on visible cracks but this method is dangerous and labor intensive . Many image analysis methods have been proposed to detect and measure the dark shadow lines of cracks reducing the need for manual pen marking . However these methods are not applicable for thin cracks which do not present clear dark lines in images . This paper presents an image analysis method to capture thin cracks and minimize the requirement for pen marking in reinforced concrete structural tests . The paper presents the mathematical models procedures and limitations of our image analysis method as well as the analysis flowchart the adopted image processing and analysis methods and the software implementation . Finally the results of applying the proposed method in full scale reinforced concrete bridge experiments are presented to demonstrate its performance . Results demonstrate that this method can capture concrete surface cracks even before dark crack lines visible to the naked eye appear . \\n'],\n",
              " [' One of the most important activities in software project planning involves scheduling tasks and assigning them to developers . Project managers must decide who will do what and when in a software project with the aim of minimizing both its duration and cost . However project managers often struggle to efficiently allocate developers and schedule tasks in a way that balances these conflicting goals . Furthermore the different criteria used to select developers could lead to inaccurate estimation of the duration and cost of tasks resulting in budget overruns delays or reduced software quality . This paper proposes an approach that makes use of multi objective optimization to handle the simultaneous minimization of project cost and duration taking into account several productivity related attributes for better estimation of task duration and cost . In particular we focus on dealing with the non interchangeable nature of human resources and the different ways in which teams carry out work by considering the relationship between the type of task interdependence and the productivity rate of developers as well as the communication overhead incurred among developers . The approach is applied to four well known optimization algorithms whose performance and scalability are compared using generated software project instances . Additionally several real world case studies are explored to help discuss the implications of such approach in the software development industry . The results and observations show positive indications that using a productivity based multi objective optimization approach has the potential to provide software project managers with more accurate developer allocation and task scheduling solutions in a more efficient manner . \\n'],\n",
              " [' Automatic segmentation of non stationary signals such as electroencephalogram electrocardiogram and brightness of galactic objects has many applications . In this paper an improved segmentation method based on fractal dimension and evolutionary algorithms for non stationary signals is proposed . After using Kalman filter to reduce existing noises FD which can detect the changes in both the amplitude and frequency of the signal is applied to reveal segments of the signal . In order to select two acceptable parameters of FD in this paper two authoritative EAs namely genetic algorithm and imperialist competitive algorithm are used . The proposed approach is applied to synthetic multi component signals real EEG data and brightness changes of galactic objects . The proposed methods are compared with some well known existing algorithms such as improved nonlinear energy operator Varri s and wavelet generalized likelihood ratio methods . The simulation results demonstrate that segmentation by using KF FD and EAs have greater accuracy which proves the significance of this algorithm . \\n'],\n",
              " [' The Colliding Bodies Optimization algorithm is a metaheuristic algorithm inspired by the physics laws of collision in which each candidate solution is modeled as an agent with mass body in proportion to the fitness of the solution . In this paper a modified version of CBO denoted by MCBO is utilized to optimize the cost of bridge superstructures . The problem consists of 17 variables and 101 implicit constraints based on AASHTO standard specifications and construction limitations . The optimization is performed for bridges with different span lengths and deck widths and with various unit costs of concrete . A comparison among the PSO CBO and MCBO algorithms is conducted which shows the efficiency and robustness of the MCBO algorithm . \\n'],\n",
              " [' Colliding Bodies Optimization is a new multi agent algorithm inspired by a collision between two objects in one dimension . Each agent is modeled as a body with a specified mass and velocity . A collision occurs between pairs of objects and the new positions of the colliding bodies are updated based on the collision laws . In this paper Enhanced Colliding Bodies Optimization which uses memory to save some best solutions is developed . In addition a mechanism is utilized to escape from local optima . The performance of the proposed algorithm is compared to those of standard CBO and some optimization techniques on some benchmark mathematical functions and three standard discrete and continuous structural design problems . Optimization results confirm the validity of the proposed approach . \\n'],\n",
              " [' Computational efficiency is still a great challenge for the generation of the Medial Axis for complicated CAD models . Current research mainly focuses on CPU based MA generation methods . However most of the methods emphasize using a single CPU . The highly efficient methods based on parallel computing are still missing . In this study a parallel method based on multi CPU is proposed for the efficient MA generation of CAD models using distance dilation . By dividing the whole model into several parts for which MAs are calculated in parallel and then combined computational efficiency can be greatly improved in theory and the computation time can be reduced nearly K times if K CPUs are used . Firstly an adaptive division method is proposed to divide the voxelized model into blocks which have nearly the same number of voxels to balance the computational burden . Secondly the local Euclidean Distance Transform is calculated for each block based on the existing distance dilation method . Thirdly the complete inter dilation method is proposed to compute the influence between different blocks to get a global EDT for each block . Finally each block generates a sub MA separately and then all the generated MAs are combined to obtain the final MA . The last three processes can be efficiently conducted in parallel by using multiple CPUs . Several groups of experiments are conducted which demonstrate the good performance of the proposed methods in terms of efficiency . \\n'],\n",
              " [' Micro and nanomanipulators are essential for a broad range of applications requiring precise micro and nanoscopic spatial control such as those in micromanufacturing and single cell analysis . These manipulators are often manually controlled using an attached joystick and can be difficult for operators to use efficiently . This paper describes a system developed in MATLAB to control a well known commercial micromanipulator in a user friendly and versatile manner through a graphical user interface . The control system and interface allows several types of flexible movement controls in three axis Cartesian space including single movements multiple queued movements and mouse following continuous movements . The system uses image processing for closed loop feedback to ensure precise and accurate control over the movement of the manipulator s end effector . The system can be used on any electronic device capable of running the free MATLAB Runtime Environment and the system is extensible to simultaneously control other instruments capable of serial communication . \\n'],\n",
              " [' In a previous work we presented algorithms which allow obtaining three dimensional models from graphs which represent a projection in conical parallel perspectives and conical oblique perspectives of polyhedral models with normalon and quasi normalon typology . In this paper the new advances that we have achieved in this field are presented allowing increasing the set of models which can be reconstructed to other typologies different from the normalon and quasi normalon ones . Moreover we present a new technique which extends the previous work in order to be implemented to conical perspectives with three vanishing points and the method proposed for the detection of the type of conical perspective represented by the graph including the detection and subsequent reconstruction of graphs which represent a flat shape has been improved . The results obtained on a total of 336 tests with a success ratio of 100 make the method a proposal to be considered for obtaining models from conical perspectives automatically . \\n'],\n",
              " [' A text independent speaker recognition system using a hybrid Probabilistic Principal Component Analysis and conventional i vector modeling technique is proposed . In this framework the total variability space is estimated using PPCA while the i vectors of target speakers and test utterances are extracted using the conventional method . This leads to appreciable decrease in development time while the time required for training and testing remains unchanged . In this a paper an algorithmic optimization to the PPCA s EM algorithm is developed . This is observed to provide a speed up of 3.7 . To simplify the testing procedure two different approximation procedures are proposed to be used in this framework . The first approximation assumes a covariance matrix computed based on the PPCA framework . The second approximation proposes an optimization to avoid inverting the precision matrix of the i vector . The comparison of time taken by these approximations with the baseline i vector extraction procedure shows speed gains with some deterioration in performance in terms of the Equal Error Rate . Among the proposed techniques a best case trade off is obtained with a speed up of 81.2 with deterioration in performance by 0.7 in absolute terms . Speaker recognition performances are studied on the telephone conditions of the benchmark NIST SRE 2010 dataset with systems built on the Mel Frequency Cepstral Co efficient feature . A trade off in the performance is observed when the proposed approximations are used . The scalability of these trade offs is tested on the Mel Filterbank Slope feature . The trade offs observed with the approximations are reduced when the two systems are fused . \\n'],\n",
              " [' Medial axis is used as an effective description for objects in many engineering fields . A difficulty for the current methods for the generation of MA of CAD models is the balance between the efficiency and the quality . In this study an approach to iteratively generating hierarchical multi resolution MA is proposed . In each iteration only a small part of MA that affects MA quality is refined by which the time cost and the space cost are reduced greatly . First the model is voxelized and its initial MA is generated by distance dilation method . Meanwhile the MA quality is computed and evaluated . Second if the MA quality does not satisfy the requirement upgrade the MA level and re compute the local MA in the affected region until the MA quality does . Finally by combining the local MA in the affected region with the reused MA in other regions hierarchical multi resolution MA is obtained . Several examples are given to demonstrate the outperformance of the proposed method in terms of time and space . \\n'],\n",
              " [' Many computer applications such as racing games and driving simulations demand high fidelity 3D road network models . However few methods exist for the automatic generation of 3D realistic road networks especially for those in the real world . On the other hand vast 2D road network data in various geographical information systems have been collected in the past and are used by a wide range of applications . A method that can automatically produce 3D high fidelity road network models from 2D real road GIS data will significantly reduce both the labor and time cost and greatly benefit applications involving road networks . Based on a set of carefully selected civil engineering rules for road design this paper proposes a novel approach that transforms existing road GIS data that contain only 2D road centerline information into high fidelity 3D road network models . The proposed method consists of several major components including road GIS data preprocessing 3D centerline modeling and 3D geometric modeling . With this approach basic road elements such as road segments road intersections and traffic interchanges are generated automatically to compose sophisticated road networks in a seamless manner . Results show that this approach provides a rapid and efficient 3D road modeling method for applications that have stringent requirements on high fidelity road models . \\n'],\n",
              " [' Application of techniques for modelling of boundary value problems implies two conflicting requirements obtaining high accuracy of the results and speed of the solution . Accurate results can be obtained only by using appropriate models and algorithms . In the previous papers the authors applied the parametric integral equations system in modelling and solving boundary value problems . The first requirement was satisfied the results were obtained with very high accuracy . This paper fulfils the second requirement by novel approach to accelerate PIES . Graphics cards programming for numerical calculations in general purpose applications using NVIDIA CUDA is used for this purpose . The speed of calculations increased up to 80 times whereas high accuracy of the solutions was maintained . Examples included in this paper concern solving elasticity problems which are modelled by three dimensional Navier Lam equations . \\n'],\n",
              " [' This work presents a contribution on the numerical modelling capabilities for the simulation of fluid flow and heat transfer in cellular solids in particular we focus on open cell aluminium foams . Rather than applying one of the classical academical or commercial numerical finite volume finite difference or finite element interface tracking methods we base our models on an interface capturing phase field method . A coupled diffuse interface lattice Boltzmann fluid flow solver and a diffuse interface heat transfer approach are combined in view of dealing with even more convoluted geometries incorporating the dynamics of interfaces and complex multiphysics applications . Numerical results for the combined fluid flow and heat transfer simulations in open cell metal foams are in very good agreement with experimental data . \\n'],\n",
              " [' The paper describes an efficient numerical model for better understanding the influence of the microstructure on the thermal conductivity of heterogeneous media . This is the extension of an approach recently proposed for simulating and evaluating effective thermal conductivities of alumina Al composites . A C code called MultiCAMG taking into account all steps of the proposed approach has been implemented in order to satisfy requirements of efficiency optimization and code unification . Thus on the one hand numerical tools such as the efficient Eyre Milton scheme for computing the thermal response of composites have been implemented for reducing the calculation cost . On the other hand statistical parameters such as the covariance and the distribution of contact angles between particles are now estimated for better analyzing the microstructure . In the present work we focus our investigations on the effects of anisotropy on the effective thermal conductivity of alumina Al composites . First of all an isotropic benchmark is set up for comparison purposes . Secondly anisotropic configurations are studied in order to direct the heat flux . A transversally isotropic structure taking benefit of wall effects is finally proposed for controlling the orientation of contact angles . Its thermal capabilities are related to the current issue of heat dissipation in automotive engine blocks . \\n'],\n",
              " [' We examine the rotational variance of the continuous parameter genetic algorithm . We show that a standard CPGA using blend crossover and standard mutation is rotationally variant . To construct a rotationally invariant CPGA it is possible to modify the crossover operation to be rotationally invariant . This however results in a loss of diversity . Hence we introduce diversity in two ways firstly using a modified mutation scheme and secondly by adding a self scaling random vector with a standard normal distribution sampled uniformly from the surface of a n dimensional unit sphere to the offspring vector . This formulation is strictly invariant albeit in a stochastic sense only . We compare the three formulations in terms of numerical efficiency for a modest set of test problems the intention not being the contribution of yet another competitive and or superior CPGA variant but rather to present formulations that are both diverse and invariant in the hope that this will stimulate additional future contributions since rotational invariance in general is a desirable salient feature for an optimization algorithm . \\n'],\n",
              " [' A simple physical model consisting of a point source displaced from its center of rotation in combination with a directivity model that includes backwards emitted energy is considered for the problem of estimating the orientation of a directional acoustic source . Such a problem arises for instance in voice commanded devices in a smart room and is usually tackled with a large or distributed microphone array . We show however that when the time difference of arrival is also taken into account a small array of only two microphones is sufficiently robust against unaccounted factors such as microphone directivity variation and mild reverberation . This is shown by comparing predicted and measured values of binaural cues and by using them and pairwise frame energies as inputs for an artificial neural network in order to estimate source orientation . \\n'],\n",
              " [' This paper introduces a method to perform a Time Scale Local Hurst Exponent analysis for time series . The traditional Hurst exponent methods usually analyze time series as a whole providing a single value that characterizes their global behavior . In contrast the methods based on the Local Hurst Exponent allow the evaluation of the fractal structure of a time series on local events . However a critical parameter in these methods is the selection of scale . Here a TS LHE method is presented based on a systematic implementation of the rescaled range method in a set of sliding windows of different sizes . This method allows calculating instantaneous values of Local Hurst Exponents at different scales associating them with individual samples of a time series . This paper is organized as follows first an overview of the TS LHE is provided then a proof of concept of this analysis is presented considering different fractional Brownian motion series a synthetic seismic signal under different noise conditions and a group of real seismic traces . Finally the obtained results show that the TS LHE analysis is particularly sensitive to sudden behavior changes of the time series such as frequency or phase variations . This sensitivity is independent of the amplitude of the data and thus it can be used to identify pattern changes as well as long and short range correlations within a time series . \\n'],\n",
              " [' Photo and physically realistic techniques are often insufficient for visualization of fluid flow simulations especially for 3D and time varying studies . Substantial research effort has been dedicated to the development of non photorealistic and illustration inspired visualization techniques for compact and intuitive presentation of such complex datasets . However a great deal of work has been reproduced in this field as many research groups have developed specialized visualization software . Additionally interoperability between illustrative visualization software is limited due to diverse processing and rendering architectures employed in different studies . In this investigation a framework for illustrative visualization is proposed and implemented in MarmotViz a ParaView plug in enabling its use on a variety of computing platforms with various data file formats and mesh geometries . Region of interest identification and feature tracking algorithms incorporated into this tool are described . Implementations of multiple illustrative effect algorithms are also presented to demonstrate the use and flexibility of this framework . By providing an integrated framework for illustrative visualization of CFD data MarmotViz can serve as a valuable asset for the interpretation of simulations of ever growing scale . cell centroid of a cell or region distance of contour points from a central axis edge feature graph constructed from mesh thickness for feature halos products of inertia tensor for a region volumetric angular momentum of a region number of unmatched regions of interest camera projection plane normal octree used for feature matching point on a contour queue used for feature matching region of interest position vector strobe silhouette curve bounding contour around a feature time user specified threshold velocity of a cell or volume average velocity of a region volume of a cell or region spatial coordinates initial value bounding contour around a feature rear point on a contour cell count threshold for regions of interest gradient threshold for regions of interest counting indices left most point on a contour inset contour around a feature minimum maximum values along a contour offset contour around a feature right most point on a contour estimated value similarity parameter for feature matching user defined time step similarity criterion for feature matching relaxation parameter for feature matching hard lower limit for the feature matching criterion volumetric average angular velocity of a region \\n'],\n",
              " [' This study deals with the asymptotic performance of a multiple spur cancellation scheme . Radio frequency transceivers are now multi standard and specific impairment can occur . The clock harmonics called spurs can leak into the signal band of the reception stage and thus degrade the performance . The performance of a fully digital approach is presented here . A one spur cancellation scheme is first described for which we exploit the a priori knowledge of the spur frequency to create a reference of the polluting tone with the same frequency . A least mean square algorithm block that uses this reference to mitigate the polluter is designed . However due to imperfections in the physical components there is a shift between the a priori frequency and the actual frequency of the spur and the spur is affected by Brownian phase noise . Under these circumstances we study the asymptotic and transient performance of the algorithm . We next improve the transient performance by adding a previously proposed adaptive step size process . In a second part of this paper we present a multiple spur parallel approach that is based on the one spur cancellation scheme for which we provide a closed form expression of the asymptotic signal plus noise interference ratio in the presence of frequency shifts and phase noise . \\n'],\n",
              " [' Fitting a pair of coupled geometric objects to a number of coordinate points is a challenging and important problem in many applications including coordinate metrology petroleum engineering and image processing . This paper derives two asymptotically efficient estimators one for concentric circles fitting and the other for concentric ellipses fitting based on the weighted equation error formulation and non linear parameter transformation . The Kanatani Cram r Rao lower bounds for the parameter estimates of the concentric circles and concentric ellipses under zero mean Gaussian noise are provided to serve as the performance benchmark . Small noise analysis shows that the proposed estimators reach the KCR lower bound performance asymptotically . The accuracy of the proposed estimators is corroborated by experiments with synthetic data and realistic images . \\n'],\n",
              " [' This paper proposes a novel training algorithm for radial basis function neural networks based on fuzzy clustering and particle swarm optimization . So far fuzzy clustering has proven to be a very efficient tool in designing such kind of networks . The motivation of the current work is to quantify the exact effect of fuzzy cluster analysis on the network s performance and use it in order to substantially improve this performance . There are two key theoretical findings resulting from the present work . First it is analytically proved that when the standard fuzzy c means algorithm is used to generate the input space fuzzy partition the main effect this partition imposes to the network s square error can be written down in terms of a distortion function that measures the ability of the partition to recreate the original data . Second using the aforementioned distortion function an upper bound of the network s square error can be constructed . Then the particle swarm optimization is put in place to minimize the above upper bound and determine the network s parameters . To further improve the accuracy the basis function widths and the connection weights are fine tuned by employing a steepest descent approach . The main experimental findings are the implementation of the PSO obtains a significant reduction of the square error while exhibiting a smooth dynamic behavior although the steepest descent further decreases the error it finally obtains smaller reduction rates meaning that the strongest impact on the error reduction is provided by the PSO and the improved performance of the proposed network is demonstrated through an extensive comparison with other related methods using a 10 fold cross validation analysis . \\n'],\n",
              " [' We propose a new method to incorporate priors on the solution of nonnegative matrix factorization . The NMF solution is guided to follow the minimum mean square error estimates of the weight combinations under a Gaussian mixture model prior . The proposed algorithm can be used for denoising or single channel source separation applications . NMF is used in SCSS in two main stages the training stage and the separation stage . In the training stage NMF is used to decompose the training data spectrogram for each source into a multiplication of a trained basis and gains matrices . In the separation stage the mixed signal spectrogram is decomposed as a weighted linear combination of the trained basis matrices for the source signals . In this work to improve the separation performance of NMF the trained gains matrices are used to guide the solution of the NMF weights during the separation stage . The trained gains matrix is used to train a prior GMM that captures the statistics of the valid weight combinations that the columns of the basis matrix can receive for a given source signal . In the separation stage the prior GMMs are used to guide the NMF solution of the gains weights matrices using MMSE estimation . The NMF decomposition weights matrix is treated as a distorted image by a distortion operator which is learned directly from the observed signals . The MMSE estimate of the weights matrix under the trained GMM prior and log normal distribution for the distortion is then found to improve the NMF decomposition results . The MMSE estimate is embedded within the optimization objective to form a novel regularized NMF cost function . The corresponding update rules for the new objectives are derived in this paper . The proposed MMSE estimates based regularization avoids the problem of computing the hyper parameters and the regularization parameters . MMSE also provides a better estimate for the valid gains matrix . Experimental results show that the proposed regularized NMF algorithm improves the source separation performance compared with using NMF without a prior or with other prior models . \\n'],\n",
              " [' One of the most important challenges in the computer vision has long been to obtain three dimensional models from the information given by a projection of the model . In this work we show an automatic system which allows obtaining three dimensional models from entities that represent the conical projection of a polyhedral model with normalon or quasi normalon typology . The results obtained on a total of 160 tests with a success ratio of 100 make the method a proposal to be considered for obtaining models from conical perspectives automatically . \\n'],\n",
              " [' Object recognition systems constitute a deeply entrenched and omnipresent component of modern intelligent systems . Research on object recognition algorithms has led to advances in factory and office automation through the creation of optical character recognition systems assembly line industrial inspection systems as well as chip defect identification systems . It has also led to significant advances in medical imaging defence and biometrics . In this paper we discuss the evolution of computer based object recognition systems over the last fifty years and overview the successes and failures of proposed solutions to the problem . We survey the breadth of approaches adopted over the years in attempting to solve the problem and highlight the important role that active and attentive approaches must play in any solution that bridges the semantic gap in the proposed object representations while simultaneously leading to efficient learning and inference algorithms . From the earliest systems which dealt with the character recognition problem to modern visually guided agents that can purposively search entire rooms for objects we argue that a common thread of all such systems is their fragility and their inability to generalize as well as the human visual system can . At the same time however we demonstrate that the performance of such systems in strictly controlled environments often vastly outperforms the capabilities of the human visual system . We conclude our survey by arguing that the next step in the evolution of object recognition algorithms will require radical and bold steps forward in terms of the object representations as well as the learning and inference algorithms used . \\n'],\n",
              " [' We present TouchCut a robust and efficient algorithm for segmenting image and video sequences with minimal user interaction . Our algorithm requires only a single finger touch to identify the object of interest in the image or first frame of video . Our approach is based on a level set framework with an appearance model fusing edge region texture and geometric information sampled local to the touched point . We first present our image segmentation solution then extend this framework to progressive video segmentation encouraging temporal coherence by incorporating motion estimation and a shape prior learned from previous frames . This new approach to visual object cut out provides a practical solution for image and video segmentation on compact touch screen devices facilitating spatially localized media manipulation . We describe such a case study enabling users to selectively stylize video objects to create a hand painted effect . We demonstrate the advantages of TouchCut by quantitatively comparing against the state of the art both in terms of accuracy and run time performance . \\n'],\n",
              " [' This paper introduces a new variation of the p norm detector which is designed for application to coherent multilook detection in compound Gaussian clutter with inverse Gamma texture . By applying what is termed a compensator enhanced detection performance can be achieved independently of the number of looks used . This is particularly useful in the case of a fast scan rate radar where the number of looks may be quite small . Conventional coherent detectors tend to experience saturation in such scenarios and so this new detection process complements recent advances in this area . Further validation is provided by applying this new decision rule to synthetic target detection in real X band radar clutter . \\n'],\n",
              " [' We consider the problem of joint tracking and classification using the information from radar and electronic support measure . For each target class a separate filter is operated in parallel and each class dependent filter is implemented by interacting multiple model regularized particle filter . The speed likelihood for each class is defined using a priori information about speed constraint and combined with the likelihoods from two sensors to improve tracking and classification . Moreover the output of classifier is also used for particle reassignment of different classes which might lead to better performance . Simulations show that our proposed method can provide reliable tracking and correct classification . joint tracking and classification point target motion model based JTC rigid target motion model based JTC electronic support measure transferable belief model particle filter regularized particle filter interacting multiple model interacting multiple model regularized particle filter IMMRPF based JTC probability density mass function root mean squared errors posterior joint state class probability density mass function target state vector ith target type known number of target classes measurement sequences from radar and ESM measurement at time k from radar and ESM flight envelope constraints for class i set of maneuver models for class i target Markovian maneuver transition matrix for class i target Gaussian white process noise vector with zero mean and variance Q state transition matrix gain matrix additive Gaussian white noise with zero mean and variance R set of all possible emitters where N is the total number of emitter types set of on board emitters of class i target number of the on board emitters of class i target ESM measurement space where ESM confusion matrix usage transition probability matrix for emitter j overall emitter usage transition matrix for class i target with emitter set posterior target class probability using ESM data only emitter usage status vector of class i target posterior target class probability using radar data only jth model probability of class i target at time k likelihood for jth model probability of class i target at time k speed likelihood functions speed feature measurement initial class probabilities number of particles for class i target at time k initial number of particles for class i indicator function effective sample size for class i target preset re sampling threshold parameter hybrid particle weights of particles \\n'],\n",
              " [' Acoustic echo canceller is used in communication and teleconferencing systems to reduce undesirable echoes resulting from the coupling between the loudspeaker and the microphone . In this paper we propose an improved variable step size normalized least mean square algorithm for acoustic echo cancellation applications based on adaptive filtering . The steady state error of the NLMS algorithm with a fixed step size is very large for a non stationary input . Variable step size algorithms can be used to decrease this error . The proposed algorithm named MESVSS NLMS combines the generalized sigmoid variable step size NLMS with the ratio of the estimation error to the mean history of the estimation error values . It is shown from single talk and double talk scenarios using speech signals from TIMIT database that the proposed algorithm achieves a better performance more than 3 dB of attenuation in the misalignment evaluation compared to GSVSS NLMS non parametric VSS NLMS and standard NLMS algorithms for a non stationary input in noisy environments . \\n'],\n",
              " [' The anisotropic diffusion is an efficient smoothing process . It is widely used in noise removing and edges preserving via different schemes . In this paper based on a mathematical background and the existing efficient anisotropic function in the literature we developed a new mathematical anisotropic diffusion function which is able to overcome the drawbacks of the traditional process such as the details loss and the image blur . The simulations results and the comparative study with other recent techniques are conducted and showed that the proposed schema generates a wide improvement in the quality of the restored images . This improvement has been shown subjectively in terms of visual quality and objectively with reference to the computation of some criteria . The simulated images are well de noised but the most important is that details and structural information are kept intact . In addition to that the proposed new function was found very interesting and presents numerous advantages like its similarity to the conventional model and the importance of the speed hence it converges faster which allows an opportunity to be well implemented in our de noising process . \\n'],\n",
              " [' Two of the main ingredients of topological persistence for shape comparison are persistence diagrams and the matching distance . Persistence diagrams are signatures capturing meaningful properties of shapes while the matching distance can be used to stably compare them . From the application viewpoint one drawback of these tools is the computational cost for evaluating the matching distance . In this paper we introduce a new framework for the matching distance estimation It preserves the reliability of the entire approach in comparing shapes extremely reducing the computational cost . Theoretical results are supported by experiments on 3D models . \\n'],\n",
              " [' In this paper we address the problem of 2D 3D pose estimation . Specifically we propose an approach to jointly track a rigid object in a 2D image sequence and to estimate its pose in 3D space . We revisit a joint 2D segmentation 3D pose estimation technique and then extend the framework by incorporating a particle filter to robustly track the object in a challenging environment and by developing an occlusion detection and handling scheme to continuously track the object in the presence of occlusions . In particular we focus on partial occlusions that prevent the tracker from extracting an exact region properties of the object which plays a pivotal role for region based tracking methods in maintaining the track . To this end a dynamical choice of how to invoke the objective functional is performed online based on the degree of dependencies between predictions and measurements of the system in accordance with the degree of occlusion and the variation of the object s pose . This scheme provides the robustness to deal with occlusions of an obstacle with different statistical properties from that of the object of interest . Experimental results demonstrate the practical applicability and robustness of the proposed method in several challenging scenarios . \\n'],\n",
              " [' First order Riesz transform based monogenic signal representation has been widely used in image processing and computer vision however it only characterizes image intrinsic one dimensional structure and is incapable of describing intrinsic two dimensional structure . To this end a novel feature extraction approach named Riesz Binary Pattern is proposed for face recognition based on image multi scale analysis and multi order Riesz transform . RBP consists of two complementary components i.e . local Riesz binary pattern and global Riesz binary pattern . LRBP is obtained by performing local binary coding operator on each Riesz transform response to extract image intrinsic two dimensional structure features . While GRBP is the global binary coding of joint information of image pixel multi scale analysis and multi order Riesz transform . Histogram of LRBP and GRBP are concatenated to form face image RBP description . Experimental results on three databases demonstrate that our proposed RBP descriptor is more discriminant in extracting image information and can provide a higher classification rate compared to some state of the art image representation methods . \\n'],\n",
              " [' Saliency detection has been researched a lot in recent years . Traditional methods are mostly conducted and evaluated on conventional RGB images . Few work has considered the incorporation of multi spectral clues . Considering the success of including near infrared spectrum in applications such as face recognition and scene categorization this paper presents a multi spectral dataset and applies it in saliency detection . Experiments demonstrate that the incorporation of near infrared band is effective in the saliency detection procedure . We also test the combinational models for integrating visible and near infrared bands . Results show that there is no single model to effect on every saliency detection method . Models should be selected according to the specific employed method . \\n'],\n",
              " [' We address the problem of predicting category labels for unlabeled videos in a large video dataset by using a ground truth set of objectively labeled videos that we have created . Large video databases like YouTube require that a user uploading a new video assign to it a category label from a prescribed set of labels . Such category labeling is likely to be corrupted by the subjective biases of the uploader . Despite their noisy nature these subjective labels are frequently used as gold standard in algorithms for multimedia classification and retrieval . Our goal in this paper is NOT to propose yet another algorithm that predicts labels for unseen videos based on the subjective ground truth . On the other hand our goal is to demonstrate that the video classification performance can be improved if instead of using subjective labels we first create an objectively labeled ground truth set of videos and then train a classifier based on such a ground truth so as to predict objective labels for the set of unlabeled videos . With regard to how we generate the objectively labeled ground truth dataset we base it on the notion that when a video is labeled by a panel of diverse individuals the majority opinion rendered by the panel may be taken to be the objective opinion . In this manner using judgments provided by multiple human annotators we have collected objective labels for a ground truth dataset consisting of randomly selected 1000 videos from the TinyVideos database that contains roughly 52 000 videos from YouTube . Through a fourfold cross validation experiment on the ground truth set we demonstrate that the objective labels have a superior consistency compared to the subjective labels when used for video classification . We show that this claim is valid for several different kinds of feature sets that one can use to compare videos and with two different types of classifiers that one can use for label prediction . Subsequently we use the ground truth dataset of 1000 videos to predict the objective category labels of the remaining 51 000 videos . We compare the objective labels thus determined with the subjective labels provided by the video uploaders and qualitatively argue for the more informative nature of the objective labels . \\n'],\n",
              " [' In order to realize the patient privacy protection in medical image opposite to traditional reversible data hiding methods which prior to embed message into the smooth area for pursuing high PSNR value the proposed method priors to embed message into the texture area of the medical images for improving the quality of the details information and helping accurate diagnosis . Furthermore in order to decrease the embedding distortion while enhancing the contrast of the texture area this paper also proposes a message sparse representation method . Experiments implemented on medical images showed that the proposed method enhances the contrast of texture area when compared with previous methods . \\n'],\n",
              " [' Organ shape plays an important role in clinical diagnosis surgical planning and treatment evaluation . Shape modeling is a critical factor affecting the performance of deformable model based segmentation methods for organ shape extraction . In most existing works shape modeling is completed in the original shape space with the presence of outliers . In addition the specificity of the patient was not taken into account . This paper proposes a novel target oriented shape prior model to deal with these two problems in a unified framework . The proposed method measures the intrinsic similarity between the target shape and the training shapes on an embedded manifold by manifold learning techniques . With this approach shapes in the training set can be selected according to their intrinsic similarity to the target image . With more accurate shape guidance an optimized search is performed by a deformable model to minimize an energy functional for image segmentation which is efficiently achieved by using dynamic programming . Our method has been validated on 2D prostate localization and 3D prostate segmentation in MRI scans . Compared to other existing methods our proposed method exhibits better performance in both studies . \\n'],\n",
              " [' Baseline correction is an important pre processing technique used to separate true spectra from interference effects or remove baseline effects . In this paper an adaptive iteratively reweighted genetic programming based on excellent community information is proposed to model baselines from spectra . Excellent community information which is abstracted from the present excellent community includes an automatic common threshold normal global and local slope information . Significant peaks can be firstly detected by an automatic common threshold . Then based on the characteristic that a baseline varies slowly with respect to wavelength normal global and local slope information are used to further confirm whether a point is in peak regions . Moreover the slope information is also used to determine the range of baseline curve fluctuation in peak regions . The proposed algorithm is more robust for different kinds of baselines and its curvature and slope can be automatically adjusted without prior knowledge . Experimental results in both simulated data and real data demonstrate the effectiveness of the algorithm . \\n'],\n",
              " [' We present a novel method that evaluates the geometric consistency of putative point matches in weakly calibrated settings i.e . when the epipolar geometry but not the camera calibration is known using only the point coordinates as information . The main idea behind our approach is the fact that each point correspondence in our data belongs to one of two classes . The classification of each point match relies on the histogram of a quantity representing the difference between cross ratios derived from a construction involving 6 tuples of point matches . Neither constraints nor scenario dependent parameters thresholds are needed . Even for few candidate point matches the ensemble of 6 tuples containing each of them turns to provide statistically reliable histograms that prove to discriminate between inliers and outliers . In fact in most cases a random sampling among this population is sufficient . Nevertheless the accuracy of the method is positively correlated to its sampling density leading to an accuracy versus resulting computational complexity trade off . Theoretical analysis and experiments are given that show the consistent performance of the proposed classification method when applied in inlier outlier discrimination . The achieved accuracy is favourably evaluated against established methods that employ geometric only information i.e . those relying on the Sampson the algebraic and the symmetric epipolar distances . Finally we also present an application of our scheme in uncalibrated stereo inside a RANSAC framework and compare it to the same as above methods . \\n'],\n",
              " [' This article presents a unified framework for detecting segmenting and tracking unknown objects in everyday scenes allowing for inspection of object hypotheses during interaction over time . A heterogeneous scene representation is proposed with background regions modeled as a combinations of planar surfaces and uniform clutter and foreground objects as 3D ellipsoids . Recent energy minimization methods based on loopy belief propagation tree reweighted message passing and graph cuts are studied for the purpose of multi object segmentation and benchmarked in terms of segmentation quality as well as computational speed and how easily methods can be adapted for parallel processing . One conclusion is that the choice of energy minimization method is less important than the way scenes are modeled . Proximities are more valuable for segmentation than similarity in colors while the benefit of 3D information is limited . It is also shown through practical experiments that with implementations on GPUs multi object segmentation and tracking using state of art MRF inference methods is feasible despite the computational costs typically associated with such methods . \\n'],\n",
              " [' Recently new high level features have been proposed to describe the semantic content of images . These features that we call supervised are obtained by exploiting the information provided by an additional set of labeled images . Supervised features were successfully used in the context of image classification and retrieval where they showed excellent results . In this paper we will demonstrate that they can be effectively used also for unsupervised image categorization that is for grouping semantically similar images . We have experimented different state of the art clustering algorithms on various standard data sets commonly used for supervised image classification evaluations . We have compared the results obtained by using four supervised features against those obtained by using low level features . The results show that supervised features exhibit a remarkable expressiveness which allows to effectively group images into the categories defined by the data sets authors . \\n'],\n",
              " [' In this paper we present the reconstructed residual error which evaluates the quality of a given segmentation of a reconstructed image in tomography . This novel evaluation method which is independent of the methods that were used to reconstruct and segment the image is applicable to segmentations that are based on the density of the scanned object . It provides a spatial map of the errors in the segmented image based on the projection data . The reconstructed residual error is a reconstruction of the difference between the recorded data and the forward projection of that segmented image . The properties and applications of the algorithm are verified experimentally through simulations and experimental micro CT data . The experiments show that the reconstructed residual error is close to the true error that it can improve gray level estimates and that it can help discriminating between different segmentations . \\n'],\n",
              " [' In this paper a framework is proposed to localize both Farsi Arabic and Latin scene texts with different sizes fonts and orientations . First candidate text regions are extracted via an MSER detector enhanced by weighted median filtering to adopt the low resolution texts . At the same time based on fuzzy inference system the input image is classified into images with a focused text content and incidental scene text images which the image does not focus on the text content . For the focused scene text images the non text candidates are filtered via an FIS . On the other hand for the incidental scene text images apart from the FIS an extra filtering algorithm based on low rank matrix recovery is proposed . Finally a new approach based on the clustering minimum area rectangle and radon transform techniques is proposed to create the single arbitrarily oriented text lines from the remaining text regions . To evaluate the proposed algorithm we created a collection of natural images containing both Farsi Arabic and Latin texts . Compared with the state of the art methods the proposed method achieves the best performance on our and Epshtein datasets and competitive performances on the ICDAR dataset . \\n'],\n",
              " [' In this paper a new pipeline of structure from motion for ground view images is proposed that uses feature points on an aerial image as references for removing accumulative errors . The challenge here is to design a method for discriminating correct matches from unreliable matches between ground view images and an aerial image . If we depend on only local image features it is not possible in principle to remove all the incorrect matches because there frequently exist repetitive and or similar patterns such as road signs . In order to overcome this difficulty we employ geometric consistency verification of matches using the RANSAC scheme that comprises two stages sampling based local verification focusing on the orientation and scale information extracted by a feature descriptor and global verification using camera poses estimated by the bundle adjustment using sampled matches . \\n'],\n",
              " [' This paper proposes methods for people re identification across non overlapping cameras . We improve the robustness of re identification by using additional group features acquired from the groups of people detected by each camera . People are grouped by discriminatively classifying the spatio temporal features of their trajectories into those of grouped people and non grouped people . Thereafter three group features are obtained in each group and utilized with other general features of each person for people re identification . Our experimental results have demonstrated improvements in people grouping and people re identification when our proposed methods have been applied to a public dataset . \\n'],\n",
              " [' In the object recognition community much effort has been spent on devising expressive object representations and powerful learning strategies for designing effective classifiers capable of achieving high accuracy and generalization . In this scenario the focus on the training sets has been historically weak by and large training sets have been generated with a substantial human intervention requiring considerable time . In this paper we present a strategy for automatic training set generation . The strategy uses semantic knowledge coming from WordNet coupled with the statistical power provided by Google Ngram to select a set of meaningful text strings related to the text class label that are subsequently fed into the Google Images search engine producing sets of images with high training value . Focusing on the classes of different object recognition benchmarks our approach collects novel training images compared to the ones obtained by exploiting Google Images with the simple text class label . In particular we show that the gathered images are better able to capture the different visual facets of a concept thus encoding in a more successful manner the intra class variance . As a consequence training standard classifiers with this data produces performances not too distant from those obtained from the classical hand crafted training sets . In addition our datasets generalize well and are stable that is they provide similar performances on diverse test datasets . This process does not require manual intervention and is completed in a few hours . \\n'],\n",
              " [' In query by semantic example image retrieval images are ranked by similarity of semantic descriptors . These descriptors are obtained by classifying each image with respect to a pre defined vocabulary of semantic concepts . In this work we consider the problem of improving the accuracy of semantic descriptors through cross modal regularization based on auxiliary text . A cross modal regularizer composed of three steps is proposed . Training images and text are first mapped to a common semantic space . A regularization operator is then learned for each concept in the semantic vocabulary . This is an operator which maps the semantic descriptors of images labeled with that concept to the descriptors of the associated texts . A convex formulation of the learning problem is introduced enabling the efficient computation of concept specific regularization operators . The third step is the selection of the most suitable operator for the image to regularize . This is implemented through a quantization of the semantic space where a regularization operator is associated with each quantization cell . Overall the proposed regularizer is a non linear mapping implemented as a piecewise linear transformation of the semantic image descriptors to regularize . This transformation is a form of cross modal domain adaptation . It is shown to achieve better performance than recent proposals in the domain adaptation literature while requiring much simpler optimization . \\n'],\n",
              " [' Recently head pose estimation in real world environments has been receiving attention in the computer vision community due to its applicability to a wide range of contexts . However this task still remains as an open problem because of the challenges presented by real world environments . The focus of most of the approaches to this problem has been on estimation from single images or video frames without leveraging the temporal information available in the entire video sequence . Other approaches frame the problem in terms of classification into a set of very coarse pose bins . In this paper we propose a hierarchical graphical model that probabilistically estimates continuous head pose angles from real world videos by leveraging the temporal pose information over frames . The proposed graphical model is a general framework which is able to use any type of feature and can be adapted to any facial classification task . Furthermore the framework outputs the entire pose distribution for a given video frame . This permits robust temporal probabilistic fusion of pose information over the video sequence and also probabilistically embedding the head pose information into other inference tasks . Experiments on large real world video sequences reveal that our approach significantly outperforms alternative state of the art pose estimation methods . The proposed framework is also evaluated on gender and facial hair estimation . By incorporating pose information into the proposed hierarchical temporal graphical mode superior results are achieved for attribute classification tasks . \\n'],\n",
              " [' We present a method for efficiently generating dense relative depth estimates from video without requiring any knowledge of the imaging system either a priori or by estimating it during processing . Instead we only require that the epipolar constraint between any two frames is satisfied and that the fundamental matrix can be estimated . By tracking sparse features across many frames and aggregating the multiple depth estimates together we are able to improve the overall estimate for any given frame . Once the depth estimates are available we treat the generation of the depth maps as a label propagation problem . This allows us to combine the automatically generated depth maps with any user corrections and modifications . \\n'],\n",
              " [' Tagging is nowadays the most prevalent and practical way to make images searchable . However in reality many manually assigned tags are irrelevant to image content and hence are not reliable for applications . A lot of recent efforts have been conducted to refine image tags . In this paper we propose to do tag refinement from the angle of topic modeling and present a novel graphical model regularized latent Dirichlet allocation . In the proposed approach tag similarity and tag relevance are jointly estimated in an iterative manner so that they can benefit from each other and the multi wise relationships among tags are explored . Moreover both the statistics of tags and visual affinities of images in the corpus are explored to help topic modeling . We also analyze the superiority of our approach from the deep structure perspective . The experiments on tag ranking and image retrieval demonstrate the advantages of the proposed method . \\n'],\n",
              " [' The interdigital palm region represents about 30 of the palm area and is inherently acquired during palmprint imaging nevertheless it has not yet attracted any noticeable attention in biometrics research . This paper investigates the ridge pattern characteristics of the interdigital palm region for its usage in biometric identification . An anatomical study of the interdigital area is initially carried out leading to the establishment of five categories according to the distribution of the singularities and three regions of interest for biometrics . With the identified regions our study analyzes the matching performance of the interdigital palm biometrics and its combination with the conventional palmprint matching approaches and presents comparative experimental results using four competing feature extraction methods . This study has been carried out with two publicly available databases . The first one consists of 2 080 images of 416 subjects acquired with a touchless low cost imaging device focused on acquiring the interdigital palm area . The second database is the publicly available BiosecurID hand database which consists of 3 200 images from 400 users . The experimental results presented in this paper suggest that features from the interdigital palm region can be used to achieve competitive performances as well as offer significant improvements for conventional palmprint recognition . \\n'],\n",
              " [' Recent developments in low cost CMOS cameras have created the opportunity of bringing imaging capabilities to sensor networks and a new field called visual sensor networks has emerged . VSNs consist of image sensors embedded processors and wireless transceivers which are powered by batteries . Since energy and bandwidth resources are limited setting up a tracking system in VSNs is a challenging problem . In this paper we present a framework for human tracking in VSN environments . The traditional approach of sending compressed images to a central node has certain disadvantages such as decreasing the performance of further processing because of low quality images . Instead in our decentralized tracking framework each camera node performs feature extraction and obtains likelihood functions . We propose a sparsity driven method that can obtain bandwidth efficient representation of likelihoods extracted by the camera nodes . Our approach involves the design of special overcomplete dictionaries that match the structure of the likelihoods and the transmission of likelihood information in the network through sparse representation in such dictionaries . We have applied our method for indoor and outdoor people tracking scenarios and have shown that it can provide major savings in communication bandwidth without significant degradation in tracking performance . We have compared the tracking results and communication loads with a block based likelihood compression scheme a decentralized tracking method and a distributed tracking method . Experimental results show that our sparse representation framework is an effective approach that can be used together with any probabilistic tracker in VSNs . \\n'],\n",
              " [' Computer vision is hard because of a large variability in lighting shape and texture in addition the image signal is non additive due to occlusion . Generative models promised to account for this variability by accurately modelling the image formation process as a function of latent variables with prior beliefs . Bayesian posterior inference could then in principle explain the observation . While intuitively appealing generative models for computer vision have largely failed to deliver on that promise due to the difficulty of posterior inference . As a result the community has favoured efficient discriminative approaches . We still believe in the usefulness of generative models in computer vision but argue that we need to leverage existing discriminative or even heuristic computer vision methods . We implement this idea in a principled way with an informed sampler and in careful experiments demonstrate it on challenging generative models which contain renderer programs as their components . We concentrate on the problem of inverting an existing graphics rendering engine an approach that can be understood as Inverse Graphics . The informed sampler using simple discriminative proposals based on existing computer vision technology achieves significant improvements of inference . \\n'],\n",
              " [' Classical image segmentation techniques in computer vision exploit visual cues such as image edges lines color and texture . Due to the complexity of real scenarios the main challenge is achieving meaningful segmentation of the imaged scene since real objects have substantial discontinuities in these visual cues . In this paper a new focus based perceptual cue is introduced the focus signal . The focus signal captures the variations of the focus level of every image pixel as a function of time and is directly related to the geometry of the scene . In a practical application a sequence of images corresponding to an autofocus sequence is processed in order to infer geometric information of the imaged scene using the focus signal . This information is integrated with the segmentation obtained using classical cues such as color and texture in order to yield an improved scene segmentation . Experiments have been performed using different off the shelf cameras including a webcam a compact digital photography camera and a surveillance camera . Obtained results using Dice s similarity coefficient and the pixel labeling error show that a significant improvement in the final segmentation can be achieved by incorporating the information obtained from the focus signal in the segmentation process . \\n'],\n",
              " ['With the rapidly increasing demands from surveillance and security industries crowd behaviour analysis has become one of the hotly pursued video event detection frontiers within the computer vision arena in recent years. This research has investigated innovative crowd behaviour detection approaches based on statistical crowd features extracted from video footages. In this paper a new crowd video anomaly detection algorithm has been developed based on analysing the extracted spatio temporal textures. The algorithm has been designed for real time applications by deploying low level statistical features and alleviating complicated machine learning and recognition processes. In the experiments the system has been proven a valid solution for detecting anomaly behaviours without strong assumptions on the nature of crowds for example subjects and density. The developed prototype shows improved adaptability and efficiency against chosen benchmark systems. \\n'],\n",
              " [' With the advent of the digital camera a common popular imaging processing is high dynamic range that aims to overcome the technological limitations of the irradiance sensor dynamic range . In this paper we will present a new method to combine low dynamic range images for HDR processing . This method is based on the theory of evidence . Without a prior knowledge of the sensor intrinsic parameters and no extra data it allows to locally maximizing the signal to noise ratio over the entire acquisition dynamic . In addition our method is less sensitive to object or people in motion into the scene that are causing ghost like artifacts with the conventional methods . This technique require that the camera be absolutely still between exposures or need a translational alignment . Simulation and experimental results are presented to demonstrate both the accuracy and efficiency of our algorithm . \\n'],\n",
              " [' We present a passive forensics method to distinguish photorealistic computer graphics from natural images . The goals of our work are to improve the detection accuracy and the robustness to content preserving image manipulations . In the proposed method Homomorphic filtering is used to highlight the detail information of image . We find that the texture changes are different between photographs and PRCG images under same Homomorphic filtering transformation and then we use the difference matrixes to describe the differences of texture changes . We define a customized statistical feature named texture similarity and combine it with the statistical features extracted from the co occurrence matrixes of differential matrixes to construct forensics features . Then we develop a statistical model and use SVM as classifier to distinguish PRCG from photographs . Experimental results show that the proposed method enjoys following advantages Proposed method reaches higher detection accuracy synchronously it is robust to tolerate content preserving manipulations such as JPEG compression adding noise histogram equalization and filtering . Proposed method is provided with satisfactory generalization capability it will be available when the training samples and the testing samples come from different sources . \\n'],\n",
              " [' The minimum barrier distance MBD introduced recently in is a pseudo metric defined on a compact subset D of the Euclidean space and whose values depend on a fixed map f from D into . The MBD is defined as the minimal value of the barrier strength of a path between the points which constitutes the length of the smallest interval containing all values of f along the path . In this paper we present a polynomial time algorithm that provably calculates the exact values of MBD for the digital images . We compare this new algorithm theoretically and experimentally with the algorithm presented in which computes the approximate values of the MBD . Moreover we notice that every generalized distance function can be naturally translated to an image segmentation algorithm . The algorithms that fall under such category include Relative Fuzzy Connectedness and those associated with the minimum barrier fuzzy distance and geodesic distance functions . In particular we compare experimentally these four algorithms on the 2D and 3D natural and medical images with known ground truth and at varying level of noise blur and inhomogeneity . \\n'],\n",
              " [' Appearance model is a key part of tracking algorithms . To attain robustness many complex appearance models are proposed to capture discriminative information of object . However such models are difficult to maintain accurately and efficiently . In this paper we observe that hashing techniques can be used to represent object by compact binary code which is efficient for processing . However during tracking online updating hash functions is still inefficient with large number of samples . To deal with this bottleneck a novel hashing method called two dimensional hashing is proposed . In our tracker samples and templates are hashed to binary matrices and the hamming distance is used to measure confidence of candidate samples . In addition the designed incremental learning model is applied to update hash functions for both adapting situation change and saving training time . Experiments on our tracker and other eight state of the art trackers demonstrate that the proposed algorithm is more robust in dealing with various types of scenarios . \\n'],\n",
              " [' This paper describes a method of gait recognition by suppressing and using gait fluctuations . Inconsistent phasing between a matching pair of gait image sequences because of temporal fluctuations degrades the performance of gait recognition . We remove the temporal fluctuations by generating a phase normalized gait image sequence with equal phase intervals . If inter period gait fluctuations within a gait image sequence are repeatedly observed for the same subject they can be regarded as a useful distinguishing gait feature . We extract phase fluctuations as temporal fluctuations as well as gait fluctuation image and trajectory fluctuations as spatial fluctuations . We combine them with the matching score using the phase normalized image sequence as additional matching scores in the score level fusion framework or as quality measures in the score normalization framework . We evaluated the methods in experiments using large scale publicly available databases and showed the effectiveness of the proposed methods . \\n'],\n",
              " [' This paper presents a disparity calculation algorithm based on stereo vision for obstacle detection and free space calculation . This algorithm incorporates line segmentation multi pass aggregation and efficient local optimisation in order to produce accurate disparity values . It is specifically designed for traffic scenes where most of the objects can be represented by planes in the disparity domain . The accurate horizontal disparity gradient for the side planes are also extracted during the disparity optimisation stage . Then an obstacle detection algorithm based on the U V disparity is introduced . Instead of using the Hough transform for line detection which is extremely sensitive to the parameter settings the G disparity image is proposed for the detection of side planes . Then the vertical planes are detected separately after removing all the side planes . Faster detection speed lower parameter sensitivity and improved performance are achieved comparing with the Hough transform based detection . After the obstacles are located and removed from the disparity map most of the remaining pixels are projections from the road surface . Using a spline as the road model the vertical profile of the road surface is estimated . Finally the free space is calculated based on the vertical road profile which is not restricted by the planar road surface assumption . \\n'],\n",
              " [' Would it be possible to automatically associate ancient pictures to modern ones and create fancy cultural heritage city maps We introduce here the task of recognizing the location depicted in an old photo given modern annotated images collected from the Internet . We present an extensive analysis on different features looking for the most discriminative and most robust to the image variability induced by large time lags . Moreover we show that the described task benefits from domain adaptation . \\n'],\n",
              " [' This paper presents a multiview model of object categories generally applicable to virtually any type of image features and methods to efficiently perform in a unified manner detection localization and continuous pose estimation in novel scenes . We represent appearance as distributions of low level fine grained image features . Multiview models encode the appearance of objects at discrete viewpoints and in addition how these viewpoints deform into one another as the viewpoint continuously varies . Using a measure of similarity between an arbitrary test image and such a model at chosen viewpoints we perform all tasks mentioned above with a common method . We leverage the simplicity of low level image features such as points extracted along edges or coarse scale gradients extracted densely over the images by building probabilistic templates i.e . distributions of features learned from one or several training examples . We efficiently handle these distributions with probabilistic techniques such as kernel density estimation Monte Carlo integration and importance sampling . We provide an extensive evaluation on a wide variety of benchmark datasets . We demonstrate performance on the ETHZ Shape dataset with single and multiple training examples well above baseline methods on par with a number of more task specific methods . We obtain remarkable performance on the recognition of more complex objects notably the cars of the 3D Object dataset of Savarese et al . with detection rates of 92.5 and an accuracy in pose estimation of 91 . We perform better than the state of the art on continuous pose estimation with the rotating cars dataset of Ozuysal et al . We also demonstrate particular capabilities with a novel dataset featuring non textured objects of undistinctive shapes the pose of which can only be determined from shading captured here by coarse scale intensity gradients . This paper is concerned with the joint recognition and pose estimation of object categories in 2D images . Recognizing that these two tasks represent two sides of a same problem we tackle them in a unified approach . In general the pose of objects can not be inferred from just one type of image information e.g . silhouette and edges to cite a common example . Additional visual cues may be necessary such as the shading onto the object surface . A key point of our contributions is thus to provide techniques generally applicable in this regard even to low level dense and or non descriptive image features . To perform continuous pose estimation our object model captures in addition to the appearance at discrete training viewpoints the deformations between these detected from the optical flow between training examples . A measure of similarity between generated views of the object and a test image allows us to perform detection recognition and pose estimation in a unified manner . The following paragraphs present the principal motivations and key points of the method comparing them to existing related work . Parts of these contributions were introduced in earlier publications . The. \\n'],\n",
              " [' Radio frequency identification technology has been used in manufacturing industries to create a RFID enabled ubiquitous environment in where ultimate real time advanced production planning and scheduling will be achieved with the goal of collective intelligence . A particular focus has been placed upon using the vast amount of RFID production shop floor data to obtain more precise and reasonable estimates of APPS parameters such as the arrival of customer orders and standard operation times . The resulting APPS model is based on hierarchical production decision making principle to formulate planning and scheduling levels . A RFID event driven mechanism is adopted to integrate these two levels for collective intelligence . A heuristic approach using a set of rules is utilized to solve the problem . The model is tested through four dimensions including the impact of rule sequences on decisions evaluation of released strategy to control the amount of production order from planning to scheduling comparison with another model and practical operations as well as model robustness . Two key findings are observed . First release strategy based on the RFID enabled real time information is efficient and effective to reduce the total tardiness by 44.46 averagely . Second it is observed that the model has the immune ability on disturbances like defects . However as the increasing of the problem size the model robustness against emergency orders becomes weak while the resistance to machine breakdown is strong oppositely . Findings and observations are summarized into a number of managerial implications for guiding associated end users for purchasing collective intelligence in practice . \\n'],\n",
              " [' Recognizing scene text is a challenging problem even more so than the recognition of scanned documents . This problem has gained significant attention from the computer vision community in recent years and several methods based on energy minimization frameworks and deep learning approaches have been proposed . In this work we focus on the energy minimization framework and propose a model that exploits both bottom up and top down cues for recognizing cropped words extracted from street images . The bottom up cues are derived from individual character detections from an image . We build a conditional random field model on these detections to jointly model the strength of the detections and the interactions between them . These interactions are top down cues obtained from a lexicon based prior i.e . language statistics . The optimal word represented by the text image is obtained by minimizing the energy function corresponding to the random field model . We evaluate our proposed algorithm extensively on a number of cropped scene text benchmark datasets namely Street View Text ICDAR 2003 2011 and 2013 datasets and IIIT 5K word and show better performance than comparable methods . We perform a rigorous analysis of all the steps in our approach and analyze the results . We also show that state of the art convolutional neural network features can be integrated in our framework to further improve the recognition performance . \\n'],\n",
              " [' We propose an automated framework for predicting gestational age and neurodevelopmental maturation of a fetus based on 3D ultrasound brain image appearance . Our method capitalizes on age related sonographic image patterns in conjunction with clinical measurements to develop for the first time a predictive age model which improves on the GA prediction potential of US images . The framework benefits from a manifold surface representation of the fetal head which delineates the inner skull boundary and serves as a common coordinate system based on cranial position . This allows for fast and efficient sampling of anatomically corresponding brain regions to achieve like for like structural comparison of different developmental stages . We develop bespoke features which capture neurosonographic patterns in 3D images and using a regression forest classifier we characterize structural brain development both spatially and temporally to capture the natural variation existing in a healthy population 447 over an age range of active brain maturation . On a routine clinical dataset 187 our age prediction results strongly correlate with true GA 0.98 accurate within 6.10 days confirming the link between maturational progression and neurosonographic activity observable across gestation . Our model also outperforms current clinical methods by 4.57 days in the third trimester a period complicated by biological variations in the fetal population . Through feature selection the model successfully identified the most age discriminating anatomies over this age range as being the Sylvian fissure cingulate and callosal sulci . \\n'],\n",
              " [' Common to much work on land cover classification in multispectral imagery is the use of single satellite images for training the classifiers for the different land types . Unfortunately more often than not decision boundaries derived in this manner do not extrapolate well from one image to another . This happens for several reasons most having to do with the fact that different satellite images correspond to different view angles on the earth s surface different sun angles different seasons and so on . In this paper we get around these limitations of the current state of the art by first proposing a new integrated representation for all of the images overlapping and non overlapping that cover a large geographic ROI . In addition to helping understand the data variability in the images this representation also makes it possible to create the ground truth that can be used for ROI based wide area learning of the classifiers . We use this integrated representation in a new Bayesian framework for data classification that is characterized by learning of the decision boundaries from a sampling of all the satellite data available for an entire geographic ROI probabilistic modeling of within class and between class variations as opposed to the more traditional probabilistic modeling of the feature vectors extracted from the measurement data and using variance based ML and MAP classifiers whose decision boundary calculations incorporate all of the multi view data for a geographic point if that point is selected for learning and testing . We show results with the new classification framework for an ROI in Chile whose size is roughly 10 000 square kilometers . This ROI is covered by 189 satellite images with varying degrees of overlap . We compare the classification performance of the proposed ROI based framework with the results obtained by extrapolating the decision boundaries learned from a single image to the entire ROI . Using a 10 fold cross validation test we demonstrate significant increases in the classification accuracy for five of the six land cover classes . In addition we show that our variance based Bayesian classifier outperforms a traditional Support Vector Machine based approach to classification for four out of six classes . \\n'],\n",
              " [' In this paper we cast multi target tracking as a dense subgraph discovering problem on the undirected relation graph of all given target hypotheses . We aim to extract multiple clusters in which each cluster contains a set of hypotheses of one particular target . In the presence of occlusion or similar moving targets or when there is no reliable evidence for the target s presence each target trajectory is expected to be fragmented into multiple tracklets . The proposed tracking framework can efficiently link such fragmented target trajectories to build a longer trajectory specifying the true states of the target . In particular a discriminative scheme is devised via learning the targets appearance models . Moreover the smoothness characteristic of the target trajectory is utilised by suggesting a smoothness tracklet affinity model to increase the power of the proposed tracker to produce persistent target trajectories revealing different targets moving paths . The performance of the proposed approach has been extensively evaluated on challenging public datasets and also in the context of team sports where team players tend to exhibit quick and unpredictable movements . Systematic experimental results conducted on a large set of sequences show that the proposed approach performs better than the state of the art trackers in particular when dealing with occlusion and fragmented target trajectory . \\n'],\n",
              " [' The performance of physical assets has become a major determinant success factor for urban flood control . However managing these assets is always challenging as there are a huge number of diverse assets involved which are distributed throughout the city and owned by different agencies . Aiming at improving the management efficiency of these assets and ensuring their performance this paper proposes the concept of cloud asset based on cloud computing mobile agent and various smart devices . Through hardware integration and software encapsulation cloud asset could sense its real time status adapt to varied working scenarios be controlled remotely and shared among agencies . It enables accurate real time control of every asset and thus improves the management efficiency and effectiveness . This paper first presents the concept of cloud asset with its technical architecture and then analyses the software agent model for cloud asset which is the key enabler to realize UPnP management of assets and provides mobility and intelligence for them . After that the framework of cloud asset enabled workflow management is built in which cloud asset could be easily found and dynamically invoked by different workflows . Finally a demonstrative case is provided to verify the effectiveness of cloud asset . \\n'],\n",
              " [' A growing conceptual and empirical literature is advancing the idea that language extends our cognitive skills . One of the most influential positions holds that language qua material symbols facilitates individual thought processes by virtue of its material properties . Extending upon this model we argue that language enhances our cognitive capabilities in a much more radical way the skilful engagement of public material symbols facilitates evolutionarily unprecedented modes of collective perception action and reasoning creating dialogically extended minds . We relate our approach to other ideas about collective minds and review a number of empirical studies to identify the mechanisms enabling the constitution of interpersonal cognitive systems . \\n'],\n",
              " [' Monocular plenoptic cameras are slightly modified off the shelf cameras that have novel capabilities as they allow for truly passive high resolution range sensing through a single camera lens . Commercial plenoptic cameras however are presently delivering range data in non metric units which is a barrier to novel applications e.g . in the realm of robotics . In this work we revisit the calibration of focused plenoptic cameras and bring forward a novel approach that leverages traditional methods for camera calibration in order to deskill the calibration procedure and to increase accuracy . First we detach the estimation of parameters related to either brightness images or depth data . Second we present novel initialization methods for the parameters of the thin lens camera model the only information required for calibration is now the size of the pixel element and the geometry of the calibration plate . The accuracy of the calibration results corroborates our belief that monocular plenoptic imaging is a disruptive technology that is capable of conquering new markets as well as traditional imaging domains . \\n'],\n",
              " [' Sensors are becoming ubiquitous in everyday life generating data at an unprecedented rate and scale . However models that assess impacts of human activities on environmental and human health have typically been developed in contexts where data scarcity is the norm . Models are essential tools to understand processes identify relationships associations and causality formalize stakeholder mental models and to quantify the effects of prevention and interventions . They can help to explain data as well as inform the deployment and location of sensors by identifying hotspots and areas of interest where data collection may achieve the best results . We identify a paradigm shift in how the integration of models and sensors can contribute to harnessing Big Data and more importantly make the vital step from Big Data to Big Information . In this paper we illustrate current developments and identify key research needs using human and environmental health challenges as an example . \\n'],\n",
              " [' This paper proposes a new extended Process to Product Modeling method for integrated and seamless information delivery manual and model view definition development . Current IDM development typically uses Business Process Modeling Notation to represent a process map . Exchange requirements and functional parts specify the information required when information is exchanged between different activities . A set of information requirements specifically defined as a subset of Industry Foundation Classes is called an MVD . Currently however PMs ERs FPs and MVDs are developed as separate documents through independent development steps . Moreover even though ERs and FPs are designed to be reused tracking and reusing the ERs and FPs developed by others is practically impossible . The xPPM method is proposed to provide a tight connection between PMs ERs FPs and MVDs and to improve the reusability of predefined ERs and FPs . The theoretical framework is based on the approach of the Georgia Tech Process to Product Modeling to suit the IDM development process . An xPPM tool is developed and the validity of xPPM is analyzed through the reproduction of existing IDMs and MVDs . The benefits and limitations of xPPM and lessons from the applicability tests are discussed . \\n'],\n",
              " [' Modeling the energy performance of existing buildings enables quick identification and reporting of potential areas for building retrofit . However current modeling practices of using energy simulation tools do not model the energy performance of buildings at their element level . As a result potential retrofit candidates caused by construction defects and degradations are not represented . Furthermore due to manual modeling and calibration processes their application is often time consuming . Current application of 2D thermography for building diagnostics is also facing several challenges due to a large number of unordered and non geo tagged images . To address these limitations this paper presents a new computer vision based method for automated 3D energy performance modeling of existing buildings using thermal and digital imagery captured by a single thermal camera . First using a new image based 3D reconstruction pipeline which consists of Graphic Processing Unit based Structure from Motion and Multi View Stereo algorithms the geometrical conditions of an existing building is reconstructed in 3D . Next a 3D thermal point cloud model of the building is generated by using a new 3D thermal modeling algorithm . This algorithm involves a one time thermal camera calibration deriving the relative transformation by forming the Epipolar geometry between thermal and digital images and the MVS algorithm for dense reconstruction . By automatically superimposing the 3D building and thermal point cloud models 3D spatio thermal models are formed which enable the users to visualize query and analyze temperatures at the level of 3D points . The underlying algorithms for generating and visualizing the 3D spatio thermal models and the 3D registered digital and thermal images are presented in detail . The proposed method is validated for several interior and exterior locations of a typical residential building and an instructional facility . The experimental results show that inexpensive digital and thermal imagery can be converted into ubiquitous reporters of the actual energy performance of existing buildings . The proposed method expedites the modeling process and has the potential to be used as a rapid and robust building diagnostic tool . \\n'],\n",
              " [' We propose a framework for the robust and fully automatic segmentation of magnetic resonance brain images called Multi Atlas Label Propagation with Expectation Maximisation based refinement . The presented approach is based on a robust registration approach highly performant label fusion and intensity based label refinement using EM . We further adapt this framework to be applicable for the segmentation of brain images with gross changes in anatomy . We propose to account for consistent registration errors by relaxing anatomical priors obtained by multi atlas propagation and a weighting scheme to locally combine anatomical atlas priors and intensity refined posterior probabilities . The method is evaluated on a benchmark dataset used in a recent MICCAI segmentation challenge . In this context we show that MALP EM is competitive for the segmentation of MR brain scans of healthy adults when compared to state of the art automatic labelling techniques . To demonstrate the versatility of the proposed approach we employed MALP EM to segment 125 MR brain images into 134 regions from subjects who had sustained traumatic brain injury . We employ a protocol to assess segmentation quality if no manual reference labels are available . Based on this protocol three independent blinded raters confirmed on 13 MR brain scans with pathology that MALP EM is superior to established label fusion techniques . We visually confirm the robustness of our segmentation approach on the full cohort and investigate the potential of derived symmetry based imaging biomarkers that correlate with and predict clinically relevant variables in TBI such as the Marshall Classification or Glasgow Outcome Score . Specifically we show that we are able to stratify TBI patients with favourable outcomes from non favourable outcomes with 64.7 accuracy using acute phase MR images and 66.8 accuracy using follow up MR images . Furthermore we are able to differentiate subjects with the presence of a mass lesion or midline shift from those with diffuse brain injury with 76.0 accuracy . The thalamus putamen pallidum and hippocampus are particularly affected . Their involvement predicts TBI disease progression . \\n'],\n",
              " [' Natural and intuitive human interaction with robotic systems is a key point to develop robots assisting people in an easy and effective way . In this paper a Human Robot Interaction system able to recognize gestures usually employed in human non verbal communication is introduced and an in depth study of its usability is performed . The system deals with dynamic gestures such as waving or nodding which are recognized using a Dynamic Time Warping approach based on gesture specific features computed from depth maps . A static gesture consisting in pointing at an object is also recognized . The pointed location is then estimated in order to detect candidate objects the user may refer to . When the pointed object is unclear for the robot a disambiguation procedure by means of either a verbal or gestural dialogue is performed . This skill would lead to the robot picking an object in behalf of the user which could present difficulties to do it by itself . The overall system which is composed by a NAO and Wifibot robots a KinectTM v2 sensor and two laptops is firstly evaluated in a structured lab setup . Then a broad set of user tests has been completed which allows to assess correct performance in terms of recognition rates easiness of use and response times . \\n'],\n",
              " [' Background estimation in video consists in extracting a foreground free image from a set of training frames . Moving and stationary objects may affect the background visibility thus invalidating the assumption of many related literature where background is the temporal dominant data . In this paper we present a temporal spatial block level approach for background estimation in video to cope with moving and stationary objects . First a Temporal Analysis module obtains a compact representation of the training data by motion filtering and dimensionality reduction . Then a threshold free hierarchical clustering determines a set of candidates to represent the background for each spatial location . Second a Spatial Analysis module iteratively reconstructs the background using these candidates . For each spatial location multiple reconstruction hypotheses are explored to obtain its neighboring locations by enforcing inter block similarities and intra block homogeneity constraints in terms of color discontinuity color dissimilarity and variability . The experimental results show that the proposed approach outperforms the related state of the art over challenging video sequences in presence of moving and stationary objects . \\n'],\n",
              " [' Graphical abstract Situation 1 An engineer has an incomplete dataset about the quantity and quality of the influent wastewater . She has some ideas about how the influent of the plant should be . She might also have some data about the catchment area . Situation 2 An engineer has information enough about the quantity and quality of the influent wastewater . She would like to translate that into some ASM family model state variables . This is the so called characterisation problem . Situation 3 The engineer has information enough about the quantity and quality of the influent wastewater . However that is only a single realisation of the problem . She would like to feature the uncertainty around these data and to generate other similar influent profiles . a Is there any method to interpolate values so as to increase the frequency of some given WWTP influent data profiles Yes the method of Devisscher et al . based on the use of databases or the ones of Langergraber et al . or Manina et al . based on the use of harmonic functions can help to complete some scarce datasets . Also the phenomenological model of Gernaey et al . has been used with this propose . b Is there any tool to derive influent WWTP data given some properties about the catchment area or the distribution of emission sources Yes the generator of Gernaey er al . provides influent data for urban WWTPs given the population and catchment area characteristics . The generator of De Keyser et al . provides emission profiles expected from a given urban population . The former describes the effect of soil first flush sewer system etc . while the later is only concerned about the very source emission patterns providing profiles that can be used as inputs for sewer WWTP or surface water models . c Are there some benchmark influent data profiles or simple models to generate influent data for urban WWTPs Influent profiles for a WWTP of 100 000 PE are available at http www.benchmarkwwtp.org including dry rain and storm weather conditions . The models of Langergraber et al . or Mannina et al . provide influent profiles for dry weather conditions . The former is accompanied by a set of parameters among which the modeller can select those corresponding to the desired size of tin WWTP under study . d Which would be the main references or tools to improve the understanding of the generating mechanisms in a given catchment area and their relationship with the WWTP influent data Interesting results can be found in the research work conducted by Butler et al . Friedler et al . or Almeida et al . about domestic wastewater generation . This research has been followed by the EU project ScorePP and the work of Ort et al . analysing the most general patterns of daily weekly and yearly emissions . Another good manner of proceed is to use the phenomenological model of. \\n'],\n",
              " [' Graphical abstract MODSIM MODFLOW MODSIM MODFLOW is the integration of two widely used and freely available codes used within the field of water resource planning and management . The nature of the coupling in MODSIM MODFLOW sets it apart from other river operations hydrologic model couplings in that the codes iterate at the time step level and share information via computer memory . Users are required to construct working MODSIM and MODFLOW models prior to using the integrated code and must adhere to each code s input file formatting requirements . Using the custom code interface available with MODSIM the models are integrated by mapping surface water features represented in both models to one another . Eric Morway with MODFLOW contributions from Rich Niswonger and MODSIM contributions from Enrique Triana Research supported by grant from the Water Sustainability Climate Program jointly funded by the National Science Foundation and U.S. Department of Agriculture National Institute of Food Agriculture and by the U.S. Geological Survey s Groundwater Resources Program . C C Fortran Windows with.NET Framework 3.5 installed Download MODSIM from http modsim.engr.colostate.edu . A version of MODFLOW compiled as dynamic link library and required by the integrated code is available upon request from the corresponding author . Finally users will need to download and install the C redistributable package for Microsoft Visual Studio 2012 available from Microsoft . Inevitably as each generation must learn the land and the waters will instruct us in the ways of community . Justice Gregory J. Hobbs Jr . \\n'],\n",
              " [' Humic substances are ubiquitous in the environment and have manifold functions . While their composition is well known information on the chemical structure and three dimensional conformation is scarce . Here we describe the Vienna Soil Organic Matter Modeler which is an online tool to generate condensed phase computer models of humic substances . Many different models can be created that reflect the diversity in composition and conformations of the constituting molecules . To exemplify the modeler 18 different models are generated based on two experimentally determined compositions to explicitly study the effect of varying e.g . the amount of water molecules in the models or the pH . Molecular dynamics simulations were performed on the models which were subsequently analyzed in terms of structure interactions and dynamics linking macroscopic observables to the microscopic composition of the systems . We are convinced that this new tool opens the way for a wide range of in silico studies on soil organic matter . \\n'],\n",
              " [' Simulation modelling in ecology is a field that is becoming increasingly compartmentalized . Here we propose a Database Approach To Modelling to create unity in dynamical ecosystem modelling with differential equations . In this approach the storage of ecological knowledge is independent of the language and platform in which the model will be run . To create an instance of the model the information in the database is translated and augmented with the language and platform specifics . This process is automated so that a new instance can be created each time the database is updated . We describe the approach using the simple Lotka Volterra model and the complex ecosystem model for shallow lakes PCLake which we automatically implement in the frameworks OSIRIS GRIND for MATLAB ACSL R DUFLOW and DELWAQ . A clear advantage of working in a database is the overview it provides . The simplicity of the approach only adds to its elegance . \\n'],\n",
              " [' Predicting the probability of wind damage in both natural and managed forests is important for understanding forest ecosystem functioning the environmental impact of storms and for forest risk management . We undertook a thorough validation of three versions of the hybrid mechanistic wind risk model ForestGALES and a statistical logistic regression model against observed damage in a Scottish upland conifer forest following a major storm . Statistical analysis demonstrated that increasing tree height and local wind speed during the storm were the main factors associated with increased damage levels . All models provided acceptable discrimination between damaged and undamaged forest stands but there were trade offs between the accuracy of the mechanistic models and model bias . The two versions of the mechanistic model with the lowest bias gave very comparable overall results at the forest scale and could form part of a decision support system for managing forest wind damage risk . Maximum width of canopy Length of the live crown Drag coefficient scale parameter Drag coefficient Regression between stem weight and resistance to overturning Critical wind speed for damage Zero plane displacement Stem diameter at base of tree Stem diameter at breast height Average spacing between trees Windiness score from Quine and White Dimensionless factor to account for additional turning moment due to crown and stem weight Dimensionless factor to account for reduction in clear wood MOR due to knots Dimensionless factor to account for gustiness of wind Tree height von Karman constant 0.4 Maximum turning moment due to wind loading only and not including additional moment due to overhanging crown and stem Modulus of rupture on wood for species of interest Parameter controlling reduction in drag coefficient with wind speed Density of air Forestry Commission sub compartment database Ratio of average tree spacing after and before a thinning Stem weight Turning moment coefficient from Hale et al . Ratio of turning moment coefficient after and before thinning Wind speed at 10 m above the zero plane displacement height Wind speed at tree height Friction velocity Weibull scale parameter Weibull shape parameter Wind speed calculated from DAMS score Wind speed calculated from WAsP airflow model Wind speed at meteorological station Distance from forest edge Yield class Aerodynamic roughness Name of software ForestGALES Developers Forest Research and INRA Contact address Forest Research Northern Research Station Roslin Midlothian EH25 9SY United Kingdom Email forestgales.support @ forestry.gsi.gov.uk Availability and Online Documentation The software along with supporting material is freely available . Go to http www.forestresearch.gov.uk forestgales to find out how to obtain the software or email forestgales.support @ forestry.gsi.gov.uk Year first available 2000 Hardware required IBM compatible PC Software required MS Windows Programming language Borland Delphi 5.0 . Versions have also been written in Python Fortran R and Java . Contact the corresponding author for further details . Program size 10 MB . With all additional support files and manuals 25 MB . \\n'],\n",
              " [' The design process of mechatronic devices which involves experts from different disciplines working together has limited time and resource constraints . These experts normally have their own domain specific designing methods and tools which can lead to incompatibilities when one needs to work together using these those methods and tools . Having a proper framework which integrates different design tools is of interest as such a framework can prevent incompatibilities between parts during the design process . In this paper we propose our co modelling methodology and co simulation tools integration framework which helps to maintain the domain specific properties of the model components during the co design process of various mechatronic devices . To avoid expensive rework later in the design phase and even possible system failure fault modelling and a layered structure with fault tolerance mechanisms for the controller software are introduced . In the end a practical mechatronic device is discussed to illustrate the methods and tools which are presented in this paper in details . \\n'],\n",
              " [' This systematic review considers how water quality and aquatic ecology models represent the phosphorus cycle. Although the focus is on phosphorus many of the observations and discussion points here relate to aquatic ecosystem models in general. The review considers how models compare across domains of application the degree to which current models are fit for purpose how to choose between multiple alternative formulations and how models might be improved. Lake and marine models have been gradually increasing in complexity with increasing emphasis on inorganic processes and ecosystems. River models have remained simpler but have been more rigorously assessed. Processes important in less eutrophic systems have often been neglected these include the biogeochemistry of organic phosphorus transformations associated with fluxes through soils and sediments transfer rate limited phosphorus uptake and responses of plants to pulsed nutrient inputs. Arguments for and against increasing model complexity physical and physiological realism are reviewed. \\n'],\n",
              " [' Graphical abstract The noise model was implemented in R to call functions from PostgreSQL and GRASS GIS packages and can be obtained from the corresponding author or the following website http www.sahsu.org content data download first available in July 2014 TRANEX requires at least one standard desktop PC . \\n'],\n",
              " [' The purpose of this research is to develop a formal knowledge e discovery methodology using advanced information technology and decision support analysis to define legal case evolution based on Collective Litigation Intelligence . In this research a decade of Australia s retail franchise and trademark litigation cases are used as the corpus to analyze and synthesize the evolution of modern retail franchise law in Australia . The formal processes used in the legal e discovery research include a LexisNexis search strategy to collect legal documents text mining to find key concepts and their representing key phrases in the documents clustering algorithms to associate the legal cases into groups and concept lattice analysis to trace the evolutionary trends of the main groups . The case analysis discovers the fundamental issues for retail modernization advantages and disadvantages of retail franchising systems and the potential litigation hazards to be avoided in the Australian market . Given the growing number of legal documents in global court systems this research provides a systematic and generalized CLI methodology to improve the efficiency and efficacy of research across international legal systems . In the context of the case study the results demonstrate the critical importance of quickly processing and interpreting existing legal knowledge using the CLI approach . For example a brand management company which purchases a successful franchise in one market is under limited time constraints to evaluate the legal environment across global markets of interest . The proposed CLI methodology can be applied to derive market entry strategies to secure growth and brand expansion of a global franchise . \\n'],\n",
              " [' In order to elucidate some basic principles for protein ligand interactions a subset of 87 structures of human proteins with their ligands was obtained from the PDB databank . After a short molecular dynamics simulation a variety of interaction energies and structural parameters were extracted . Linear regression was performed to determine which of these parameters have a potentially significant contribution to the protein ligand interaction . The parameters exhibiting relatively high correlation coefficients were selected . Important factors seem to be the number of ligand atoms the ratio of N O and S atoms to total ligand atoms the hydrophobic polar aminoacid ratio and the ratio of cavity size to the sum of ligand plus water atoms in the cavity . An important factor also seems to be the immobile water molecules in the cavity . Nine of these parameters were used as known inputs to train a neural network in the prediction of seven other . Eight structures were left out of the training to test the quality of the predictions . After optimization of the neural network the predictions were fairly accurate given the relatively small number of structures especially in the prediction of the number of nitrogen and sulfur atoms of the ligand . \\n'],\n",
              " [' Bacteria are increasingly resistant to existing antibiotics which target a narrow range of pathways . New methods are needed to identify targets including repositioning targets among distantly related species . We developed a novel combination of systems and structural modeling and bioinformatics to reposition known antibiotics and targets to new species . We applied this approach to Mycoplasma genitalium a common cause of urethritis . First we used quantitative metabolic modeling to identify enzymes whose expression affects the cellular growth rate . Second we searched the literature for inhibitors of homologs of the most fragile enzymes . Next we used sequence alignment to assess that the binding site is shared by M. genitalium but not by humans . Lastly we used molecular docking to verify that the reported inhibitors preferentially interact with M. genitalium proteins over their human homologs . Thymidylate kinase was the top predicted target and piperidinylthymines were the top compounds . Further work is needed to experimentally validate piperidinylthymines . In summary combined systems and structural modeling is a powerful tool for drug repositioning . \\n'],\n",
              " [' Kansei evaluation plays a vital role in the implementation of Kansei engineering however it is difficult to quantitatively evaluate customer preferences of a product s Kansei attributes as such preferences involve human perceptual interpretation with certain subjectivity uncertainty and imprecision . An effective Kansei evaluation requires justifying the classification of Kansei attributes extracted from a set of collected Kansei words establishing priorities for customer preferences of product alternatives with respect to each attribute and synthesizing the priorities for the evaluated alternatives . Moreover psychometric Kansei evaluation systems essentially require dealing with Kansei words . This paper presents a Kansei evaluation approach based on the technique of computing with words . The aims of this study were to classify collected Kansei words into a set of Kansei attributes by using cluster analysis based on fuzzy relations to model Kansei preferences based on semantic labels for the priority analysis and to synthesize priority information and rank the order of decision alternatives by means of the linguistic aggregation operation . An empirical study is presented to demonstrate the implementation process and applicability of the proposed Kansei evaluation approach . The theoretical and practical implications of the proposed approach are also discussed . \\n'],\n",
              " [' As a pivotal domain within envelope protein fusion peptide plays a crucial role in pathogenicity and therapeutic intervention . Taken into account the limited FP annotations in NCBI database and absence of FP prediction software it is urgent and desirable to develop a bioinformatics tool to predict new putative FPs in retroviruses . In this work a sequence based FP model was proposed by combining Hidden Markov Method with similarity comparison . The classification accuracies are 91.97 and 92.31 corresponding to 10 fold and leave one out cross validation . After scanning sequences without FP annotations this model discovered 53 946 np FPs . The statistical results on FPs or np FPs reveal that FP is a conserved and hydrophobic domain . The FP software programmed for windows environment is available at https sourceforge.net projects fptool files source navbar . \\n'],\n",
              " [' Motivation Protein fold space is a conceptual framework where all possible protein folds exist and ideas about protein structure function and evolution may be analyzed . Classification of protein folds in this space is commonly achieved by using similarity indexes and or machine learning approaches each with different limitations . Results We propose a method for constructing a compact vector space model of protein fold space by representing each protein structure by its residues local contacts . We developed an efficient method to statistically test for the separability of points in a space and showed that our protein fold space representation is learnable by any machine learning algorithm . Availability An API is freely available at https code.google.com p pyrcc . \\n'],\n",
              " [' The collection and analysis of data on the three dimensional as built status of large scale civil infrastructure whether under construction newly put into service or in operation has been receiving increasing attention on the part of researchers and practitioners in the civil engineering field . Such collection and analysis of data is essential for the active monitoring of production during the construction phase of a project and for the automatic 3D layout of built assets during their service lives . This review outlines recent research efforts in this field and technological developments that aim to facilitate the analysis of 3D data acquired from as built civil infrastructure and applications of such data not only to the construction process per se but also to facility management in particular to production monitoring and automated layout . This review also considers prospects for improvement and addresses challenges that can be expected in future research and development . It is hoped that the suggestions and recommendations made in this review will serve as a basis for future work and as motivation for ongoing research and development . \\n'],\n",
              " [' In this paper the physical and chemical characteristics biological structure and function of a non specific nuclease from Yersinia enterocolitica subsp . palearctica found in our group were studied using multiple bioinformatics approaches . The results showed that Y. NSN had 283 amino acids a weight of 30 692.5ku and a certain hydrophilic property . Y. NSN had a signal peptide no transmembrane domains and disulphide bonds . Cleavage site in Y. NSN was between pos . 23 and 24 . The prediction result of the secondary structure showed Y. NSN was a coil structure based protein . The ratio of helix folded and random coil were 18.73 16.96 and 64.31 respectively . Active sites were pos . 124 125 127 157 165 and 169 . Mg2 binding site was pos . 157 . Substrate binding sites were pos . 124 125 and 169 . The analysis of multisequencing alignment and phylogenetic tree indicated that Y. NSN shared high similarity with the nuclease from Y. enterocolitica subsp . enterocolitica 8081 . The enzyme activity results showed that Y. NSN was a nuclease with good thermostability . \\n'],\n",
              " [' Reptin functions in a wide range of biological processes including chromatin remodelling nucleolar organization and transcriptional regulation of WNT signalling . As catenin dependent transcriptional repression and activation events involve binding of Reptin and histone deacetylase 1 to APPL endocytic proteins this complex has become an important target to identify molecules governing endocytic processes and WNT signalling . Here we describe the structural basis of APPL binding to Reptin to explore their mode of binding in context with APPL1 APPL2 dimerization . There is an evidence that both PH and BAR domains of APPL proteins exhibit alternately conserved regions involved in hetero dimerization process and our in silico data also corroborate this fact . Moreover APPL2PH domain binds to the BAR domain region encompassing a nuclear localization signal . We conclude that APPLPH binding to BAR domain and Reptin is mutually exclusive which regulates the nucleocytoplasmic shuttling of Reptin . Furthermore Reptin is unable to bind with membrane associated APPL proteins . These observations were further expanded by experimental approaches where we identified a novel point mutation D316N lying in the APPL1PH domain which resulted in a significantly reduced binding with Reptin . By luciferase assays we observed that overexpression of APPL1D316N and APPL1WT stimulated catenin TCF dependent transcriptional activity in a similar manner which suggested that binding of Reptin to APPL1 is not necessary for catenin dependent target gene expression . Overall our data attempt to highlight a comparative role of APPL proteins in controlling catenin dependent transcription mechanism which may improve our understanding of gene regulation . \\n'],\n",
              " [' Background Bacillus anthracis is a gram positive spore forming rod shaped bacteria which is the etiologic agent of anthrax cutaneous pulmonary and gastrointestinal . A recent outbreak of anthrax in a tropical region uncovered natural and in vitro resistance against penicillin ciprofloxacin quinolone due to over exposure of the pathogen to these antibiotics . This fact combined with the ongoing threat of using B. anthracis as a biological weapon proves that the identification of new therapeutic targets is urgently needed . Methods In this computational approach various databases and online based servers were used to detect essential proteins of B. anthracis A0248 . Protein sequences of B. anthracis A0248 strain were retrieved from the NCBI database which was then run in CD hit suite for clustering . NCBI BlastP against the human proteome and similarity search against DEG were done to find out essential human non homologous proteins . Proteins involved in unique pathways were analyzed using KEGG genome database and PSORTb CELLO v.2.5 ngLOC these three tools were used to deduce putative cell surface proteins . Results Successive analysis revealed 116 proteins to be essential human non homologs among which 17 were involved in unique metabolic pathways and 28 were predicted as membrane associated proteins . Both types of proteins can be exploited as they are unlikely to have homologous counterparts in the human host . Conclusion Being human non homologous these proteins can be targeted for potential therapeutic drug development in future . Targets on unique metabolic and membrane bound proteins can block cell wall synthesis bacterial replication and signal transduction respectively . \\n'],\n",
              " [' Inferring transcriptional regulatory interactions between transcription factors and their targets has utmost importance for understanding the complex regulatory mechanisms in cellular system . In this paper we introduced a computational method to predict regulatory interactions in Arabidopsis based on gene expression data and sequence information . Support vector machine and Jackknife cross validation test were employed to perform our method on a collected dataset including 178 positive samples and 1068 negative samples . Results showed that our method achieved an overall accuracy of 98.39 with the sensitivity of 94.88 and the specificity of 93.82 which suggested that our method can serve as a potential and cost effective tool for predicting regulatory interactions in Arabidopsis . \\n'],\n",
              " [' A short partial sequence of 28 amino acids is all the information we have so far about the putative allergen 2S albumin from almond . The aim of this work was to analyze this information using mainly bioinformatics tools in order to verify its rightness . Based on the results reported in the paper describing this allergen from almond we analyzed the original data of amino acids sequencing through available software . The degree of homology of the almond 12kDa protein with any other known 2S albumin appears to be much lower than the one reported in the paper that firstly described it . In a publicly available cDNA library we discovered an expressed sequence tag which translation generates a protein that perfectly matches both of the sequencing outputs described in the same paper . A further analysis indicated that the latter protein seems to belong to the vicilin superfamily rather than to the prolamin one . The fact that also vicilins are seed storage proteins known to be highly allergenic would explain the IgE reactivity originally observed . Based on our observations we suggest that the IgE reactive 12kDa protein from almond currently known as Pru du 2S albumin is in reality the cleaved N terminal region of a 7S vicilin like protein . \\n'],\n",
              " [' Although image based phenotypic assays are considered a powerful tool for siRNA library screening the reproducibility and biological implications of various image based assays are not well characterized in a systematic manner . Here we compared the resolution of high throughput assays of image based cell count and typical cell viability measures for cancer samples . It was found that the optimal plating density of cells was important to obtain maximal resolution in both types of assays . In general cell counting provided better resolution than the cell viability measure in diverse batches of siRNAs . In addition to cell count diverse image based measures were simultaneously collected from a single screening and showed good reproducibility in repetitions . They were classified into a few functional categories according to biological process based on the differential patterns of hit prioritization from the same screening data . The presented systematic analyses of image based parameters provide new insight to a multitude of applications and better biological interpretation of high content cell based assays . \\n'],\n",
              " [' Mammalian target of rapamycin a key mediator of PI3K Akt mTOR signaling pathway has recently emerged as a compelling molecular target in glioblastoma . The mTOR is a member of serine threonine protein kinase family that functions as a central controller of growth proliferation metabolism and angiogenesis but its signaling is dysregulated in various human diseases especially in certain solid tumors including the glioblastoma . Here considering that there are various kinase inhibitors being approved or under clinical or preclinical development it is expected that some of them can be re exploited as new potent agents to target mTOR for glioblastoma therapy . To achieve this a synthetic pipeline that integrated molecular grafting consensus scoring virtual screening kinase assay and structure analysis was described to systematically profile the binding potency of various small molecule inhibitors deposited in the protein kinase inhibitor database against the kinase domain of mTOR . Consequently a number of structurally diverse compounds were successfully identified to exhibit satisfactory inhibition profile against mTOR with IC50 values at nanomolar level . In particular few sophisticated kinase inhibitors as well as a flavonoid myricetin showed high inhibitory activities which could thus be considered as potential lead compounds to develop new potent selective mTOR inhibitors . Structural examination revealed diverse nonbonded interactions such as hydrogen bonds hydrophobic forces and van der Waals contacts across the complex interface of mTOR with myricetin conferring both stability and specificity for the mTOR inhibitor binding . \\n'],\n",
              " [' To analyze the evolutionary dynamics of a mutant population in an evolutionary experiment it is necessary to sequence a vast number of mutants by high throughput sequencing technologies which enable rapid and parallel analysis of multikilobase sequences . However the observed sequences include many errors of base call . Therefore if next generation sequencing is applied to analysis of a heterogeneous population of various mutant sequences it is necessary to discriminate between true bases as point mutations and errors of base call in the observed sequences and to subject the sequences to error correction processes . To address this issue we have developed a novel method of error correction based on the Potts model and a maximum a posteriori probability estimate of its parameters corresponding to the true sequences . Our method of error correction utilizes the quality scores which are assigned to individual bases in the observed sequences and the neighborhood relationship among the observed sequences mapped in sequence space . The computer experiments of error correction of artificially generated sequences supported the effectiveness of our method showing that 50 90 of errors were removed . Interestingly this method is analogous to a probabilistic model based method of image restoration developed in the field of information engineering . \\n'],\n",
              " [' The modulation of the properties and function of cell membranes by small volatile substances is important for many biomedical applications . Despite available experimental results molecular mechanisms of action of inhalants and organic solvents such as acetone on lipid membranes remain not well understood . To gain a better understanding of how acetone interacts with membranes we have performed a series of molecular dynamics simulations of a POPC bilayer in aqueous solution in the presence of acetone whose concentration was varied from 2.8 to 11.2mol . The MD simulations of passive distribution of acetone between a bulk water phase and a lipid bilayer show that acetone favors partitioning into the water free region of the bilayer located near the carbonyl groups of the phospholipids and at the beginning of the hydrocarbon core of the lipid membrane . Using MD umbrella sampling we found that the permeability barrier of 0.5kcal mol exists for acetone partitioning into the membrane . In addition a Gibbs free energy profile of the acetone penetration across a bilayer demonstrates a favorable potential energy well of 3.6kcal mol located at 15 16 from the bilayer center . The analysis of the structural and dynamics properties of the model membrane revealed that the POPC bilayer can tolerate the presence of acetone in the concentration range of 2.8 5.6mol . The accumulation of the higher acetone concentration of 11.2mol results however in drastic disordering of phospholipid packing and the increase in the membrane fluidity . The acetone molecules push the lipid heads apart and hence act as spacers in the headgroup region . This effect leads to the increase in the average headgroup area per molecule . In addition the acyl tail region of the membrane also becomes less dense . We suggest therefore that the molecular mechanism of acetone action on the phospholipid bilayer has many common features with the effects of short chain alcohols DMSO and chloroform . \\n'],\n",
              " [' An attempt was made to develop a computational model based on artificial neural network and ant colony optimization to estimate the composition of medium components for maximizing the productivity of Penicillin G Acylase enzyme from Escherichia coli DH5 strain harboring the plasmid pPROPAC . As a first step an artificial neural network model was developed to predict the PGA activity by considering the concentrations of seven important components of the medium . Design of experiments employing central composite design technique was used to obtain the training samples . In the second step ant colony optimization technique for continuous domain was employed to maximize the PGA activity by finding the optimal inputs for the developed ANN model . Further the effect of a combination of ant colony optimization for continuous domain with a preferential local search strategy was studied to analyze the performance . For a comparative study the training samples were fed into the response surface methodology optimization software to maximize the PGA production . The obtained PGA activity by the proposed approach was found to be higher than that of the obtained value with the response surface methodology . The optimum solution obtained computationally was experimentally verified . The observed PGA activity exhibited a close agreement with the model predictions . \\n'],\n",
              " [' Gene silencing is an important function as it keeps newly acquired foreign DNA repressed thereby avoiding possible deleterious effects in the host organism . Known transcriptional regulators associated with this process are called xenogeneic silencers and belong to either the H NS Lsr2 MvaT or Rok families . In the work described here we looked for XS like regulators and their distribution in prokaryotic organisms was evaluated . Our analysis showed that putative XS regulators similar to H NS Lsr2 MvaT or Rok are present only in bacteria . This does not exclude the existence of alternative XS in the rest of the organisms analyzed . Additionally of the four XS groups evaluated in this work those from the H NS family have diversified more than the other groups . In order to compare the distribution of these putative XS regulators we also searched for other nucleoid associated proteins not included in this group such as Fis EbfC YbaB HU IHF and Alba . Results showed that NAPs from the Fis EbfC YbaB HU IHF and Alba families are widely distributed among prokaryotes . These NAPs were found in multiple combinations with or without XS like proteins . In regard with XS regulators results showed that only XS proteins from one family were found in those organisms containing them . This suggests specificity for this type of regulators and their corresponding genomes . \\n'],\n",
              " [' Reconstructions of genome scale metabolic networks from different organisms have become popular in recent years . Metabolic engineering can simulate the reconstruction process to obtain desirable phenotypes . In previous studies optimization algorithms have been implemented to identify the near optimal sets of knockout genes for improving metabolite production . However previous works contained premature convergence and the stop criteria were not clear for each case . Therefore this study proposes an algorithm that is a hybrid of the ant colony optimization algorithm and flux balance analysis to predict near optimal sets of gene knockouts in an effort to maximize growth rates and the production of certain metabolites . Here we present a case study that uses Baker s yeast also known as Saccharomyces cerevisiae as the model organism and target the rate of vanillin production for optimization . The results of this study are the growth rate of the model organism after gene deletion and a list of knockout genes . The ACOFBA algorithm was found to improve the yield of vanillin in terms of growth rate and production compared with the previous algorithms . \\n'],\n",
              " [' Naturally inspired evolutionary algorithms prove effectiveness when used for solving feature selection and classification problems . Artificial Bee Colony is a relatively new swarm intelligence method . In this paper we propose a new hybrid gene selection method namely Genetic Bee Colony algorithm . The proposed algorithm combines the used of a Genetic Algorithm along with Artificial Bee Colony algorithm . The goal is to integrate the advantages of both algorithms . The proposed algorithm is applied to a microarray gene expression profile in order to select the most predictive and informative genes for cancer classification . In order to test the accuracy performance of the proposed algorithm extensive experiments were conducted . Three binary microarray datasets are use which include colon leukemia and lung . In addition another three multi class microarray datasets are used which are SRBCT lymphoma and leukemia . Results of the GBC algorithm are compared with our recently proposed technique mRMR when combined with the Artificial Bee Colony algorithm . We also compared the combination of mRMR with GA and Particle Swarm Optimization algorithms . In addition we compared the GBC algorithm with other related algorithms that have been recently published in the literature using all benchmark datasets . The GBC algorithm shows superior performance as it achieved the highest classification accuracy along with the lowest average number of selected genes . This proves that the GBC algorithm is a promising approach for solving the gene selection problem in both binary and multi class cancer classification . \\n'],\n",
              " [' Cse1p and Xpot are two karyopherin proteins that transport the corresponding cargos during the nucleocytoplasmic transport . We utilized Elastic Network Model and Finite Element Analysis to study their conformational dynamics . These dynamics were interpreted by their intrinsic modes that played key roles in the flexibility of karyopherins which further affected the binding affinities . The findings included that it was the karyopherin s versatile conformations composed of the same superhelices of HEAT repeats that produced different degrees of functional flexibilities . We presented evidence that these coarse grained methods could help to elucidate the biological function behind the structures of the two karyopherins . \\n'],\n",
              " [' Organisms thriving at extreme cold surroundings are called as psychrophiles and they present a wealth of knowledge about sequence adjustments in proteins that had occurred during the adaptation to low temperatures . In this paper we propose a new cascading model to investigate the basis for psychrophilicity . In this model a superior classifier was used to discriminate psychrophilic from mesophilic protein sequences and then the PART rule generating algorithm was applied on the input instances that are correctly classified by the classifier to generate human interpretable rules . These derived rules were further validated on a structural dataset and finally analyzed to discover the underlying biological basis about the psychrophilicity . In this study we have used one of the key features of psychrophilic proteins accountable for remaining functional in extreme cold temperature surroundings i.e . global patterns of amino acid composition as the input features . The rotation forest classifier outperformed all the other classifiers with maximum accuracy of 70.5 and maximum AUC of 0.78 . The effect of sequence length on the classification accuracy was also investigated . The analysis of the derived rules and interpretation of the analyzed results had revealed some interesting phenomena such as the amino acids A D G F and S are over represented and T is under represented in psychrophilic proteins . These findings augment the existing domain knowledge for psychrophilic sequence features . \\n'],\n",
              " [' Receptor like kinase is an important member in protein kinase family which is widely involved in plant growth development and defense responses . It is significant to analyze the kinase structure and evolution of pollen RLKs in order to study their mechanisms . In our study 64 and 73 putative pollen RLKs were chosen from maize and Arabidopsis . Phylogenetic analysis showed that the pollen RLKs were conservative and might had existed before divergence between monocot and dicot which were mainly concentrated in RLCK VII and LRR III two subfamilies . Chromosomal localization and gene duplication analysis showed the expansion of pollen RLKs were mainly caused by segmental duplication . By calculating Ka Ks value of extracellular domain intracellular domain and kinase domain in pollen RLKs we found that the pollen RLKs duplicated genes had mainly experienced the purifying selection while maize might have experienced weaker purifying selection . Meanwhile extracellular domain might have experienced stronger diversifying selection than intracellular domain in both species . Estimation of duplication time showed that the duplication events of Arabidopsis have occurred approximately between 18 and 69 million years ago compared to 0.67 170 million years ago of maize . \\n'],\n",
              " [' Continuous collision detection is a key technique to meet non penetration requirements in many applications . Even though it is possible to perform efficient culling operations in the broad stage of a continuous collision detection algorithm such as bounding volume hierarchies a huge number of potentially colliding triangles still survive and go to the succeeding narrow stage . This heavily burdens the elementary collision tests in a collision detection algorithm and affects the performance of the entire pipeline especially for fast moving or deforming objects . This paper presents a low cost filtering algorithm using algebraic analysis techniques . It can significantly reduce the number of elementary collision tests that occur in the narrow stage . We analyze the root existence during the time interval for a standard cubic equation defining an elementary collision test . We demonstrate the efficiency of the algebraic filter in our experiments . Cubic solvers augmented by our filtering algorithm are able to achieve up to 99 filtering ratios and more than 10 performance improvement against the standard cubic solver without any filters . \\n'],\n",
              " [' We present algorithms for computing the differential geometry properties of lines of curvature of parametric surfaces . We derive a unit tangent vector curvature vector binormal vector torsion and algorithms to evaluate their higher order derivatives of lines of curvature of parametric surfaces . Among these quantities it is shown that the curvature and its first derivative of the lines of curvature lend a hand for the formation of curved plates in shipbuilding . We also visualize the twist of lines of curvature which enables us to observe how much the osculating plane of the line of curvature turns about the tangent vector . \\n'],\n",
              " [' In recent years computer aided redesigning methods based on genome scale metabolic network models have played important roles in metabolic engineering studies however most of these methods are hindered by intractable computing times . In particular methods that predict knockout strategies leading to overproduction of desired biochemical are generally unable to do high level prediction because the computational time will increase exponentially . In this study we propose a new framework named IdealKnock which is able to efficiently evaluate potentials of the production for different biochemical in a system by merely knocking out pathways . In addition it is also capable of searching knockout strategies when combined with the OptKnock or OptGene framework . Furthermore unlike other methods IdealKnock suggests a series of mutants with targeted overproduction which enables researchers to select the one of greatest interest for experimental validation . By testing the overproduction of a large number of native metabolites IdealKnock showed its advantage in successfully breaking through the limitation of maximum knockout number in reasonable time and suggesting knockout strategies with better performance than other methods . In addition gene reaction relationship is well considered in the proposed framework . \\n'],\n",
              " [' Genome wide association studies and other genetic analyses have identified a large number of genes and variants implicating a variety of disease etiological mechanisms . It is imperative for the study of human diseases to put these genetic findings into a coherent functional context . Here we use system biology tools to examine disease connections of five master genes for CD4 T cell subtypes . We compiled a list of genes functionally interacting with the master genes then we surveyed the disease connections either by experimental evidence or by genetic association . Embryonic lethal genes are over represented in master genes and their interacting genes . Transcription factors are significantly enriched among genes interacting with the master genes . Predicted haploinsufficiency is a feature of most these genes . Disease connected genes are enriched in this list of genes 42 of these genes have a disease connection according to Online Mendelian Inheritance in Man and 74 are associated with some diseases or phenotype in a Genome Wide Association Study . Seemingly not all of the diseases connected to genes surveyed were immune related which may indicate pleiotropic functions of the master regulator genes and associated genes . \\n'],\n",
              " [' Efficient identification of patient intervention comparison and outcome components in medical articles is helpful in evidence based medicine . The purpose of this study is to clarify whether first sentences of these components are good enough to train naive Bayes classifiers for sentence level PICO element detection . We extracted 19 854 structured abstracts of randomized controlled trials with any P I O label from PubMed for naive Bayes classifiers training . Performances of classifiers trained by first sentences of each section and those trained by all sentences were compared using all sentences by ten fold cross validation . The results measured by recall precision and F measures show that there are no significant differences in performance between CF and CA for detection of O element . However CA perform better for I elements in terms of recall and F measures . For P elements CF have higher precision but lower recall . CF are not always better than CA in sentence level PICO element detection . Their performance varies in detecting different elements . \\n'],\n",
              " [' In a reconfigurable system the response to contextual or internal change may trigger reconfiguration events which on their turn activate scripts that change the system s architecture at runtime . To be safe however such reconfigurations are expected to obey the fundamental principles originally specified by its architect . This paper introduces an approach to ensure that such principles are observed along reconfigurations by verifying them against concrete specifications in a suitable logic . Architectures reconfiguration scripts and principles are specified in Archery an architectural description language with formal semantics . Principles are encoded as constraints which become formulas of a two layer graded hybrid logic where the upper layer restricts reconfigurations and the lower layer constrains the resulting configurations . Constraints are verified by translating them into logic formulas which are interpreted over models derived from Archery specifications of architectures and reconfigurations . Suitable notions of bisimulation and refinement to which the architect may resort to compare configurations are given and their relationship with modal validity is discussed . \\n'],\n",
              " [' Given a set of symmetric antisymmetric filter vectors containing only regular multiresolution filters the method we present in this article can establish a balanced multiresolution scheme for images allowing their balanced decomposition and subsequent perfect reconstruction without the use of any extraordinary boundary filters . We define balanced multiresolution such that it allows balanced decomposition i.e . decomposition of a high resolution image into a low resolution image and corresponding details of equal size . Such a balanced decomposition makes on demand reconstruction of regions of interest efficient in both computational load and implementation aspects . We find this balanced decomposition and perfect reconstruction based on an appropriate combination of symmetric antisymmetric extensions near the image and detail boundaries . In our method exploiting such extensions correlates to performing sample split operations . Our general approach is demonstrated for some commonly used symmetric antisymmetric multiresolution filters . We also show the application of such a balanced multiresolution scheme in real time focus context visualization . \\n'],\n",
              " [' Purpose Despite years of effort and millions of dollars spent to create unified electronic communicable disease reporting systems the goal remains elusive . A major barrier has been a lack of understanding by system designers of communicable disease work and the public health workers who perform this work . This study reports on the application of user centered design representations traditionally used for improving interface design to translate the complex CD work identified through ethnographic studies to guide designers and developers of CD systems . The purpose of this work is to better understand public health practitioners and their information workflow with respect to CD monitoring and control at a local health agency and to develop evidence based design representations that model this CD work to inform the design of future disease surveillance systems . Methods We performed extensive onsite semi structured interviews targeted work shadowing and a focus group to characterize local health agency CD workflow . Informed by principles of design ethnography and user centered design we created persona scenarios and user stories to accurately represent the user to system designers . Results We sought to convey to designers the key findings from ethnographic studies public health CD work is mobile and episodic in contrast to current CD reporting systems which are stationary and fixed health agency efforts are focused on CD investigation and response rather than reporting and current CD information systems must conform to public health workflow to ensure their usefulness . In an effort to illustrate our findings to designers we developed three contemporary design support representations persona scenario and user story . Conclusions Through application of user centered design principles we were able to create design representations that illustrate complex public health communicable disease workflow and key user characteristics to inform the design of CD information systems for public health . \\n'],\n",
              " [' Polymorphic programming languages have been adapted for constructing distributed access control systems where a program represents a proof of eligibility according to a given policy . As a security requirement it is typically stated that the programs of such languages should satisfy noninterference . However this property has not been defined and proven semantically . In this paper we first propose a semantics based on Henkin models for a predicative polymorphic access control language based on lambda calculus . A formal semantic definition of noninterference is then proposed through logical relations . We prove a type soundness theorem which states that any well typed program of our language meets the noninterference property defined in this paper . In this way it is guaranteed that access requests from an entity do not interfere with those from unrelated or more trusted entities . \\n'],\n",
              " [' We propose a language independent symbolic execution framework for languages endowed with a formal operational semantics based on term rewriting . Starting from a given definition of a language a new language definition is generated with the same syntax as the original one but whose semantical rules are transformed in order to rewrite over logical formulas denoting possibly infinite sets of program states . Then the symbolic execution of concrete programs is by definition the execution of the same programs with the symbolic semantics . We prove that the symbolic execution thus defined has the properties naturally expected from it . A prototype implementation of our approach was developed in the framework . We demonstrate the tool s genericity by instantiating it on several languages and illustrate it on the reachability analysis and model checking of several programs . \\n'],\n",
              " [' In this paper we present an efficient approach for parameterizing a genus zero triangular mesh onto the sphere with an optimal radius in an as rigid as possible manner which is an extension of planar ARAP parametrization approach to spherical domain . We analyze the smooth and discrete ARAP energy and formulate our spherical parametrization energy from the discrete ARAP energy . The solution is non trivial as the energy involves a large system of non linear equations with additional spherical constraints . To this end we propose a two step iterative algorithm . In the first step we adopt a local global iterative scheme to calculate the parametrization coordinates . In the second step we optimize a best approximate sphere on which parametrization triangles can be embedded in a rigidity preserving manner . Our algorithm is simple robust and efficient . Experimental results show that our approach provides almost isometric spherical parametrizations with lowest rigidity distortion over state of the art approaches . \\n'],\n",
              " [' Intrinsic shape matching has become the standard approach for pose invariant correspondence estimation among deformable shapes . Most existing approaches assume global consistency . While global isometric matching is well understood only a few heuristic solutions are known for partial matching . Partial matching is particularly important for robustness to topological noise which is a common problem in real world scanner data . We introduce a new approach to partial isometric matching based on the observation that isometries are fully determined by local information a map of a single point and its tangent space fixes an isometry . We develop a new representation for partial isometric maps based on equivalence classes of correspondences between pairs of points and their tangent spaces . We apply our approach to register partial point clouds and compare it to the state of the art methods where we obtain significant improvements over global methods for real world data and stronger guarantees than previous partial matching algorithms . \\n'],\n",
              " [' In this paper we discuss the design and development of TRAK an ontology that formally models information relevant for the rehabilitation of knee conditions . TRAK provides the framework that can be used to collect coded data in sufficient detail to support epidemiologic studies so that the most effective treatment components can be identified new interventions developed and the quality of future randomized control trials improved to incorporate a control intervention that is well defined and reflects clinical practice . TRAK follows design principles recommended by the Open Biomedical Ontologies Foundry . TRAK uses the Basic Formal Ontology as the upper level ontology and refers to other relevant ontologies such as Information Artifact Ontology Ontology for General Medical Science and Phenotype And Trait Ontology . TRAK is orthogonal to other bio ontologies and represents domain specific knowledge about treatments and modalities used in rehabilitation of knee conditions . Definitions of typical exercises used as treatment modalities are supported with appropriate illustrations which can be viewed in the OBO Edit ontology editor . The vast majority of other classes in TRAK are cross referenced to the Unified Medical Language System to facilitate future integration with other terminological sources . TRAK is implemented in OBO a format widely used by the OBO community . TRAK is available for download from http www.cs.cf.ac.uk trak . In addition its public release can be accessed through BioPortal where it can be browsed searched and visualized . \\n'],\n",
              " [' In this paper we compose six different Python and Prolog VMs into 4 pairwise compositions one using C interpreters one running on the JVM one using meta tracing interpreters and one using a C interpreter and a meta tracing interpreter . We show that programs that cross the language barrier frequently execute faster in a meta tracing composition and that meta tracing imposes a significantly lower overhead on composed programs relative to mono language programs . \\n'],\n",
              " [' The actor model is a message passing concurrency model that avoids deadlocks and low level data races by construction . This facilitates concurrent programming especially in the context of complex interactive applications where modularity security and fault tolerance are required . The tradeoff is that the actor model sacrifices expressiveness and safety guarantees with respect to parallel access to shared state . In this paper we present domains as a set of novel language abstractions for safely encapsulating and sharing state within the actor model . We introduce four types of domains namely immutable isolated observable and shared domains that each is tailored to a certain access pattern on that shared state . The domains are characterized with an operational semantics . For each we discuss how the actor model s safety guarantees are upheld even in the presence of conceptually shared state . Furthermore the proposed language abstractions are evaluated with a case study in Scala comparing them to other synchronization mechanisms to demonstrate their benefits in deadlock freedom parallel reads and enforced isolation . \\n'],\n",
              " [' The Trapezoid Step Functions domain is introduced in order to approximate continuous functions by a finite sequence of trapezoids adopting linear functions to abstract the upper and the lower bounds of a continuous variable in each time slot . The lattice structure of TSF is studied showing how to build and compute a sound abstraction of a given continuous function . Experimental results underline the effectiveness of the approach in terms of both precision and efficiency with respect to the domain of Interval Valued Step Functions . \\n'],\n",
              " [' Objective To report on the results of a review concerning the use of mobile phones for health with older adults . Methods PubMed and CINAHL were searched for articles using older adults and mobile phones along with related terms and synonyms between 1965 and June 2012 . Identified articles were filtered by the following inclusion criteria original research project utilizing a mobile phone as an intervention involve target adults 60years of age or older and have an aim emphasizing the mobile phone s use in health . Results Twenty one different articles were found and categorized into ten different clinical domains including diabetes activities of daily life and dementia care among others . The largest group of articles focused on diabetes care followed by COPD Alzheimer s dementia Care and osteoarthritis . Areas of interest studied included feasibility acceptability and effectiveness . While there were many different clinical domains the majority of studies were pilot studies that needed more work to establish a stronger base of evidence . Conclusions Current work in using mobile phones for older adult use are spread across a variety of clinical domains . While this work is promising current studies are generally smaller feasibility studies and thus future work is needed to establish more generalizable stronger base of evidence for effectiveness of these interventions . \\n'],\n",
              " [' Although biomedical information available in articles and patents is increasing exponentially we continue to rely on the same information retrieval methods and use very few keywords to search millions of documents . We are developing a fundamentally different approach for finding much more precise and complete information with a single query using predicates instead of keywords for both query and document representation . Predicates are triples that are more complex datastructures than keywords and contain more structured information . To make optimal use of them we developed a new predicate based vector space model and query document similarity function with adjusted tf idf and boost function . Using a test bed of 107 367 PubMed abstracts we evaluated the first essential function retrieving information . Cancer researchers provided 20 realistic queries for which the top 15 abstracts were retrieved using a predicate based and keyword based approach . Each abstract was evaluated double blind by cancer researchers on a 0 5 point scale to calculate precision and relevance . Precision was significantly higher for the predicate based than for the keyword based approach . Relevance was almost doubled with the predicate based approach 2.1 versus 1.6 without rank order adjustment and 1.34 versus 0.98 with rank order adjustment for predicate versus keyword based approach respectively . Predicates can support more precise searching than keywords laying the foundation for rich and sophisticated information search . \\n'],\n",
              " [' Most of today s embedded systems are very complex . These systems controlled by computer programs continuously interact with their physical environments through network of sensory input and output devices . Consequently the operations of such embedded systems are highly reactive and concurrent . Since embedded systems are deployed in many safety critical applications where failures can lead to catastrophic events an approach that combines mathematical logic and formal verification is employed in order to ensure correct behavior of the control algorithm . This paper presents What You Prove Is What You Execute compilation strategy for a Globally Asynchronous Locally Synchronous programming language called Safey Critical SystemJ . SC SystemJ is a safety critical subset of the SystemJ language . A formal big step transition semantics of SC SystemJ is developed for compiling SC SystemJ programs into propositional Linear Temporal Logic formulas . These LTL formulas are then converted into a network of Mealy automata using a novel and efficient compilation algorithm . The resultant Mealy automata have a straightforward syntactic translation into Promela code . The resultant Promela models can be used for verifying correctness properties via the SPIN model checker . Finally there is a single translation procedure to compile both Promela and C Java code for execution which satisfies the De Bruijn index i.e . this final translation step is simple enough that is can be manually verified . \\n'],\n",
              " [' This paper presents the design implementation and applications of a software testing tool TAO which allows users to specify and generate test cases and oracles in a declarative way . Extended from its previous grammar based test generation tool TAO provides a declarative notation for defining denotational semantics on each productive grammar rule such that when a test case is generated its expected semantics will be evaluated automatically as well serving as its test oracle . TAO further provides a simple tagging mechanism to embed oracles into test cases for bridging the automation between test case generation and software testing . Two practical case studies are used to illustrate how automated oracle generation can be effectively integrated with grammar based test generation in different testing scenarios locating fault inducing input patterns on Java applications and Selenium based automated web testing . \\n'],\n",
              " [' The chaetognaths constitute a small and enigmatic phylum of little marine invertebrates . Both nuclear and mitochondrial genomes have numerous originalities some phylum specific . Until recently their mitogenomes seemed containing only one tRNA gene but a recent study found in two chaetognath mitogenomes two and four tRNA genes . Moreover apparently two conspecific mitogenomes have different tRNA gene numbers . Reanalyses by tRNAscan SE and ARWEN softwares of the five available complete chaetognath mitogenomes suggest numerous additional tRNA genes from different types . Their total number never reaches the 22 found in most other invertebrates using that genetic code . Predicted error compensation between codon anticodon mismatch and tRNA misacylation suggests translational activity by tRNAs predicted solely according to secondary structure for tRNAs predicted by tRNAscan SE not ARWEN . Numbers of predicted stop suppressor tRNAs coevolve with predicted overlapping frameshifted protein coding genes including stop codons . Sequence alignments in secondary structure prediction with non chaetognath tRNAs suggest that the most likely functional tRNAs are in intergenic regions as regular mt tRNAs . Due to usually short intergenic regions generally tRNA sequences partially overlap with flanking genes . Some tRNA pairs seem templated by sense antisense strands . Moreover 16S rRNA genes but not 12S rRNAs appear as tRNA nurseries as previously suggested for multifunctional ribosomal like protogenomes . \\n'],\n",
              " [' Background and purpose Poor device design that fails to adequately account for user needs cognition and behavior is often responsible for use errors resulting in adverse events . This poor device design is also often latent and could be responsible for No Fault Found reporting in which medical devices sent for repair by clinical users are found to be operating as intended . Unresolved NFF reports may contribute to incident under reporting clinical user frustration and biomedical engineering technologist inefficacy . This study uses human factors engineering methods to investigate the relationship between NFF reporting frequency and device usability . Material and methods An analysis of medical equipment maintenance data was conducted to identify devices with a high NFF reporting frequency . Subsequently semi structured interviews and heuristic evaluations were performed in order to identify potential usability issues . Finally usability testing was conducted in order to validate that latent usability related design faults result in a higher frequency of NFF reporting . Results The analysis of medical equipment maintenance data identified six devices with a high NFF reporting frequency . Semi structured interviews heuristic evaluations and usability testing revealed that usability issues caused a significant portion of the NFF reports . Other factors suspected to contribute to increased NFF reporting include accessory issues intermittent faults and environmental issues . Usability testing conducted on three of the devices revealed 23 latent usability related design faults . Conclusions These findings demonstrate that latent usability related design faults manifest themselves as an increase in NFF reporting and that devices containing usability related design faults can be identified through an analysis of medical equipment maintenance data . \\n'],\n",
              " [' Objective Publications are a key data source for investigator profiles and research networking systems . We developed ReCiter an algorithm that automatically extracts bibliographies from PubMed using institutional information about the target investigators . Methods ReCiter executes a broad query against PubMed groups the results into clusters that appear to constitute distinct author identities and selects the cluster that best matches the target investigator . Using information about investigators from one of our institutions we compared ReCiter results to queries based on author name and institution and to citations extracted manually from the Scopus database . Five judges created a gold standard using citations of a random sample of 200 investigators . Results About half of the 10 471 potential investigators had no matching citations in PubMed and about 45 had fewer than 70 citations . Interrater agreement for the gold standard was 0.81 . Scopus achieved the best recall of 0.81 while name based queries had 0.78 and ReCiter had 0.69 . ReCiter attained the best precision of 0.93 while Scopus had 0.85 and name based queries had 0.31 . Discussion ReCiter accesses the most current citation data uses limited computational resources and minimizes manual entry by investigators . Generation of bibliographies using named based queries will not yield high accuracy . Proprietary databases can perform well but requite manual effort . Automated generation with higher recall is possible but requires additional knowledge about investigators . \\n'],\n",
              " [' Objective Electronic medical records data is increasingly incorporated into genome phenome association studies . Investigators hope to share data but there are concerns it may be re identified through the exploitation of various features such as combinations of standardized clinical codes . Formal anonymization algorithms can prevent such violations but prior studies suggest that the size of the population available for anonymization may influence the utility of the resulting data . We systematically investigate this issue using a large scale biorepository and EMR system through which we evaluate the ability of researchers to learn from anonymized data for genome phenome association studies under various conditions . Methods We use a k anonymization strategy to simulate a data protection process for resources of similar size to those found at nine academic medical institutions within the United States . Following the protection process we replicate an existing genome phenome association study and compare the discoveries using the protected data and the original data through the correlation of the p values of association significance . Results Our investigation shows that anonymizing an entire dataset with respect to the population from which it is derived yields significantly more utility than small study specific datasets anonymized unto themselves . When evaluated using the correlation of genome phenome association strengths on anonymized data versus original data all nine simulated sites results from largest scale anonymizations retained better utility to those on smaller sizes . We observed a general trend of increasing for larger data set sizes 0.9481 for small sized datasets 0.9493 for moderately sized datasets 0.9934 for large sized datasets . Conclusions This research implies that regardless of the overall size of an institution s data there may be significant benefits to anonymization of the entire EMR even if the institution is planning on releasing only data about a specific cohort of patients . \\n'],\n",
              " [' Background Gene name recognition and normalization is together with detection of other named entities a crucial step in biomedical text mining and the underlying basis for development of more advanced techniques like extraction of complex events . While the current state of the art solutions achieve highly promising results on average performance can drop significantly for specific genes with highly ambiguous synonyms . Depending on the topic of interest this can cause the need for extensive manual curation of such text mining results . Our goal was to enhance this curation step based on tools widely used in pharmaceutical industry utilizing the text processing and classification capabilities of the Konstanz Information Miner along with publicly available sources . Results F score achieved on gene specific test corpora for highly ambiguous genes could be improved from values close to zero due to very low precision to values 0.9 for several cases . Interestingly the presented approach even resulted in an increased F score for genes showing already good results in initial gene name normalization . For most test cases we could significantly improve precision while retaining a high recall . Conclusions We could show that KNIME can be used to assist in manual curation of text mining results containing high numbers of false positive hits . Our results also indicate that it could be beneficial for future development in the field of gene name normalization to create gene specific training corpora based on incorrectly identified genes common to current state of the art algorithms . \\n'],\n",
              " [' The ubiquity of Online Social Networks is creating new sources for healthcare information particularly in the context of pharmaceutical drugs . We aimed to examine the impact of a given OSN s characteristics on the content of pharmaceutical drug discussions from that OSN . We compared the effect of four distinguishing characteristics from ten different OSNs on the content of their pharmaceutical drug discussions General versus Health OSN OSN moderation OSN registration requirements and OSNs with a question and answer format . The effects of these characteristics were measured both quantitatively and qualitatively . Our results show that an OSN s characteristics indeed affect the content of its discussions . Based on their information needs healthcare providers may use our findings to pick the right OSNs or to advise patients regarding their needs . Our results may also guide the creation of new and more effective domain specific health OSNs . Further future researchers of online healthcare content in OSNs may find our results informative while choosing OSNs as data sources . We reported several findings about the impact of OSN characteristics on the content of pharmaceutical drug discussion and synthesized these findings into actionable items for both healthcare providers and future researchers of healthcare discussions on OSNs . Future research on the impact of OSN characteristics could include user demographics quality and safety of information and efficacy of OSN usage . \\n'],\n",
              " [' The workflow models of the patient journey in a Pediatric Emergency Department seems to be an effective approach to develop an accurate and complete representation of the PED processes . This model can drive the collection of comprehensive quantitative and qualitative service delivery and patient treatment data as an evidence base for the PED service planning . Our objective in this study is to identify crowded situation indicators and bottlenecks that contribute to over crowding . The greatest source of delay in patient flow is the waiting time from the health care request and especially the bed request to exit from the PED for hospital admission . It represented 70 of the time that these patients occupied in the PED waiting rooms . The use of real data to construct the workflow model of the patient path is effective in identifying sources of delay in patient flow and aspects of the PED activity that could be improved . The development of this model was based on accurate visits made in the PED of the Regional University Hospital Center of Lille . This modeling which has to represent most faithfully possible the reality of the PED of CHRU of Lille is necessary . It must be detailed enough to produce an analysis allowing to identify the dysfunctions of the PED and also to propose and to estimate prevention indicators of crowded situations . Our survey is integrated into the French National Research Agency project titled Hospital Optimization Simulation and avoidance of strain . H pital Optimisation Simulation et vitement des Tensions . \\n'],\n",
              " [' Risk stratification is instrumental to modern clinical decision support systems . Comprehensive risk stratification should be able to provide the clinicians with not only the accurate assessment of a patient s risk but also the clinical context to be acted upon . However existing risk stratification techniques mainly focus on predicting the risk score for individual patients at the cohort level they offer little insight beyond a flat score based segmentation . This essentially reduces a patient to a score and thus removes him her from his her clinical context . To address this limitation in this paper we propose a bilinear model for risk stratification that simultaneously captures the three key aspects of risk stratification it predicts the risk of each individual patient it stratifies the patient cohort based on not only the risk score but also the clinical characteristics and it embeds all patients into clinical contexts with clear interpretation . We apply our model to a cohort of 4977 patients 1127 among which were diagnosed with Congestive Heart Failure . We demonstrate that our model can not only accurately predict the onset risk of CHF but also provide rich and actionable clinical insights into the patient cohort . \\n'],\n",
              " [' The focus of this paper is on the challenges and opportunities presented by developing scenarios of use for interactive medical devices . Scenarios are integral to the international standard for usability engineering of medical devices and are also applied to the development of health software . The 62366 standard lays out a process for mitigating risk during normal use . However this begs the question of whether real use matches normal use . In this paper we present an overview of the product lifecycle and how it impacts on the type of scenario that can be practically applied . We report on the development and testing of a set of scenarios intended to inform the design of infusion pumps based on real use . The scenarios were validated by researchers and practitioners experienced in clinical practice and their utility was assessed by developers and practitioners representing different stages of the product lifecycle . These evaluations highlighted previously unreported challenges and opportunities for the use of scenarios in this context . Challenges include integrating scenario based design with usability engineering practice covering the breadth of uses of infusion devices and managing contradictory evidence . Opportunities included scenario use beyond design to guide marketing to inform purchasing and as resources for training staff . This study exemplifies one empirically grounded approach to communicating and negotiating the realities of practice . \\n'],\n",
              " [' The task of recognizing and normalizing protein name mentions in biomedical literature is a challenging task and important for text mining applications such as protein protein interactions pathway reconstruction and many more . In this paper we present ProNormz an integrated approach for human proteins tagging and normalization . In Homo sapiens a greater number of biological processes are regulated by a large human gene family called protein kinases by post translational phosphorylation . Recognition and normalization of human protein kinases is considered to be important for the extraction of the underlying information on its regulatory mechanism from biomedical literature . ProNormz distinguishes HPKs from other HPs besides tagging and normalization . To our knowledge ProNormz is the first normalization system available to distinguish HPKs from other HPs in addition to gene normalization task . ProNormz incorporates a specialized synonyms dictionary for human proteins and protein kinases a set of 15 string matching rules and a disambiguation module to achieve the normalization . Experimental results on benchmark BioCreative II training and test datasets show that our integrated approach achieve a fairly good performance and outperforms more sophisticated semantic similarity and disambiguation systems presented in BioCreative II GN task . As a freely available web tool ProNormz is useful to developers as extensible gene normalization implementation to researchers as a standard for comparing their innovative techniques and to biologists for normalization and categorization of HPs and HPKs mentions in biomedical literature . URL http www.biominingbu.org pronormz . \\n'],\n",
              " [' Objective In medical information retrieval research semantic resources have been mostly used by expanding the original query terms or estimating the concept importance weight . However implicit term dependency information contained in semantic concept terms has been overlooked or at least underused in most previous studies . In this study we incorporate a semantic concept based term dependence feature into a formal retrieval model to improve its ranking performance . Design Standardized medical concept terms used by medical professionals were assumed to have implicit dependency within the same concept . We hypothesized that by elaborately revising the ranking algorithms to favor documents that preserve those implicit dependencies the ranking performance could be improved . The implicit dependence features are harvested from the original query using MetaMap . These semantic concept based dependence features were incorporated into a semantic concept enriched dependence model . We designed four different variants of the model with each variant having distinct characteristics in the feature formulation method . Measurements We performed leave one out cross validations on both a clinical document corpus and a medical literature corpus which are representative test collections in medical information retrieval research . Results Our semantic concept enriched dependence model consistently outperformed other state of the art retrieval methods . Analysis shows that the performance gain has occurred independently of the concept s explicit importance in the query . Conclusion By capturing implicit knowledge with regard to the query term relationships and incorporating them into a ranking model we could build a more robust and effective retrieval model independent of the concept importance . \\n'],\n",
              " [' An ever increasing amount of medical data such as electronic health records is being collected stored shared and managed in large online health information systems and electronic medical record systems . From such rich collections data is often published in the form of census and statistical data sets for the purpose of knowledge sharing and enabling medical research . This brings with it an increasing need for protecting individual people privacy and it becomes an issue of great importance especially when information about patients is exposed to the public . While the concept of data privacy has been comprehensively studied for relational data models and algorithms addressing the distinct differences and complex structure of XML data are yet to be explored . Currently the common compromise method is to convert private XML data into relational data for publication . This ad hoc approach results in significant loss of useful semantic information previously carried in the private XML data . Health data often has very complex structure which is best expressed in XML . In fact XML is the standard format for exchanging and publishing health information . Lack of means to deal directly with data in XML format is inevitably a serious drawback . In this paper we propose a novel privacy protection model for XML and an algorithm for implementing this model . We provide general rules both for transforming a private XML schema into a published XML schema and for mapping private XML data to the new privacy protected published XML data . In addition we propose a new privacy property dependency which can be applied to both relational and XML data and that takes into consideration the hierarchical nature of sensitive data . Lastly we provide an implementation of our model algorithm and privacy property and perform an experimental analysis to demonstrate the proposed privacy scheme in practical application . \\n'],\n",
              " [' Objectives New DNA sequencing technologies have revolutionized the search for genetic disruptions . Targeted sequencing of all protein coding regions of the genome called exome analysis is actively used in research oriented genetics clinics with the transition to exomes as a standard procedure underway . This transition is challenging identification of potentially causal mutation amongst 106 variants requires specialized computation in combination with expert assessment . This study analyzes the usability of user interfaces for clinical exome analysis software . There are two study objectives To ascertain the key features of successful user interfaces for clinical exome analysis software based on the perspective of expert clinical geneticists To assess user system interactions in order to reveal strengths and weaknesses of existing software inform future design and accelerate the clinical uptake of exome analysis . Methods Surveys interviews and cognitive task analysis were performed for the assessment of two next generation exome sequence analysis software packages . The subjects included ten clinical geneticists who interacted with the software packages using the think aloud method . Subjects interactions with the software were recorded in their clinical office within an urban research and teaching hospital . All major user interface events were time stamped and annotated with coding categories to identify usability issues in order to characterize desired features and deficiencies in the user experience . Results We detected 193 usability issues the majority of which concern interface layout and navigation and the resolution of reports . Our study highlights gaps in specific software features typical within exome analysis . The clinicians perform best when the flow of the system is structured into well defined yet customizable layers for incorporation within the clinical workflow . The results highlight opportunities to dramatically accelerate clinician analysis and interpretation of patient genomic data . Conclusion We present the first application of usability methods to evaluate software interfaces in the context of exome analysis . Our results highlight how the study of user responses can lead to identification of usability issues and challenges and reveal software reengineering opportunities for improving clinical next generation sequencing analysis . While the evaluation focused on two distinctive software tools the results are general and should inform active and future software development for genome analysis software . As large scale genome analysis becomes increasingly common in healthcare it is critical that efficient and effective software interfaces are provided to accelerate clinical adoption of the technology . Implications for improved design of such applications are discussed . \\n'],\n",
              " [' This paper proposes the all IP WSNs for real time patient monitoring . In this paper the all IP WSN architecture based on gateway trees is proposed and the hierarchical address structure is presented . Based on this architecture the all IP WSN can perform routing without route discovery . Moreover a mobile node is always identified by a home address and it does not need to be configured with a care of address during the mobility process so the communication disruption caused by the address change is avoided . Through the proposed scheme a physician can monitor the vital signs of a patient at any time and at any places and according to the IPv6 address he can also obtain the location information of the patient in order to perform effective and timely treatment . Finally the proposed scheme is evaluated based on the simulation and the simulation data indicate that the proposed scheme might effectively reduce the communication delay and control cost and lower the packet loss rate . \\n'],\n",
              " [' One of the main reasons that leads to a low adoption rate of telemedicine systems is poor usability . An aspect that influences usability during the reporting of findings is the input mode e.g . if a free text or a structured report interface is employed . The objective of our study is to compare the usability of FT and ST telemedicine systems specifically in terms of user satisfaction efficiency and general usability . We comparatively evaluate the usability of these two input modes in a telecardiology system for issuing electrocardiography reports in the context of a statewide telemedicine system in Brazil with more than 350.000 performed tele electrocardiography examinations . We adopted a multiple method research strategy applying three different kinds of usability evaluations user satisfaction was evaluated through interviews with seven medical professionals using the System Usability Scale questionnaire and specific questions related to adequacy and user experience . Efficiency was evaluated by estimating execution time using the Keystroke Level Model . General usability was assessed based on the conformity of the systems to a set of e health specific usability heuristics . The results of this comparison provide a first indication that a structured report input mode for such a system is more satisfactory and efficient with a larger conformity to usability heuristics than free text input . User satisfaction using the SUS questionnaire has been scored in average with 58.8 and 77.5 points for the FT and SR system respectively which means that the SR system was rated 18.65 points higher than the FT system . In terms of efficiency the completion of a findings report using the SR mode is estimated to take 8.5s 3.74 times faster than using the FT system . The SR system also demonstrated less violations to usability heuristics in comparison to 14 points observed in the FT system . These results provide a first indication that the usage of structured reporting as an input mode in telecardiology systems may enhance usability . This also seems to confirm the advantages of the usage of structured reporting as already described in the literature for other areas such as teleradiology . \\n'],\n",
              " [' To date the utilitarian benefits of online consumption have only been partially investigated . This study undertakes an exhaustive approach to fully delimit the dimensional structure related to the utilitarian motivations for online consumption . First an in depth literature review is carried out in order to allow the proposal of an aprioristic base structure of eleven categories of utilitarian motivations . Next qualitative analyses are applied to assess and eventually refine the structure of utilitarian motivations proposed after the literature review their labels and respective measurement scales . Finally this qualitative phase concludes with ten motivational categories and 46 items . Then quantitative analyses are applied based on a questionnaire administered to a sample of 667 Internet users to keep refining and to eventually validate both the dimensional structure of motivations and the related measurement scales . Finally a structure of 9 utilitarian motivations is established with the following labels assortment economy convenience availability of information adaptability customization desire for control payment services anonymity and absence of social interaction . The nomological validity of this structure is satisfactorily tested using a second order factor model . The article finishes by discussing some implications for practitioners . \\n'],\n",
              " [' Researchers have found that price dispersion and market inefficiency exists in electronic marketplaces . Little attention has been bestowed to explore difference in market efficiency between traditional and electronic marketplaces . This study integrates both product and channel preference factors to analyze differences in market efficiency between electronic and traditional shopping environments . Data Envelopment Analysis is applied to calculate market efficiency for single channel and multi channel shoppers . Results show that market efficiencies vary across consumer segments and products . In summary this paper enhances understanding of market efficiency by incorporating behavioral segment and product characteristics into the explanatory framework . \\n'],\n",
              " [' Objective Structured data on mammographic findings are difficult to obtain without manual review . We developed and evaluated a rule based natural language processing system to extract mammographic findings from free text mammography reports . Materials and Methods The NLP system extracted four mammographic findings mass calcification asymmetry and architectural distortion using a dictionary look up method on 93 705 mammography reports from Group Health . Status annotations and anatomical location annotation were associated to each NLP detected finding through association rules . After excluding negated uncertain and historical findings affirmative mentions of detected findings were summarized . Confidence flags were developed to denote reports with highly confident NLP results and reports with possible NLP errors . A random sample of 100 reports was manually abstracted to evaluate the accuracy of the system . Results The NLP system correctly coded 96 99 out of our sample of 100 reports depending on findings . Measures of sensitivity specificity and negative predictive values exceeded 0.92 for all findings . Positive predictive values were relatively low for some findings due to their low prevalence . Discussion Our NLP system was implemented entirely in SAS Base which makes it portable and easy to implement . It performed reasonably well with multiple applications such as using confidence flags as a filter to improve the efficiency of manual review . Refinements of library and association rules and testing on more diverse samples may further improve its performance . Conclusion Our NLP system successfully extracts clinically useful information from mammography reports . Moreover SAS is a feasible platform for implementing NLP algorithms . \\n'],\n",
              " [' This article aims at assessing the progress of mobile payment research over the last 8years . A previous literature review covering articles published between 1999 and 2006 showed that the majority of research had only focused on a few topics . In order to address this issue a research agenda was formulated to encourage researchers to explore new topics . Almost a decade later our review reveals that researchers have continued to focus on the same topics with a limited accumulation of new knowledge and similar findings . In addition to reviewing the literature we discuss the possible reasons for the lack of research diversity and propose new recommendations to enhance future mobile payment research . \\n'],\n",
              " [' In 1982 Betamax the world s first personal recording service was ruled as a fair use in court . Although the copyright holders of TV content claimed that Betamax was an infringement of copyright the court determined that the benefits of personal recording services were significant and that the copyright holder s profits could be protected because the original service was of better quality and had a better cost structure . It also ruled that the loss from manual advertisement skip was minimal . However recent advancements in information technology have allowed new kinds of personal recording services such as a cloud DVR that provides unlimited storage and flawless quality and an Auto hop feature that automatically removes embedded advertisements . This paper introduces a microeconomic model for reviewing the copyright holder s business model and social welfare under the court s decision in relation to newer personal recording services powered by information technologies . Before cloud DVR existed applying fair use to personal recording services increased social welfare while protecting the copyright holder s profits however after the introduction of cloud DVR it may no longer do so . \\n'],\n",
              " [' For the purpose of post marketing drug safety surveillance which has traditionally relied on the voluntary reporting of individual cases of adverse drug events other sources of information are now being explored including electronic health records which give us access to enormous amounts of longitudinal observations of the treatment of patients and their drug use . Adverse drug events which can be encoded in EHRs with certain diagnosis codes are however heavily underreported . It is therefore important to develop capabilities to process by means of computational methods the more unstructured EHR data in the form of clinical notes where clinicians may describe and reason around suspected ADEs . In this study we report on the creation of an annotated corpus of Swedish health records for the purpose of learning to identify information pertaining to ADEs present in clinical notes . To this end three key tasks are tackled recognizing relevant named entities labeling attributes of the recognized entities and relationships between them . For each of the three tasks leveraging models of distributional semantics i.e . unsupervised methods that exploit co occurrence information to model typically in vector space the meaning of words and in particular combinations of such models is shown to improve the predictive performance . The ability to make use of such unsupervised methods is critical when faced with large amounts of sparse and high dimensional data especially in domains where annotated resources are scarce . \\n'],\n",
              " [' This study uses eye tracking to explore the Elaboration Likelihood Model in online shopping . The results show that the peripheral cue did not have moderating effect on purchase intention but had moderating effect on eye movement . Regarding purchase intention the high elaboration had higher purchase intention than the low elaboration with a positive peripheral cue but there was no difference in purchase intention between the high and low elaboration with a negative peripheral cue . Regarding eye movement with a positive peripheral cue the high elaboration group was observed to have longer fixation duration than the low elaboration group in two areas of interest however with a negative peripheral cue the low elaboration group had longer fixation on the whole page and two AOIs . In addition the relationship between purchase intention and eye movement of the AOIs is more significant in the high elaboration group when given a negative peripheral cue and in the low elaboration group when given a positive peripheral cue . This study not only examines the postulates of the ELM but also contributes to a better understanding of the cognitive processes of the ELM . These findings have practical implications for e sellers to identify characteristics of consumers elaboration in eye movement and designing customization and persuasive context for different elaboration groups in e commerce . \\n'],\n",
              " [' Despite recent progress in prediction and prevention heart disease remains a leading cause of death . One preliminary step in heart disease prediction and prevention is risk factor identification . Many studies have been proposed to identify risk factors associated with heart disease however none have attempted to identify all risk factors . In 2014 the National Center of Informatics for Integrating Biology and Beside issued a clinical natural language processing challenge that involved a track for identifying heart disease risk factors in clinical texts over time . This track aimed to identify medically relevant information related to heart disease risk and track the progression over sets of longitudinal patient medical records . Identification of tags and attributes associated with disease presence and progression risk factors and medications in patient medical history were required . Our participation led to development of a hybrid pipeline system based on both machine learning based and rule based approaches . Evaluation using the challenge corpus revealed that our system achieved an F1 score of 92.68 making it the top ranked system of the 2014 i2b2 clinical NLP challenge . \\n'],\n",
              " [' Advertisement options are a recent development in online advertising . Simply an ad option is a first look contract in which a publisher or search engine grants an advertiser a right but not obligation to enter into transactions to purchase impressions or clicks from a specific ad slot at a pre specified price on a specific delivery date . Such a structure provides advertisers with more flexibility of their guaranteed deliveries . The valuation of ad options is an important topic and previous studies on ad options pricing have been mostly restricted to the situations where the underlying prices follow a geometric Brownian motion . This assumption is reasonable for sponsored search however some studies have also indicated that it is not valid for display advertising . In this paper we address this issue by employing a stochastic volatility model and discuss a lattice framework to approximate the proposed SV model in option pricing . Our developments are validated by experiments with real advertising data we find that the SV model has a better fitness over the GBM model we validate the proposed lattice model via two sequential Monte Carlo simulation methods we demonstrate that advertisers are able to flexibly manage their guaranteed deliveries by using the proposed options and publishers can have an increased revenue when some of their inventories are sold via ad options . \\n'],\n",
              " [' Online reviews as one kind of quality indicator of products or service are becoming increasingly important in influencing purchase decisions of prospective consumers on electronic commerce websites . With the fast growth of the Chinese e commerce industry it is thus indispensable to design effective online review systems for e commerce websites in the Chinese context by taking into account cultural factors . In this paper we conduct two empirical studies on online reviews . Firstly we study how culture differences across countries impact the way in which consumers provide online reviews . Secondly we investigate the impact of online reviews on product sales in the Chinese context and show that directly copying the ideas of successful online review systems in the USA will deteriorate the effectiveness of the systems in China . Finally we propose several suggestions for the development of effective online review systems in the Chinese context based on the results of our two empirical studies and the findings in previous studies . \\n'],\n",
              " [' Background Drug repositioning is the process of finding new indications for existing drugs . Its importance has been dramatically increasing recently due to the enormous increase in new drug discovery cost . However most of the previous molecular centered drug repositioning work is not able to reflect the end point physiological activities of drugs because of the inherent complexity of human physiological systems . Methods Here we suggest a novel computational framework to make inferences for alternative indications of marketed drugs by using electronic clinical information which reflects the end point physiological results of drug s effects on the biological activities of humans . In this work we use the concept of complementarity between clinical disease signatures and clinical drug effects . With this framework we establish disease related clinical variable vectors and drug related clinical variable vectors by applying two methodologies . Finally we assign a repositioning possibility score to each disease drug pair by the calculation of complementarity and association between clinical states of disease signatures and clinical effects of drugs . A total of 717 clinical variables in the electronic clinical dataset are considered in this study . Results The statistical significance of our prediction results is supported through two benchmark datasets . We discovered not only lots of known relationships between diseases and drugs but also many hidden disease drug relationships . For example glutathione and edetic acid may be investigated as candidate drugs for asthma treatment . We examined prediction results by using statistical experiments and presented evidences for those with already published literature . Conclusion The results show that electronic clinical information is a feasible data resource and utilizing the complementarity between clinical signatures of disease and clinical effects of drugs is a potentially predictive concept in drug repositioning research . It makes the proposed approach useful to identity novel relationships between diseases and drugs that have a high probability of being biologically valid . \\n'],\n",
              " [' Studies have shown that perceptual maps derived from online consumer generated data are effective for depicting market structure such as demonstrating positioning of competitive brands . However most text mining algorithms would require manual reading to merge extracted product features with synonyms . In response Topic modeling is introduced to group synonyms together under a topic automatically leading to convenient and accurate evaluation of brands based on consumers online reviews . To ensure the feasibility of employing Topic modeling in assessing competitive brands we developed a unique and novel framework named WVAP based on Scree plot technique . WVAP can filter the noises in posterior distribution obtained from Topic modeling and improve accuracy in brand evaluation . A case study exploring online reviews of mobile phones is conducted . We extract topics to reflect the features of the cell phones with a qualified validity . In addition to perceptual maps derived by multi dimensional scaling for product positioning we also rank these products by TOPSIS so as to visualize the market structure from different perspectives . Our case study of cell phones shows that the proposed framework is effective in mining online reviews and providing insights into the competitive landscape . \\n'],\n",
              " [' Owing to the limited information possessed by patients there exists significant information asymmetry between patients and physicians . In addition as services are intangible inseparable and heterogeneous patients are difficult to judge the physicians service quality . With the development of online healthcare services healthcare websites currently provide more information for patients such as patient generated and system generated information . Those kinds of information can reflect the quality of physicians service outcome and delivery process to help patients to select physicians . However there is scant research on the role of patient generated and system generated information in patients online behavior . Collecting data from a healthcare website this paper develops a two equation model to verify the effects of two kinds of information on patients search evaluation and decision making on healthcare websites . The results of our empirical research show that positive patient generated and system generated information on physicians service quality positively impact patients reactions at different stages . Moreover we also find that synergies between patient generated and system generated information are positively associated with patients decisions to consult a physician . These findings provide valuable contributions to the online healthcare research . \\n'],\n",
              " [' In e commerce applications vendors can construct detailed profiles about customers preferences which is known as buyer profiling . These profiles can then be used by vendors in order to perform practices such as price discrimination poor judgment etc . The use of pseudonyms and specially changing pseudonyms from time to time are known to minimize profiling minimizing the capacity of vendors to perform such practices in turn . Although there are some frameworks and tools that support pseudonym change there are few proposals that suggest or directly change the pseudonym in an automated fashion . Instead users are usually provided with the mechanisms to change pseudonyms but without any advise on when they should actually use these mechanisms . In this paper we present an approach to control buyer profiling by means of automated pseudonym changes performed according to human privacy attitudes . We also present an application scenario and an evaluation of our proposal . \\n'],\n",
              " [' Cognitive Informatics is a burgeoning interdisciplinary domain comprising of the cognitive and information sciences that focuses on human information processing mechanisms and processes within the context of computing and computer applications . Based on a review of articles published in the Journal of Biomedical Informatics between January 2001 and March 2014 we identified 57 articles that focused on topics related to cognitive informatics . We found that while the acceptance of CI into the mainstream informatics research literature is relatively recent its impact has been significant from characterizing the limits of clinician problem solving and reasoning behavior to describing coordination and communication patterns of distributed clinical teams to developing sustainable and cognitively plausible interventions for supporting clinician activities . Additionally we found that most research contributions fell under the topics of decision making usability and distributed team activities with a focus on studying behavioral and cognitive aspects of clinical personnel as they performed their activities or interacted with health information systems . We summarize our findings within the context of the current areas of CI research future research directions and current and future challenges for CI researchers . We are at a turbulent yet exciting phase in healthcare turbulent as the transformations in healthcare practice have been driven by paradigmatic shift toward the use of health information technology both as a result of necessity and federal mandates exciting as such transformations have highlighted the central role of cognitive and behavioral sciences in developing usable systems that can provide high quality patient care . While there is a bright future in terms of opportunities for researchers and practitioners who seek to engage in cognitive science research it is also important to reflect on past research to understand the historical context and foundations of the development of cognitive research in biomedical informatics the theories constructs and frameworks that drive the current research and the potential directions for future research . Within this focus this special communication provides a broader context of the cognitive and behavioral research on HIT in biomedical informatics . In addition we have also created a virtual issue of the Journal of Biomedical Informatics that will provide a snapshot of the research that has been published in JBI pertaining to cognitive and social science research . Cognitive science is an interdisciplinary field that draws from psychology computer science linguistics philosophy and anthropology to understand human activities including reasoning decision making and problem solving . Principles from cognitive science have been applied for studying the usability of medical devices and interfaces developing training educational interventions and guidelines streamlining and improving workflow and clinical processes and for understanding the process of clinical judgment reasoning and decision making . In summary cognitive science provides a viable mechanism to inform our understanding in technology rich clinical environments and represents an important component of biomedical informatics . Additionally cognitive research has been a key to shaping and structuring the use of HIT adapting to the various needs of the clinical environment . Cognitive informatics by. \\n'],\n",
              " [' Introduction While mammography notably contributes to earlier detection of breast cancer it has its limitations including a large number of false positive exams . Improved radiology education could potentially contribute to alleviating this issue . Toward this goal in this paper we propose an algorithm for modeling of false positive error making among radiology trainees . Identifying troublesome locations for the trainees could focus their training and in turn improve their performance . Methods The algorithm proposed in this paper predicts locations that are likely to result in a false positive error for each trainee based on the previous annotations made by the trainee . The algorithm consists of three steps . First the suspicious false positive locations are identified in mammograms by Difference of Gaussian filter and suspicious regions are segmented by computer vision based segmentation algorithms . Second 133 features are extracted for each suspicious region to describe its distinctive characteristics . Third a random forest classifier is applied to predict the likelihood of the trainee making a false positive error using the extracted features . The random forest classifier is trained using previous annotations made by the trainee . We evaluated the algorithm using data from a reader study in which 3 experts and 10 trainees interpreted 100 mammographic cases . Results The algorithm was able to identify locations where the trainee will commit a false positive error with accuracy higher than an algorithm that selects such locations randomly . Specifically our algorithm found false positive locations with 40 accuracy when only 1 location was selected for all cases for each trainee and 12 accuracy when 10 locations were selected . The accuracies for randomly identified locations were both 0 for these two scenarios . Conclusions In this first study on the topic we were able to build computer models that were able to find locations for which a trainee will make a false positive error in images that were not previously seen by the trainee . Presenting the trainees with such locations rather than randomly selected ones may improve their educational outcomes . \\n'],\n",
              " [' Concept drift is a common phenomenon in stock market that can cause the devaluation of the knowledge learned from cross sectional analysis as the market changes over time in unforeseen ways . The widely used cross sectional regression analysis based on expert knowledge has obvious limitations in handling problems that involve concept drift and high dimensional data . To discover causal relations between portfolio selection factors and stock returns and identify concept drifts of these relations we apply a novel causal discovery technique called modified Additive Noise Model with Conditional Probability Table . In evaluation experiments we compares ANMCPT to the conventional cross sectional analysis approach in mining relationships between portfolio selection factors and stock returns . Results indicate that the factors selected by ANMCPT outperform the factors adopted in most previous cross sectional researches that followed the Fama French framework . To the best of our knowledge this paper is the first to compare causal inference technique with Fama French framework in concept drift mining of stock portfolio selection factors . Our causal inference based concept drift mining method provides a new approach to accurate knowledge discovery in stock market . \\n'],\n",
              " [' In Electronic Health Records much of valuable information regarding patients conditions is embedded in free text format . Natural language processing techniques have been developed to extract clinical information from free text . One challenge faced in clinical NLP is that the meaning of clinical entities is heavily affected by modifiers such as negation . A negation detection algorithm NegEx applies a simplistic approach that has been shown to be powerful in clinical NLP . However due to the failure to consider the contextual relationship between words within a sentence NegEx fails to correctly capture the negation status of concepts in complex sentences . Incorrect negation assignment could cause inaccurate diagnosis of patients condition or contaminated study cohorts . We developed a negation algorithm called DEEPEN to decrease NegEx s false positives by taking into account the dependency relationship between negation words and concepts within a sentence using Stanford dependency parser . The system was developed and tested using EHR data from Indiana University and it was further evaluated on Mayo Clinic dataset to assess its generalizability . The evaluation results demonstrate DEEPEN which incorporates dependency parsing into NegEx can reduce the number of incorrect negation assignment for patients with positive findings and therefore improve the identification of patients with the target clinical findings in EHRs . \\n'],\n",
              " [' Despite the huge growth potential that has been predicted for in app purchases and the mobile game market little is known about what motivates game players to make such purchases . The purpose of this paper is to build a research model based on the loyalty literature and studies of value theory to identify the antecedents of in app purchase intention in the context of mobile games . The proposed model was empirically evaluated using a web survey of 3309 mobile game players 813 nonpaying players and 2496 paying players . Structural equation modeling was used to assess the research model . The results reveal that loyalty to the mobile game has significant influence on a player s intention to make an in app purchase . The perceived values of the game have direct influence on the loyalty of all players but appear to have relatively little impact on the purchase intentions of nonpaying players . Two values were found to have a direct impact on a player s intention to make an in app purchase . Specifically our study revealed differences between paying users and nonpaying users . This study provides a better understanding of how the values influence loyalty among all players of the game and the purchase intentions of paying and nonpaying players . Further insights into mobile game app marketing strategies are provided as well . \\n'],\n",
              " [' Objective Some phase 1 clinical trials offer strong financial incentives for healthy individuals to participate in their studies . There is evidence that some individuals enroll in multiple trials concurrently . This creates safety risks and introduces data quality problems into the trials . Our objective was to construct a privacy preserving protocol to track phase 1 participants to detect concurrent enrollment . Design A protocol using secure probabilistic querying against a database of trial participants that allows for screening during telephone interviews and on site enrollment was developed . The match variables consisted of demographic information . Measurement The accuracy of the matching and its computational performance in seconds were measured under simulated environments . Accuracy was also compared to non secure matching methods . Results The protocol performance scales linearly with the database size . At the largest database size of 20 000 participants a query takes under 20s on a 64 cores machine . Sensitivity precision and negative predictive value of the queries were consistently at or above 0.9 and were very similar to non secure versions of the protocol . Conclusion The protocol provides a reasonable solution to the concurrent enrollment problems in phase 1 clinical trials and is able to ensure that personal information about participants is kept secure . \\n'],\n",
              " [' In this study a novel approach via the composite of fuzzy controllers and dithers is presented . According to this approach we can synthesize a set of fuzzy controllers and find appropriate dithers to stabilize nonlinear multiple time delay interconnected systems . A robustness design of model based fuzzy control is first proposed to overcome the effect of modeling errors between the NMTD interconnected subsystems and Takagi Sugeno fuzzy models . In terms of Lyapunov s direct method a delay dependent stability criterion is then derived to guarantee the asymptotic stability of NMTD interconnected systems . Based on this criterion and the decentralized control scheme a set of model based fuzzy controllers is synthesized via the technique of parallel distributed compensation to stabilize the NMTD interconnected system . When the designed fuzzy controllers can not stabilize the NMTD interconnected systems a batch of high frequency signals is simultaneously introduced to stabilize it . If the frequencies of dithers are high enough the outputs of the dithered interconnected system and those of its corresponding mathematical model the relaxed interconnected system can be made as close as desired . This makes it possible to obtain a rigorous prediction of the stability of the dithered interconnected system based on the one of the relaxed interconnected system . Finally a numerical example is given to illustrate the feasibility of our approach . \\n'],\n",
              " [' Analytic Hierarchy Process is increasingly applied to healthcare and medical research and applications . However knowledge representation of pairwise reciprocal matrix is still dubious . This research discusses the related drawbacks and recommends pairwise opposite matrix as the ideal alternative . Pairwise opposite matrix is the key foundation of Primitive Cognitive Network Process which revises the AHP approach with practical changes . A medical decision treatment evaluation using AHP is revised by P CNP with a step by step tutorial . Comparisons with AHP have been discussed . The proposed method could be a promising decision tool to replace AHP to share information among patients or and doctors and to evaluate therapies medical treatments health care technologies medical resources and healthcare policies . \\n'],\n",
              " [' This paper proposes a novel multi objective model for an unrelated parallel machine scheduling problem considering inherent uncertainty in processing times and due dates . The problem is characterized by non zero ready times sequence and machine dependent setup times and secondary resource constraints for jobs . Each job can be processed only if its required machine and secondary resource are available at the same time . Finding optimal solution for this complex problem in a reasonable time using exact optimization tools is prohibitive . This paper presents an effective multi objective particle swarm optimization algorithm to find a good approximation of Pareto frontier where total weighted flow time total weighted tardiness and total machine load variation are to be minimized simultaneously . The proposed MOPSO exploits new selection regimes for preserving global as well as personal best solutions . Moreover a generalized dominance concept in a fuzzy environment is employed to find locally Pareto optimal frontier . Performance of the proposed MOPSO is compared against a conventional multi objective particle swarm optimization algorithm over a number of randomly generated test problems . Statistical analyses based on the effect of each algorithm on each objective space show that the proposed MOPSO outperforms the CMOPSO in terms of quality diversity and spacing metrics . \\n'],\n",
              " [' Merging sustainable development with the business and taking goals into account from its three dimensions which are derived from customer and stakeholder requirements have been a potential source of competitive differentiation for firms . Academic and corporate interest in sustainable supply chain management has also risen considerably in recent years . This paper examines the components and elements of SSC management and how they serve as a foundation for an evaluation framework . By using quality function deployment as a product system planning and improvement tool an effective SSC structure can be obtained . QFD uses a matrix called the House of Quality and constructing the HoQ is a critical step in the application of QFD as it translates customer requirements into engineering characteristics . However participants of HoQ construction sessions tend to provide information about their individual judgments in multiple formats such as numerically or linguistically depending on their different knowledge experience culture and circumstance . Furthermore they can generate incomplete preferences which are challenging to assess in a consistent way . Therefore the objective of this study is to apply an extended QFD methodology in SSC by introducing a new group decision making approach that takes multiple preference formats and incomplete information into account and fusions different formats of expressions into one uniform group decision by means of the fuzzy set theory . To assess the validity of the proposed approach a case study conducted at HAVI Logistics Turkey is also presented in the paper . \\n'],\n",
              " [' Missing data in large insurance datasets affects the learning and classification accuracies in predictive modelling . Insurance datasets will continue to increase in size as more variables are added to aid in managing client risk and will therefore be even more vulnerable to missing data . This paper proposes a hybrid multi layered artificial immune system and genetic algorithm for partial imputation of missing data in datasets with numerous variables . The multi layered artificial immune system creates and stores antibodies that bind to and annihilate an antigen . The genetic algorithm optimises the learning process of a stimulated antibody . The evaluation of the imputation is performed using the RIPPER k nearest neighbour na ve Bayes and logistic discriminant classifiers . The effect of the imputation on the classifiers is compared with that of the mean mode and hot deck imputation methods . The results demonstrate that when missing data imputation is performed using the proposed hybrid method the classification improves and the robustness to the amount of missing data is increased relative to the mean mode method for data missing completely at random missing at random and not missing at random .The imputation performance is similar to or marginally better than that of the hot deck imputation . \\n'],\n",
              " [' This paper presents a novel soft computing based solution to a complex optimal control or dynamic optimization problem that requires the solution to be available in real time . The complexities in this problem of optimal guidance of interceptors launched with high initial heading errors include the more involved physics of a three dimensional missile target engagement and those posed by the assumption of a realistic dynamic model such as time varying missile speed thrust drag and mass besides gravity and upper bound on the lateral acceleration . The classic pure proportional navigation law is augmented with a polynomial function of the heading error and the values of the coefficients of the polynomial are determined using differential evolution . The performance of the proposed DE enhanced guidance law is compared against the existing conventional laws in the literature on the criteria of time and energy optimality peak lateral acceleration demanded terminal speed and robustness to unanticipated target maneuvers to illustrate the superiority of the proposed law . \\n'],\n",
              " [' A methodology for designing semi physical fuzzy models is proposed . Prior physical knowledge about the dynamics of the system is modeled with continuous time differential equations . Fuzzy knowledge bases are embedded in these equations as nonlinear constructive blocks . Rules comprising the knowledge bases are fitted to interval valued data with metaheuristics . A possibilistic filter is proposed that is able to gradually evolve an initial estimation of the latent variables of the model on the basis of successive prediction errors . This methodology has been applied to the prediction of voltage and state of charge of LiFePO4 batteries . An empirical study has been carried over data gathered in experiments at the Battery Laboratory at Oviedo University . Fitting between the proposed model and actual measurements is studied for four different manufacturers and different charge discharge patterns . Predictions of the evolution of the voltage during charge discharge and inactivity compare favorably to different models in the literature . The possibilistic filter allows to estimate the state of charge of batteries after an arbitrary path that may include partial charges and discharges . It is shown that the accuracy of the open loop model improves that of other approaches in the literature and at the same time the observer based online model is able to approximate the effective remnant charge of the battery after a reasonably short time . \\n'],\n",
              " [' This paper proposes a method for finding solutions of arbitrarily nonlinear systems of functional equations through stochastic global optimization . The original problem is transformed into a global optimization one by synthesizing objective functions whose global minima if they exist are also solutions to the original system . The global minimization task is carried out by the stochastic method known as fuzzy adaptive simulated annealing triggered from different starting points aiming at finding as many solutions as possible . To demonstrate the efficiency of the proposed method solutions for several examples of nonlinear systems are presented and compared with results obtained by other approaches . We consider systems composed of n equations on Euclidean spaces . \\n'],\n",
              " [' In the bacteria foraging optimization algorithm the chemotactic process is randomly set imposing that the bacteria swarm together and keep a safe distance from each other . In hybrid bacteria foraging optimization algorithm and particle swarm optimization algorithm the principle of swarming is introduced in the framework of BFAO . The hBFOA PSO algorithm is based on the adjustment of each bacterium position according to the neighborhood environment . In this paper the effectiveness of the hBFOA PSO algorithm has been tested for automatic generation control of an interconnected power system . A widely used linear model of two area non reheat thermal system equipped with proportional integral controller is considered initially for the design and analysis purpose . At first a conventional integral time multiply absolute error based objective function is considered and the performance of hBFOA PSO algorithm is compared with PSO BFOA and GA. Further a modified objective function using ITAE damping ratio of dominant eigenvalues and settling time with appropriate weight coefficients is proposed to increase the performance of the controller . Further robustness analysis is carried out by varying the operating load condition and time constants of speed governor turbine tie line power in the range of 50 to 50 as well as size and position of step load perturbation to demonstrate the robustness of the proposed hBFOA PSO optimized PI controller . The proposed approach is also extended to a non linear power system model by considering the effect of governor dead band non linearity and the superiority of the proposed approach is shown by comparing the results of craziness based particle swarm optimization approach for the identical interconnected power system . Finally the study is extended to a three area system considering both thermal and hydro units with different PI coefficients and comparison between ANFIS and proposed approach has been provided . \\n'],\n",
              " [' The ability of artificial neural networks to model the rainfall discharge relationships of karstic aquifers has been studied in the Terminio massif which supplies the Naples area with a yearly mean discharge of approximately 1 3.5m3 s. The Mediterranean climate causes a rapid increase in evapotranspiration and a decrease in rainfall towards spring summer . Especially during drought and in combination with highly sensitive climatic parameters there are dramatic changes in the discharge amount especially during the July and August months . A neural network model was developed based on MLP network to forecast of water resources three and six month before the main stress months of July and August . Example data were extracted on an ultra centenarian hydrological serial . The training and validation phases confirmed by a ten fold cross validation methodology led to a very satisfactory calibration of the ANN model with errors in forecasting discharge values of just 5 and 10 . \\n'],\n",
              " [' Stress is a major health problem in our world today . For this reason it is important to gain an objective understanding of how average individuals respond to real life events they observe in environments they encounter . Our aim is to estimate an objective stress signal for an observer of a real world environment stimulated by meditation . A computational stress signal predictor system is proposed which was developed based on a support vector machine genetic algorithm and an artificial neural network to predict the stress signal from a real world data set . The data set comprised of physiological and physical sensor response signals for stress over the time of the meditation activity . A support vector machine based individual independent classification model was developed to determine the overall shape of the stress signal and results suggested that it matched the curves formed by a linear function a symmetric saturating linear function and a hyperbolic tangent function . Using this information of the shape of the stress signal an artificial neural network based stress signal predictor was developed . Compared to the curves formed from a linear function symmetric saturating linear function and hyperbolic tangent function the stress signal produced by the stress signal predictor for the observers was the most similar to the curve formed by a hyperbolic tangent function with p 0.01 according to statistical analysis . The research presented in this paper is a new dimension in stress research it investigates developing an objective stress measure that is dependent on time . \\n'],\n",
              " [' Models based on data mining and machine learning techniques have been developed to detect the disease early or assist in clinical breast cancer diagnoses . Feature selection is commonly applied to improve the performance of models . There are numerous studies on feature selection in the literature and most of the studies focus on feature selection in supervised learning . When class labels are absent feature selection methods in unsupervised learning are required . However there are few studies on these methods in the literature . Our paper aims to present a hybrid intelligence model that uses the cluster analysis techniques with feature selection for analyzing clinical breast cancer diagnoses . Our model provides an option of selecting a subset of salient features for performing clustering and comprehensively considers the use of most existing models that use all the features to perform clustering . In particular we study the methods by selecting salient features to identify clusters using a comparison of coincident quantitative measurements . When applied to benchmark breast cancer datasets experimental results indicate that our method outperforms several benchmark filter and wrapper based methods in selecting features used to discover natural clusters maximizing the between cluster scatter and minimizing the within cluster scatter toward a satisfactory clustering quality . \\n'],\n",
              " [' Diabetes mellitus is a disease that affects to hundreds of millions of people worldwide . Maintaining a good control of the disease is critical to avoid severe long term complications . In recent years several artificial pancreas systems have been proposed and developed which are increasingly advanced . However there is still a lot of research to do . One of the main problems that arises in the automatic control of diabetes is to get a model explaining how glycemia varies with insulin food intakes and other factors fitting the characteristics of each individual or patient . This paper proposes the application of evolutionary computation techniques to obtain customized models of patients unlike most of previous approaches which obtain averaged models . The proposal is based on a kind of genetic programming based on grammars known as Grammatical Evolution . The proposal has been tested with in silico patient data and results are clearly positive . We present also a study of four different grammars and five objective functions . In the test phase the models characterized the glucose with a mean percentage average error of 13.69 modeling well also both hyper and hypoglycemic situations . \\n'],\n",
              " [' Control of power electronics converters used in PV system is very much essential for the efficient operation of the solar system . In this paper a modified incremental conduction maximum power point tracking algorithm in conjunction with an adaptive fuzzy controller is proposed to control the DC DC boost converter in the PV system under rapidly varying atmospheric and partial shading conditions . An adaptive hysteresis current controller is proposed to control the inverter . The proposed current controller provides constant switching frequency with less harmonic content compared with fixed hysteresis current control algorithm and sinusoidal PWM controller . The modeling and simulation of PV system along with the proposed controllers are done using MATLAB SIMSCAPE software . Simulation results show that the proposed MPPT algorithm is faster in transient state and presents smoother signal with less fluctuations in steady state . The hardware implementation of proposed MPPT algorithm and inverter current control algorithms using Xilinx spartran 3 FPGA is also presented . The experimental results show satisfactory performance of the proposed approaches . \\n'],\n",
              " [' Brushless DC machines are found increasing use in applications that demand high and rugged performance . In some critical circumstance such as aerospace the motor must be highly reliable . In this context a novel model based fault diagnosis system is developed for brushless DC motor speed control system . Under the consideration of the complexity of characterizing the dynamic of BLDC motor control system with analytic expression a LRGF neural network with pole assignment technique is carried out for modeling the system . During the diagnosis process fault signal of the motor is isolated with LRGFNN online . Meanwhile adaptive lifting scheme and adaptive threshold method are presented for detecting the faults from the isolated fault signal under the existence of mechanical error and electrical error . The effectiveness of the diagnosis system is demonstrated in the simulation of electrical and mechanical fault in the motor . The detection of the incipient fault is also given . \\n'],\n",
              " [' A semi integrated system for driver assistance and pedestrian safety is presented . This system is composed of a single camera which focuses on the driver for picking up visual cues and a stereo rig that focus on the road ahead for the detection of road obstructions and pedestrians . While the car is in motion the driver s viewing direction is obtained and analyzed along with information of road condition and any moving vehicle ahead in order to determine if the current driving condition is safe . In addition when the vehicle is moving slowly the system can also detect the existence of a pedestrian ahead and warns the driver if the pedestrian moves in front of the car . This system contains algorithm based safety analysis as well as fuzzy rules based analysis for interaction between variables . Our experimental results show that the condition for driver safety can be accurately classified in 94.5 of the tested driving conditions and the pedestrians can be identified in 93.18 of the tested cases . These were compared to the results of similar systems and shown to be superior . \\n'],\n",
              " [' Large number of population based Differential Evolution algorithms has been proposed in the literature . Their good performance is often reported for benchmark problems . However when applied to Neural Networks training for regression these methods usually perform poorer than classical Levenberg Marquardt algorithm . The major aim of the present paper is to clarify why In this research in which Neural Networks are used for a real world regression problem it is empirically shown that various Differential Evolution algorithms are falling into stagnation during Neural Network training . It means that after some time the individuals stop improving or improve very occasionally although the population diversity remains high . Similar behavior of Differential Evolution algorithms is observed for some but not the majority of benchmark problems . In the paper the impact of Differential Evolution population size the initialization range and bounds on Neural Networks performance is also discussed . Among tested algorithms only the Differential Evolution with Global and Local neighborhood based mutation operators performs better than the Levenberg Marquardt algorithm for Neural Networks training . This version of Differential Evolution also shows the symptoms of stagnation but much weaker than the other tested variants . To enhance exploitation in the final stage of Neural Networks training it is proposed to merge the Differential Evolution with Global and Local neighborhood based mutation operators algorithm with the Trigonometric mutation operator . This method does not rule out the stagnation problem but slightly improves the performance of trained Neural Networks . \\n'],\n",
              " [' It is undeniably crucial for a firm to be able to make a forecast regarding the sales volume of new products . However the current economic environments invariably have uncertain factors and rapid fluctuations where decision makers must draw conclusions from minimal data . Previous studies combine scenario analysis and technology substitution models to forecast the market share of multigenerational technologies . However a technology substitution model based on a logistic curve will not always fit the S curve well . Therefore based on historical data and the data forecast by both the Scenario and Delphi methods a two stage fuzzy piecewise logistic growth model with multiple objective programming is proposed herein . The piecewise concept is adopted in order to reflect the market impact of a new product such that it can be possible to determine the effective length of sales forecasting intervals even when handling a large variation in data or small size data . In order to demonstrate the model s performance two cases in the Television and Telecommunication industries are treated using the proposed method and the technology substitution model or the Norton and Bass diffusion model . A comparison of the results shows that the proposed model outperforms the technology substitution model and the Norton and Bass diffusion model . \\n'],\n",
              " [' This paper presents a detailed study about a biomedical image retrieval framework by extracting Phase Congruency features from L a b triplets of images and representing them in fuzzy feature space . These features correspond to an edge corner map of the given image . The resulting map is then processed by Scale Invariant Feature Transform to derive keypoints that are invariant to affine transformations . The ensuing features were vector quantized to build a codebook of keypoints . The codebook was produced using a Spherical Self Organizing Map built with a geodesic data structure termed as GeoSOM . Then keypoints of the query image are mapped with the codebook and their occurrences are counted to formulate a histogram termed as Phase Congruency based Bag of Keypoints . This histogram is generated offline for target images and a similarity measure was performed with the query image to yield the nearest match based on a global fuzzy membership function . Exhaustive experiments of the proposed framework named as BIRS were performed on a diverse medical image collection . Finally performance of BIRS demonstrates the advantage of the proposed image representation approach in terms of Precision Recall parameters . Furthermore relative comparison of the proposed scheme with existing feature descriptors depicts improved P R values . The proposed feature extraction and representation scheme was also robust against quantization errors . \\n'],\n",
              " [' This research addresses system reliability analysis using weakest t norm based approximate intuitionistic fuzzy arithmetic operations where failure probabilities of all components are represented by different types of intuitionistic fuzzy numbers . Due to the incomplete imprecise vague and conflicting information about the component of system the present study evaluates the reliability of system in terms of membership function and non membership function by using weakest t norm based approximate intuitionistic fuzzy arithmetic operations on different types of intuitionistic fuzzy numbers . In general interval arithmetic operations have been used to analyze the fuzzy system reliability . In complicated systems interval arithmetic operations may occur the accumulating phenomenon of fuzziness . In order to overcome the accumulating phenomenon of fuzziness this research adopts approximate intuitionistic fuzzy arithmetic operations under the weakest t norm arithmetic operations to analyze fuzzy system reliability . The approximate intuitionistic fuzzy arithmetic operations employ principle of interval arithmetic under the weakest t norm arithmetic operations . The proposed novel fuzzy arithmetic operations may obtain fitter decision values which have smaller fuzziness accumulating and successfully analyze the system reliability . Also weakest t norm arithmetic operations provide more exact fuzzy results and effectively reduce fuzzy spreads . Using proposed approach fuzzy reliability of series system and parallel system are also constructed . For numerical verification of proposed approach a malfunction of printed circuit board assembly is presented as a numerical example . The result of the proposed method is compared with the listing approaches of reliability analysis methods . \\n'],\n",
              " [' The present paper proposes a multidimensional coupled chaotic map as a pseudo random number generator . Based on an introduced dynamical systems a watermark scheme is presented . By modifying the original image and embedding a watermark in the difference values within the original image the proposed scheme overcomes the problem of embedding a watermark in the spatial domain . As the watermark extraction does not require the original image the introduced model can be employed for practical applications . This algorithm tries to improve the problem of failure of embedding in small key space embedding speed and level of security . \\n'],\n",
              " [' Inspired by the ideas of multi swarm information sharing and elitist perturbation guiding a novel multi swarm cooperative multistage perturbation guiding particle swarm optimizer is proposed in this paper . The multi swarm information sharing idea is to harmoniously improve the evolving efficiency via information communicating and sharing among different sub swarms with different evolution mechanisms . It is possible to drive a stagnated sub swarm to revitalize once again with the beneficial information obtained from other sub swarms . Multistage elitist perturbation guiding strategy aims to slow down the learning speed and intensity in a certain extent from the global best individual while keeping the elitist learning mechanism . It effectively enlarges the exploration domain and diversifies the flying tracks of particles . Extensive experiments indicate that the proposed strategies are necessary and cooperative both of which construct a promising algorithm MCpPSO when comparing with other particle swarm optimizers and state of the art algorithms . The ideas of central position perturbation along the global best particle different computing approaches for central position and important parameters influence analysis are presented and analyzed . \\n'],\n",
              " [' In actuality for example the review of the National Science Foundation and the blind peer review of doctoral dissertation in China the evaluation experts are requested to provide two types of information such as the performance of the evaluation objects and the familiarity with the evaluation areas . However existing information aggregation research achievements can not be used to fusion the two types information described above effectively . In this paper we focus on the information aggregation issue in the situation where there are confidence levels of the aggregated arguments under intuitionistic fuzzy environment . Firstly we develop some confidence intuitionistic fuzzy weighted aggregation operators such as the confidence intuitionistic fuzzy weighted averaging operator and the confidence intuitionistic fuzzy weighted geometric operator . Then based on the Einstein operations we proposed the confidence intuitionistic fuzzy Einstein weighted averaging operator and the confidence intuitionistic fuzzy Einstein weighted geometric operator . Finally a practical example about the review of the doctoral dissertation in Chinese universities is provided to illustrate the developed intuitionistic fuzzy information aggregation operators . \\n'],\n",
              " [' Autonomy and adaptability are key features of intelligent agents . Many applications of intelligent agents such as the control of ambient intelligence environments and autonomous intelligent robotic systems require the processing of information coming in from many available sensors to produce adequate output responses in changing scenarios . Autonomy in these cases applies not only to the ability of the agent to produce correct outputs without human guidance but also to its ubiquity and or portability low power consumption and integrability . In this sense an embedded electronic system implementation paradigm can be applied to the design of autonomous intelligent agents in order to satisfy the above mentioned characteristics . However processing complex computational intelligence algorithms with tight delay constraints in resource constrained and low power embedded systems is a challenging engineering problem . In this paper a single chip intelligent agent based on a computationally efficient neuro fuzzy information processing core is described . The system has been endowed with an information preprocessing module based on Principal Component Analysis that permits a substantial reduction of the input space dimensionality with little loss of modeling capability . Moreover the PCA module has been tested as a means to achieve deep adaptability in changing environment dynamics and to endow the agent with fault tolerance in the presence of sensor failures . For data driven trials and research a data set obtained from an experimental intelligent inhabited environment has been used as a benchmark system . \\n'],\n",
              " [' Traditional parametric software reliability growth models are based on some assumptions or distributions and none such single model can produce accurate prediction results in all circumstances . Non parametric models like the artificial neural network based models can predict software reliability based on only fault history data without any assumptions . In this paper initially we propose a robust feedforward neural network based dynamic weighted combination model for software reliability prediction . Four well known traditional SRGMs are combined based on the dynamically evaluated weights determined by the learning algorithm of the proposed FFNN . Based on this proposed FFNN architecture we also propose a robust recurrent neural network based dynamic weighted combination model to predict the software reliability more justifiably . A real coded genetic algorithm is proposed to train the ANNs . Predictability of the proposed models are compared with the existing ANN based software reliability models through three real software failure data sets . We also compare the performances of the proposed models with the models that can be developed by combining three or two of the four SRGMs . Comparative studies demonstrate that the PFFNNDWCM and PRNNDWCM present fairly accurate fitting and predictive capability than the other existing ANN based models . Numerical and graphical explanations show that PRNNDWCM is promising for software reliability prediction since its fitting and prediction error is much less relative to the PFFNNDWCM . \\n'],\n",
              " [' This paper proposes a new evolutionary algorithm for continuous non linear optimization problems . This optimization algorithm is inspired by the procedure of trading the shares on stock market and it is called exchange market algorithm . Evaluation of how the stocks are traded on the stock market by elites has formed this evolutionary as an optimization algorithm . In the proposed method there are two different modes in EMA . In the first mode there is no oscillation in the market whereas in the second mode the market has oscillation . It is noticeable that at the end of each mode the individuals finesses are evaluated . For the first mode the algorithm s duty is to recruit people toward successful individuals while in the second case the algorithm seeks optimal points . In this algorithm the generation and organization of random numbers are performed in the best way due to the existence of two absorbent operators and two searching operators leading to high capability in global optimum point extraction . To evaluate the performance of the proposed algorithm this algorithm has been implemented on 12 different benchmark functions with 10 20 30 and 50 dimension variables . The results obtained by 30 dimension variables are compared with the results obtained by the eight new and efficient algorithms . The results indicate the ability of the proposed algorithm in finding the global optimum point of the functions for each run of the program . \\n'],\n",
              " [' Mechanical and physical properties of sandstone are interesting scientifically and have great practical significance as well as their relations to the mineralogy and pore features . These relations are however highly nonlinear and can not be easily formulated by conventional methods . This paper investigates the potential of the technique named as the relevance vector machine for prediction of the elastic compressibility of sandstone based on its characteristics of physical properties . Based on the fact that the hyper parameters may have effects on the RVM performance an iteration method is proposed in this paper to search for optimal hyper parameter value so that it can produce best predictions . Also the qualitative sensitivity of the physical properties is investigated by the backward regression analysis . Meanwhile the hyper parameter effect of the RVM approach is discussed in the prediction of the elastic compressibility of sandstone . The predicted results of the RVM demonstrate that hyper parameter values have evident effects on the RVM performance . Comparisons on the results of the RVM the artificial neural network and the support vector machine prove that the proposed strategy is feasible and reliable for prediction of the elastic compressibility of sandstone based on its physical properties . \\n'],\n",
              " [' Human performance evaluation is one of the most important fields to analyze for the continuity of an organization . Evaluation files filled by the managers generally end up in dusty folders where no one looks . This decreases the credibility of the evaluators and the process itself . Whereas the management thinks that they are taking the valuable time from the people who can do better things instead of these evaluations . In this paper we add an engineering point of view to this process by giving a Hybrid Multicriteria Decision Making approach to evaluate employees performances working for a same task and explain an efficient way of handling the qualitative and quantitative data simultaneously . The real life situations where performance criteria show interaction will be possible to solve and the different types of interactions will be handled with the proposed hybrid method using Analytical Network Process and Choquet Integral simultaneously . We also give a numerical illustration at the end of the study with the appropriate concluding remarks including the advantages of the proposed method . \\n'],\n",
              " [' Scalar and vector drives have been the cornerstones of control of industrial motors for decades . In both the elimination of mechanical speed sensor consists in a trend of modern drives . This work proposes the development of an adaptive neuro fuzzy inference system angular rotor speed estimator applied to vector and scalar drives . A multi frequency training of ANFIS is proposed initially for a V f scheme and after that a vector drive with magnetizing flux oriented control is proposed . In the literature ANFIS has been commonly proposed as a speed controller in substitution of the classical PI controller of the speed control loop . This paper investigates the ANFIS as an open loop speed estimator instead . The subtractive clustering technique was used as strategy for generating the membership functions for all the incoming signal inputs of ANFIS . This provided a better analysis of the training data set improving the comprehension of the estimator . Additionally the subtractive cluster technique allowed the training with experimental data corrupted by noise improving the estimator robustness . Simulations to evaluate the performance of the estimator considering the V f and vector drive system were realized using the Matlab Simulink software . Finally experimental results are presented to validate the ANFIS open loop estimator . angular speed of the magnetizing flux oriented reference frame angular slip speed direct sum overall function implemented by the adaptive network arbitrary function instantaneous value of magnetizing current instantaneous value of direct component stator current instantaneous value of quadrature component stator current magnetizing inductance stator inductance predicted network output for the pattern k differential operator positive constant positive constant stator resistance set of parameters desired network output for the pattern k rotor leakage time constant rotor time constant instantaneous value of direct component stator voltage instantaneous value of quadrature component stator voltage vector of input variables moment of inertia number of training pattern number of pole pairs \\n'],\n",
              " [' Most of the real world problems have dynamic characteristics where one or more elements of the underlying model for a given problem including the objective constraints or even environmental parameters may change over time . Hyper heuristics are problem independent meta heuristic techniques that are automating the process of selecting and generating multiple low level heuristics to solve static combinatorial optimization problems . In this paper we present a novel hybrid strategy for applicability of hyper heuristic techniques on dynamic environments by integrating them with the memory search algorithm . The memory search algorithm is an important evolutionary technique that have applied on various dynamic optimization problems . We validate performance of our method by considering both the dynamic generalized assignment problem and the moving peaks benchmark . The former problem is extended from the generalized assignment problem by changing resource consumptions capacity constraints and costs of jobs over time and the latter one is a well known synthetic problem that generates and updates a multidimensional landscape consisting of several peaks . Experimental evaluation performed on various instances of the given two problems validates that our hyper heuristic integrated framework significantly outperforms the memory search algorithm . \\n'],\n",
              " [' Artificial chromosomes with genetic algorithm is one of the latest versions of the estimation of distribution algorithms . This algorithm has already been applied successfully to solve different kinds of scheduling problems . However due to the fact that its probabilistic model does not consider variable interactions ACGA may not perform well in some scheduling problems particularly if sequence dependent setup times are considered . This is due to the fact that the previous job will influence the processing time of the next job . Simply capturing ordinal information from the parental distribution is not sufficient for a probabilistic model . As a result this paper proposes a bi variate probabilistic model to add into the ACGA . This new algorithm is called the ACGA2 and is used to solve single machine scheduling problems with sequence dependent setup times in a common due date environment . A theoretical analysis is given in this paper . Some heuristics and local search algorithm variable neighborhood search are also employed in the ACGA2 . The results indicate that the average error ratio of this ACGA2 is half the error ratio of the ACGA . In addition when ACGA2 is applied in combination with other heuristic methods and VNS the hybrid algorithm achieves optimal solution quality in comparison with other algorithms in the literature . Thus the proposed algorithms are effective for solving the scheduling problems . \\n'],\n",
              " [' Four wave mixing crosstalk is the dominant nonlinear effect in long haul repeaterless wavelength division multiplexing lightwave fiber optical communication systems . To reduce FWM crosstalk in optical communication systems unequally spaced channel allocation method is used . One of the unequal bandwidth channel allocation techniques is designed by using the concept of Golomb ruler . It allows the gradual computation of an optimally allocated channel set such that degradations caused by inter channel interference and FWM is minimal . This paper applies two soft computing based approaches i.e . Genetic Algorithm and Biogeography Based Optimization to generate near optimal Golomb ruler sequences in reasonable time . The generated sequences have been compared with the two other classical approaches namely Extended Quadratic Congruence and Search Algorithm . It has been observed that BBO GA outperforms the other two approaches . \\n'],\n",
              " [' In this paper some multi item inventory models for deteriorating items are developed in a random planning horizon under inflation and time value money with space and budget constraints . The proposed models allow stock dependent consumption rate and partially backlogged shortages . Here the time horizon is a random variable with exponential distribution . The inventory parameters other than planning horizon are deterministic in one model and in the other the deterioration and net value of the money are fuzzy available budget and space are fuzzy and random fuzzy respectively . Fuzzy and random fuzzy constraints have been defuzzified using possibility and possibility probability chance constraint techniques . The fuzzy objective function also has been defuzzified using possibility chance constraint against a goal . Both deterministic optimization problems are formulated for maximization of profit and solved using genetic algorithm and fuzzy simulation based genetic algorithm . The models are illustrated with some numerical data . Results for different achievement levels are obtained and sensitivity analysis on expected profit function is also presented . Scope and purpose The traditional inventory model considers the ideal case in which depletion of inventory is caused by a constant demand rate . However for more sale inventory should be maintained at a higher level . Of course this would result in higher holding or procurement cost etc . Also in many real situations during a shortage period the longer the waiting time is the smaller the backlogging rate would be . For instance for fashionable commodities and high tech products with short product life cycle the willingness for a customer to wait for backlogging diminishes with the length of the waiting time . Most of the classical inventory models did not take into account the effects of inflation and time value of money . But at present the economic situation of most of the countries has been much deteriorated due to large scale inflation and consequent sharp decline in the purchasing power of money . So it has not been possible to ignore the effects of inflation and time value of money any further . The purpose of this article is to maximize the expected profit of two inventory control systems in the random planning horizon . \\n'],\n",
              " [' This study applies the Multiple Criteria Decision Making to evaluate the service quality of some Turkish hospitals . In general the service quality has abstract properties which mean that using the previously known measurement approach is insufficient . It is for this reason that the fuzzy set theory is adopted as a research template . In Istanbul Turkey there are four B class hospitals classed as private hospitals that are covered by the Social Security Institution and for which we propose to represent the service performance measurement using triangular fuzzy numbers . In this study importance weights of performance criteria are found with AHP . Then the Multiple Criteria Decision Making methods TOPSIS and Yager s min max approach are applied to find and rank the crisp performance values . In a second step an aggregation of performance criteria with OWA and Compensatory AND operators are looked at instead of the TOPSIS method and min max approach . Thereby numerical applications are supplied by the four methods and the obtained results are compared . \\n'],\n",
              " [' In this paper we suggest a block image encryption algorithm which can give us an efficient scheme to hide and encrypt image data . Only the diffusion function instead of classical permutation plus diffusion operations is adopted . The plain image is firstly divided into two equal parts randomly by vertical horizontal or diagonal directions . Then encryption of one part depends on the other part in which the keystream is generated by the plain image i.e . one of the two parts . An error concept is added in the initial conditions in every round . It means that the keystreams are different in the process of encryption steps . The error may be positive or negative decided by a rule of sign function . Experiment results show that the proposed method can provide a high security of cryptosystem and can reduce the computation redundancy compared with that of the traditional architectures such as Arnold map based method and totally shuffling based method . \\n'],\n",
              " [' We develop an orthogonal forward selection approach to construct radial basis function network classifiers for two class problems . Our approach integrates several concepts in probabilistic modelling including cross validation mutual information and Bayesian hyperparameter fitting . At each stage of the OFS procedure one model term is selected by maximising the leave one out mutual information between the classifier s predicted class labels and the true class labels . We derive the formula of LOOMI within the OFS framework so that the LOOMI can be evaluated efficiently for model term selection . Furthermore a Bayesian procedure of hyperparameter fitting is also integrated into the each stage of the OFS to infer the l 2 norm based local regularisation parameter from the data . Since each forward stage is effectively fitting of a one variable model this task is very fast . The classifier construction procedure is automatically terminated without the need of using additional stopping criterion to yield very sparse RBF classifiers with excellent classification generalisation performance which is particular useful for the noisy data sets with highly overlapping class distribution . A number of benchmark examples are employed to demonstrate the effectiveness of our proposed approach . \\n'],\n",
              " [' In medical system there may be many critical diseases where experts do not have sufficient knowledge to handle those problems . For these cases experts may provide their opinion only about certain aspects of the disease and remain silent for those unknown features . Feeling the need of prioritizing different experts based on their given information this article uses a novel concept for assigning confident weights to different experts which are mainly based on their provided information . Experts provide their opinions about various symptoms using intuitionistic fuzzy soft matrix . In this article we propose an algorithmic approach based on intuitionistic fuzzy soft set which explores a particular disease reflecting the agreement of all experts . This approach is guided by the group decision making model and uses cardinals of IFSS as novel concept . We have used choice matrix as an important parameter which is based on choice parameters of individual expert . This article has also validated the proposed approach using distance measurements and consents of the majority of experts . The effectiveness of the proposed approach is demonstrated using a suitable case study . \\n'],\n",
              " [' JCSE SPIHT an algorithm of joint compression and selective encryption based on set partitioning in hierarchical trees is proposed to achieve image encryption and compression simultaneously . It can protect SPIHT compressed images by only fast scrambling a tiny portion of crucial data during the coding process while keeping all the virtues of SPIHT intact . Intensive experiments are conducted to validate and evaluate the proposed algorithm the results show that the efficiency and the compression performance of JCSE SPIHT are very close to original SPIHT . In security analysis JCSE SPIHT is proved to be immune to various attacks not only from traditional cryptanalysis but also by utilizing sophisticated image processing techniques . \\n'],\n",
              " [' All dynamic crop models for growth and development have several parameters whose values are usually determined by using measurements coming from the real system . The parameter estimation problem is raised as an optimization problem and optimization algorithms are used to solve it . However because the model generally is nonlinear the optimization problem likely is multimodal and therefore classical local search methods fail in locating the global minimum and as a consequence the model parameters could be inaccurate estimated . This paper presents a comparison of several evolutionary and bio inspired algorithms considered as global optimization methods such as Differential Evolution Covariance Matrix Adaptation Evolution Strategy Particle Swarm Optimization and Artificial Bee Colony on parameter estimation of crop growth SUCROS model . Subsequently the SUCROS model for potential growth was applied to a husk tomato crop using data coming from an experiment carried out in Chapingo Mexico . The objective was to determine which algorithm generates parameter values that give the best prediction of the model . An analysis of variance was carried out to statistically evaluate the efficiency and effectiveness of the studied algorithms . Algorithm s efficiency was evaluated by counting the number of times the objective function was required to approximate an optimum . On the other hand the effectiveness was evaluated by counting the number of times that the algorithm converged to an optimum . Simulation results showed that standard DE rand 1 bin got the best result . \\n'],\n",
              " [' This paper presents a novel parameter automation strategy for particle swarm optimization algorithm for solving non convex emission constrained economic dispatch problems . Many evolutionary techniques such as particle swarm optimization differential evolution have been applied to solve these problems and found to perform in a better way in comparison with conventional optimization methods . But often these methods converge to a sub optimal solution prematurely . This paper presents a new improved particle swarm optimization technique called self organizing hierarchical particle swarm optimization technique with time varying acceleration coefficients for non convex emission constrained economic dispatch problems to avoid premature convergence . Generator ramp rate limits and prohibited operating zones are taken into account in problem formulation . Non convex emission constrained economic dispatch problem is obtained by considering both the economy and emission objectives . The performance of the proposed method is demonstrated on two sample test systems . The results of the proposed method are compared with other methods . It is found that the results obtained by the proposed method are superior in terms of fuel cost emission output and losses . \\n'],\n",
              " [' This paper presents a real coded chemical reaction based algorithm to solve the short term hydrothermal scheduling problem . Hydrothermal system is highly complex and related with every problem variables in a nonlinear way . The objective of the hydro thermal scheduling is to determine the optimal hourly schedule of power generation for different hydrothermal power system for certain intervals of time such that cost of power generation is minimum . Chemical reaction optimization mimics the interactions of molecules in term of chemical reaction to reach a low energy stable state . A real coded version of chemical reaction optimization known as real coded chemical reaction optimization is considered here . To check the effectiveness of the RCCRO 3 different test systems are considered and mathematical remodeling of the algorithm is done to make it suitable for solving short term hydrothermal scheduling problem . Simulation results confirm that the proposed approach outperforms several other existing optimization techniques in terms quality of solution obtained and computational efficiency . Results also establish the robustness of the proposed methodology to solve STHS problems . \\n'],\n",
              " [' Although greedy algorithms possess high efficiency they often receive suboptimal solutions of the ensemble pruning problem since their exploration areas are limited in large extent . And another marked defect of almost all the currently existing ensemble pruning algorithms including greedy ones consists in they simply abandon all of the classifiers which fail in the competition of ensemble selection causing a considerable waste of useful resources and information . Inspired by these observations an interesting greedy Reverse Reduce Error pruning algorithm incorporated with the operation of subtraction is proposed in this work . The RRE algorithm makes the best of the defeated candidate networks in a way that the Worst Single Model is chosen and then its votes are subtracted from the votes made by those selected components within the pruned ensemble . The reason is because for most cases the WSM might make mistakes in its estimation for the test samples . And different from the classical RE the near optimal solution is produced based on the pruned error of all the available sequential subensembles . Besides the backfitting step of RE algorithm is replaced with the selection step of a WSM in RRE . Moreover the problem of ties might be solved more naturally with RRE . Finally soft voting approach is employed in the testing to RRE algorithm . The performances of RE and RRE algorithms and two baseline methods i.e . the method which selects the Best Single Model in the initial ensemble and the method which retains all member networks of the initial ensemble are evaluated on seven benchmark classification tasks under different initial ensemble setups . The results of the empirical investigation show the superiority of RRE over the other three ensemble pruning algorithms . \\n'],\n",
              " [' PieceWise AutoRegressive eXogenous models represent one of the broad classes of the hybrid dynamical systems . Among many classes of HDS PWARX model used as an attractive modeling structure due to its equivalence to other classes . This paper presents a novel fuzzy distance weight matrix based parameter identification method for PWARX model . In the first phase of the proposed method estimation for the number of affine submodels present in the HDS is proposed using fuzzy clustering validation based algorithm . For the given set of input output data points generated by predefined PWARX model fuzzy c means clustering procedure is used to classify the data set according to its affine submodels . The fuzzy distance weight matrix based weighted least squares algorithm is proposed to identify the parameters for each PWARX submodel which minimizes the effect of noise and classification error . In the final phase fuzzy validity function based model selection method is applied to validate the identified PWARX model . The effectiveness of the proposed method is demonstrated using three benchmark examples . Simulation experiments show validation of the proposed method . \\n'],\n",
              " [' In this research we propose a novel framework referred to as collective game behavior decomposition where complex collective behavior is assumed to be generated by aggregation of several groups of agents following different strategies and complexity emerges from collaboration and competition of individuals . The strategy of an agent is modeled by certain simple game theory models with limited information . Genetic algorithms are used to obtain the optimal collective behavior decomposition based on history data . The trained model can be used for collective behavior prediction . For modeling individual behavior two simple games the minority game and mixed game are investigated in experiments on the real world stock prices and foreign exchange rate . Experimental results are presented to show the effectiveness of the new proposed model . \\n'],\n",
              " [' Large scale software systems are in general difficult to manage and monitor . In many cases these systems display unexpected behavior especially after being updated or when changes occur in their environment . Therefore to handle a changing environment it is desirable to base fault detection and performance monitoring on self adaptive techniques . Several studies have been carried out in the past which inspired on the immune system aim at solving complex technological problems . Among them anomaly detection pattern recognition system security and data mining are problems that have been addressed in this framework . There are similarities between the software fault detection problem and the identification of the pathogens that are found in natural immune systems . Being inspired by vaccination and negative and clonal selection observed in these systems we developed an effective self adaptive model to monitor software applications analyzing the metrics of system resources . \\n'],\n",
              " [' We propose a new image encryption algorithm which is based on the spatiotemporal non adjacent coupled map lattices . The system of non adjacent coupled map lattices has more outstanding cryptography features in dynamics than the logistic map or coupled map lattices does . In the proposed image encryption we employ a bit level pixel permutation strategy which enables bit planes of pixels permute mutually without any extra storage space . Simulations have been carried out and the results demonstrate the superior security and high efficiency of the proposed algorithm . \\n'],\n",
              " [' Multilayer perceptron and support vector machine two popular learning machines are increasingly being used as alternatives to classical statistical models for ground level ozone prediction . However employing learning machines without sufficient awareness about their limitations can lead to unsatisfactory results in modeling the ozone evolving mechanism especially during ozone formation episodes . With the spirit of literature review and justification this paper discusses with respect to the concerning of ozone prediction the recently developed algorithms technologies for treating the most prominent model performance degradation limitations . MLP has the black box property i.e . it hardly provides physical explanation for the trained model overfitting and local minima problems and SVM has parameters identification and class imbalance problems . This commentary article aims to stress that the underlying philosophy of using learning machines is by no means as trivial as simply fitting models to the data because it causes difficulties controversies or unresolved problems . This article also aims to serve as a reference point for further technical readings for experts in relevant fields . \\n'],\n",
              " [' This paper presents fully fuzzy fixed charge multi item solid transportation problems in which direct costs fixed charges supplies demands conveyance capacities and transported quantities are fuzzy in nature . Objective is to minimize the total fuzzy cost under fuzzy decision variables . In this paper some approaches are proposed to find the fully fuzzy transported amounts for a fuzzy solid transportation problem . Proposed approaches are applicable for both balanced and unbalanced FFFCMISTPs . Another fuzzy fixed charge multi item solid transportation problem in which transported amounts are not fuzzy is also presented and solved by some other techniques . The models are illustrated with numerical examples and nature of the solutions is discussed . \\n'],\n",
              " [' The proposed work involves the multiobjective PSO based adaption of optimal neural network topology for the classification of multispectral satellite images . It is per pixel supervised classification using spectral bands . This paper also presents a thorough experimental analysis to investigate the behavior of neural network classifier for given problem . Based on 1050 number of experiments we conclude that following two critical issues needs to be addressed selection of most discriminative spectral bands and determination of optimal number of nodes in hidden layer . We propose new methodology based on multiobjective particle swarm optimization technique to determine discriminative spectral bands and the number of hidden layer node simultaneously . The accuracy with neural network structure thus obtained is compared with that of traditional classifiers like MLC and Euclidean classifier . The performance of proposed classifier is evaluated quantitatively using Xie Beni and indexes . The result shows the superiority of the proposed method to the conventional one . \\n'],\n",
              " [' In this study two induced generalized hesitant fuzzy hybrid operators called the induced generalized hesitant fuzzy Shapley hybrid weighted averaging operator and the induced generalized hesitant fuzzy Shapley hybrid geometric mean operator are defined . The prominent characteristics of these two operators are that they do not only globally consider the importance of elements and their ordered positions but also overall reflect their correlations . Furthermore when the weight information of the attributes and the ordered positions is partly known using grey relational analysis method and the Shapley function models for the optimal fuzzy measures on an attribute set and on an ordered set are respectively established . Finally an approach to hesitant fuzzy multi attribute decision making with incomplete weight information and interactive conditions is developed and an illustrative example is provided to show its practicality and effectivity . \\n'],\n",
              " [' Bibliometrics is a discipline that analyzes bibliographic material from a quantitative perspective . It is very useful for classifying information according to different variables including journals institutions and countries . This paper presents a general overview of research in the fuzzy sciences using bibliometric indicators . The main advantage is that these indicators provide a general picture identifying some of the most influential research in this area . The analysis is divided into key sections focused on relevant journals papers authors institutions and countries . Most of the results are in accordance with our common knowledge although some unexpected results are also found . Note that the aim of this paper is to be informative and these indicators identify most of the fundamental research in this field . However some very influential issues may be omitted if they are not included in the Web of Science database which is used for carrying out the bibliometric analysis . \\n'],\n",
              " [' In this paper we investigate the deviation of the priority weights from hesitant multiplicative preference relations in group decision making environments . As basic elements of HMPRs hesitant multiplicative elements usually have different numbers of possible values . To correctly compute or compare HMEs there are two principles to normalize them i.e . the normalization and the normalization . Based on the normalization we develop a new goal programming model to derive the priority weights from HMPRs in group decision making environments . Based on the normalization a consistent HMPR and an acceptably consistent HMPR are defined and their desired properties are studied . A convex combination method is then developed to obtain interval weights from an acceptably consistent HMPR . This approach is further extended to group decision making situations in which the experts evaluate their preferences as several HMPRs . Finally some numerical examples are provided to illustrate the validity and applicability of the proposed models . \\n'],\n",
              " [' Clustering is an efficient topology control method which balances the traffic load of the sensor nodes and improves the overall scalability and the life time of the wireless sensor networks . However in a cluster based WSN the cluster heads consume more energy due to extra work load of receiving the sensed data data aggregation and transmission of aggregated data to the base station . Moreover improper formation of clusters can make some CHs overloaded with high number of sensor nodes . This overload may lead to quick death of the CHs and thus partitions the network and thereby degrade the overall performance of the WSN . It is worthwhile to note that the computational complexity of finding optimum cluster for a large scale WSN is very high by a brute force approach . In this paper we propose a novel differential evolution based clustering algorithm for WSNs to prolong lifetime of the network by preventing faster death of the highly loaded CHs . We incorporate a local improvement phase to the traditional DE for faster convergence and better performance of our proposed algorithm . We perform extensive simulation of the proposed algorithm . The experimental results demonstrate the efficiency of the proposed algorithm . \\n'],\n",
              " [' Evolutionary algorithms start with an initial population vector which is randomly generated when no preliminary knowledge about the solution is available . Recently it has been claimed that in solving continuous domain optimization problems the simultaneous consideration of randomness and opposition is more effective than pure randomness . In this paper it is mathematically proven that this scheme called opposition based learning also does well in binary spaces . The proposed binary opposition based scheme can be embedded inside many binary population based algorithms . We applied it to accelerate the convergence rate of binary gravitational search algorithm as an application . The experimental results and mathematical proofs confirm each other . \\n'],\n",
              " [' Application of machine learning techniques to the functional Magnetic Resonance Imaging data is recently an active field of research . There is however one area which does not receive due attention in the literature preparation of the fMRI data for subsequent modelling . In this study we focus on the issue of synchronization of the stream of fMRI snapshots with the mental states of the subject which is a form of smart filtering of the input data performed prior to building a predictive model . We demonstrate investigate and thoroughly discuss the negative effects of lack of alignment between the two streams and propose an original data driven approach to efficiently address this problem . Our solution involves casting the issue as a constrained optimization problem in combination with an alternative classification accuracy assessment scheme applicable to both batch and on line scenarios and able to capture information distributed across a number of input samples lifting the common simplifying i.i.d . assumption . The proposed method is tested using real fMRI data and experimentally compared to the state of the art ensemble models reported in the literature outperforming them by a wide margin . \\n'],\n",
              " [' Global positioning system is the most widely used military and commercial positioning tool for real time navigation and location . Geometric dilution of precision stands as a relevant measure of positioning accuracy and consequently the performance quality of the GPS positioning algorithm . Since the calculation of GPS GDOP has a time and power burden that involves complicated transformation and inversion of measurement matrices in this paper we propose hybrid intelligent methods namely adaptive neuro fuzzy inference system improved ANFIS and radial basis function for GPS GDOP classification . Through investigation it is verified that the ANFIS is a high performance and valuable classifier . In the ANFIS training the radius vector has very important role for its recognition accuracy . Therefore in the optimization module bee algorithm is proposed for finding the optimum vector of radius . In order to improve the performance of the proposed method a new improvement for the BA is used . In addition to enhance the accuracy of the method principal component analysis is utilized as a pre processing step . Experimental results clearly indicate that the proposed intelligent methods have high classification accuracy rates comparing with conventional ones . \\n'],\n",
              " [' Removal of miscible hazardous materials from aqueous solutions is an alarming problem for the environmental scientists . Several linear and nonlinear regression models like Langmuir Freundlich D R Tempkin isotherm models are in vogue for determining the adsorbing capacity of standard adsorbents used for this purpose . In this article we propose a novel quantum inspired backpropagation multilayer perceptron based on quantum gates for the prediction of this adsorption behavior exhibited by calcareous soil oftentimes used in adsorbing miscible iron from aqueous solutions . The backpropagation learning formulae for the proposed QBMLP architecture has also been generalized for multiple number of layers in both field homogeneous and field heterogeneous configurations characterized by three standard activations viz . sigmoid tanh and tan1.5h functions . Applications of the efficiency of the proposed QBMLP over the regression models are demonstrated with regards to the prediction behavior of the adsorption of iron by calcareous soil from an aqueous solution with effect to various characteristic adsorbent parameters . The adsorption process is considered to be a physical one since the activation energy of ferrous ion adsorption is 9.469kJmol 1 due to Arrhenius . Moreover the thermodynamic parameters of Gibb s free energy enthalpy and entropy values indicate it be spontaneous . Results indicate that QBMLP predicts the adsorption behavior of calcareous soil to a very closer extent thereby obviating the need for further regression experimental analysis . Comparison with the performance of a similar classical multilayer perceptron architecture also reveals the prediction and time efficiency of the proposed QBMLP architecture . \\n'],\n",
              " [' Traditionally clustering is the task of dividing samples into homogeneous clusters based on their degrees of similarity . As samples are assigned to clusters users need to manually give descriptions for all clusters . In this paper a rapid fuzzy rule clustering method based on granular computing is proposed to give descriptions for all clusters . A new and simple unsupervised feature selection method is employed to endow every sample with a suitable description . Exemplar descriptions are selected from sample s descriptions by relative frequency and data granulation is guided by the selected exemplar fuzzy descriptions . Every cluster is depicted by a single fuzzy rule which make the clusters understandable for humans . The experimental results show that our proposed model is able to discover fuzzy IF THEN rules to obtain the potential clusters . \\n'],\n",
              " [' Recently a novel probabilistic model building evolutionary algorithm named probabilistic model building genetic network programming has been proposed . PMBGNP uses graph structures for its individual representation which shows higher expression ability than the classical EDAs . Hence it extends EDAs to solve a range of problems such as data mining and agent control . This paper is dedicated to propose a continuous version of PMBGNP for continuous optimization in agent control problems . Different from the other continuous EDAs the proposed algorithm evolves the continuous variables by reinforcement learning . We compare the performance with several state of the art algorithms on a real mobile robot control problem . The results show that the proposed algorithm outperforms the others with statistically significant differences . \\n'],\n",
              " [' The evaluation of bone development is a complex and time consuming task for the physicians since it may cause intraobserver and interobserver differences . In this study we present a new training algorithm for support vector machines in order to determine the bone age in young children from newborn to 6 years old . By the new algorithm we aimed to assist the radiologists so as to eliminate the disadvantages of the methods used in bone age determination . To achieve this purpose primarily feature extraction procedure was performed to the left hand wrist X ray images by using image processing techniques and the features related with the carpal bones and distal epiphysis of radius bone were obtained . Then these features were used for the input arguments of the classifier . In the classification process a new training algorithm for support vector machines was proposed by using particle swarm optimization . When training support vector machines particle swarm optimization was used for generating a new training instance which will represent the whole training set of the related class by using the training set . Finally these new instances were used as the support vectors and classification process was carried out by using these new instances . The performance of the proposed method was compared with the naive Bayes k nearest neighborhood support vector machines and C4.5 algorithms . As a result it was determined that the proposed method was found successful than the other methods for bone age determination with a classification performance of 74.87 . \\n'],\n",
              " [' This paper deals with Hamilton Jacobi Bellman equation based stabilized optimal control of hybrid dynamical systems . This paper presents the fuzzy clustering based event wise multiple linearized modeling approaches for HDS to describe the continuous dynamic in each event . In the present work a fuzzy clustering validation approach is presented for the selection of number of linearized models which span entire HDS . The method also describes how to obtain event wise operating point using fuzzy membership function which is used to find the event wise model bank by linearizing the first principles model . The event wise linearized models are used for the formulation of the optimal control law . The HJB equation is formulated using a suitable quadratic term in the objective function . By use of the direct method of Lyapunov stability the control law is shown to be optimal with respect to objective functional and stabilized the event wise linearized models . The global Lyapunov function is proposed with discrete variables which stabilized the HDS . The proposed modeling and control algorithm have been applied on two HDSs . Necessary theoretical and simulation experiments are presented to demonstrate the performance and validation of the proposed algorithm . \\n'],\n",
              " [' We present an optimization based unsupervised approach to automatic document summarization . In the proposed approach text summarization is modeled as a Boolean programming problem . This model generally attempts to optimize three properties namely relevance summary should contain informative textual units that are relevant to the user redundancy summaries should not contain multiple textual units that convey the same information and length summary is bounded in length . The approach proposed in this paper is applicable to both tasks single and multi document summarization . In both tasks documents are split into sentences in preprocessing . We select some salient sentences from document to generate a summary . Finally the summary is generated by threading all the selected sentences in the order that they appear in the original document . We implemented our model on multi document summarization task . When comparing our methods to several existing summarization methods on an open DUC2005 and DUC2007 data sets we found that our method improves the summarization results significantly . This is because first when extracting summary sentences this method not only focuses on the relevance scores of sentences to the whole sentence collection but also the topic representative of sentences . Second when generating a summary this method also deals with the problem of repetition of information . The methods were evaluated using ROUGE 1 ROUGE 2 and ROUGE SU4 metrics . In this paper we also demonstrate that the summarization result depends on the similarity measure . Results of the experiment showed that combination of symmetric and asymmetric similarity measures yields better result than their use separately . \\n'],\n",
              " [' In this paper a Multitree Genetic Programming based method is developed to learn an INTerpretable and ACcurate Takagi Sugeno Kang fuzzy rule based sYstem for dynamic portfolio trading . The MGP INTACTSKY utilizes a TSK model with a new structure to develop a more interpretable and accurate system for dynamic portfolio trading . In the new structure of TSK disjunctive normal form rules with variable structured consequent parts are developed in which the absence of some input variables is allowed . Input variables are the most influential technical indices which are selected by stepwise regression analysis . The technical indices are computed using wavelet transformed stock price series to eliminate the noise . The proposed system directly induces the preferred portfolio weights from the stock s technical indices through time . Here genetic programming with the multitree structure is applied to learn the TSK fuzzy rule bases with the Pittsburgh approach . With this approach the correlation of different stocks is properly considered during the evolutionary process . To evaluate the performance of the MGP INTACTSKY for portfolio trading the proposed model is implemented on the Tehran Stock Exchange as an emerging market as well as Toronto and Frankfurt Stock Exchanges as two mature markets . The experimental results show that the proposed model outperforms other methods such as the momentum strategy the multitree genetic programming based crisp system the genetic algorithm based first order TSK system the buy and hold approach and the market s main index in terms of accuracy and interpretability . \\n'],\n",
              " [' In this article the dynamic multi swarm particle swarm optimizer and a new cooperative learning strategy are hybridized to obtain DMS PSO CLS . DMS PSO is a recently developed multi swarm optimization algorithm and has strong exploration ability for the use of a novel randomly regrouping schedule . However the frequently regrouping operation of DMS PSO results in the deficiency of the exploitation ability . In order to achieve a good balance between the exploration and exploitation abilities the cooperative learning strategy is hybridized to DMS PSO which makes information be used more effectively to generate better quality solutions . In the proposed strategy for each sub swarm each dimension of the two worst particles learns from the better particle of two randomly selected sub swarms using tournament selection strategy so that particles can have more excellent exemplars to learn and can find the global optimum more easily . Experiments are conducted on some well known benchmarks and the results show that DMS PSO CLS has a superior performance in comparison with DMS PSO and several other popular PSO variants . \\n'],\n",
              " [' Designing of scalable routing protocol with prolonged network lifetime for a wireless sensor network is a challenging task . WSN consists of large number of power communication and computational constrained inexpensive nodes . It is difficult to replace or recharge battery of a WSN node when operated in a hostile environment . Cluster based routing is one of the techniques to provide prolonged network lifetime along with scalability . This paper proposes a technique for cluster formation derived from Grid based method . We have also proposed a new decentralized cluster head election method based on Bollinger Bands . Bollinger Bands are based on Upper Bollinger Band and Lower Bollinger Band both of these bands are extremely reactive to any change in the inputs provided to them . We have used this property of Bollinger Bands to elect CH . Simulation result shows significant improvement in the network lifetime in comparison with other decentralized and ant based algorithms . \\n'],\n",
              " [' The pyramidal dual tree directional filter bank transform is a new image decomposition which has many advantages such as multiscale and multidirectional transform efficient implementation high angular resolution low redundant ratio and shiftable subbands . In this paper we present a new color image segmentation algorithm based on PDTDFB domain hidden Markov tree model . Firstly the joint statistics and mutual information of the PDTDFB coefficients are studied . Then the PDTDFB coefficients are modeled using a HMT model with Gaussian mixtures which can effectively capture the intra scale inter scale and inter direction dependencies . Finally a color image segmentation using PDTDFB domain HMT model is developed in which expectation maximization parameter estimation Bayesian multiscale raw segmentation context based multiscale fusion and majority vote based color component fusion are used . Experimental evidence shows that the proposed color image segmentation algorithm has very effective segmentation results in comparison with the state of the art segmentation methods recently proposed in the literature . \\n'],\n",
              " [' This paper presents a modified version of the water cycle algorithm . The fundamental concepts and ideas which underlie the WCA are inspired based on the observation of water cycle process and how rivers and streams flow to the sea . New concept of evaporation rate for different rivers and streams is defined so called evaporation rate based WCA which offers improvement in search . Furthermore the evaporation condition is also applied for streams that directly flow to sea based on the new approach . The ER WCA shows a better balance between exploration and exploitation phases compared to the standard WCA . It is shown that the ER WCA offers high potential in finding all global optima of multimodal and benchmark functions . The WCA and ER WCA are tested using several multimodal benchmark functions and the obtained optimization results show that in most cases the ER WCA converges to the global solution faster and offers more accurate results than the WCA and other considered optimizers . Based on the performance of ER WCA on a number of well known benchmark functions the efficiency of the proposed method with respect to the number of function evaluations and accuracy of function value are represented . \\n'],\n",
              " [' In this paper a hybrid gravitational search algorithm and pattern search technique is proposed for load frequency control of multi area power system . Initially various conventional error criterions are considered the PI controller parameters for a two area power system are optimized employing GSA and the effect of objective function on system performance is analyzed . Then GSA control parameters are tuned by carrying out multiple runs of algorithm for each control parameter variation . After that PS is employed to fine tune the best solution provided by GSA . Further modifications in the objective function and controller structure are introduced and the controller parameters are optimized employing the proposed hybrid GSA and PS approach . The superiority of the proposed approach is demonstrated by comparing the results with some recently published modern heuristic optimization techniques such as firefly algorithm differential evolution bacteria foraging optimization algorithm particle swarm optimization hybrid BFOA PSO NSGA II and genetic algorithm for the same interconnected power system . Additionally sensitivity analysis is performed by varying the system parameters and operating load conditions from their nominal values . Also the proposed approach is extended to two area reheat thermal power system by considering the physical constraints such as reheat turbine generation rate constraint and governor dead band nonlinearity . Finally to demonstrate the ability of the proposed algorithm to cope with nonlinear and unequal interconnected areas with different controller coefficients the study is extended to a nonlinear three unequal area power system and the controller parameters of each area are optimized using proposed hGSA PS technique . \\n'],\n",
              " [' In this paper we present a clustering framework for type 2 fuzzy clustering which covers all steps of the clustering process including clustering algorithm parameters estimation and validation and verification indices . The proposed clustering algorithm is developed based on dual centers type 2 fuzzy clustering model . In this model the centers of clusters are defined by a pair of objects rather than a single object . The membership values of the objects to the clusters are defined by type 2 fuzzy numbers and there are not any type reduction or defuzzification steps in the proposed clustering algorithm . In addition the relation among the size of the cluster bandwidth distance between dual centers and fuzzifier parameter are indicated and analyzed to facilitate the parameters estimation step . To determine the optimum number of clusters we develop a new validation index which is compatible with the proposed model structure . A new compatible verification index is also defined to compare the results of the proposed model with existing type 1 fuzzy clustering model . Finally the results of computational experiments are presented to show the efficiency of the proposed approach . \\n'],\n",
              " [' Unplanned dilution and ore loss are the most critical challenges in underground stoping operations . These problems are the main cause behind a mine closure and directly influencing the productivity of the underground stope mining and the profitability of the entire operation . Despite being aware of the significance of unplanned dilution and ore loss prediction of these phenomena is still unexplained as they occur through complex mechanisms and causative factors . Current management practices primarily rely on similar stope reconciliation data and the intuition of expert mining engineers . In this study an innovative unplanned dilution and ore loss management system is established using a neuro fuzzy system . The aim of the proposed decision support system is to overcome the UB phenomenon in underground stope blasting which provides quantitative prediction of unplanned dilution and ore loss with practical recommendations simultaneously . To achieve the method proposed an uneven break prediction system was developed by an artificial neural network considering 1076 datasets covering 10 major UB causative factors collected from three underground stoping mines in Western Australia . In succession the UB consultation system was established via a fuzzy expert system in reference to surveyed results of fifteen underground mining experts . The UB prediction and consultation system were combined as one concurrent neuro fuzzy system that is named the uneven break optimiser . Because the current UB prediction systems in investigated mines were highly unsatisfactory with correlation coefficient of 0.088 and limited to only unplanned dilution the performance of the proposed UB prediction system is a remarkable achievement . The uneven break optimiser can be directly employed to improve underground stoping production and this tool will be beneficial not only for underground stope planning and design but also for production management . \\n'],\n",
              " [' Feature selection is often required as a preliminary step for many pattern recognition problems . However most of the existing algorithms only work in a centralized fashion i.e . using the whole dataset at once . In this research a new method for distributing the feature selection process is proposed . It distributes the data by features i.e . according to a vertical distribution and then performs a merging procedure which updates the feature subset according to improvements in the classification accuracy . The effectiveness of our proposal is tested on microarray data which has brought a difficult challenge for researchers due to the high number of gene expression contained and the small samples size . The results on eight microarray datasets show that the execution time is considerably shortened whereas the performance is maintained or even improved compared to the standard algorithms applied to the non partitioned datasets . \\n'],\n",
              " [' In this paper we aim at proposing a switching adaptive control scheme using a Hopfield based dynamic neural network for nonlinear systems with external disturbances . In our proposed scheme an auxiliary direct adaptive controller ensures the system stability when the indirect adaptive controller is failed that is approaches to zero where is the denominator of an indirect adaptive control law . The IAC s limitation of then can be solved by simply switching the IAC to the DAC where is a positive desired value . The Hopfield dynamic neural network is used to not only design DAC but also approximate the unknown plant nonlinearities in IAC design . The designed simple structure of HDNN keeps the tracking performance well and also makes the practical implementation much easier because of the use of less and fixed number of neurons . \\n'],\n",
              " [' Hill climbing constitutes one of the simplest way to produce approximate solutions of a combinatorial optimization problem and is a central component of most advanced metaheuristics . This paper focuses on evaluating climbing techniques in a context where deteriorating moves are not allowed in order to isolate the intensification aspect of metaheuristics . We aim at providing guidelines to choose the most adequate method for climbing efficiently fitness landscapes with respect to their size and some ruggedness and neutrality measures . To achieve this we compare best and first improvement strategies as well as different neutral move policies on a large set of combinatorial fitness landscapes derived from academic optimization problems including NK landscapes . The conclusions highlight that first improvement is globally more efficient to explore most landscapes while best improvement superiority is observed only on smooth landscapes and on some particular structured landscapes . The empirical analysis realized on neutral move policies shows that a stochastic hill climbing reaches in average better configurations and requires fewer evaluations than other climbing techniques . Results indicate that accepting neutral moves at each step of the search should be useful on all landscapes especially those having a significant rate of neutrality . Last we point out that reducing adequately the precision of a fitness function makes the climbing more efficient and helps to solve combinatorial optimization problems . \\n'],\n",
              " [' The classification of imbalanced data is a major challenge for machine learning . In this paper we presented a fuzzy total margin based support vector machine method to handle the class imbalance learning problem in the presence of outliers and noise . The proposed method incorporates total margin algorithm different cost functions and the proper approach of fuzzification of the penalty into FTM SVM and formulates them in nonlinear case . We considered an excellent type of fuzzy membership functions to assign fuzzy membership values and got six FTM SVM settings . We evaluated the proposed FTM SVM method on two artificial data sets and 16 real world imbalanced data sets . Experimental results show that the proposed FTM SVM method has higher G Mean and F Measure values than some existing CIL methods . Based on the overall results we can conclude that the proposed FTM SVM method is effective for CIL problem especially in the presence of outliers and noise in data sets . \\n'],\n",
              " [' Support vector machine is sensitive to the outliers which reduces its generalization ability . This paper presents a novel support vector regression together with fuzzification theory inconsistency matrix and neighbors match operator to address this critical issue . Fuzzification method is exploited to assign similarities on the input space and on the output response to each pair of training samples respectively . The inconsistency matrix is used to calculate the weights of input variables followed by searching outliers through a novel neighborhood matching algorithm and then eliminating them . Finally the processed data is sent to the original SVR and the prediction results are acquired . A simulation example and three real world applications demonstrate the proposed method for data set with outliers . \\n'],\n",
              " [' In this study a dynamic screening strategy is proposed to discriminate subjects with autistic spectrum disorder from healthy controls . The ASD is defined as a neurodevelopmental disorder that disrupts normal patterns of connectivity between the brain regions . Therefore the potential use of such abnormality for autism screening is investigated . The connectivity patterns are estimated from electroencephalogram data collected from 8 brain regions under various mental states . The EEG data of 12 healthy controls and 6 autistic children were collected during eyes open and eyes close resting states as well as when subjects were exposed to affective faces . Subsequently the subjects were classified as autistic or healthy groups based on their brain connectivity patterns using pattern recognition techniques . Performance of the proposed system in each mental state is separately evaluated . The results present higher recognition rates using functional connectivity features when compared against other existing feature extraction methods . \\n'],\n",
              " [' In this paper a new fuzzy peer assessment methodology that considers vagueness and imprecision of words used throughout the evaluation process in a cooperative learning environment is proposed . Instead of numerals words are used in the evaluation process in order to provide greater flexibility . The proposed methodology is a synthesis of perceptual computing and a fuzzy ranking algorithm . Per C is adopted because it allows uncertainties of words to be considered in the evaluation process . Meanwhile the fuzzy ranking algorithm is deployed to obtain appropriate performance indices that reflect a student s contribution in a group and subsequently rank the student accordingly . A case study to demonstrate the effectiveness of the proposed methodology is described . Implications of the results are analyzed and discussed . The outcomes clearly demonstrate that the proposed fuzzy peer assessment methodology can be deployed as an effective evaluation tool for cooperative learning of students . \\n'],\n",
              " [' Image quality assessment of distorted or decompressed images without any reference to the original image is challenging from computational point of view . Quality of an image is best judged by human observers without any reference image and evaluated using subjective measures . The paper aims at designing a generic no reference image quality assessment method by incorporating human visual perception in assigning quality class labels to the images . Using fuzzy logic approach we consider information theoretic entropies of visually salient regions of images as features and assess quality of the images using linguistic values . The features are transformed into fuzzy feature space by designing an algorithm based on interval type 2 fuzzy sets . The algorithm measures uncertainty present in the input output feature space to predict image quality accurately as close to human observations . We have taken a set of training images belonging to five different pre assigned quality class labels for calculating foot print of uncertainty corresponding to each class . To assess the quality class label of the test images maximum of T conorm applied on the lower and upper membership functions of the test images belonging to different classes is calculated . Our proposed image quality metric is compared with other no reference quality metrics demonstrating more accurate results and compatible with subjective mean opinion score metric . \\n'],\n",
              " [' The job shop scheduling problem with operators is an extension of the classic job shop problem in which an operation must be assisted by one of a limited set of human operators so it models many real life situations . In this paper we tackle the JSO by means of memetic algorithms with the objective of minimizing the makespan . We define and analyze a neighborhood structure which is then exploited in local search and tabu search algorithms . These algorithms are combined with a conventional genetic algorithm to improve a fraction of the chromosomes in each generation . We also consider two different schedule builders for chromosome decoding . All these elements are combined to obtain memetic algorithms which are evaluated over an extensive set of instances . The results of the experimental study show that they reach high quality solutions in very short time comparing favorably with the state of the art methods . \\n'],\n",
              " [' A new disturbance detection and classification technique based on modified Adaline and adaptive neuro fuzzy information system is proposed for a distributed generation system comprising a wind power generating system and a photovoltaic array . The proposed technique is based on a fast Gauss Newton parameter updating rule rather than the conventional Widrow Hoff delta rule for the Adaline network . The voltage and current signals near the target distributed generation particularly the DFIG whose speed varies from minimum to the maximum cut off speed are processed through the modified Adaline network to yield the features like the negative sequence power harmonic amplification factor total harmonic distortion etc . These features are then used as training sets for the ANFIS which employs a gradient descent algorithm to update its parameters . The proposed technique distinguishes the islanding condition of the distributed generation system with some other disturbances such as switching faults capacitor bank switching voltage swell voltage sag distorted grid voltage unbalanced load switching etc . which are referred to as non islanding cases in this paper . \\n'],\n",
              " [' Classifying walking patterns helps the diagnosis of health status disease progression and the effect of interventions . In this paper we develop previous research on human gait to extract a meaningful set of parameters that allow us to design a highly interpretable system capable of identifying different gait styles with linguistic fuzzy if then rules . The model easily discriminates among five different walking patterns namely normal walk on tiptoes dragging left limb dragging right limb and dragging both limbs . We have carried out a complete experimentation to test the performance of the extracted parameters to correctly classify these five chosen gait styles . \\n'],\n",
              " [' Recent research revealed that model assisted parameter tuning can improve the quality of supervised machine learning models . The tuned models were especially found to generalize better and to be more robust compared to other optimization approaches . However the advantages of the tuning often came along with high computation times meaning a real burden for employing tuning algorithms . While the training with a reduced number of patterns can be a solution to this it is often connected with decreasing model accuracies and increasing instabilities and noise . Hence we propose a novel approach defined by a two criteria optimization task where both the runtime and the quality of ML models are optimized . Because the budgets for this optimization task are usually very restricted in ML the surrogate assisted Efficient Global Optimization algorithm is adapted . In order to cope with noisy experiments we apply two hypervolume indicator based EGO algorithms with smoothing and re interpolation of the surrogate models . The techniques do not need replicates . We find that these EGO techniques can outperform traditional approaches such as latin hypercube sampling as well as EGO variants with replicates . \\n'],\n",
              " [' In this paper speed control of Brushless DC motor using Bat algorithm optimized online Adaptive Neuro Fuzzy Inference System is presented . Learning parameters of the online ANFIS controller i.e . Learning Rate Forgetting Factor and Steepest Descent Momentum Constant are optimized for different operating conditions of Brushless DC motor using Genetic Algorithm Particle Swarm Optimization and Bat algorithm . In addition tuning of the gains of the Proportional Integral Derivative Fuzzy PID and Adaptive Fuzzy Logic Controller is optimized using Genetic Algorithm Particle Swarm Optimization and Bat Algorithm . Time domain specification of the speed response such as rise time peak overshoot undershoot recovery time settling time and steady state error is obtained and compared for the considered controllers . Also performance indices such as Root Mean Squared Error Integral of Absolute Error Integral of Time Multiplied Absolute Error and Integral of Squared Error are evaluated and compared for the above controllers . In order to validate the effectiveness of the proposed controller simulation is performed under constant load condition varying load condition and varying set speed conditions of the Brushless DC motor . The real time experimental verification of the proposed controller is verified using an advanced DSP processor . The simulation and experimental results confirm that bat algorithm optimized online ANFIS controller outperforms the other controllers under all considered operating conditions . \\n'],\n",
              " [' The single machine scheduling problem with sequence dependent setup times with the objective of minimizing the total weighted tardiness is a challenging problem due to its complexity and has a huge number of applications in real production environments . In this paper we propose a memetic algorithm that combines and extends several ideas from the literature including a crossover operator that respects both the absolute and relative position of the tasks a replacement strategy that improves the diversity of the population and an effective but computationally expensive neighborhood structure . We propose a new decomposition of this neighborhood that can be used by a variable neighborhood descent framework and also some speed up methods for evaluating the neighbors . In this way we can obtain competitive running times . We conduct an experimental study to analyze the proposed algorithm and prove that it is significantly better than the state of the art in standard benchmarks . \\n'],\n",
              " [' The traditional visual and acoustic embolic signal detection methods based on the expert analysis of individual spectral recordings and Doppler shift sounds are the gold standards . However these types of detection methods are high cost subjective and can only be applied by experts . In order to overcome these drawbacks computer based automated embolic detection systems which employ spectral properties of emboli speckle and artifact using Fourier and Wavelet Transforms have been proposed . In this study we propose a fast accurate and robust automated emboli detection system based on the Dual Tree Complex Wavelet Transform . Employing the DTCWT which does not suffer from the lack of shift invariance property of ordinary Discrete Wavelet Transform increases the robustness of the coefficients extracted from the Doppler ultrasound signals . In this study a Doppler ultrasound dataset including 100 samples from each embolic Doppler speckle and artifact signal is used . Each sample obtained from forward and reverse blood flow directions is represented by 1024 points . In our method we first extract the forward and reverse blood flow coefficients separately using DTCWT from the samples . Then dimensionality reduction is applied to each set of coefficients and both of the reduced set of coefficients are fed to classifiers individually . Subsequently in the view that the forward and reverse blood flow coefficients carry different characteristics the individual predictors of these classifiers are combined using ensemble stacking method . We compare the obtained results with Fast Fourier Transform and DWT based emboli detection systems and show that the features extracted using DTCWT give the highest accuracy and emboli detection rate . It is also observed that combining forward and reverse coefficients using stacking ensemble method improves the emboli and artifact detection rates and overall accuracy . \\n'],\n",
              " [' This paper presents a biologically inspired sequential learning spiking neural classifier for pattern classification problems . It consists of a two layered neural network and a separate decision block which estimates the predicted class label . Inspired by observations in the neuroscience literature the input layer employs a new neuron model which converts real valued stimuli into spikes with varying amplitudes and firing times . The intermediate layer neurons are modeled as integrate and fire spiking neurons . The decision block identifies that intermediate neuron which fires first and returns the class label associated with that neuron as the predicted class label . The sequential learning algorithm for the spiking neural network automatically determines the network structure from the training samples and adapts its synaptic weights by long term potentiation and long term depression . Performance of SLSNC has been evaluated using a number of benchmark classification problems and the results have been compared with other well known spiking neural network classifiers in the literature as well as with the standard support vector machine with a Gaussian kernel and the fast learning Extreme Learning Machine classifiers . The results clearly indicate that the described spiking neural network produces similar or better generalization performance with a smaller network . \\n'],\n",
              " [' The ability of artificial immune systems to adapt to varying pathogens makes such systems a suitable choice for various robotic applications . Generally immunity based robotic applications map local instantaneous sensory information into either an antigen or a co stimulatory signal according to the choice of representation schema . Algorithms then use relevant immune functions to output either evolved antibodies or maturity of dendritic cells in terms of actuation signals . It is observed that researchers do not try to replicate the biological immunity but select necessary immune functions instead resulting in an ad hoc manner these applications are reported . On the other hand the paradigm shift in robotics research from reactive to probabilistic approaches is also not being reflected in these applications . Authors therefore present a detailed review of immuno inspired robotic applications in an attempt to identify the possible areas to explore . Moreover the literature has been categorized according to the underlying immuno definitions . Implementation details have been critically reviewed in terms of corresponding mathematical expressions and their representation schema that include binary real or hybrid approaches . Limitations of reported applications have also been identified in light of modern immunological interpretations including the danger theory . As a result of this study authors suggest a renewed focus on innate immunity action contextualization prior to B T cell invocation and behavior evolution instead of arbitration . In this context a multi tier immunological framework for robotics research combining innate and adaptive components together is also suggested and skeletonized . \\n'],\n",
              " [' Genetic algorithm is a branch of evolutionary algorithm has proved its effectiveness in solving constrain based complex real world problems in variety of dimensions . The individual phases of GA are the mimic of the basic biological processes and hence the self adaptability of GA varied in accordance to the adjustable natural processes . In some instances self adaptability in GA fails in identifying adaptable genes to form a solution set after recombination which leads converge toward infeasible solution sometimes this infeasible solution could not be converted into feasible form by means of any of the repairing techniques . In this perspective Gene Suppressor a bio inspired process is being proposed as a new phase after recombination in the classical GA life cycle . This phase works on new individuals generated after recombination to attain self adaptability by adapting best genes in the environment to regulate chromosomes expression for achieving desired phenotype expression . Repairing in this phase converts infeasible solution into feasible solution by suppressing conflicting gene from the environment . Further the solution vector expression is improved by inducing best genes expression in the environment within the set of intended constrains . Multiobjective Multiple Knapsack Problems one of the popular NP hard combinatorial problems is being considered as the test bed for proving the competence of the proposed new phase of GA . The standard MMKP benchmark instances obtained from OR library are used for the experiments reported in this paper . The outcomes of the proposed method is compared with the existing repairing techniques where the analyses proved the proficiency of the proposed GS model in terms of better error and convergence rates for all instances . \\n'],\n",
              " [' In this paper we propose a H.264 AVC compressed domain human action recognition system with projection based metacognitive learning classifier . The features are extracted from the quantization parameters and the motion vectors of the compressed video stream for a time window and used as input to the classifier . Since compressed domain analysis is done with noisy sparse compression parameters it is a huge challenge to achieve performance comparable to pixel domain analysis . On the positive side compressed domain allows rapid analysis of videos compared to pixel level analysis . The classification results are analyzed for different values of Group of Pictures parameter time window including full videos . The functional relationship between the features and action labels are established using PBL McRBFN with a cognitive and meta cognitive component . The cognitive component is a radial basis function while the meta cognitive component employs self regulation to achieve better performance in subject independent action recognition task . The proposed approach is faster and shows comparable performance with respect to the state of the art pixel domain counterparts . It employs partial decoding which rules out the complexity of full decoding and minimizes computational load and memory usage . This results in reduced hardware utilization and increased speed of classification . The results are compared with two benchmark datasets and show more than 90 accuracy using the PBL McRBFN . The performance for various GOP parameters and group of frames are obtained with twenty random trials and compared with other well known classifiers in machine learning literature . \\n'],\n",
              " [' In this paper we investigate the reduction in total transmission time and the energy consumption of wireless sensor networks using multi hop data aggregation by forming coordination in hierarchical clustering . Novel algorithm handles wireless sensor network in numerous circumstances as in large extent and high density deployments . One of the major purposes is to collect information from inaccessible areas by using factorization of the area into subareas and appointing cluster head in each of the subarea . Coordination and cooperation among the local nodes via relay nodes in local cluster helped to serve each and every node . Routing is based on the predefined path proposed by new transmission algorithm . Transmission distance is minimized by using cluster coordinators for inter cluster communication and relay nodes within the cluster . We show by extended simulations that Chain Based Cluster Cooperative Protocol performs very well in terms of energy and time . To prove it we compare it with LEACH SEP genetic HCR and ERP and found that new protocol consumes six times less energy than LEACH five times less energy than SEP four time less energy than genetic HCR and three times less energy than ERP which further validate our work . \\n'],\n",
              " [' The analysis of internal connective operators of fuzzy reasoning is very significant and the robustness of fuzzy reasoning has been calling for study . An interesting and important question is that how to choose suitable internal connective operators to guarantee good robustness of rule based fuzzy reasoning This paper is intended to answer it . In this paper Lipschitz aggregation property and copula characteristic of t norms and implications are discussed . The robustness of rule based fuzzy reasoning is investigated and the relationships among input perturbation rule perturbation and output perturbation are presented . The suitable t norm and implication can be chosen to satisfy the need of robustness of fuzzy reasoning . In 1 Lipschitz operators if both t norm and implication are copulas the rule based fuzzy reasoning is much more stable and more reliable . In copulas if both t norm and implication are 1 l Lipschitz they can guarantee good robustness of fuzzy reasoning . The experiments not only illustrate the ideas proposed in the paper but also can be regarded as applications of soft computing . The approach in the paper also provides guidance for choosing suitable fuzzy connective operators and decision making application in rule based fuzzy reasoning . \\n'],\n",
              " [' Time series forecasting has been widely used to determine future prices of stocks and the analysis and modeling of finance time series is an important task for guiding investors decisions and trades . Nonetheless the prediction of prices by means of a time series is not trivial and it requires a thorough analysis of indexes variables and other data . In addition in a dynamic environment such as the stock market the non linearity of the time series is a pronounced characteristic and this immediately affects the efficacy of stock price forecasts . Thus this paper aims at proposing a methodology that forecasts the maximum and minimum day stock prices of three Brazilian power distribution companies which are traded in the S o Paulo Stock Exchange BM FBovespa . When compared to the other papers already published in the literature one of the main contributions and novelty of this paper is the forecast of the range of closing prices of Brazilian power distribution companies stocks . As a result of its application investors may be able to define threshold values for their stock trades . Moreover such a methodology may be of great interest to home brokers who do not possess ample knowledge to invest in such companies . The proposed methodology is based on the calculation of distinct features to be analysed by means of attribute selection defining the most relevant attributes to predict the maximum and minimum day stock prices of each company . Then the actual prediction was carried out by Artificial Neural Networks which had their performances evaluated by means of Mean Absolute Error Mean Absolute Percentage Error and Root Mean Square Error calculations . The proposed methodology for addressing the problem of prediction of maximum and minimum day stock prices for Brazilian distribution companies is effective . In addition these results were only possible to be achieved due to the combined use of attribute selection by correlation analysis and ANNs . \\n'],\n",
              " [' The main aim in network anomaly detection is effectively spotting hostile events within the traffic pattern associated to network operations by distinguishing them from normal activities . This can be only accomplished by acquiring the a priori knowledge about any kind of hostile behavior that can potentially affect the network or more easily by building a model that is general enough to describe the normal network behavior and detect the violations from it . Earlier detection frameworks were only able to distinguish already known phenomena within traffic data by using pre trained models based on matching specific events on pre classified chains of traffic patterns . Alternatively more recent statistics based approaches were able to detect outliers respect to a statistic idealization of normal network behavior . Clearly while the former approach is not able to detect previously unknown phenomena the latter one has limited effectiveness since it can not be aware of anomalous behaviors that do not generate significant changes in traffic volumes . Machine learning allows the development of adaptive non parametric detection strategies that are based on understanding the network dynamics by acquiring through a proper training phase a more precise knowledge about normal or anomalous phenomena in order to classify and handle in a more effective way any kind of behavior that can be observed on the network . Accordingly we present a new anomaly detection strategy based on supervised machine learning and more precisely on a batch relevance based fuzzyfied learning algorithm known as U BRAIN aiming at understanding through inductive inference the specific laws and rules governing normal or abnormal network traffic in order to reliably model its operating dynamics . The inferred rules can be applied in real time on online network traffic . This proposal appears to be promising both in terms of identification accuracy and robustness flexibility when coping with uncertainty in the detection classification process as verified through extensive evaluation experiments . \\n'],\n",
              " [' In this paper a new approach called evolving principal component clustering is applied to a data stream . Regions of the data described by linear models are identified . The method recursively estimates the data variance and the linear model parameters for each cluster of data . It enables good performance robust operation low computational complexity and simple implementation on embedded computers . The proposed approach is demonstrated on real and simulated examples from laser range finder data measurements . The performance complexity and robustness are validated through a comparison with the popular split and merge algorithm . \\n'],\n",
              " [' Surface Electromyography is a non invasive easy to record signal of superficial muscles from the skin surface . The sEMG is widely used in evaluating the functional status of the hand to assist in hand gesture recognition prosthetics and rehabilitation applications . Considering the nonlinear and non stationary characteristics of sEMG hand gesture recognition using sEMG signals necessitate designers to use Maximal Lyapunov Exponent or ensemble Empirical Mode Decomposition based MLEs . In this research we propose a hand gesture recognition method of sEMG based on nonlinear multiscale MLE . The aim is to increase the classification accuracy of sEMG features while reducing the complexity of EMD . The nonlinear MLE features are classified using Flexible Neural Tree which can solve highly structured dependent problems of the Artificial Neural Network . The testing has been conducted using several experiments with five participants . The classification performance of nonlinear multiscale MLE method is compared with MLE and EMD based MLE through simulations . Experimental results demonstrate that the former algorithm outperforms the two latter algorithms and can classify six different hand gestures up to 97.6 accuracy . \\n'],\n",
              " [' Despite the wide application of evolutionary computation techniques to rule discovery in stock algorithmic trading a comprehensive literature review on this topic is unavailable . Therefore this paper aims to provide the first systematic literature review on the state of the art application of EC techniques for rule discovery in stock AT . Out of 650 articles published before 2013 51 relevant articles from 24 journals were confirmed . These papers were reviewed and grouped into three analytical method categories and three EC technique categories . A significant bias toward the applications of genetic algorithm based and genetic programming based techniques in technical trading rule discovery is observed . Other EC techniques and fundamental analysis lack sufficient study . Furthermore we summarize the information on the evaluation scheme of selected papers and particularly analyze the researches which compare their models with buy and hold strategy . We observe an interesting phenomenon where most of the existing techniques perform effectively in the downtrend and poorly in the uptrend and considering the distribution of research in the classification framework we suggest that this phenomenon can be attributed to the inclination of factor selections and problem in transaction cost selections . We also observe the significant influence of the transaction cost change on the margins of excess return . Other influenced factors are also presented in detail . The absence of ways for market trend prediction and the selection of transaction cost are two major limitations of the studies reviewed . In addition the combination of trading rule discovery techniques and portfolio selection is a major research gap . Our review reveals the research focus and gaps in applying EC techniques for rule discovery in stock AT and suggests a roadmap for future research . \\n'],\n",
              " [' This paper introduces a novel approach for identity authentication system based on metacarpophalangeal joint patterns . A discriminative common vector based method is utilized for feature selection . In the literature there is no study using whole MJP for identity authentication exceptionally a work using the hand knuckle pattern which is some part of the MJP draws the attention as a similar study . The originality of this approach is that whole MJP is firstly used as a biometric identifier and DCV method is firstly applied for extracting the feature set of MJP . The developed system performs some basic tasks like image acquisition image pre processing feature extraction matching and performance evaluation . The feasibility and effectiveness of this approach is rigorously evaluated using the k fold cross validation technique on two different databases a publicly available database and a specially established database . The experimental results indicate that the MJPs are very distinctive biometric identifiers and can be securely used in biometric identification and verification systems DCV method is successfully employed for obtaining the feature set of MJPs and proposed MJP based authentication approach is very successful according to state of the art techniques with a recognition rate of between 95.33 and 100.00 . \\n'],\n",
              " [' This paper introduces a novel approach to detect and classify power quality disturbance in the power system using radial basis function neural network . The proposed method requires less number of features as compared to conventional approach for the identification . The feature extracted through the wavelet is trained by a radial basis function neural network for the classification of events . After training the neural network the weight obtained is used to classify the Power Quality problems . For the classification 20 types of disturbances are taken into account . The classification performance of RBFNN is compared with feed forward multilayer network learning vector quantization probabilistic neural network and generalized regressive neural network . The classification accuracy of the RBFNN network is improved just by rewriting the weights and updating the weights with the help of cognitive as well as the social behavior of particles along with fitness value . The simulation results possess significant improvement over existing methods in signal detection and classification . \\n'],\n",
              " [' In this paper using the Dempster Shafer theory of evidence a new decision criterion is proposed which can quickly classify airborne objects without any a priori knowledge whose data are laced with environmental noise characteristics within 10seconds from the time it is detected . Kinematic parameters of an airborne object received from radars are used to classify it into one of the six classes which include three levels of ballistic target discrimination aerodynamic satellite and unknown . The DST is chosen as it can suitably handle the element of uncertainty limited a priori data and short observation times that exist with the data acquired for the purpose of classification . The focus of the work is on ballistic targets in a theater of war . The approach is compared with the popularly known k NN and decision tree techniques and is found to perform better with the chosen data sets . This approach is tested using both real flight test data and simulated data . \\n'],\n",
              " [' In this paper a new version of the particle swarm optimization algorithm suitable for discrete optimization problems is presented and applied for the solution of the capacitated location routing problem and for the solution of a new formulation of the location routing problem with stochastic demands . The proposed algorithm combines three different topologies which are incorporated in a constriction particle swarm optimization algorithm and thus a very effective new algorithm the global and local combinatorial expanding neighborhood topology particle swarm optimization was developed . The algorithm was tested initially in the three classic sets of benchmark instances for the capacitated location routing problem with discrete demands and then as there are no benchmark instances for the location routing problem with stochastic demands these instances were transformed appropriately in order to be suitable for the problem with stochastic demands . The algorithm was tested in the problem with the stochastic demands using these transformed sets of benchmark instances . The algorithm was compared with a number of different implementations of the PSO and with metaheuristic evolutionary and nature inspired algorithms from the literature for the location routing problem with discrete and stochastic demands . \\n'],\n",
              " [' The Bayesian neural networks are useful tools to estimate the functional structure in the nonlinear systems . However they suffer from some complicated problems such as controlling the model complexity the training time the efficient parameter estimation the random walk and the stuck in the local optima in the high dimensional parameter cases . In this paper to alleviate these mentioned problems a novel hybrid Bayesian learning procedure is proposed . This approach is based on the full Bayesian learning and integrates Markov chain Monte Carlo procedures with genetic algorithms and the fuzzy membership functions . In the application sections to examine the performance of proposed approach nonlinear time series and regression analysis are handled separately and it is compared with the traditional training techniques in terms of their estimation and prediction abilities . \\n'],\n",
              " [' Cognitive radio network enables unlicensed users to sense for and opportunistically operate in underutilized licensed channels which are owned by the licensed users . Cognitive radio network has been regarded as the next generation wireless network centered on the application of artificial intelligence which helps the SUs to learn about as well as to adaptively and dynamically reconfigure its operating parameters including the sensing and transmission channels for network performance enhancement . This motivates the use of artificial intelligence to enhance security schemes for CRNs . Provisioning security in CRNs is challenging since existing techniques such as entity authentication are not feasible in the dynamic environment that CRN presents since they require pre registration . In addition these techniques can not prevent an authenticated node from acting maliciously . In this article we advocate the use of reinforcement learning to achieve optimal or near optimal solutions for security enhancement through the detection of various malicious nodes and their attacks in CRNs . RL which is an artificial intelligence technique has the ability to learn new attacks and to detect previously learned ones . RL has been perceived as a promising approach to enhance the overall security aspect of CRNs . RL which has been applied to address the dynamic aspect of security schemes in other wireless networks such as wireless sensor networks and wireless mesh networks can be leveraged to design security schemes in CRNs . We believe that these RL solutions will complement and enhance existing security solutions applied to CRN To the best of our knowledge this is the first survey article that focuses on the use of RL based techniques for security enhancement in CRNs . \\n'],\n",
              " [' This paper deals with the problem of parameter estimation in the generalized Mallows model by using both local and global search metaheuristic algorithms . The task we undertake is to learn parameters for defining the GMM from a dataset of complete rankings permutations . Several approaches can be found in the literature some of which are based on greedy search and branch and bound search . The greedy approach has the disadvantage of usually becoming trapped in local optima while the branch and bound approach basically A search usually comes down to approximate search because of memory requirements losing in this way its guaranteed optimality . Here we carry out a comparative study of several MH algorithms methods variable neighborhood search methods genetic algorithms and estimation of distribution algorithms and a tailored algorithm A to address parameter estimation in GMMs . We use 22 real datasets of different complexity all but one of which were created by the authors by preprocessing real raw data . We provide a complete analysis of the experiments in terms of accuracy number of iterations and CPU time requirements . \\n'],\n",
              " [' In this research flexible flow shop scheduling with unrelated parallel machines at each stage are considered . The number of stages and machines vary at each stage and each machine can process specific operations . In other words machines have eligibility and parts have different release times . In addition the blocking restriction is considered for the problem . Parts should pass each stage and process on only one machine at each stage . In the proposed problem transportation of parts loading and unloading parts are done by robots and the objective function is finding an optimal sequence of processing parts and robots movements to minimize the makespan and finding the closest number to the optimal number of robots . The main contribution of this study is to present the mixed integer linear programming model for the problem which considers release times for parts in scheduling area loading and unloading times of parts which transferred by robots . New methodologies are investigated for solving the proposed model . Ant Colony Optimization with double pheromone and genetic algorithm are proposed . Finally two meta heuristic algorithms are compared to each other computational results show that the GA performs better than ACO and the near optimal numbers of robots are determined . \\n'],\n",
              " [' With the advent of paralleling and implementation of restructuring in the power market some routine rules and patterns of traditional market should be accomplished in a way different from the past . To this end the unit commitment scheduling that has once been aimed at minimizing operating costs in an integrated power market is metamorphosed to profit based unit commitment by adopting a new schema in which generation companies have a common tendency to maximize their own profit . In this paper a novel optimization technique called imperialist competitive algorithm as well as an improved version of this evolutionary algorithm are employed for solving the PBUC problem . Moreover traditional binary approach of coding of initial solutions is replaced with an improved integer based coding method in order to reduce computational complexity and subsequently ameliorate convergence procedure of the proposed method . Then a sub ICA algorithm is proposed to obtain optimal generation power of thermal units . Simulation results validate effectiveness and applicability of the proposed method on two scenarios a set of unimodal and multimodal standard benchmark functions two GENCOs consist of 10 and 100 generating units . fuel consumption coefficient of unit fuel consumption coefficient of unit fuel consumption coefficient of unit cost of imperialist cost of country cost function of unit i for units with ON status P P normalized cost of nth imperialist cold start up cost of unit i cooling constant of unit i ramp down rate of unit i cost of best obtained solution at last iteration of algorithm hot start up cost of unit i initial state of unit i commitment state of unit i at time t number of decades of main sub algorithm total number of generating units number of colonies number of imperialists initial number of colonies of empire n a chaotic vector produced by a chose map total system demand at time t positions of a colony at the current decade positions of an imperialist at the current decade positions of a colony at the next decade maximum active generation of unit i maximum response rate limited active power output of unit i at time t minimum active generation of unit i maximum response rate limited active power output of unit i at time t generation of unit i at time t a randomly generated number within total revenue start up cost of unit i at time t stopping criterion of the algorithm when it converges into 10 6 tolerance dispatch period total cost of generation total cost of imperialist empire n minimum OFF time of unit i minimum ON time of unit i ramp up rate of unit i a vector that is directed toward the imperialist locations and starts from the previous location of the colony v 1 1 a vector perpendicular to vector V time duration for which unit i has been OFF at time t time duration for which unit i has been ON at time t colonies corporation factor in imperialist power a chaotic. \\n'],\n",
              " [' The theoretical studies of differential evolution algorithm have gradually attracted the attention of more and more researchers . According to recent researches the classical DE can not guarantee global convergence in probability except for some special functions . Along this perspective a problem aroused is that on which functions DE can not guarantee global convergence . This paper firstly addresses that DE variants are difficult on solving a class of multimodal functions identified by two characteristics . One is that the global optimum of the function is near a boundary of the search space . The other is that the function has a larger deceptive optima set in the search space . By simplifying the class of multimodal functions this paper then constructs a Linear Deceptive function . Finally this paper develops a random drift model of the classical DE algorithm to prove that the algorithm can not guarantee global convergence on the class of functions identified by the two above characteristics . \\n'],\n",
              " [' Detecting discontinuities in electrical signals from recorded oscillograms makes it possible to segment them . This is the first step in implementing automated methods which will ensure disturbances in electrical power systems are detected classified and stored . In this context this paper presents a way of determining an adaptive threshold based on the decomposition of electrical signals through the Discrete Wavelet Transform using Daubechies family filter banks allowing for the segmentation of signals and as a consequence the analysis of disturbances related to Power Quality . Considering this the proposed approach was initially evaluated for signals originating from mathematical models representing short term voltage fluctuations transients and harmonic distortions . In the synthetic signal database either single or combined occurrences of more than one disturbance were considered . By applying the DWT the amount of energy and entropy of energy were then calculated for the leaves of the second level of decomposition . Based on these calculations a unique adaptive threshold could be determined for each analyzed signal . Afterwards the amount of existing intersections between the threshold and the curve of details obtained for the second level of decomposition was then defined . Thus the intersections determine the beginning and end of the segments . In order to validate the approach the performance of the proposed methodology was analyzed considering the signals obtained from oscillograms provided by IEEE 1159.3 Task Force as well as real oscillograms obtained from a regional distribution utility . After these analyses it was observed that the proposed approach is efficient and applicable to automatic segmentation of events related to PQ . \\n'],\n",
              " [' Time series forecasting is an important and widely popular topic in the research of system modeling and stock index forecasting is an important issue in time series forecasting . Accurate stock price forecasting is a challenging task in predicting financial time series . Time series methods have been applied successfully to forecasting models in many domains including the stock market . Unfortunately there are 3 major drawbacks of using time series methods for the stock market some models can not be applied to datasets that do not follow statistical assumptions most time series models that use stock data with a significant amount of noise involutedly have worse forecasting performance and the rules that are mined from artificial neural networks are not easily understandable . To address these problems and improve the forecasting performance of time series models this paper proposes a hybrid time series adaptive network based fuzzy inference system model that is centered around empirical mode decomposition to forecast stock prices in the Taiwan Stock Exchange Capitalization Weighted Stock Index and Hang Seng Stock Index . To measure its forecasting performance the proposed model is compared with Chen s model Yu s model the autoregressive model the ANFIS model and the support vector regression model . The results show that our model is superior to the other models based on root mean squared error values . \\n'],\n",
              " [' We consider the problem faced by a company that must outsource reverse logistics activities to third party providers . Addressing RL outsourcing problems has become increasingly relevant issue in the management science and decision making literatures . The correct evaluation and ranking of the decision criteria priorities determining the selection of the best third party RL providers is essential for the competitive performance of the outsourcing company . The method proposed in this study allows to identify and classify these decision criteria . First the relevant criteria and sub criteria are identified using a SWOT analysis . Then Intuitionistic Fuzzy AHP is used to evaluate the relative importance weights among the criteria and the corresponding sub criteria . These relative weights are implemented in a novel extension of Mikhailov s fuzzy preference programming method to produce local weights for all criteria and sub criteria . Finally these local weights are used to assign a global weight to each sub criterion and create a ranking . We discuss the results obtained by applying the proposed model to a case study of a real company . In particular these results show that the most important priority for the company when delegating RL activities to 3PRLPs is to focus on the core business while reducing costs constitutes one of its least important priorities . \\n'],\n",
              " [' This paper presents a new algorithm designed to find the optimal parameters of PID controller . The proposed algorithm is based on hybridizing between differential evolution and Particle Swarm Optimization with an aging leader and challengers algorithms . The proposed algorithm is tested on twelve benchmark functions to confirm its performance . It is found that it can get better solution quality higher success rate in finding the solution and yields in avoiding unstable convergence . Also ALC PSODE is used to tune PID controller in three tanks liquid level system which is a typical nonlinear control system . Compared to different PSO variants genetic algorithm differential evolution and Ziegler Nichols method the proposed algorithm achieve the best results with least standard deviation for different swarm size . These results show that ALC PSODE is more robust and efficient while keeping fast convergence . \\n'],\n",
              " [' Many variants of particle swarm optimization both enhance the performance of the original method and greatly increase its complexity . Motivated by this fact we investigate factors that influence the convergence speed and stability of basic PSO without increasing its complexity from which we develop an evaluation index called Control Strategy PSO . The evaluation index is based on the oscillation properties of the transition process in a control system . It provides a method of selection parameters that promote system convergence to the optimal value and thus helps manage the optimization process . In addition it can be applied to the characteristic analyses and parameter confirmation processes associated with other intelligent algorithms . We present a detailed theoretical and empirical analysis in which we compare the performance of CSPSO with published results on a suite of well known benchmark optimization functions including rotated and shifted functions . We used the convergence rates and iteration numbers as metrics to compare simulation data and thereby demonstrate the effectiveness of our proposed evaluation index . We applied CSPSO to antenna array synthesis and our experimental results show that it offers high performance in pattern synthesis . \\n'],\n",
              " [' Advanced sensing technologies have produced a significant amount of discrete point data in the past decade . Measurement uncertainty frequently occurs at the geometric discontinuity of mechanical parts . In this paper a genetic search algorithm is developed for optimally constrained multiple line fitting of discrete data points . It contains two important technical components constrained least squares fitting of multiple lines and genetic search for optimal corner edge points . The algorithm is designed for both two dimensional and three dimensional cases . Numerical experiments demonstrate the effectiveness of the proposed approach compared to the conventional least squares fitting method as well as exhaustive search method . A comparative study with a particle swarm method indicates that both the genetic search and particle swarm search produce similar results in terms of minimum fitting errors . It can be used for the effective determination of sharp edges or corners based on discrete data points measured for high precision industrial inspection and manufacturing . direction numbers of a line in a three dimensional space binary numbers for a chromosome cognitive and social parameters decimal value corresponding to binary bits the shortest distance of the ith data point and the fitting line fitness function in two dimensional cases fitness function in three dimensional cases the fitness value of the ith candidate corner mean and maximum fitness the probability of crossover the probability of mutation the position of the globally best individual the best previous position of the ith particle random numbers that are uniformly distributed within a range the kth position of the ith particle in 2D The kth position of the ith particle in 3D the coordination of the ith data point in a three dimension space The coordination of start point for genetic search orthogonal projection of data points to fitted lines the real increment in a given genetic search the coordination of candidate constrained corner the size of genetic search in three coordinate directions the coordinates of the point passed by a line the coordinates of any point in a three dimension space the kth velocity of the ith particle inertial weight the coefficients of a line equation in two and three dimensions the sum of squared coordinate difference in x coordinate between data points and their corresponding points on the fitted line the sum of squared coordinate difference in y coordinate between data points and their corresponding points on the fitted line the gradient of E the gradient of E standard deviation of data points \\n'],\n",
              " [' As social media and e commerce on the Internet continue to grow opinions have become one of the most important sources of information for users to base their future decisions on . Unfortunately the large quantities of opinions make it difficult for an individual to comprehend and evaluate them all in a reasonable amount of time . The users have to read a large number of opinions of different entities before making any decision . Recently a new retrieval task in information retrieval known as Opinion Based Entity Ranking has emerged . OpER directly ranks relevant entities based on how well opinions on them are matched with a user s preferences that are given in the form of queries . With such a capability users do not need to read a large number of opinions available for the entities . Previous research on OpER does not take into account the importance and subjectivity of query keywords in individual opinions of an entity . Entity relevance scores are computed primarily on the basis of occurrences of query keywords match by assuming all opinions of an entity as a single field of text . Intuitively entities that have positive judgments and strong relevance with query keywords should be ranked higher than those entities that have poor relevance and negative judgments . This paper outlines several ranking features and develops an intuitive framework for OpER in which entities are ranked according to how well individual opinions of entities are matched with the user s query keywords . As a useful ranking model may be constructed from many ranking features we apply learning to rank approach based on genetic programming to combine features in order to develop an effective retrieval model for OpER task . The proposed approach is evaluated on two collections and is found to be significantly more effective than the standard OpER approach . \\n'],\n",
              " [' Mining frequent itemsets is an essential problem in data mining and plays an important role in many data mining applications . In recent years some itemset representations based on node sets have been proposed which have shown to be very efficient for mining frequent itemsets . In this paper we propose DiffNodeset a novel and more efficient itemset representation for mining frequent itemsets . Based on the DiffNodeset structure we present an efficient algorithm named dFIN to mining frequent itemsets . To achieve high efficiency dFIN finds frequent itemsets using a set enumeration tree with a hybrid search strategy and directly enumerates frequent itemsets without candidate generation under some case . For evaluating the performance of dFIN we have conduct extensive experiments to compare it against with existing leading algorithms on a variety of real and synthetic datasets . The experimental results show that dFIN is significantly faster than these leading algorithms . \\n'],\n",
              " [' Cluster ensemble is a powerful method for improving both the robustness and the stability of unsupervised classification solutions . This paper introduced group method of data handling to cluster ensemble and proposed a new cluster ensemble framework which named cluster ensemble framework based on the group method of data handling . CE GMDH consists of three components an initial solution a transfer function and an external criterion . Several CE GMDH models can be built according to different types of transfer functions and external criteria . In this study three novel models were proposed based on different transfer functions least squares approach cluster based similarity partitioning algorithm and semidefinite programming . The performance of CE GMDH was compared among different transfer functions and with some state of the art cluster ensemble algorithms and cluster ensemble frameworks on synthetic and real datasets . Experimental results demonstrate that CE GMDH can improve the performance of cluster ensemble algorithms which used as the transfer functions through its unique modelling process . It also indicates that CE GMDH achieves a better or comparable result than the other cluster ensemble algorithms and cluster ensemble frameworks . \\n'],\n",
              " [' Minimum class variance support vector machine and large margin linear projection classifier in contrast with traditional support vector machine take the distribution information of the data into consideration and can obtain better performance . However in the case of the singularity of the within class scatter matrix both MCVSVM and LMLP only exploit the discriminant information in a single subspace of the within class scatter matrix and discard the discriminant information in the other subspace . In this paper a so called twin space support vector machine algorithm is proposed to deal with the high dimensional data classification task where the within class scatter matrix is singular . TSSVM is rooted in both the non null space and the null space of the within class scatter matrix takes full advantage of the discriminant information in the two subspaces and so can achieve better classification accuracy . In the paper we first discuss the linear case of TSSVM and then develop the nonlinear TSSVM . Experimental results on real datasets validate the effectiveness of TSSVM and indicate its superior performance over MCVSVM and LMLP . \\n'],\n",
              " [' Power quality issues have become more important than before due to increased use of sensitive electrical loads . In this paper a new hybrid algorithm is presented for PQ disturbances detection in electrical power systems . The proposed method is constructed based on four main steps simulation of PQ events extraction of features selection of dominant features and classification of selected features . By using two powerful signal processing tools i.e . variational mode decomposition and S transform some potential features are extracted from different PQ events . VMD as a new tool decomposes signals into different modes and ST also analyzes signals in both time and frequency domains . In order to avoid large dimension of feature vector and obtain a detection scheme with optimum structure sequential forward selection and sequential backward selection as wrapper based methods and Gram Schmidt orthogonalization based feature selection method as filter based method are used for elimination of redundant features . In the next step PQ events are discriminated by support vector machines as classifier core . Obtained results of the extensive tests prove the satisfactory performance of the proposed method in terms of speed and accuracy even in noisy conditions . Moreover the start and end points of PQ events can be detected with high precision . \\n'],\n",
              " [' Fabrication of three dimensional structures has gained increasing importance in the bone tissue engineering field . Mechanical properties and permeability are two important requirement for BTE scaffolds . The mechanical properties of the scaffolds are highly dependent on the processing parameters . Layer thickness delay time between spreading each powder layer and printing orientation are the major factors that determine the porosity and compression strength of the 3D printed scaffold . In this study the aggregated artificial neural network was used to investigate the simultaneous effects of layer thickness delay time between spreading each layer and print orientation of porous structures on the compressive strength and porosity of scaffolds . Two optimization methods were applied to obtain the optimal 3D parameter settings for printing tiny porous structures as a real BTE problem . First particle swarm optimization algorithm was implemented to obtain the optimum topology of the AANN . Then Pareto front optimization was used to determine the optimal setting parameters for the fabrication of the scaffolds with required compressive strength and porosity . The results indicate the acceptable potential of the evolutionary strategies for the controlling and optimization of the 3DP process as a complicated engineering problem . \\n'],\n",
              " [' In this paper novel interval and general type 2 self organizing fuzzy logic controllers are proposed for the automatic control of anesthesia during surgical procedures . The type 2 SOFLC is a hierarchical adaptive fuzzy controller able to generate and modify its rule base in response to the controller s performance . The type 2 SOFLC uses type 2 fuzzy sets derived from real surgical data capturing patient variability in monitored physiological parameters during anesthetic sedation which are used to define the footprint of uncertainty of the type 2 fuzzy sets . Experimental simulations were carried out to evaluate the performance of the type 2 SOFLCs in their ability to control anesthetic delivery rates for maintaining desired physiological set points for anesthesia under signal and patient noise . Results show that the type 2 SOFLCs can perform well and outperform previous type 1 SOFLC and comparative approaches for anesthesia control producing lower performance errors while using better defined rules in regulating anesthesia set points while handling the control uncertainties . The results are further supported by statistical analysis which also show that zSlices general type 2 SOFLCs are able to outperform interval type 2 SOFLC in terms of their steady state performance . \\n'],\n",
              " [' This paper proposes relaxed conditions for control synthesis of discrete time Takagi Sugeno fuzzy control systems under unreliable communication links . To widen the applicability of the fuzzy control approach under network environments a novel fuzzy controller which is homogenous polynomially parameter dependent on both the current time normalized fuzzy weighting functions and the multi steps past normalized fuzzy weighting functions is provided to make much more use of the information of the underlying system . Moreover a new kind of slack variable approach is also developed and thus the algebraic properties of these multi instant normalized fuzzy weighting functions are collected into some augmented matrices . As a result the conservatism of control synthesis of discrete time Takagi Sugeno fuzzy control systems under unreliable communication links can be significantly reduced . Two illustrative examples are presented to demonstrate the effectiveness of the theoretical development . \\n'],\n",
              " [' An accurate contour estimation plays a significant role in classification and estimation of shape size and position of thyroid nodule . This helps to reduce the number of false positives improves the accurate detection and efficient diagnosis of thyroid nodules . This paper introduces an automated delineation method that integrates spatial information with neutrosophic clustering and level sets for accurate and effective segmentation of thyroid nodules in ultrasound images . The proposed delineation method named as Spatial Neutrosophic Distance Regularized Level Set is based on Neutrosophic L Means clustering which incorporates spatial information for Level Set evolution . The SNDRLS takes rough estimation of region of interest as input provided by Spatial NLM clustering for precise delineation of one or more nodules . The performance of the proposed method is compared with level set NLM clustering Active Contour Without Edges Fuzzy C Means clustering and Neutrosophic based Watershed segmentation methods using the same image dataset . To validate the SNDRLS method the manual demarcations from three expert radiologists are employed as ground truth . The SNDRLS yields the closest boundaries to the ground truth compared to other methods as revealed by six assessment measures . The experimental results show that the SNDRLS is able to delineate multiple nodules in thyroid ultrasound images accurately and effectively . The proposed method achieves the automated nodule boundary even for low contrast blurred and noisy thyroid ultrasound images without any human intervention . Additionally the SNDRLS has the ability to determine the controlling parameters adaptively from SNLM clustering . \\n'],\n",
              " [' Two soft computing techniques are implemented to model and optimize the compressive strength of carbon polymer composites . Artificial neural network is used to establish a relationship between the uniaxial compressive strength of fabricated materials and the most significant processing parameters . To put together a database three different types of wood are carbonized at various heat treatment temperatures in specific pyrolysis time periods . Compression tests are then conducted at room temperature on the composites at a constant strain rate . The collected data of compressive strength and the related fabrication parameters are used as sets of data for training a neural network . A nested cross validation scheme is used to ensure the efficiency of the network . Results are indicative of a very good network which generalizes very well . Next an attempt is made to optimize the compressive behavior of the composites by controlling carbonization temperature time and also starting material type with the aid of a genetic algorithm coupled with the trained network . The optimization system yields promising results significantly enhancing the compressive strength . The validity of the optimal experiment as proposed by the soft computing system is verified by subsequent laboratory testing . \\n'],\n",
              " [' The present study applies a Hybrid method for identification of unknown parameters in a semi empirical tire model the so called Magic Formula . Firstly the Hybrid method used a Genetic Algorithm as a global search methodology with high exploration power . Secondly the results of the Genetic Algorithm were used as starting values for the Levenberg Marquardt algorithm as a gradient based method with high exploitation power . In this way the beneficial aspects of both methods are simultaneously exploited and their shortcomings are avoided . In order to establish the effectiveness of the proposed method performance of the Hybrid method has been compared with other methods available in the literature . In addition the use of GA as a Heuristic method for tire parameters identification has been discussed . Moreover the extrapolation power of Magic Formula identified with Hybrid method has been properly investigated . Finally the performance of the Hybrid method has been examined through tire parameter identification with priori known model . The results indicated that the Hybrid method has outstanding benefits such as high convergence speed high accuracy and null sensitivity to the starting values of unknown parameters . \\n'],\n",
              " [' In this paper we studied fuzzy linear fractional differential equations under Riemann Liouville H differentiability as the fuzzy initial value problems . Some of the previous results on solutions of these equations are concreted . We obtained new solutions by using the fractional hyperbolic functions and their properties in details . Finally an application and two examples are given to illustrate our results . \\n'],\n",
              " [' In this paper an exchange market algorithm approach is applied to solve highly non linear power system optimal reactive power dispatch problems . ORPD is most vital optimization problems in power system study and are usually devised as optimal power flow problem . The problem is formulated as nonlinear non convex constrained optimization problem with the presence of both continuous and discrete control variables . The EMA searches for optimal solution via two main phases namely balanced market and oscillation market . Each of the phases comprises of both exploration and exploitation which makes the algorithm unique . This uniqueness of EMA is exploited in this paper to solve various vital objectives associated with ORPD problems . Programs are developed in MATLAB and tested on standard IEEE 30 and IEEE 118 bus systems . The results obtained using EMA are compared with other contemporary methods in the literature . Simulation results demonstrate the superiority of EMA in terms of its computational efficiency and robustness . Consumed function evaluation for each case study is mentioned in the convergence plot itself for better clarity . Parametric study is also performed on different case studies to obtain the suitable values of tuneable parameters . susceptance of the lines connecting between bus and bus objective function conductance of transmission line between bus and bus stability index of th load bus number of generator busses or PV busses number of load busses or PQ busses number of tap changing transformers number of transmission lines number of shunt capacitors active power loss in MW minimum limit of active power output of slack generator of slack generator of slack generator of slack generator G1 active power generation of th generator active power load demand at th bus minimum limit of reactive power output of th generator maximum limit of reactive power output of ith generator reactive power generation for th generator reactive power load demand at th bus limiting value of reactive power generation of th generator reactive power output of slack generator G1 reactive power output of Shunt capacitor minimum value of reactive power output of th shunt capacitor maximum value of reactive power output of th shunt capacitor reactive power output of st shunt capacitor reactive power output of th shunt capacitor apparent power flow through th transmission line apparent power flow limit of th transmission line max . limit of apparent power flow through th transmission line shares of shareholders lower limit of thshare of th shareholder upper limit of th share of th shareholder variation in the share of th shareholder of the third group tap positions of tap changing transformers minimum position of th tap changing transformer maximum position of th tap changing transformer tap position of the first tap changing transformer tap position of the th tap changing transformer voltage of first generator G . magnitude of the load voltage voltage magnitude of the first load bus voltage magnitude of the th bus voltage magnitude of the th bus minimum voltage of. \\n'],\n",
              " [' In this paper we have made a humble attempt to automate an ophthalmologic diagnostic system based on signal processing using wavelets . Electroretinographic signals indicate the activity of the retinal cells from different layers of the inner retina and therefore these signals are used to predict various dreadful diseases . In this work we have analyzed 95 subjects from four different classes viz . Controls Congenital Stationery Night Blindness rod cone dystrophy and Central Retinal Vein Occlusion . The signal features extracted by wavelets are used for morphological and statistical analysis and for getting the subtle parameters like entropy . The results found comprises of difference in the values of wavelet coefficients a wave and b wave amplitude in the case of normal and pathological signals . The colour intensity distribution of scalograms shows highlighting variations in the case of maximum response and oscillatory potentials of the ERG signals for specific type of diseases . Furthermore we propose an Electroretinographic Index from different entropy parameters which can be used to distinguish between the normal and abnormal classes . This new method based on ERG signal analysis can be reliable enough to build a solution for the constraints in the field of ophthalmology . \\n'],\n",
              " [' Complex biological systems such as the human brain can be expected to be inherently nonlinear and hence difficult to model . Most of the previous studies on investigations of brain function have either used linear models or parametric nonlinear models . In this paper we propose a novel application of a nonlinear measure of phase synchronization based on recurrences correlation between probabilities of recurrence to study seizures in the brain . The advantage of this nonparametric method is that it makes very few assumptions thus making it possible to investigate brain functioning in a data driven way . We have demonstrated the utility of CPR measure for the study of phase synchronization in multichannel seizure EEG recorded from patients with global as well as focal epilepsy . For the case of global epilepsy brain synchronization using thresholded CPR matrix of multichannel EEG signals showed clear differences in results obtained for epileptic seizure and pre seizure . Brain headmaps obtained for seizure and pre seizure cases provide meaningful insights about synchronization in the brain in those states . The headmap in the case of focal epilepsy clearly enables us to identify the focus of the epilepsy which provides certain diagnostic value . Comparative studies with linear correlation have shown that the nonlinear measure CPR outperforms the linear correlation measure . \\n'],\n",
              " [' A novel quadrature clutter rejection approach based on multivariate empirical mode decomposition which is an extension of empirical mode decomposition to multivariate for processing multichannel signals is proposed in this paper to suppress the quadrature clutter signals induced by the vascular wall and the surrounding stationary or slowly moving tissues in composite Doppler ultrasound signals and extract more blood flow components with low velocities . In this approach the MEMD algorithms which include the bivariate empirical mode decomposition with a nonuniform sampling scheme for adaptive selection of projection directions and the trivariate empirical mode decomposition with noise assistance are directly employed to adaptively decompose the complex valued quadrature composite signals echoed from both bidirectional blood flow and moving wall into a small number of zero mean rotation components which are defined as complex intrinsic mode functions . Then the relevant CIMFs contributed to blood flow components are automatically distinguished in terms of the break of the CIMFs power and then directly added up to give the quadrature blood flow signal . Specific simulation and human subject experiments are taken up to demonstrate the advantages and limitations of this novel method for quadrature clutter rejection in bidirectional Doppler ultrasound signals . Due to eliminating the extra errors induced by the Hilbert transform or complex FIR filter algorithms used in the traditional clutter rejection approaches based on the directional separation process the proposed method provides improved accuracy for clutter rejection and preserve more slow blood blow components which could be helpful to early diagnose arterial diseases . \\n'],\n",
              " [' The analysis of heart rate variability is central for cardiac diagnostics but its essential nonstationarity has started to gain attention only recently . The aim of this work is to develop a method for finding mathematical indicators of HRV spectral properties considering frequency nonstationarity . The analysis is done both for the new model of rhythmogram taking into account frequency modulation and for the true rhythmogram record during head up tilt test . Continuous wavelet transformation of the frequency modulated signal has been derived in analytical form . The local frequency of heart rhythm giving the maximum of CWT has been determined . Treated as another non stationary signal this frequency has been subjected to CWT following double CWT procedure . The transient periods for local frequency the frequencies of local frequency fluctuation against the main trend and the periods of emergence and attenuation of such fluctuations have been defined by estimating the spectral integrals in the ranges ULF VLF LF HF . The presented technique allows to use HRV control in the cases of arrhythmia ectopic beats heart turbulence and other non stationary violence of heart rhythm and also while studying long term cardiac records . \\n'],\n",
              " [' Materials informatics is a growing field in materials science . Materials scientists have begun to use soft computing techniques to discover novel materials . In order to apply these techniques the descriptors of a material must be selected thereby deciding the resulting performance . As a way of describing a material the properties of each element in the material are used directly as the features of the input variable . Depending on the number of elements in the material the dimensionality of the input may differ . Hence it is not possible to apply the same model to materials with different numbers of elements for tasks such as regression or discrimination . In the present paper we present a novel method of uniforming the dimensionality of the input that allows regression or discriminative tasks to be performed using soft computing techniques . The main contribution of the proposed method is to provide a solution for uniforming the dimensionality among input vectors of different size . The proposed method is a variant of the denoising autoencoder Vincent et al . using neural networks and gives a latent representation with uniformed dimensionality of the input . In the experiments of the present study we consider compounds with ionic conductivity and hydrogen storage materials . The results of the experiments indicate that the regression tasks can be performed using the uniformed latent data learned by the proposed method . Moreover in the clustering task using these latent data we observed distance preservation in data space which is also the case for the denoising autoencoder . This result may enable the proposed method to be used in a broad range of applications . \\n'],\n",
              " [' We study the theoretical performance of using Electrical Impedance Tomography to measure the conductivity of the main tissues of the head . The governing equations are solved using the Finite Element Method for realistically shaped head models with isotropic and anisotropic electrical conductivities . We focus on the Electroencephalography signal frequency range since EEG source localization is the assumed application . We obtain the Cram r Rao Lower Bound to find the minimum conductivity estimation error expected with EIT measurements . The more convenient electrode pairs selected for current injection from a typical EEG array are determined from the CRLB . Moreover using simulated data the Maximum Likelihood Estimator of the conductivity parameters is shown to be close to the CRLB for a relatively low number of measurements . The results support the idea of using EIT as a low cost and practical tool for individually measure the conductivity of the head tissues and to use them when solving the EEG source localization . Even when the conductivity of the soft tissues of the head is available from Diffusion Tensor Imaging EIT can complement the electrical model with the estimation of the skull and scalp conductivities . \\n'],\n",
              " [' Conventionally optimal reactive power dispatch is described as the minimization of active power transmission losses and or total voltage deviation by controlling a number of control variables while satisfying certain equality and inequality constraints . This article presents a newly developed meta heuristic approach chaotic krill herd algorithm for the solution of the ORPD problem of power system incorporating flexible AC transmission systems devices . The proposed CKHA is implemented and its performance is tested successfully on standard IEEE 30 bus test power system . The considered power system models are equipped with two types of FACTS controllers . Simulation results indicate that the proposed approach yields superior solution over other popular methods surfaced in the recent state of the art literature including chaos embedded few newly developed optimization techniques . The obtained results indicate the effectiveness for the solution of ORPD problem of power system considering FACTS devices . Finally simulation is extended to some large scale power system models like IEEE 57 bus and IEEE 118 bus test power systems for the same objectives to emphasis on the scalability of the proposed CKHA technique . The scalability the robustness and the superiority of the proposed CKHA are established in this paper . \\n'],\n",
              " [' This work proposes a comparative study of a pair of electrocardiographic 2D representations the frontal plane and a preferential plane obtained from ECG data . During depolarization and repolarization main electrical vectors were analyzed and compared between healthy subjects and patients referred for percutaneous transluminal coronary angioplasty . Recordings were obtained at rest . Many patients from the latter group presented normal ECGs thus the hypothesis to prove was that electrical axis in any of the studied planes would effectively discriminate silent ischemia records from healthy ones . The FP was constructed with I and aVF leads while the PP used the two first eigenvectors of the spatial correlation matrix of the ECG . Although the depolarization and repolarization vectors from both groups resulted normal those from the silent ischemia group appeared strongly biased to the left closer to the limit of the normality range . This slight change originated a significant separation between health and disease in the FP . Here most of the parameters resulted highly informative even those related to the depolarization phase . The cardiac vector integrating both depolarization and repolarization information presented the highest performance . Parameters in the PP however did not produce an acceptable discrimination except for the amplitude of the T wave . Additionally the repolarization orientation in the FP was the only marker that simultaneously discriminated three different groups of patients according to their occlusion sites . In conclusion the FP offered a 2D representation general enough to enable the separation of silent ischemia versus health populations while the PP did not due mainly to its individually optimized nature failing to provide a unique referencial frame for all the subjects . \\n'],\n",
              " [' Due to noises speckles etc . automatic prostate segmentation is rather challenging and using only low level information such as intensity gradient is insufficient and unable to tackle the problem . In this paper we propose an automatic prostate segmentation method combining intrinsic properties of TRUS images with the high level shape prior information . First intrinsic properties of TRUS images such as the intensity transition near the prostate boundary as well as the speckle induced texture features obtained by Gabor filter banks are integrated to deform the model to the target contour . These properties make our method insensitive to high gradient regions introduced by noises and speckles . Then the preliminary segmentation is fine tuned by the non parametric shape prior which is optimally distilled by non parametric kernel density estimation as it can approximate arbitrary distributions . The refinement is along the direction of mean shift vector and considerably strengthens the robustness of the method . The performance of our method is validated by experimental results . Compared with the state of the art the accuracy and robustness of the method is quite promising and the mean absolute distance is only 1.21 0.85mm . \\n'],\n",
              " [' A spectral angle based feature extraction method Spectral Clustering Independent Component Analysis is proposed in this work to improve the brain tissue classification from Magnetic Resonance Images . SC ICA provides equal priority to global and local features thereby it tries to resolve the inefficiency of conventional approaches in abnormal tissue extraction . First input multispectral MRI is divided into different clusters by a spectral distance based clustering . Then Independent Component Analysis is applied on the clustered data in conjunction with Support Vector Machines for brain tissue analysis . Normal and abnormal datasets consisting of real and synthetic T1 weighted T2 weighted and proton density fluid attenuated inversion recovery images were used to evaluate the performance of the new method . Comparative analysis with ICA based SVM and other conventional classifiers established the stability and efficiency of SC ICA based classification especially in reproduction of small abnormalities . Clinical abnormal case analysis demonstrated it through the highest Tanimoto Index accuracy values 0.75 98.8 observed against ICA based SVM results 0.17 96.1 for reproduced lesions . Experimental results recommend the proposed method as a promising approach in clinical and pathological studies of brain diseases . \\n'],\n",
              " [' Automatic analysis of biomedical time series such as electroencephalogram and electrocardiographic signals has attracted great interest in the community of biomedical engineering due to its important applications in medicine . In this work a simple yet effective bag of words representation that is originally developed for text document analysis is extended for biomedical time series representation . In particular similar to the bag of words model used in text document domain the proposed method treats a time series as a text document and extracts local segments from the time series as words . The biomedical time series is then represented as a histogram of codewords each entry of which is the count of a codeword appeared in the time series . Although the temporal order of the local segments is ignored the bag of words representation is able to capture high level structural information because both local and global structural information are well utilized . The performance of the bag of words model is validated on three datasets extracted from real EEG and ECG signals . The experimental results demonstrate that the proposed method is not only insensitive to parameters of the bag of words model such as local segment length and codebook size but also robust to noise . \\n'],\n",
              " [' Technology credit scoring models have been used to screen loan applicant firms based on their technology . Typically a logistic regression model is employed to relate the probability of a loan default of the firms with several evaluation attributes associated with technology . However these attributes are evaluated in linguistic expressions represented by fuzzy number . Besides the possibility of loan default can be described in verbal terms as well . To handle these fuzzy input and output data we proposed a fuzzy credit scoring model that can be applied to predict the default possibility of loan for a firm that is approved based on its technology . The method of fuzzy logistic regression as an appropriate prediction approach for credit scoring with fuzzy input and output was presented in this study . The performance of the model is improved compared to that of typical logistic regression . This study is expected to contribute to practical utilization of the technology credit scoring with linguistic evaluation attributes . \\n'],\n",
              " [' Diaphragmatic electromyogram signal plays an important role in the diagnosis and analysis of respiratory diseases . However EMGdi recordings are often contaminated by electrocardiographic interference which posing serious obstacle to traditional denoising approaches due to overlapped spectra of these signals . In this paper a novel method based on wavelet transform and independent component analysis is proposed to remove the ECG interference from noisy EMGdi signals . With the proposed method the original independent components of contaminated EMGdi signal were first obtained with ICA . Then the ECG components contained were removed by a specially designed wavelet domain filter . After that the purified independent components were reconstructed back to the original signal space by ICA to obtain clean EMGdi signals . Experimental results achieved on practical clinical data show that the proposed approach is better than several traditional methods include wavelet transform ICA digital filter and adaptive filter in ECG interference removing . \\n'],\n",
              " [' The minimum vertex cover problem is a classical combinatorial optimization problem . This paper studies this problem based on rough sets . We show that finding the minimal vertex cover of a graph can be translated into finding the attribute reduction of a decision information table . At the same time finding a minimum vertex cover of graphs is equivalent to finding an optimal reduct of a decision information table . As an application of the theoretical framework a new algorithm for the minimum vertex cover problem based on rough sets is constructed . Experiments show that the proposed algorithm gets better performance in terms of the ratio value when compared with some other algorithms . \\n'],\n",
              " [' In this paper I introduce a new method for feature extraction to classify digital mammograms using fast finite shearlet transform . Initially fast finite shearlet transform was performed over mammogram images and feature vectors were built using coefficients of the transform . In subsequent calculations features were ranked according to t test statistics and capabilities were distinguished between different classes . To maximize differences between class representatives a thresholding process was implemented as a final stage of feature extraction and classifications were calculated over the optimal feature set using 5 fold cross validation and a support vector machine classifier . The present results show that the proposed method provides satisfactory classification accuracy . \\n'],\n",
              " [' Eye activity has larger electrical potential than the average electroencephalogram recording thus making it one of the major sources of artefacts . Ocular artefacts must be removed as completely as possible with little or no loss of EEG to obtain a higher quality EEG . Using independent component analysis the EEG is separated into independent components and the contaminated component is removed thus removing the OA . However ICA does not separate the sources completely and some of the meaningful EEG is lost . In this paper a new method combining ICA and wavelet neural networking is proposed . In this method WNN is applied to the contaminated ICs correcting the OA and thus lowering the data lost . The method was evaluated using simulated and real datasets and the results show that the OA are successfully removed with very little data loss . \\n'],\n",
              " [' Pulse transit time and pulse wave velocity are the markers most widely used to evaluate the vascular effects of aging hypertension arterial stiffness and atherosclerosis . To calculate these markers it is necessary to determine the location of the onset and systolic peak of the arterial pulse wave . In this paper a method employed for electrocardiography R peak detection with a slight modification is applied for both the onset and systolic peak detections in APW . The method employs Shannon energy envelope estimator Hilbert transform and moving average filter . The minimum value and the positive zero crossing points of the odd symmetry function of the HT correspond to the locations of the onset and systolic peak respectively . The algorithm was evaluated using expert s annotations with 10 records of 5min length and different signal to noise ratios and achieved a good performance and precision . When compared to expert s annotation the algorithm detected these fiducial points with average sensitivity positive predictivity and accuracy of 100 and presented errors less than 10ms . In APW signals contaminated with noise in both cases the relative error is less than 2 respect to pulse wave periods of 800ms . The performance of algorithm was compared with both foot approximation and adaptive threshold methods and the results show that the algorithm outperforms theses reported methods with respect to manuals annotation . The results are promising suggesting that the method provides a simple but accurate onset and systolic peak detection and can be used in the measurement of pulse transit time pulse wave velocity and pulse rate variability . \\n'],\n",
              " [' We propose a methodological study for the optimization of surface EMG based hand gesture classification effective to implement a human computer interaction device for both healthy subjects and transradial amputees . The widely commonly used unsupervised Principal Component Analysis approach was compared to the promising supervised common spatial pattern methodology to identify the best classification strategy and the related tuning parameters . A low density array of sEMG sensors was built to record the muscular activity of the forearm and classify five different hand gestures . Twenty healthy subjects were recruited to compute optimized parameters for and to compare between the two strategies . The system was also tested on a transradial amputee subject in order to assess the robustness of the optimization in recognizing disabled users gestures . Results show that RMS WA ANN is the best feature vector classifier pair for the PCA approach whereas M RMS WA ANN is the best pair for the CSP methodology . Statistical analysis on classification results shows no significant differences between the two strategies . Moreover we found out that the optimization computed for healthy subjects was proven to be sufficiently robust to be used on the amputee subject . This motivates further investigation of the proposed methodology on a larger sample of amputees . Our results are useful to boost EMG based hand gesture recognition and constitute a step toward the definition of an efficient EMG controlled system for amputees . \\n'],\n",
              " [' In this paper we propose a novel approach for the compression of multichannel electroencephalograph signals . The method assumes that EEG signals are the linear mixture of several independent components . To retain the ICs the proposed scheme first applies an independent component analysis with a preprocessing step of principal component analysis to EEG signals . Then the compression scheme is composed of two parts the ICs compression part and the residue compression part . Each IC is arranged in the form of matrix and then compressed with the algorithm of set partitioning in hierarchical trees . The residue signals are compressed in the same way as ICs but with a higher compression ratio . The appropriate combination of compression ratios of the ICs and the residue is explored to achieve desired performance . The compression scheme is tested with eight datasets sampled at two different frequencies . The experimental results demonstrate the high compression performance of the proposed approach and its potential usage in the EEG related telemedicine applications . \\n'],\n",
              " [' Features greatly influence the results of speech emotion recognition among which Mel frequency Cepstral Coefficients is the most commonly used in speech emotion . However MFCC does not consider both the relationship among neighbor coefficients of Mel filters of a frame and the relationship among coefficients of Mel filters of neighbor frames which possibly leads to lose many useful features from spectrogram . This paper presents novel weighted spectral features based on Local Hu moments . The idea is motivated by that the energy on spectrogram would drastically vary with some emotion types such as angry and happy while it would slightly change with other emotion types such as sadness and fear . This phenomenon would affect the local energy distribution of spectrogram in both time axis and frequency axis of spectrogram . To describe local energy distribution of spectrogram Hu moments computed from local regions of spectrogram are used as Hu moments can evaluate the degree how the energy is concentrated to the center of energy gravity of local region of spectrogram and can significantly vary with the speech emotion types . The conducted experiments validate the proposed features in terms of the effectiveness of the speech emotion recognition . \\n'],\n",
              " [' The feature extraction of event related potentials is a significant prerequisite for many types of P300 BCIs . In this paper we proposed a multi ganglion artificial neural network based feature learning method to extract a deep feature structure of single trial multi channel ERP signals and improve classification accuracy . Five subjects took part in the Imitating Reading ERP experiments . We recorded the target electroencephalography samples and non target samples for each subjects . Then we applied ANNFL method to extract the feature vectors and classified them by using support vector machine . The ANNFL method outperforms the principal component analysis method and conventional three layer auto encoder and then leads to higher classification accuracies of five subjects BCI signals than using the single channel temporal features . ANNFL is an unsupervised feature learning method which can automatically learn feature vector from EEG data and provide more effective feature representation than PCA method and single channel temporal feature extraction method . \\n'],\n",
              " [' One of the most common signs of tiredness or fatigue is yawning . Naturally identification of fatigued individuals would be helped if yawning is detected . Existing techniques for yawn detection are centred on measuring the mouth opening . This approach however may fail if the mouth is occluded by the hand as it is frequently the case . The work presented in this paper focuses on a technique to detect yawning whilst also allowing for cases of occlusion . For measuring the mouth opening a new technique which applies adaptive colour region is introduced . For detecting yawning whilst the mouth is occluded local binary pattern features are used to also identify facial distortions during yawning . In this research the Strathclyde Facial Fatigue database which contains genuine video footage of fatigued individuals is used for training testing and evaluation of the system . \\n'],\n",
              " [' Feature extraction and automatic classification of mental states is an interesting and open area of research in the field of brain computer interfacing . A well trained classifier would allow the BCI system to control an external assistive device in real world problems . Sometimes standard existing classifiers fail to generalize the components of a non stationary signal like Electroencephalography which may pose one or more problems during real time usage of the BCI system . In this paper we aim to tackle this issue by designing an interval type 2 fuzzy classifier which deals with the uncertainties of the EEG signal over various sessions . Our designed classifier is used to decode various movements concerning the wrist and finger . For this purpose we have employed extreme energy ratio to construct the feature vector . The average classification accuracy achieved during offline training and online testing over eight subjects are 86.45 and 78.44 respectively . On comparison with other related works it is shown that our designed IT2FS classifier presents a better performance . \\n'],\n",
              " [' The signal preprocessing is prerequisite for reduction of noise and for better estimation of sources from the measured field distribution of multichannel data since different measurement channels may be contaminated by different types of artifacts and noise . Toward this we use a combination of independent component analysis and ensemble empirical mode decomposition to denoise the multichannel magnetocardiography data . In this technique MCG time series data is first subjected to ICA to obtain the statistically independent components and subsequently the EEMD interval threshold based denoising is applied to the ICs prior to the reconstruction of the signal . We compare the results obtained from EEMD ICA with those obtained using the conventional ICA alone and also using the wavelet enhanced ICA . We illustrate the effect of these denoising techniques on the pseudo current density maps which aid in visualizing the source location . The results obtained from the EEMD ICA are seen to be decidedly superior compared to those obtained by ICA alone and wICA methods . \\n'],\n",
              " [' A novel feedback controlled hydrodynamic human circulatory system simulator well suited for in vitro validation of cardiac assist devices is presented in this paper . The cardiovascular system simulator consists of high bandwidth actuators allowing a high precision hardware in the loop hydrodynamic interface in connection with physiological circulatory models calculated in real time . The hydrodynamically coupled process dynamics consist of several actuator loops and demand a multivariable control design approach in the face of system nonlinearities and uncertainties . Based on a detailed model employing the Lagrange formalism a robust decentralised controller is designed . Fixed structural constraints and the minimisation of the norm necessitate the application of nonsmooth optimisation techniques . The robust decentralised norm optimal controller is tested in extensive in vitro experiments and shows good performance with regard to reference tracking and system coupling . In vitro experiments include multivariable reference step tests and frequency analysis tests of the vascular impedance transfer function . \\n'],\n",
              " [' The response of the brain to a sensory stimulus may present itself in the electroencephalogram as evoked and or induced activity . While the evoked response is given by peaks and troughs in the signal time locked and phase locked to the stimuli the induced response is time but not phase locked and can be considered as an increase or a decrease in the power of EEG in a specific frequency band at a specific time range with regard to the stimulus onset . The induced response does not have the same phase following successive stimuli . It is believed that cognition and perception of a stimulus present themselves primarily as the induced response in the EEG . In this paper the induced response of the brain to auditory speech stimuli is investigated and different approaches to detect induced activity are compared . It is shown that there is an increase in theta and delta power in response to words compared to the baseline starting around 500ms after their onset . During this time there is also an increase in pairwise coherence between the posterior electrodes . In response to tone bursts a change in pairwise coherence was observed in the beta band starting around 200ms . To the best of our knowledge this is the first time such responses have been described using simple protocols without complex stimulus manipulations being involved . Responses in the EEG to speech rather than the more conventional tone bursts or clicks suggests that it may be feasible to use the EEG as an objective means to demonstrate brain activation to salient real world stimuli . This would be of particular benefit in investigating access to speech in patients who are unable or unwilling to reliably respond to conventional subjective experimental protocols such as infants . \\n'],\n",
              " [' Bipolar disorders are characterized by a mood swing ranging from mania to depression . A system that could monitor and eventually predict these changes would be useful to improve therapy and avoid dangerous events . Speech might convey relevant information about subjects mood and there is a growing interest to study its changes in presence of mood disorders . In this work we present an automatic method to characterize fundamental frequency dynamics in voiced part of syllables . The method performs a segmentation of voiced sounds from running speech samples and estimates two categories of features . The first category is borrowed from Taylor s Tilt intonational model . However the meaning of the proposed features is different from the meaning of Taylor s ones since the former are estimated from all voiced segments without performing any analysis of intonation . A second category of features takes into account the speed of change of F0 . In this work the proposed features are first estimated from an emotional speech database . Then an analysis on speech samples acquired from eleven psychiatric patients experiencing different mood states and eighteen healthy control subjects is introduced . Subjects had to perform a text reading task and a picture commenting task . The results of the analysis on the emotional speech database indicate that the proposed features can discriminate between high and low arousal emotions . This was verified both at single subject and group level . An intra subject analysis was performed on bipolar patients and it highlighted significant changes of the features with different mood states although this was not observed for all the subjects . The directions of the changes estimated for different patients experiencing the same mood swing were not coherent and were task dependent . Interestingly a single subject analysis performed on healthy controls and on bipolar patients recorded twice with the same mood label resulted in a very small number of significant differences . In particular a very good specificity was highlighted for the Taylor inspired features and for a subset of the second category of features thus strengthening the significance of the results obtained with patients . Even if the number of enrolled patients is small this work suggests that the proposed features might give a relevant contribution to the demanding research field of speech based mood classifiers . Moreover the results here presented indicate that a model of speech changes in bipolar patients might be subject specific and that a richer characterization of subject status could be necessary to explain the observed variability . \\n'],\n",
              " [' Muscle fiber conduction velocity can be measured by estimating the time delay between surface EMG signals recorded by electrodes aligned with the fiber direction . In the case of dynamic contractions the EMG signal is highly non stationary and the time delay between recording sites may vary rapidly over time . Thus the processing methods usually applied in the case of static contractions do not hold anymore and the delay estimation requires processing techniques that are adapted to non stationary conditions . The current paper investigates several methods based on time frequency approaches or adaptive filtering in order to solve the time varying delay estimation problem . These approaches are theoretically analyzed and compared by Monte Carlo simulations in order to determine if their performance is sufficient for practical applications . Moreover results obtained on experimental signals recorded during cycling from the vastus medialis muscle are also shown . The study presents for the first time a set of approaches for instantaneous delay estimation from two channels EMG signals . \\n'],\n",
              " [' Fatty liver or steatosis is a pathology characterized by fat accumulation in the liver cells . Ultrasound is the most common technique used for its evaluation however the diagnosis is strongly dependent on the physician s expertise and system settings . These drawbacks have motivated the development of procedures for the quantitative analysis of ultrasound images to help the steatosis diagnosis . In this work three approaches are presented and tested with human liver images . The first one addresses textural analysis of the hepatic parenchyma using five classifiers 357 features a feature selector and classifiers fusion . Its performance is measured by two parameters accuracy and area under the ROC curve . The second makes use of the hepatorenal coefficient followed by a statistical analysis to discriminate echogenicity differences between liver and kidney . The third is based on the acoustical attenuation coefficient evaluated over a line traced in the images with parallel orientation to the acoustical beam . The use of classifiers fusion has provided better results when compared with the performance of the best one considered alone . The hepatorenal coefficient proved to be a good parameter for steatosis detection with calculated sensitivity and specificity of 0.90 and 0.88 respectively . It was observed the hepatorenal coefficient is not influenced by the ultrasound machine parameters . The attenuation coefficient provided lower sensitivity and specificity values than the ones from the hepatorenal coefficient . \\n'],\n",
              " [' When multiple acquisition systems are used to simultaneously acquire signals synchronization issues may arise potentially causing errors in the determination of acquisition starting points and continuous clock offsets and shifts on each device . This paper introduces a processing method to efficiently synchronize these signals in the presence of white noise sources without the requirement of clock sharing or any other digital line exchange . The use of a signal source such as white noise with a very wide frequency band is of great interest for synchronization purposes due to its aperiodic nature . This high bandwidth signal is simultaneously acquired by all the acquisition channels on distinct systems and synchronized afterwards using cross correlation methods . Two different correlation methods were tested a global method used when clock system frequencies are exactly known and a local method used when independent clocks evidence shifts over time that cumulatively account for long term acquisition errors in the synchronization process . In a computational simulation with known clock frequencies the results show a synchronization error of 1 10 of the time resolution for both methods . For unknown clock frequencies the global method achieved an error of 24 10 the time resolution indicating a much poorer performance . In the experimental set up only the local method was tested . The best result shows a synchronization error of 4 10 of the time resolution . All the signal conditioning and acquisition parameters were chosen taking into account potential biomedical applications . \\n'],\n",
              " [' This review discusses the critical issues and recommended practices from the perspective of myoelectric interfaces . The major benefits and challenges of myoelectric interfaces are evaluated . The article aims to fill gaps left by previous reviews and identify avenues for future research . Recommendations are given for example for electrode placement sampling rate segmentation and classifiers . Four groups of applications where myoelectric interfaces have been adopted are identified assistive technology rehabilitation technology input devices and silent speech interfaces . The state of the art applications in each of these groups are presented . \\n'],\n",
              " [' A minimal recruitment model can be used to guide mechanical ventilation PEEP selection for acute respiratory distress syndrome patients . However implementation of this model requires a specific clinical protocol and is computationally expensive and thus not suitable for bedside application . This work aims to improve the performance and bedside utility of the minimal recruitment model through simplifying the model and improving the identification algorithm without compromising the model s physiological relevance to the disease . Identifying the model requires fitting of 8 unique parameters to pressure volume data at multiple levels of positive end expiratory pressure . A minimal algorithm is proposed to improve the model s performance . The algorithm utilises a non linear least squares solver to estimate a global set of the parameters to a pressure volume curve at one PEEP level . These global parameters were then subsequently used to estimate other parameters at different pressure volume curves . The accuracy and computational performance of the minimal algorithm is compared to the grid search algorithm for 2 ARDS patient cohorts . The median absolute percentage curve fitting error over all patients for grid search algorithm is 1.40 and for the minimal algorithm is 2.43 . The median computational time for all patients for the grid search algorithm is 394.51s and for the minimal algorithm is 0.72s where a 600 of reduction of computational time was found for the minimal algorithm . The estimated parameters using the minimal algorithm are correlated with the grid search algorithm with median person s correlation of R 0.9 . The model fitting error for the minimal algorithm is higher than the grid search algorithm . However the model was able capture similar trends in physiologically relevant parameters without the loss of important clinical information . The minimal algorithm is less computationally intensive than the grid search algorithm whilst still providing a means of selecting PEEP with only a small increase in model fitting error . The minimal algorithm is able to improve computational performance while maintaining the ability to capture physiological parameters as the grid search algorithm . The significant reduction in computational time encourages its clinical application at the bedside for decision making . \\n'],\n",
              " [' Multiple watermarking is used to share the copyright of multiple users increase robustness and high security . The proposed method is comparison of multiple watermarking techniques based on Discrete Wavelet Transform and Singular Value Decomposition using Genetic algorithm . This research elaborates the three main categories of multiple watermarking techniques such as successive segmented and composite watermarking . The experimental results show that the DWT based watermarking algorithms possess multi resolution description characteristics achieving imperceptibility . The SVD based watermarking algorithms add the watermark information to the singular values of the diagonal matrix achieving robustness requirements . The optimization is to maximize the performance of peak signal to noise ratio and normalized correlation in multiple watermarking techniques using genetic algorithms . \\n'],\n",
              " [' There are two versions in the literature of counting co author pairs . Whereas the first version leads to a two dimensional power function distribution the other version shows three dimensional graphs totally rotatable around and their shapes are visible in space from all possible points of view . As a result these new 3 D computer graphs called Social Gestalts deliver more comprehensive information about social network structures than simple 2 D power function distributions . The mathematical model of Social Gestalts and the corresponding methods for the 3 D visualization and animation of collaboration networks are presented in Part I of this paper . Fundamental findings in psychology sociology and physics are used as a basis for the development of this model . The application of these new methods to male and to female networks is shown in Part II . After regression analysis the visualized Social Gestalts are rather identical with the corresponding empirical distributions . The structures of female co authorship networks differ markedly from the structures of the male co authorship networks . For female co author pairs networks accentuation of productivity dissimilarities of the pairs is becoming visible but on the contrary for male co author pairs networks accentuation of productivity similarities of the pairs is expressed . \\n'],\n",
              " ['3D modeling in rock art studies involves different techniques according to the size and morphology of the subject. It has mainly been used for reconstructing the volume of caves morphology of walls and as a substitute to graphic and photographic recording of the prehistoric pictures. Little work has been done at macroscopic and microscopic scale partly because lasergrammetry which is the most common technique is poorly adequate under the centimetric scale while for patrimonial purposes recording at high resolution was of little interest. Thanks to the increasing performance of personal computers new modeling techniques are becoming available based on photographic recording and no longer depending on costly and cumbersome equipments. We have tested in open air and underground sites in France Portugal and Russia the potential of photogrammetry and focus stacking for 3D recording of millimetric and submillimetric details of prehistoric petroglyphs and paintings along with original simple optical solutions. \\n'],\n",
              " [' Recent psychoacoustic studies found that across band envelope correlation carried important information for speech intelligibility . This motivated the present work to propose an ABEC based intelligibility measure that could be used to non intrusively predict speech intelligibility in noise using only temporal envelope waveforms extracted from the noise corrupted speech . The proposed ABEC based metric was computed by averaging the correlation coefficients of mean removed envelope waveforms from adjacent frequency bands of the noise corrupted speech . The ABECm measures were evaluated with intelligibility scores obtained from normal hearing listeners presented with sentences corrupted by four types of maskers in a total of 22 conditions . High correlation was obtained between ABECm values and listeners sentence recognition scores and this correlation was comparable to those computed with existing intrusive and non intrusive intelligibility measures . This suggests that across band envelope correlation may work as a simple but efficient predictor of speech intelligibility in noise whose computation does not need access to the clean reference signal . \\n'],\n",
              " ['A designer is mainly supported by two essential factors in design decisions. These two factors are intelligence and experience aiding the designer by predicting the interconnection between the required design parameters. Through classification of product data and similarity recognition between new and existing designs it is partially possible to replace the required experience for an inexperienced designer. Given this context the current paper addresses a framework for recognition and flexible retrieval of similar models in product design. The idea is to establish an infrastructure for transferring design as well as the required PLM Product Lifecycle Management know how to the design phase of product development in order to reduce the design time. Furthermore such a method can be applied as a brainstorming method for a new and creative product development as well. The proposed framework has been tested and benchmarked while showing promising results. \\n'],\n",
              " [' To injection mould polymers designing mould is a key task involving several critical decisions with direct implications to yield quality productivity and frugality . One prominent decision among them is specifying sprue bush conduit expansion as it significantly influences overall injection moulding abstruseness anguish in its design criteria deceives direct determination . Intuitively designers decide it wisely and then exasperate by optimising or manipulating processing parameters . To overwhelm that anomaly this research aims at proposing an ideal design criteria holistically for all polymeric materials also tend as a functional assessment metric towards perfection i.e . criteria to specify sprue conduit size before mould development . Accordingly a priori analytical criterion was deduced quantitatively as expansion ratio from ubiquitous empirical relationships specifically a.k.a an exclusive expansion angle imperatively configured for injectant properties . Its computational intelligence advantage was leveraged to augment functionality of perfectly injecting into an impression gap while synchronising both injector capacity and desired moulding features . For comprehensiveness it was continuously sensitised over infinite scale as an explicit factor dependent on in situ spatio temporal injectant state perplexity with discrete slope and altitude for each polymeric character . In which congregant ranges of apparent viscosity and shear thinning index were conceived to characteristically assort most thermoplastics . Thereon results accorded aggressive conduit expansion widening for viscous incrust while a very aggressive narrowing for shear thinning encrust among them apparent viscosity had relative dominance . This important rationale would certainly form a priori design basis as well diagnose filling issues causing several defects . Like this the proposed generic design criteria being simple would immensely benefit mould designers besides serve as an inexpensive preventive clich to moulders . Its adaption ease to practice manifests a hope of injection moulding extremely alluring polymers . Therefore we concluded that appreciating injectant s polymeric character to design exclusive sprue bush offers a definite a priori advantage . \\n'],\n",
              " [' We study the benefits and limits of parallelised Markov chain Monte Carlo sampling in cosmology . MCMC methods are widely used for the estimation of cosmological parameters from a given set of observations and are typically based on the Metropolis Hastings algorithm . Some of the required calculations can however be computationally intensive meaning that a single long chain can take several hours or days to calculate . In practice this can be limiting since the MCMC process needs to be performed many times to test the impact of possible systematics and to understand the robustness of the measurements being made . To achieve greater speed through parallelisation MCMC algorithms need to have short autocorrelation times and minimal overheads caused by tuning and burn in . The resulting scalability is hence influenced by two factors the MCMC overheads and the parallelisation costs . In order to efficiently distribute the MCMC sampling over thousands of cores on modern cloud computing infrastructure we developed a Python framework called CosmoHammer which embeds emcee an implementation by Foreman Mackey et al . of the affine invariant ensemble sampler by Goodman and Weare . We test the performance of CosmoHammer for cosmological parameter estimation from cosmic microwave background data . While Metropolis Hastings is dominated by overheads CosmoHammer is able to accelerate the sampling process from a wall time of 30 h on a dual core notebook to 16 min by scaling out to 2048 cores . Such short wall times for complex datasets open possibilities for extensive model testing and control of systematics . \\n'],\n",
              " [' Internet interventions constitute a promising and cost effective treatment alternative for a wide range of psychiatric disorders and somatic conditions . Several clinical trials have provided evidence for its efficacy and effectiveness and recent research also indicate that it can be helpful in the treatment of conditions that are debilitating but do not necessarily warrant more immediate care for instance procrastination a self regulatory failure that is associated with decreased well being and mental health . However providing treatment interventions for procrastination via the Internet is a novel approach making it unclear how the participants themselves perceive their experiences . The current study thus investigated participants own apprehension of undergoing Internet based cognitive behavior therapy for procrastination by distributing open ended questions at the post treatment assessment for instance What did you think about the readability of the texts How valuable do you believe that this treatment has been for you and The thing that I am most displeased with is . In total 75 participants responded and the material was examined using thematic analysis . The results indicate that there exist both positive and negative aspects of the treatment program . Many participants increased their self efficacy and were able to gain momentum on many tasks and assignments that had been deferred in their everyday life . Meanwhile several participants lacked motivation to complete the exercises had too many conflicting commitments and were unable to keep up with the tight treatment schedule . Hence the results suggest that Internet interventions for procrastination could profit from individual tailoring shorter and more manageable modules and that the content need to be adapted to the reading comprehension and motivational level of the participant . \\n'],\n",
              " ['Normally all manufacturing and fabrication processes introduce residual stresses in a component. These stresses exist even after all service or external loads have been removed. Residual stresses have been studied elaborately in the past and even in depth research have been done to determine their magnitude and distribution during different manufacturing processes. But very few works have dealt with the study of residual stresses formation during the casting process. Even though these stresses are less in magnitude they still result in crack formation and subsequent failure in later phases of the component usage. In this work the residual stresses developed in a shifter during casting process are first determined by finite element analysis using ANSYS Mechanical APDL Release 12.0 software. Initially the analysis was done on a simple block to determine the optimum element size and boundary conditions. With these values the actual shifter component was analyzed. All these simulations are done in an uncoupled thermal and structural environment. The results showed the areas of maximum residual stress. This was followed by the geometrical optimization of the cast part for minimum residual stresses. The resulting shape gave lesser and more evenly distributed residual stresses. Crack compliance method was used to experimentally determine the residual stresses in the modified cast part. The results obtained from the measurements are verified by finite element analysis findings. \\n'],\n",
              " ['In this paper we present an efficient technique for sketch based 3D modeling using automatically extracted image features. Creating a 3D model often requires a drawing of irregular shapes composed of curved lines as a starting point but it is difficult to hand draw such lines without introducing awkward bumps and edges along the lines. We propose an automatic alignment of a user s hand drawn sketch lines to the contour lines of an image facilitating a considerable level of ease with which the user can carelessly continue sketching while the system intelligently snaps the sketch lines to a background image contour no longer requiring the strenuous effort and stress of trying to make a perfect line during the modeling task. This interactive technique seamlessly combines the efficiency and perception of the human user with the accuracy of computational power applied to the domain of 3D modeling where the utmost precision of on screen drawing has been one of the hurdles of the task hitherto considered a job requiring a highly skilled and careful manipulation by the user. We provide several examples to demonstrate the accuracy and efficiency of the method with which complex shapes were achieved easily and quickly in the interactive outline drawing task. \\n'],\n",
              " [' A classical result in algebraic specification states that a total function defined on an initial algebra is a homomorphism if and only if the kernel of that function is a congruence . We expand on the discussion of that result from an earlier paper extending it from total to partial functions simplifying the proofs using relational calculus and generalising the setting to regular categories . \\n'],\n",
              " [' Sustainable online peer to peer support groups require engaged members . A metric commonly used to identify these members is the number of posts they have made . The 90 9 1 principle has been proposed as a rule of thumb for classifying members using this metric with a recent study demonstrating the applicability of the principal to digital health social networks . Using data from a depression Internet support group the current study sought to replicate this finding and to investigate in more detail the model of best fit for classifying participant contributions . Our findings replicate previous results and also find the fit of a power curve to account for 98.6 of the variance . The Zipf distribution provides a more nuanced image of the data and may have practical application in assessing the coherence of the sample . \\n'],\n",
              " [' Permutation entropy is a well known and fast method extensively used in many physiological signal processing applications to measure the irregularity of time series . Multiscale PE is based on assessing the PE for a number of coarse grained sequences representing temporal scales . However the stability of the conventional MPE may be compromised for short time series . Here we propose an improved MPE to reduce the variability of entropy measures over long temporal scales leading to more reliable and stable results . We gain insight into the dependency of MPE and IMPE on several straightforward signal processing concepts which appear in biomedical activity via a set of synthetic signals . We also apply these techniques to real biomedical signals via publicly available electroencephalogram recordings acquired with eyes open and closed and to ictal and non ictal intracranial EEGs . We conclude that IMPE improves the reliability of the entropy estimations in comparison with the traditional MPE and that it is a promising technique to characterize physiological changes affecting several temporal scales . We provide the source codes of IMPE and the synthetic data in the public domain . \\n'],\n",
              " [' Contemporary rock art researchers have a wide choice of 3D laser scanners available to them for recording stone surfaces and this is complimented by numerous software packages that are able to process point cloud data . Though ESRI s ArcGIS primarily handles geographical data it also offers the ability to visualise XYZ data from a stone surface . In this article the potential of ArcGIS for rock art research is explored by focusing on 3D data obtained from two panels of cup and ring marks found at Loups s Hill County Durham England . A selection of methods commonly utilised in LiDAR studies which enhance the identification of landscape features are also conducted upon the rock panels including DSM normalisation and raster data Principle Component Analysis . Collectively the visualisations produced from these techniques facilitate the identification of the rock art motifs but there are limitations to these enhancements that are also discussed . \\n'],\n",
              " [' Psychotherapy process research examines the content of treatment sessions and their association with outcomes in an attempt to better understand the interactions between therapists and clients and to elucidate mechanisms of behavior change . A similar approach is possible in technology delivered interventions which have an interaction process that is always perfectly preserved and rigorously definable . The present study sought to examine the process of participants interactions with a computer delivered brief intervention for drug use from a study comparing computer and therapist delivered brief interventions among adults at two primary health care centers in New Mexico . Specifically we sought to describe the pattern of participants choices and reactions throughout the computer delivered brief intervention and to examine associations between that process and intervention response at 3 month follow up . Participants were most likely to choose marijuana as the first substance they wished to discuss . Most participants indicated that they had not experienced any problems as a result of their drug use but nearly a third of these nevertheless indicated a desire to stop or reduce its use participants who did report negative consequences were most likely to endorse financial or relationship concerns . However participant ratings of the importance of change or of the helpfulness of personalized normed feedback were unrelated to changes in substance use frequency . Design of future e interventions should consider emphasizing possible benefits of quitting rather than the negative consequences of drug use and when addressing consequences should consider focusing on the impacts of substance use on relationship and financial aspects . These findings are an early but important step toward using process evaluation to optimize e intervention content . \\n'],\n",
              " [' Growth models have the capability of studying change at the group as well as the individual level . In addition these methods have documented advantages over traditional data analytic approaches in the analysis of repeated measures data . These advantages include but are not limited to the ability to incorporate time varying predictors handle dependence among repeated observations in a very flexible manner and to provide accurate estimates with missing data under fairly unrestrictive missing data assumptions . The flexibility of the growth curve modeling approach to the analysis of change makes it the preferred choice in the evaluation of direct indirect and moderated intervention effects . Although offering many benefits growth models present challenges in terms of design analysis and reporting of results . This paper provides a nontechnical overview of growth models in the analysis of change in randomized experiments and advocates for their use in the field of internet interventions . Practical recommendations for design analysis and reporting of results from growth models are provided . \\n'],\n",
              " [' The aim of this pilot study was to explore the effects of an early and customized CBT intervention mainly delivered via internet for adolescents with coexisting recurrent pain and emotional distress . The intervention was based on a transdiagnostic approach to concurrently target pain and emotional distress . A single case experimental design was employed with six participants 17 21years old who were recruited via school health care professionals at the student health care team at an upper secondary school in a small town in Sweden . The intervention consisted of 5 9 modules of CBT delivered via internet in combination with personal contacts and face to face sessions . The content and length of the program was customized depending on needs . The effects of the program were evaluated based on self report inventories which the participants filled out before and after the intervention and at a six month follow up . They did also fill out a diary where they rated symptoms on a daily basis . The results were promising at least when considering changes during the intervention as well as pre and posttest ratings . However the results were more modest when calculating the reliable change index and most of the treatment effects were not sustained at the follow up assessment which raises questions about the durability of the effects . Taken together this study indicates that this type of program is promising as an early intervention for adolescents with pain and concurrent emotional distress although the outcomes need to be explored further especially in terms of long term effects . \\n'],\n",
              " [' We present the project Asteroids @ home that uses distributed computing to solve the time consuming inverse problem of shape reconstruction of asteroids . The project uses the Berkeley Open Infrastructure for Network Computing framework to distribute collect and validate small computational units that are solved independently at individual computers of volunteers connected to the project . Shapes rotational periods and orientations of the spin axes of asteroids are reconstructed from their disk integrated photometry by the lightcurve inversion method . \\n'],\n",
              " [' The aims of the current study were to 1 establish the efficacy of two Internet based prevention programmes to reduce anxiety and depressive symptoms in adolescents and 2 investigate the distribution of psychological symptoms in a large sample of Australian adolescents prior to the implementation of the intervention . A cluster randomised controlled trial was conducted with 976 Year 9 10 students from twelve Australian secondary schools in 2009 . Four schools were randomly allocated to the Anxiety Internet based prevention programme five schools to the Depression Internet based prevention programme and three to their usual health classes . The Thiswayup Schools for Anxiety and Depression prevention courses were presented over the Internet and consist of 6 7 evidence based curriculum consistent lessons to improve the ability to manage anxiety and depressive symptoms . Participants were assessed at baseline and post intervention . Data analysis was constrained by both study attrition and data corruption . Thus post intervention data were only available for 265 976 students . Compared to the control group students in the depression intervention group showed a significant improvement in anxiety and depressive symptoms at the end of the course whilst students in the anxiety intervention demonstrated a reduction in symptoms of anxiety . No significant differences were found in psychological distress . The Thiswayup Schools Depression and Anxiety interventions appear to reduce anxiety and depressive symptoms in adolescents using a curriculum based blended online and offline cognitive behavioural therapy programme that was implemented by classroom teachers . Given the study limitations particularly the loss of post intervention data these findings can only be considered preliminary and need to be replicated in future research . \\n'],\n",
              " [' In this study a new methodology in predicting a system output has been investigated by applying a data mining technique and a hybrid type II fuzzy system in CNC turning operations . The purpose was to generate a supplemental control function under the dynamic machining environment where unforeseeable changes may occur frequently . Two different types of membership functions were developed for the fuzzy logic systems and also by combining the two types a hybrid system was generated . Genetic algorithm was used for fuzzy adaptation in the control system . Fuzzy rules are automatically modified in the process of genetic algorithm training . The computational results showed that the hybrid system with a genetic adaptation generated a far better accuracy . The hybrid fuzzy system with genetic algorithm training demonstrated more effective prediction capability and a strong potential for the implementation into existing control functions . \\n'],\n",
              " [' There is evidence from randomized control trials that internet based cognitive behavioral therapy is efficacious in the treatment of anxiety and depression and recent research demonstrates the effectiveness of iCBT in routine clinical care . The aims of this study were to implement and evaluate a new pathway by which patients could access online treatment by completing an automated assessment rather than seeing a specialist health professional . We compared iCBT treatment outcomes in patients who received an automated pre treatment questionnaire assessment with patients who were assessed by a specialist psychiatrist prior to treatment . Participants were treated as part of routine clinical care and were therefore not randomized . The results showed that symptoms of anxiety and depression decreased significantly with iCBT and that the mode of assessment did not affect outcome . That is a pre treatment assessment by a psychiatrist conferred no additional treatment benefits over an automated assessment . These findings suggest that iCBT is effective in routine care and may be implemented with an automated assessment . By providing wider access to evidence based interventions and reducing waiting times the use of iCBT within a stepped care model is a cost effective way to reduce the burden of disease caused by these common mental disorders . \\n'],\n",
              " [' Geriatric depression is a pathological process that causes a great number of symptoms resulting in limited mental and physical functionality . The computation of oscillatory and synchronization patterns in certain brain areas may facilitate the early and robust identification of depressive symptoms . In this study electroencephalographic data were recorded from 34 participants suffering from both cognitive impairment and geriatric depression and 32 control subjects . Both groups were matched according to their cognitive status . The study aims at evaluating neurophysiological features of elderly participants suffering from depression and neurodegeneration . The current work focuses on the identification of depression symptoms that coexist with cognitive decline the correlation of the examined neurophysiological features with geriatric depression combined with cognitive impairment and the investigation of the role of data mining techniques in the analysis of EEG data . The EEG features were estimated through synchronization analysis . Depressive patterns were detected through data mining techniques . Random Forest Random Tree Multilayer Perceptron and Support Vector Machines were employed for data classification . The efficiency of the classifiers varied from 92.42 to 95.45 . Random Forest demonstrated the highest accuracy . Both synchronization and oscillatory features contributed to the decision trees formation with the former prevailing . Moreover synchronization features significantly contributed to the decision trees formation . In line with previous neuroscientific findings synchronization among right frontal midline anteriofrontal regions showed great correlation with depressive symptoms . Evaluation of the classifiers indicated the Random Forest as being the most robust algorithm . Synchronization of certain brain regions is more indicative of identifying depression symptoms than oscillatory since synchronization features contributed the most to the formation of the classification trees . \\n'],\n",
              " [' Objectives The first aim of this study was to describe parental attitudes towards and intentions to access computer based therapies for youth mental health problems . The second aim was to assess parental factors predicting attitudes and intentions to access computer based therapies for youth . Method Three hundred and seventy three Australian parents completed an online survey measuring demographics mental health service experience personality technology factors mental health knowledge and attitudes perceived benefits problems and helpfulness of computer based therapies and intentions to access services . Results Approximately 50 of parents reported accessing support for their child s mental health yet only 6 had used a computer based therapy . The majority of parents strongly endorsed all benefits of computer based therapies and appeared relatively less concerned by potential service problems . Computer based therapies were perceived as somewhat to extremely helpful by 87 of parents and 94 indicated that they would utilise a computer based therapy if their child required support and one was offered to them . Parental knowledge of computer based therapies significantly predicted perceived helpfulness p .001 and intentions to access p .001 computer based therapies above that of parent demographic characteristics clinical factors and engagement with technology . Conclusions Australian parents hold positive attitudes to the use of computer based therapies . \\n'],\n",
              " [' In recent years the automotive industry has known a remarkable development in order to satisfy the customer requirements . In this paper we will study one of the components of the automotive which is the twist beam . The study is focused on the multicriteria design of the automotive twist beam undergoing linear elastic deformation . Indeed for the design of this automotive part there are some criteria to be considered as the rigidity and the resistance to fatigue . Those two criteria are known to be conflicting therefore our aim is to identify the Pareto front of this problem . To do this we used a Normal Boundary Intersection algorithm coupling with a radial basis function metamodel in order to reduce the high calculation time needed for solving the multicriteria design problem . Otherwise we used the free form deformation technique for the generation of the 3D shapes of the automotive part studied during the optimization process . \\n'],\n",
              " [' Many trials on Internet delivered psychological treatments have had problems with nonadherence but not much is known about the subjective reasons for non adhering . The aim of this study was to explore participants experiences of non adherence to Internet delivered psychological treatment . Grounded theory was used to analyze data from seven in depth interviews with persons who had non adhered to a study on Internet delivered cognitive behavioral therapy for generalized anxiety disorder . The process of non adherence is described as an interaction between patient factors and treatment factors . A working model theory was generated to illustrate the experience of nonadherence . The model describes a process where treatment features such as workload text content complexity and treatment process don t match personal prerequisites regarding daily routines perceived language skills and treatment expectations respectively resulting in the decision to nonadhere . Negative effects were also stated as a reason for non adherence . Several common strategies used for increasing adherence to Internet delivered therapy in general are by these non completers regarded as factors directly related to their reason for non adherence . \\n'],\n",
              " [' Mean field models of the mammalian cortex treat this part of the brain as a two dimensional excitable medium . The electrical potentials generated by the excitatory and inhibitory neuron populations are described by nonlinear coupled partial differential equations that are known to generate complicated spatio temporal behaviour . We focus on the model by Liley et al . 67 113 . Several reductions of this model have been studied in detail but a direct analysis of its spatio temporal dynamics has to the best of our knowledge never been attempted before . Here we describe the implementation of implicit time stepping of the model and the tangent linear model and solving for equilibria and time periodic solutions using the open source library PETSc . By using domain decomposition for parallelization and iterative solving of linear problems the code is capable of parsing some dynamics of a macroscopic slice of cortical tissue with a sub millimetre resolution . \\n'],\n",
              " [' The increasing number of astronomical surveys in mid and far infrared as well as in submillimetre and radio wavelengths brings more difficulties to the already challenging task of detecting sources in an automatic way . These specific images are characterized by a more complex background than in shorter wavelengths with a higher level of noise more noticeable flux variations and both unresolved and extended sources with a higher dynamic range . In order to improve the source detection efficiency in long wavelength images in this paper we present a new approach based on the combined use of a multiscale decomposition and a recently developed method called Distilled Sensing . Its application minimizes the impact of the contaminants from the background unveiling and highlighting the sources at the same time . The experimental results achieved using infrared and radio images illustrate the good performance of the approach identifying greater percentages of true sources than using both the widely used SExtractor algorithm and the Distilled Sensing method alone . \\n'],\n",
              " [' Several solutions have been proposed till date to achieve effective replication policies and admission control methods that can ensure a high Quality of Service to Video on Demand systems . In this paper we have chosen to study admission control and a replication strategy in a video on demand system called Feedback Control Architecture for Distributed Multimedia Systems . The proposed approaches are founded on the supervision and auto adaptation of the system load continued knowledge of the network and of the video servers workload and the development of a dynamic replication strategy of segments for popular video that remade to replica placement problem by discussing the problem of the replicas number to create . The workload of video servers and of network are calculated by a QoS controller . The proposed approach takes into account of replica detail and offers a replica removal policy to avoid waste and for a good management of storage space . The goal is to maintain an updated broad view of the system s behaviour from the collected measurements to databases of available videos and databases of available video servers . To demonstrate the efficiency and feasibility of our approaches and validate the results obtained we conducted a series of experiments based on a simulator that we implemented . \\n'],\n",
              " ...]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "text"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "imqqQOsDv8oN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "576a28a9-cb73-4ab3-e97c-e480eaa12687"
      },
      "id": "imqqQOsDv8oN",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.21.2-py3-none-any.whl (4.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.7 MB 8.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.9.1-py3-none-any.whl (120 kB)\n",
            "\u001b[K     |████████████████████████████████| 120 kB 64.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.12.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 41.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.9.1 tokenizers-0.12.1 transformers-4.21.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "import os"
      ],
      "metadata": {
        "id": "5aw2GggJwLOz"
      },
      "id": "5aw2GggJwLOz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
      ],
      "metadata": {
        "id": "y5bHIn0TwbCr"
      },
      "id": "y5bHIn0TwbCr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summarizer = pipeline(\"summarization\")"
      ],
      "metadata": {
        "id": "4vMWntyZwg7E",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212,
          "referenced_widgets": [
            "1e7a613c8710465399a901bfc12951cc",
            "0df08209f41141c9b1fce29a3f0608be",
            "c0f08f7f41944c7b84276a00cdbc9d18",
            "3e8a49e7a7a94e77a0af514073b6aadd",
            "3dcce6d11f554314bb727538ef44964d",
            "3eeb37d6865f46d2a1434e649dd2d133",
            "da2bd2c5298f4d3cbea42379f870336d",
            "805c41cb378649fb8bc0a4c04816336e",
            "61a20e5f9f684615a2cb43a2fd6ae199",
            "f20cdb5c35c54a61a2ad1dce2ee37848",
            "a711319a0983451ca7643b58d6524640",
            "2173d6e1edb64a648f7e9549709ded67",
            "b3baee21f115444fb3de11298b048156",
            "f79865069828487eb1e2e4db8918ffb7",
            "da0b03298bc44c22a28b9a94eeef5e27",
            "62aa418cc35a4454af73999602450d96",
            "8183a5cb2e9b4a30bc20d7beebb0c118",
            "9266fd9991284055841a6af2efc42d04",
            "35cf72af739f4d77951eebe1cff1f4cd",
            "1bce10dae6014704a5e0ae7c91c37985",
            "744a6d0533e54da1a4498cee2579b8df",
            "c22d81453a034b0b844ec41d7984edae",
            "bda20703fa3e45069d858fec53a7baac",
            "888b431d8ee4425087f66aaba84ec547",
            "e10e4a1892ed4b32883e80411e72ad15",
            "1c8e79bd12a44792a49ce956d54f9e7e",
            "424220a9927f4ed682b7ff0c93a999c7",
            "0abcafc9de8d4a6ab2e1b4413bfb8ffc",
            "5f21add9a63b4fd7b0233451d9862696",
            "69964ea9c23040ccb3fe057f6c1e4053",
            "dda9a3d52299409d95b80f6d7514b1f3",
            "2e41862ac45c4206a6276717efe7e334",
            "2169b54b8eaf466d9b15a6a8876ba485",
            "d4fe0f2908b041f3bd61dedb8f32806a",
            "280001d5a33f42f4ad9a174c2dca12f9",
            "32d65e67163b4f7594dd401ebb6c5aad",
            "75feaa5186684d618271988113b4b7c3",
            "896ed465d16a4575bff55e94bd70d20f",
            "b51e3c0822f54d37b0499f38d905a206",
            "e532048672ee4c638781a73834df7a6d",
            "a47b4ed055b2422f9fb00bfbb9f3f505",
            "7d03c58110c046049ac36507c66898b0",
            "5d7111662216473d8540f2a5d7806d46",
            "dd4d88dc5092474080a4ca34ae5aa434",
            "949830752c8846128b4188fe4da1197b",
            "28ac29efe361406fbee17c1015850264",
            "c9eccfd5a17c4a23b20472a539fac418",
            "6b80a316d862453ca3ee3a423d11ec9c",
            "b3fed85f60ee42e490fe39a6e07b1bab",
            "827be06e0b064b0ab7dea66653301abd",
            "27b6cdcc16334c5daec0ac4a1ee1e52a",
            "91bc5ca8582c42a7a53a3fca5734154e",
            "77a134ceb23b47e89c74269f9e10341c",
            "cb45d0695a9c4688b76aaa860d51d408",
            "24169045690e417c9465cea23fb04dd6"
          ]
        },
        "outputId": "612dd8f7-4700-4b83-feed-cb2044b2b4e1"
      },
      "id": "4vMWntyZwg7E",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading config.json:   0%|          | 0.00/1.76k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1e7a613c8710465399a901bfc12951cc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading pytorch_model.bin:   0%|          | 0.00/1.14G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2173d6e1edb64a648f7e9549709ded67"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bda20703fa3e45069d858fec53a7baac"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading vocab.json:   0%|          | 0.00/878k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d4fe0f2908b041f3bd61dedb8f32806a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading merges.txt:   0%|          | 0.00/446k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "949830752c8846128b4188fe4da1197b"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "summarizer = pipeline(\"summarization\", model=\"t5-base\", tokenizer=\"t5-base\", framework=\"tf\")"
      ],
      "metadata": {
        "id": "OB7-2NJqw71o",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339,
          "referenced_widgets": [
            "9f7172d72a1049b183f3ab67c5eeaf75",
            "8d920cdc9f294c45a0c51ce8e496b45b",
            "c8dedc59aa994e33a716870c0942d5d7",
            "b1cb33778ae14b18a024d2501d7fe97b",
            "3f8d8e3c5b8b4bc6a27d1612681b5def",
            "5624bf8982674c439d940f8c5cef0d4a",
            "adecf808224944198a1ee0ea56ca55cb",
            "bc77b655dc134ac19297c01ac2584990",
            "ced33ad39c8b44a796af95b85811d52b",
            "d34b435ca05d421d8f253e1196c4c806",
            "75350df6ede74320ac8f1a0040ee4306",
            "1b5d8e0af6204ddbb84bc5981bc9a107",
            "5930fc681e0a4b2185327888e0ac6643",
            "761ea4e618914de385781cf8d534b6ea",
            "3916a333e4d14041bb55ae94e10bced6",
            "d20f74f60fe54b45a9db1078cb3f0d95",
            "06d450e226974ef18a74fd79181ace6c",
            "3ea1b6bf4dd24bc3808ff70c823d9004",
            "25a518e683974715bb56dccbab4c226a",
            "4771e33872094547b385f1d32b3ac2ec",
            "3f83c2b3b8d94ff98db20fce1031a425",
            "6703952f8edc47e5a83de901c357fbef",
            "daa55cc26fcc415ab54ba5a86b1ef848",
            "2416c0e5591a4ee29ab46e708b003dbd",
            "3924a3c6e80349308510e5ae91a89c60",
            "e13752b7beeb403c9073faf1fdc9f7b2",
            "ff0836c31f9f44dba10f23a8ce68b634",
            "4afaf9ff404d4482ba55cef5211fe0a3",
            "23ebc32d938e47f38284a8882c36590f",
            "ff3da6b3eaed42a1a3700734ebeb6bed",
            "1155f945fb7f433a82ec074a803fe030",
            "19aadf810c6a4018825806f2a8517326",
            "d1418033f6e74fb5bd6d4d459f416133",
            "6a5499de06a24397b029f88251d6ba7f",
            "e95a6a03a2e843fd996f22c26dea2323",
            "fefb64b77d6a49cb8db89be9a5fec440",
            "e2bbeb7879924d7bbdfb4d13ec463ebd",
            "dbff55e92edb4320bc25185ad990d234",
            "3d227249fd174e419db05655cc80a1a6",
            "0f88fb466e4a4a3da44f1c5e1ff1714e",
            "2f53e8a5ca0b4e6c9ccf737b43542f47",
            "a3ea20a734f941a2a59a6538e0bc4897",
            "aba64c855048455f99f0dec51c334370",
            "a8785c2ca66e47d187d034d8ae1bb738"
          ]
        },
        "outputId": "5c1d4e69-c648-4cdb-ff2a-2b73b58a521e"
      },
      "id": "OB7-2NJqw71o",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading config.json:   0%|          | 0.00/1.17k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9f7172d72a1049b183f3ab67c5eeaf75"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading tf_model.h5:   0%|          | 0.00/851M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1b5d8e0af6204ddbb84bc5981bc9a107"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFT5ForConditionalGeneration.\n",
            "\n",
            "All the layers of TFT5ForConditionalGeneration were initialized from the model checkpoint at t5-base.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading spiece.model:   0%|          | 0.00/773k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "daa55cc26fcc415ab54ba5a86b1ef848"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading tokenizer.json:   0%|          | 0.00/1.32M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6a5499de06a24397b029f88251d6ba7f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/models/t5/tokenization_t5_fast.py:166: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
            "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
            "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
            "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
            "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
            "  FutureWarning,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "result=[]\n",
        "for i in range(6):\n",
        "      summary_text = summarizer(text[i][0], max_length=120, min_length=30, do_sample=False)[0]['summary_text']\n",
        "      result.append(summary_text)\n",
        "      print(i)\n"
      ],
      "metadata": {
        "id": "sBVxcKH2xHTd",
        "outputId": "b213214d-b077-4f01-f120-a45f68bb7d9b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "sBVxcKH2xHTd",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rouge"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XWYMEjP56ZcF",
        "outputId": "c18b5d65-3127-4c59-a953-af70f739cb7c"
      },
      "id": "XWYMEjP56ZcF",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting rouge\n",
            "  Downloading rouge-1.0.1-py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from rouge) (1.15.0)\n",
            "Installing collected packages: rouge\n",
            "Successfully installed rouge-1.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from rouge import Rouge\n",
        "rouge = Rouge()\n",
        "for i in range(6):\n",
        " print(rouge.get_scores(rhs[i][0], result[i][0], avg=True))\n",
        "  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kb2N999U1beS",
        "outputId": "e9d0da13-4ccc-489d-965e-f3cacc62f7b2"
      },
      "id": "Kb2N999U1beS",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'rouge-1': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.0, 'p': 0.0, 'f': 0.0}}\n",
            "{'rouge-1': {'r': 1.0, 'p': 0.02127659574468085, 'f': 0.04166666625868056}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 1.0, 'p': 0.02127659574468085, 'f': 0.04166666625868056}}\n",
            "{'rouge-1': {'r': 1.0, 'p': 0.024390243902439025, 'f': 0.04761904715419502}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 1.0, 'p': 0.024390243902439025, 'f': 0.04761904715419502}}\n",
            "{'rouge-1': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.0, 'p': 0.0, 'f': 0.0}}\n",
            "{'rouge-1': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.0, 'p': 0.0, 'f': 0.0}}\n",
            "{'rouge-1': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.0, 'p': 0.0, 'f': 0.0}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df1= pd.DataFrame(list(zip(df['FileName'], df['Abstract'], result)),\n",
        "               columns =['FileName','Abstract','Predicted Summary'])"
      ],
      "metadata": {
        "id": "n5NtoLr4yqKG"
      },
      "id": "n5NtoLr4yqKG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1\n"
      ],
      "metadata": {
        "id": "_2Vyj0MUERFj"
      },
      "id": "_2Vyj0MUERFj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "df1.to_csv('output1.csv')\n",
        "\n",
        "files.download('output1.csv')"
      ],
      "metadata": {
        "id": "7jfIE01sErgV"
      },
      "id": "7jfIE01sErgV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['RHS'] = df['RHS'].apply(lambda x : 'sostok '+ x + ' eostok')\n"
      ],
      "metadata": {
        "id": "yoDNLAlpkxUB"
      },
      "id": "yoDNLAlpkxUB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_tr,x_val,y_tr,y_val=train_test_split(np.array(df['Abstract']),np.array(df['RHS']),test_size=0.2,random_state=4,shuffle=True)"
      ],
      "metadata": {
        "id": "2_VBhlTkJqOd"
      },
      "id": "2_VBhlTkJqOd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_tr"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SjxQHXu5NEWT",
        "outputId": "68279a4a-8440-4e54-a04b-dbb89ff30344"
      },
      "id": "SjxQHXu5NEWT",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([' This paper deals with Hamilton Jacobi Bellman equation based stabilized optimal control of hybrid dynamical systems . This paper presents the fuzzy clustering based event wise multiple linearized modeling approaches for HDS to describe the continuous dynamic in each event . In the present work a fuzzy clustering validation approach is presented for the selection of number of linearized models which span entire HDS . The method also describes how to obtain event wise operating point using fuzzy membership function which is used to find the event wise model bank by linearizing the first principles model . The event wise linearized models are used for the formulation of the optimal control law . The HJB equation is formulated using a suitable quadratic term in the objective function . By use of the direct method of Lyapunov stability the control law is shown to be optimal with respect to objective functional and stabilized the event wise linearized models . The global Lyapunov function is proposed with discrete variables which stabilized the HDS . The proposed modeling and control algorithm have been applied on two HDSs . Necessary theoretical and simulation experiments are presented to demonstrate the performance and validation of the proposed algorithm . \\n',\n",
              "       ' Mechanical stimuli play a significant role in the process of long bone development as evidenced by clinical observations and in vivo studies . Up to now approaches to understand stimuli characteristics have been limited to the first stages of epiphyseal development . Furthermore growth plate mechanical behavior has not been widely studied . In order to better understand mechanical influences on bone growth we used Carter and Wong biomechanical approximation to analyze growth plate mechanical behavior and explore stress patterns for different morphological stages of the growth plate . To the best of our knowledge this work is the first attempt to study stress distribution on growth plate during different possible stages of bone development from gestation to adolescence . Stress distribution analysis on the epiphysis and growth plate was performed using axisymmetric finite element analysis in a simplified generic epiphyseal geometry using a linear elastic model as the first approximation . We took into account different growth plate locations morphologies and widths as well as different epiphyseal developmental stages . We found stress distribution during bone development established osteogenic index patterns that seem to influence locally epiphyseal structures growth and coincide with growth plate histological arrangement . \\n',\n",
              "       ' The assessment of the state of the acrosome is a priority in artificial insemination centres since it is one of the main causes of function loss . In this work boar spermatozoa present in gray scale images acquired with a phase contrast microscope have been classified as acrosome intact or acrosome damaged after using fluorescent images for creating the ground truth . Based on shape prior criteria combined with Otsu s thresholding regional minima and watershed transform the spermatozoa heads were segmented and registered . One of the main novelties of this proposal is that unlike what previous works stated the obtained results show that the contour information of the spermatozoon head is important for improving description and classification . Other of this work novelties is that it confirms that combining different texture descriptors and contour descriptors yield the best classification rates for this problem up to date . The classification was performed with a Support Vector Machine backed by a Least Squares training algorithm and a linear kernel . Using the biggest acrosome intact damaged dataset ever created the early fusion approach followed provides a 0.9913 F Score outperforming all previous related works . \\n',\n",
              "       ...,\n",
              "       ' Considering the inherent connection between supplier selection and inventory management in supply chain networks this article presents a multi period inventory lot sizing model for a single product in a serial supply chain where raw materials are purchased from multiple suppliers at the first stage and external demand occurs at the last stage . The demand is known and may change from period to period . The stages of this production distribution serial structure correspond to inventory locations . The first two stages stand for storage areas for raw materials and finished products in a manufacturing facility and the remaining stages symbolize distribution centers or warehouses that take the product closer to customers . The problem is modeled as a time expanded transshipment network which is defined by the nodes and arcs that can be reached by feasible material flows . A mixed integer nonlinear programming model is developed to determine an optimal inventory policy that coordinates the transfer of materials between consecutive stages of the supply chain from period to period while properly placing purchasing orders to selected suppliers and satisfying customer demand on time . The proposed model minimizes the total variable cost including purchasing production inventory and transportation costs . The model can be linearized for certain types of cost structures . In addition two continuous and concave approximations of the transportation cost function are provided to simplify the model and reduce its computational time . \\n',\n",
              "       ' Background Structural changes of the brain s third ventricle have been acknowledged as an indicative measure of the brain atrophy progression in neurodegenerative and endocrinal diseases . To investigate the ventricular enlargement in relation to the atrophy of the surrounding structures shape analysis is a promising approach . However there are hurdles in modeling the third ventricle shape . First it has topological variations across individuals due to the inter thalamic adhesion . In addition as an interhemispheric structure it needs to be aligned to the midsagittal plane to assess its asymmetric and regional deformation . Method To address these issues we propose a model based shape assessment . Our template model of the third ventricle consists of a midplane and a symmetric mesh of generic shape . By mapping the template s midplane to the individuals brain midsagittal plane we align the symmetric mesh on the midline of the brain before quantifying the third ventricle shape . To build the vertex wise correspondence between the individual third ventricle and the template mesh we employ a minimal distortion surface deformation framework . In addition to account for topological variations we implement geometric constraints guiding the template mesh to have zero width where the inter thalamic adhesion passes through preventing vertices crossing between left and right walls of the third ventricle . The individual shapes are compared using a vertex wise deformity from the symmetric template . Results Experiments on imaging and demographic data from a study of aging showed that our model was sensitive in assessing morphological differences between individuals in relation to brain volume gender and the fluid intelligence at age 72 . It also revealed that the proposed method can detect the regional and asymmetrical deformation unlike the conventional measures volume and width of the third ventricle . Similarity measures between binary masks and the shape model showed that the latter reconstructed shape details with high accuracy . Conclusions We have demonstrated that our approach is suitable to morphometrical analyses of the third ventricle providing high accuracy and inter subject consistency in the shape quantification . This shape modeling method with geometric constraints based on anatomical landmarks could be extended to other brain structures which require a consistent measurement basis in the morphometry . \\n',\n",
              "       ' Sample variations are one of the main problems associated with speaker recognition . Most approaches use multiple templates in the gallery database . But this requires enormous memory space . In order to minimize classification errors and intra class variations adaptive online and offline template update methods using vector quantization and Gaussian mixture model are proposed . Online and offline feature update as well as model update techniques are considered here . Feature update utilizes the vector quantization approach while Gaussian mixture model approach is considered for model updating . The proposed methods automatically update the feature in accordance with the biometric sample variations over time and they continually adapt the templates based on semi supervised learning strategies . Experiments with 50 subjects reveal that the proposed template update strategies improve the recognition accuracy and reduce the classification errors for voice recognition systems even under sample variations . \\n'],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_tr"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dbdCYuWyNnOP",
        "outputId": "08e3e75d-ee13-4dfa-9754-eede60f5e7a7"
      },
      "id": "dbdCYuWyNnOP",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['sostok Hamilton Jacobi Bellman HJB equation based stabilized optimal control of hybrid dynamical systems HDS is presented. The fuzzy validity based method is used to find the number of linear models present in the HDS. Stability proof for the event wise and generalized HJB solution based optimal control is proposed. The proposed modeling and control algorithm have been applied on two HDSs. eostok',\n",
              "       'sostok Growth plate characteristics affect SOC development. Mechanical environment within growth plate varies according to epiphyseal ossification state. Mechanical stimuli may affect similarly growth plate and epiphyseal ossification. eostok',\n",
              "       'sostok A new early fusion approach for acrosome integrity classification is proposed. Specific segmentation based on shape priors was carried out. The biggest acrosome intact damaged dataset ever created was created and published. Acrosome contour is important for improving description and classification. The best result up to date combining shape and texture features has been obtained. eostok',\n",
              "       ...,\n",
              "       'sostok This article provides an inventory model for a serial supply chain with multiple suppliers and time varying demand. The model minimizes the total cost of purchasing production inventory setup and holding and transportation. Considering lead times necessary feasibility conditions regarding the demand at the last stage are established. Actual transportation costs are modeled by exact piecewise linear functions. Our results show that inventory setup and holding costs can affect supplier selection and order lot sizing decisions. eostok',\n",
              "       'sostok Present a model based approach to investigate the morphology of the third ventricle. Assess the regional deformations in relation to the atrophy of surrounding structures. Use a symmetric template model with the midplane definition for unbiased analysis. Achieve a robust surface modeling using a progressive surface deformation. Validate the method on a healthy aging sample with different clinical variables. eostok',\n",
              "       'sostok Created a new database for speaker template updating. MFCC super template for speaker recognition is proposed. Proposed an online and offline MFCC feature and GMM based model update. Secondary template for speaker template model update is also suggested. eostok'],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.text import Tokenizer \n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "#prepare a tokenizer for reviews on training data\n",
        "x_tokenizer = Tokenizer() \n",
        "x_tokenizer.fit_on_texts(list(x_tr))"
      ],
      "metadata": {
        "id": "OLXJKjo1N4bS"
      },
      "id": "OLXJKjo1N4bS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "thresh=3\n",
        "\n",
        "cnt=0\n",
        "tot_cnt=0\n",
        "freq=0\n",
        "tot_freq=0\n",
        "\n",
        "for key,value in x_tokenizer.word_counts.items():\n",
        "    tot_cnt=tot_cnt+1\n",
        "    tot_freq=tot_freq+value\n",
        "    if(value<thresh):\n",
        "        cnt=cnt+1\n",
        "        freq=freq+value\n",
        "    \n",
        "print(\"% of rare words in vocabulary:\",(cnt/tot_cnt)*100)\n",
        "print(\"Total Coverage of rare words:\",(freq/tot_freq)*100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iFi1im_WOAmN",
        "outputId": "95122608-7a16-4fe6-8a34-7aaecfdb3618"
      },
      "id": "iFi1im_WOAmN",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "% of rare words in vocabulary: 50.14319014319014\n",
            "Total Coverage of rare words: 3.293208560853649\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_summary_len=100"
      ],
      "metadata": {
        "id": "JLGWCY_zlmo-"
      },
      "id": "JLGWCY_zlmo-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_text_len=0\n",
        "length=[]\n",
        "for i in df['Abstract']:\n",
        "  length.append(len(i.split(' ')))\n",
        "  print(len(i.split(' ')))\n",
        "\n",
        "max_text_len=max(length)\n",
        "max_text_len"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dl6r7GuQPNio",
        "outputId": "6ed2b92f-a745-491d-e707-44e8c8f80839"
      },
      "id": "Dl6r7GuQPNio",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "157\n",
            "115\n",
            "231\n",
            "140\n",
            "167\n",
            "364\n",
            "278\n",
            "200\n",
            "158\n",
            "122\n",
            "331\n",
            "255\n",
            "126\n",
            "340\n",
            "311\n",
            "148\n",
            "298\n",
            "190\n",
            "211\n",
            "282\n",
            "208\n",
            "165\n",
            "279\n",
            "486\n",
            "193\n",
            "275\n",
            "166\n",
            "251\n",
            "102\n",
            "198\n",
            "132\n",
            "292\n",
            "196\n",
            "247\n",
            "229\n",
            "321\n",
            "141\n",
            "171\n",
            "89\n",
            "290\n",
            "243\n",
            "169\n",
            "330\n",
            "208\n",
            "205\n",
            "229\n",
            "255\n",
            "193\n",
            "283\n",
            "141\n",
            "269\n",
            "146\n",
            "184\n",
            "275\n",
            "251\n",
            "276\n",
            "178\n",
            "142\n",
            "192\n",
            "275\n",
            "163\n",
            "196\n",
            "165\n",
            "208\n",
            "341\n",
            "137\n",
            "312\n",
            "186\n",
            "187\n",
            "271\n",
            "157\n",
            "213\n",
            "260\n",
            "277\n",
            "217\n",
            "133\n",
            "197\n",
            "372\n",
            "281\n",
            "440\n",
            "304\n",
            "126\n",
            "138\n",
            "212\n",
            "274\n",
            "327\n",
            "170\n",
            "183\n",
            "161\n",
            "154\n",
            "293\n",
            "165\n",
            "203\n",
            "204\n",
            "364\n",
            "204\n",
            "318\n",
            "251\n",
            "279\n",
            "202\n",
            "145\n",
            "126\n",
            "189\n",
            "135\n",
            "226\n",
            "164\n",
            "218\n",
            "354\n",
            "248\n",
            "105\n",
            "256\n",
            "253\n",
            "215\n",
            "205\n",
            "72\n",
            "185\n",
            "286\n",
            "209\n",
            "350\n",
            "176\n",
            "221\n",
            "301\n",
            "269\n",
            "225\n",
            "134\n",
            "199\n",
            "258\n",
            "142\n",
            "194\n",
            "167\n",
            "288\n",
            "187\n",
            "186\n",
            "180\n",
            "159\n",
            "324\n",
            "236\n",
            "352\n",
            "133\n",
            "218\n",
            "210\n",
            "349\n",
            "129\n",
            "98\n",
            "163\n",
            "241\n",
            "216\n",
            "215\n",
            "229\n",
            "132\n",
            "103\n",
            "130\n",
            "233\n",
            "211\n",
            "176\n",
            "228\n",
            "263\n",
            "234\n",
            "228\n",
            "212\n",
            "372\n",
            "211\n",
            "177\n",
            "146\n",
            "192\n",
            "223\n",
            "252\n",
            "146\n",
            "282\n",
            "146\n",
            "121\n",
            "165\n",
            "125\n",
            "238\n",
            "378\n",
            "201\n",
            "324\n",
            "283\n",
            "260\n",
            "198\n",
            "243\n",
            "194\n",
            "196\n",
            "299\n",
            "193\n",
            "166\n",
            "175\n",
            "191\n",
            "229\n",
            "256\n",
            "323\n",
            "209\n",
            "252\n",
            "243\n",
            "282\n",
            "206\n",
            "158\n",
            "249\n",
            "191\n",
            "254\n",
            "295\n",
            "169\n",
            "205\n",
            "120\n",
            "186\n",
            "264\n",
            "171\n",
            "167\n",
            "142\n",
            "127\n",
            "238\n",
            "237\n",
            "172\n",
            "168\n",
            "503\n",
            "136\n",
            "174\n",
            "259\n",
            "97\n",
            "247\n",
            "320\n",
            "159\n",
            "160\n",
            "175\n",
            "267\n",
            "239\n",
            "239\n",
            "121\n",
            "190\n",
            "270\n",
            "159\n",
            "145\n",
            "121\n",
            "157\n",
            "98\n",
            "238\n",
            "152\n",
            "141\n",
            "177\n",
            "297\n",
            "210\n",
            "219\n",
            "196\n",
            "188\n",
            "152\n",
            "186\n",
            "203\n",
            "151\n",
            "125\n",
            "255\n",
            "232\n",
            "185\n",
            "182\n",
            "195\n",
            "178\n",
            "138\n",
            "225\n",
            "178\n",
            "161\n",
            "147\n",
            "172\n",
            "186\n",
            "397\n",
            "162\n",
            "204\n",
            "104\n",
            "151\n",
            "209\n",
            "146\n",
            "247\n",
            "306\n",
            "186\n",
            "171\n",
            "147\n",
            "229\n",
            "154\n",
            "226\n",
            "149\n",
            "274\n",
            "213\n",
            "155\n",
            "192\n",
            "201\n",
            "240\n",
            "73\n",
            "178\n",
            "163\n",
            "245\n",
            "205\n",
            "211\n",
            "201\n",
            "146\n",
            "141\n",
            "156\n",
            "155\n",
            "201\n",
            "174\n",
            "167\n",
            "191\n",
            "180\n",
            "198\n",
            "262\n",
            "114\n",
            "220\n",
            "195\n",
            "105\n",
            "231\n",
            "213\n",
            "250\n",
            "153\n",
            "177\n",
            "181\n",
            "268\n",
            "153\n",
            "154\n",
            "180\n",
            "273\n",
            "194\n",
            "225\n",
            "231\n",
            "169\n",
            "177\n",
            "131\n",
            "220\n",
            "291\n",
            "213\n",
            "177\n",
            "211\n",
            "360\n",
            "211\n",
            "191\n",
            "199\n",
            "250\n",
            "120\n",
            "87\n",
            "193\n",
            "130\n",
            "161\n",
            "174\n",
            "258\n",
            "138\n",
            "235\n",
            "161\n",
            "180\n",
            "230\n",
            "161\n",
            "240\n",
            "137\n",
            "149\n",
            "213\n",
            "174\n",
            "171\n",
            "214\n",
            "268\n",
            "107\n",
            "182\n",
            "135\n",
            "167\n",
            "142\n",
            "236\n",
            "222\n",
            "204\n",
            "196\n",
            "165\n",
            "152\n",
            "174\n",
            "228\n",
            "200\n",
            "224\n",
            "217\n",
            "190\n",
            "222\n",
            "186\n",
            "178\n",
            "157\n",
            "172\n",
            "189\n",
            "204\n",
            "183\n",
            "179\n",
            "295\n",
            "239\n",
            "161\n",
            "261\n",
            "205\n",
            "111\n",
            "240\n",
            "322\n",
            "208\n",
            "264\n",
            "348\n",
            "253\n",
            "208\n",
            "141\n",
            "186\n",
            "258\n",
            "97\n",
            "275\n",
            "244\n",
            "142\n",
            "185\n",
            "126\n",
            "131\n",
            "118\n",
            "178\n",
            "178\n",
            "310\n",
            "271\n",
            "150\n",
            "256\n",
            "245\n",
            "228\n",
            "276\n",
            "113\n",
            "213\n",
            "136\n",
            "266\n",
            "154\n",
            "219\n",
            "209\n",
            "192\n",
            "241\n",
            "145\n",
            "158\n",
            "259\n",
            "154\n",
            "191\n",
            "206\n",
            "180\n",
            "193\n",
            "154\n",
            "139\n",
            "163\n",
            "203\n",
            "168\n",
            "151\n",
            "206\n",
            "177\n",
            "321\n",
            "241\n",
            "192\n",
            "183\n",
            "179\n",
            "139\n",
            "143\n",
            "177\n",
            "218\n",
            "138\n",
            "257\n",
            "179\n",
            "214\n",
            "144\n",
            "244\n",
            "243\n",
            "136\n",
            "167\n",
            "196\n",
            "150\n",
            "142\n",
            "239\n",
            "151\n",
            "180\n",
            "191\n",
            "199\n",
            "129\n",
            "98\n",
            "193\n",
            "341\n",
            "253\n",
            "81\n",
            "261\n",
            "247\n",
            "158\n",
            "188\n",
            "192\n",
            "200\n",
            "250\n",
            "172\n",
            "215\n",
            "185\n",
            "149\n",
            "166\n",
            "160\n",
            "256\n",
            "262\n",
            "278\n",
            "226\n",
            "202\n",
            "209\n",
            "258\n",
            "88\n",
            "170\n",
            "139\n",
            "240\n",
            "282\n",
            "244\n",
            "227\n",
            "242\n",
            "207\n",
            "218\n",
            "205\n",
            "141\n",
            "90\n",
            "176\n",
            "210\n",
            "148\n",
            "228\n",
            "176\n",
            "251\n",
            "172\n",
            "138\n",
            "220\n",
            "162\n",
            "136\n",
            "113\n",
            "413\n",
            "120\n",
            "153\n",
            "154\n",
            "91\n",
            "144\n",
            "217\n",
            "183\n",
            "134\n",
            "101\n",
            "230\n",
            "206\n",
            "108\n",
            "141\n",
            "106\n",
            "224\n",
            "106\n",
            "172\n",
            "90\n",
            "223\n",
            "120\n",
            "252\n",
            "220\n",
            "166\n",
            "127\n",
            "278\n",
            "178\n",
            "253\n",
            "78\n",
            "150\n",
            "503\n",
            "256\n",
            "108\n",
            "146\n",
            "197\n",
            "214\n",
            "230\n",
            "178\n",
            "185\n",
            "153\n",
            "153\n",
            "202\n",
            "250\n",
            "117\n",
            "503\n",
            "266\n",
            "215\n",
            "197\n",
            "201\n",
            "209\n",
            "161\n",
            "121\n",
            "181\n",
            "275\n",
            "215\n",
            "165\n",
            "278\n",
            "291\n",
            "224\n",
            "214\n",
            "149\n",
            "503\n",
            "197\n",
            "196\n",
            "237\n",
            "129\n",
            "313\n",
            "105\n",
            "94\n",
            "149\n",
            "165\n",
            "133\n",
            "213\n",
            "239\n",
            "181\n",
            "219\n",
            "289\n",
            "397\n",
            "107\n",
            "258\n",
            "129\n",
            "237\n",
            "99\n",
            "262\n",
            "238\n",
            "131\n",
            "264\n",
            "310\n",
            "232\n",
            "137\n",
            "113\n",
            "105\n",
            "217\n",
            "106\n",
            "139\n",
            "270\n",
            "300\n",
            "476\n",
            "148\n",
            "209\n",
            "162\n",
            "314\n",
            "333\n",
            "313\n",
            "341\n",
            "250\n",
            "333\n",
            "216\n",
            "260\n",
            "180\n",
            "270\n",
            "177\n",
            "171\n",
            "188\n",
            "172\n",
            "203\n",
            "228\n",
            "229\n",
            "139\n",
            "180\n",
            "139\n",
            "205\n",
            "193\n",
            "267\n",
            "206\n",
            "229\n",
            "214\n",
            "294\n",
            "170\n",
            "174\n",
            "149\n",
            "186\n",
            "205\n",
            "192\n",
            "195\n",
            "196\n",
            "131\n",
            "201\n",
            "201\n",
            "268\n",
            "168\n",
            "123\n",
            "131\n",
            "249\n",
            "164\n",
            "164\n",
            "279\n",
            "182\n",
            "221\n",
            "143\n",
            "129\n",
            "218\n",
            "181\n",
            "140\n",
            "238\n",
            "380\n",
            "224\n",
            "130\n",
            "286\n",
            "356\n",
            "94\n",
            "267\n",
            "175\n",
            "119\n",
            "389\n",
            "172\n",
            "192\n",
            "101\n",
            "210\n",
            "181\n",
            "125\n",
            "384\n",
            "110\n",
            "198\n",
            "172\n",
            "250\n",
            "178\n",
            "152\n",
            "150\n",
            "206\n",
            "143\n",
            "109\n",
            "249\n",
            "233\n",
            "250\n",
            "120\n",
            "160\n",
            "213\n",
            "258\n",
            "198\n",
            "211\n",
            "135\n",
            "151\n",
            "194\n",
            "190\n",
            "170\n",
            "158\n",
            "221\n",
            "82\n",
            "503\n",
            "266\n",
            "237\n",
            "244\n",
            "372\n",
            "222\n",
            "208\n",
            "111\n",
            "180\n",
            "157\n",
            "205\n",
            "350\n",
            "346\n",
            "224\n",
            "183\n",
            "503\n",
            "255\n",
            "164\n",
            "160\n",
            "484\n",
            "171\n",
            "151\n",
            "51\n",
            "266\n",
            "210\n",
            "180\n",
            "201\n",
            "142\n",
            "123\n",
            "186\n",
            "188\n",
            "261\n",
            "267\n",
            "117\n",
            "217\n",
            "174\n",
            "244\n",
            "211\n",
            "323\n",
            "214\n",
            "235\n",
            "188\n",
            "254\n",
            "108\n",
            "235\n",
            "198\n",
            "181\n",
            "106\n",
            "195\n",
            "199\n",
            "177\n",
            "172\n",
            "167\n",
            "309\n",
            "143\n",
            "141\n",
            "166\n",
            "164\n",
            "243\n",
            "78\n",
            "178\n",
            "89\n",
            "222\n",
            "236\n",
            "234\n",
            "143\n",
            "223\n",
            "276\n",
            "239\n",
            "311\n",
            "256\n",
            "234\n",
            "263\n",
            "203\n",
            "230\n",
            "248\n",
            "266\n",
            "315\n",
            "416\n",
            "173\n",
            "360\n",
            "199\n",
            "105\n",
            "268\n",
            "118\n",
            "192\n",
            "242\n",
            "225\n",
            "187\n",
            "229\n",
            "168\n",
            "323\n",
            "203\n",
            "200\n",
            "159\n",
            "503\n",
            "349\n",
            "194\n",
            "219\n",
            "232\n",
            "217\n",
            "230\n",
            "125\n",
            "215\n",
            "250\n",
            "212\n",
            "169\n",
            "234\n",
            "118\n",
            "311\n",
            "159\n",
            "259\n",
            "213\n",
            "198\n",
            "176\n",
            "160\n",
            "191\n",
            "253\n",
            "217\n",
            "230\n",
            "254\n",
            "104\n",
            "186\n",
            "177\n",
            "246\n",
            "235\n",
            "247\n",
            "207\n",
            "180\n",
            "325\n",
            "199\n",
            "235\n",
            "142\n",
            "375\n",
            "176\n",
            "167\n",
            "201\n",
            "190\n",
            "116\n",
            "246\n",
            "188\n",
            "183\n",
            "293\n",
            "188\n",
            "132\n",
            "153\n",
            "93\n",
            "177\n",
            "118\n",
            "162\n",
            "142\n",
            "159\n",
            "161\n",
            "207\n",
            "114\n",
            "196\n",
            "198\n",
            "258\n",
            "139\n",
            "127\n",
            "259\n",
            "205\n",
            "271\n",
            "257\n",
            "184\n",
            "161\n",
            "165\n",
            "207\n",
            "265\n",
            "194\n",
            "296\n",
            "152\n",
            "142\n",
            "229\n",
            "168\n",
            "126\n",
            "157\n",
            "157\n",
            "228\n",
            "158\n",
            "180\n",
            "107\n",
            "195\n",
            "227\n",
            "158\n",
            "298\n",
            "210\n",
            "257\n",
            "282\n",
            "262\n",
            "206\n",
            "208\n",
            "328\n",
            "324\n",
            "100\n",
            "188\n",
            "294\n",
            "208\n",
            "168\n",
            "167\n",
            "196\n",
            "136\n",
            "281\n",
            "187\n",
            "212\n",
            "503\n",
            "165\n",
            "270\n",
            "229\n",
            "223\n",
            "146\n",
            "196\n",
            "419\n",
            "315\n",
            "155\n",
            "193\n",
            "186\n",
            "217\n",
            "201\n",
            "205\n",
            "148\n",
            "257\n",
            "203\n",
            "196\n",
            "66\n",
            "503\n",
            "205\n",
            "221\n",
            "257\n",
            "199\n",
            "281\n",
            "199\n",
            "206\n",
            "282\n",
            "199\n",
            "197\n",
            "212\n",
            "169\n",
            "156\n",
            "118\n",
            "115\n",
            "146\n",
            "270\n",
            "243\n",
            "173\n",
            "203\n",
            "165\n",
            "147\n",
            "177\n",
            "171\n",
            "161\n",
            "304\n",
            "430\n",
            "172\n",
            "238\n",
            "251\n",
            "97\n",
            "368\n",
            "123\n",
            "216\n",
            "151\n",
            "171\n",
            "145\n",
            "318\n",
            "262\n",
            "284\n",
            "224\n",
            "207\n",
            "67\n",
            "143\n",
            "192\n",
            "156\n",
            "282\n",
            "171\n",
            "267\n",
            "83\n",
            "280\n",
            "149\n",
            "214\n",
            "282\n",
            "220\n",
            "161\n",
            "189\n",
            "160\n",
            "169\n",
            "217\n",
            "95\n",
            "250\n",
            "213\n",
            "201\n",
            "122\n",
            "137\n",
            "117\n",
            "148\n",
            "152\n",
            "292\n",
            "147\n",
            "132\n",
            "170\n",
            "156\n",
            "151\n",
            "153\n",
            "101\n",
            "176\n",
            "199\n",
            "152\n",
            "174\n",
            "125\n",
            "162\n",
            "159\n",
            "211\n",
            "147\n",
            "217\n",
            "132\n",
            "161\n",
            "151\n",
            "151\n",
            "121\n",
            "157\n",
            "125\n",
            "136\n",
            "167\n",
            "140\n",
            "162\n",
            "153\n",
            "127\n",
            "126\n",
            "230\n",
            "238\n",
            "115\n",
            "214\n",
            "243\n",
            "248\n",
            "123\n",
            "215\n",
            "147\n",
            "149\n",
            "173\n",
            "147\n",
            "158\n",
            "160\n",
            "151\n",
            "51\n",
            "198\n",
            "162\n",
            "142\n",
            "173\n",
            "136\n",
            "165\n",
            "156\n",
            "152\n",
            "152\n",
            "155\n",
            "154\n",
            "195\n",
            "140\n",
            "157\n",
            "122\n",
            "163\n",
            "123\n",
            "167\n",
            "190\n",
            "148\n",
            "149\n",
            "150\n",
            "147\n",
            "117\n",
            "153\n",
            "162\n",
            "148\n",
            "186\n",
            "163\n",
            "164\n",
            "160\n",
            "176\n",
            "158\n",
            "221\n",
            "208\n",
            "182\n",
            "151\n",
            "141\n",
            "154\n",
            "142\n",
            "142\n",
            "157\n",
            "153\n",
            "111\n",
            "143\n",
            "215\n",
            "155\n",
            "122\n",
            "199\n",
            "108\n",
            "157\n",
            "147\n",
            "169\n",
            "114\n",
            "154\n",
            "105\n",
            "178\n",
            "163\n",
            "188\n",
            "153\n",
            "156\n",
            "151\n",
            "148\n",
            "154\n",
            "185\n",
            "176\n",
            "134\n",
            "151\n",
            "138\n",
            "125\n",
            "503\n",
            "151\n",
            "149\n",
            "215\n",
            "257\n",
            "163\n",
            "106\n",
            "178\n",
            "178\n",
            "211\n",
            "207\n",
            "147\n",
            "149\n",
            "140\n",
            "216\n",
            "111\n",
            "162\n",
            "338\n",
            "155\n",
            "150\n",
            "150\n",
            "164\n",
            "160\n",
            "226\n",
            "157\n",
            "160\n",
            "150\n",
            "146\n",
            "157\n",
            "171\n",
            "163\n",
            "154\n",
            "168\n",
            "147\n",
            "261\n",
            "131\n",
            "282\n",
            "247\n",
            "145\n",
            "155\n",
            "151\n",
            "157\n",
            "161\n",
            "119\n",
            "181\n",
            "161\n",
            "149\n",
            "105\n",
            "141\n",
            "226\n",
            "165\n",
            "167\n",
            "210\n",
            "140\n",
            "123\n",
            "167\n",
            "137\n",
            "164\n",
            "160\n",
            "145\n",
            "156\n",
            "166\n",
            "163\n",
            "154\n",
            "173\n",
            "143\n",
            "151\n",
            "167\n",
            "119\n",
            "155\n",
            "134\n",
            "152\n",
            "106\n",
            "242\n",
            "125\n",
            "177\n",
            "188\n",
            "168\n",
            "127\n",
            "171\n",
            "321\n",
            "121\n",
            "164\n",
            "155\n",
            "172\n",
            "165\n",
            "157\n",
            "237\n",
            "177\n",
            "143\n",
            "158\n",
            "159\n",
            "162\n",
            "141\n",
            "160\n",
            "163\n",
            "136\n",
            "157\n",
            "153\n",
            "145\n",
            "137\n",
            "141\n",
            "113\n",
            "144\n",
            "131\n",
            "503\n",
            "147\n",
            "112\n",
            "136\n",
            "446\n",
            "158\n",
            "215\n",
            "167\n",
            "181\n",
            "148\n",
            "162\n",
            "167\n",
            "82\n",
            "175\n",
            "165\n",
            "191\n",
            "150\n",
            "206\n",
            "144\n",
            "184\n",
            "154\n",
            "184\n",
            "173\n",
            "155\n",
            "142\n",
            "153\n",
            "152\n",
            "161\n",
            "156\n",
            "173\n",
            "194\n",
            "108\n",
            "148\n",
            "182\n",
            "141\n",
            "212\n",
            "150\n",
            "179\n",
            "206\n",
            "153\n",
            "152\n",
            "196\n",
            "160\n",
            "178\n",
            "161\n",
            "155\n",
            "167\n",
            "149\n",
            "152\n",
            "163\n",
            "154\n",
            "112\n",
            "205\n",
            "155\n",
            "154\n",
            "138\n",
            "269\n",
            "166\n",
            "285\n",
            "151\n",
            "134\n",
            "159\n",
            "163\n",
            "147\n",
            "137\n",
            "159\n",
            "157\n",
            "114\n",
            "175\n",
            "189\n",
            "160\n",
            "175\n",
            "166\n",
            "106\n",
            "119\n",
            "204\n",
            "150\n",
            "171\n",
            "175\n",
            "184\n",
            "129\n",
            "151\n",
            "161\n",
            "259\n",
            "187\n",
            "130\n",
            "249\n",
            "170\n",
            "210\n",
            "91\n",
            "108\n",
            "268\n",
            "92\n",
            "181\n",
            "197\n",
            "328\n",
            "229\n",
            "110\n",
            "131\n",
            "172\n",
            "186\n",
            "62\n",
            "114\n",
            "200\n",
            "179\n",
            "192\n",
            "65\n",
            "236\n",
            "190\n",
            "167\n",
            "173\n",
            "171\n",
            "129\n",
            "163\n",
            "223\n",
            "98\n",
            "83\n",
            "225\n",
            "140\n",
            "177\n",
            "210\n",
            "212\n",
            "78\n",
            "313\n",
            "245\n",
            "161\n",
            "182\n",
            "204\n",
            "163\n",
            "165\n",
            "76\n",
            "79\n",
            "191\n",
            "76\n",
            "207\n",
            "138\n",
            "151\n",
            "102\n",
            "176\n",
            "503\n",
            "159\n",
            "143\n",
            "194\n",
            "95\n",
            "114\n",
            "243\n",
            "44\n",
            "140\n",
            "192\n",
            "178\n",
            "220\n",
            "74\n",
            "139\n",
            "126\n",
            "224\n",
            "223\n",
            "131\n",
            "196\n",
            "121\n",
            "212\n",
            "146\n",
            "145\n",
            "156\n",
            "154\n",
            "254\n",
            "39\n",
            "163\n",
            "128\n",
            "134\n",
            "158\n",
            "116\n",
            "107\n",
            "156\n",
            "94\n",
            "74\n",
            "257\n",
            "169\n",
            "313\n",
            "126\n",
            "144\n",
            "191\n",
            "190\n",
            "161\n",
            "157\n",
            "233\n",
            "145\n",
            "149\n",
            "103\n",
            "206\n",
            "95\n",
            "184\n",
            "197\n",
            "266\n",
            "140\n",
            "136\n",
            "224\n",
            "209\n",
            "306\n",
            "210\n",
            "188\n",
            "179\n",
            "236\n",
            "206\n",
            "132\n",
            "136\n",
            "142\n",
            "298\n",
            "133\n",
            "121\n",
            "176\n",
            "221\n",
            "213\n",
            "201\n",
            "236\n",
            "220\n",
            "142\n",
            "143\n",
            "145\n",
            "213\n",
            "229\n",
            "259\n",
            "180\n",
            "97\n",
            "202\n",
            "154\n",
            "89\n",
            "234\n",
            "183\n",
            "141\n",
            "267\n",
            "192\n",
            "178\n",
            "191\n",
            "108\n",
            "237\n",
            "151\n",
            "206\n",
            "106\n",
            "208\n",
            "187\n",
            "125\n",
            "161\n",
            "187\n",
            "130\n",
            "170\n",
            "152\n",
            "217\n",
            "145\n",
            "270\n",
            "268\n",
            "175\n",
            "160\n",
            "214\n",
            "139\n",
            "108\n",
            "191\n",
            "227\n",
            "128\n",
            "144\n",
            "199\n",
            "135\n",
            "185\n",
            "253\n",
            "141\n",
            "157\n",
            "218\n",
            "285\n",
            "119\n",
            "235\n",
            "179\n",
            "267\n",
            "110\n",
            "104\n",
            "210\n",
            "126\n",
            "150\n",
            "115\n",
            "157\n",
            "177\n",
            "182\n",
            "87\n",
            "174\n",
            "205\n",
            "243\n",
            "153\n",
            "189\n",
            "202\n",
            "100\n",
            "189\n",
            "174\n",
            "215\n",
            "220\n",
            "150\n",
            "144\n",
            "141\n",
            "219\n",
            "193\n",
            "150\n",
            "56\n",
            "222\n",
            "235\n",
            "105\n",
            "159\n",
            "197\n",
            "147\n",
            "84\n",
            "111\n",
            "164\n",
            "153\n",
            "125\n",
            "117\n",
            "160\n",
            "286\n",
            "111\n",
            "65\n",
            "125\n",
            "51\n",
            "243\n",
            "197\n",
            "221\n",
            "128\n",
            "81\n",
            "202\n",
            "195\n",
            "176\n",
            "221\n",
            "257\n",
            "238\n",
            "280\n",
            "214\n",
            "274\n",
            "180\n",
            "224\n",
            "113\n",
            "303\n",
            "124\n",
            "278\n",
            "164\n",
            "65\n",
            "184\n",
            "218\n",
            "226\n",
            "191\n",
            "71\n",
            "202\n",
            "151\n",
            "102\n",
            "170\n",
            "235\n",
            "175\n",
            "122\n",
            "190\n",
            "208\n",
            "168\n",
            "85\n",
            "157\n",
            "237\n",
            "147\n",
            "115\n",
            "164\n",
            "253\n",
            "229\n",
            "116\n",
            "257\n",
            "90\n",
            "196\n",
            "141\n",
            "149\n",
            "75\n",
            "166\n",
            "147\n",
            "105\n",
            "141\n",
            "187\n",
            "158\n",
            "94\n",
            "176\n",
            "157\n",
            "113\n",
            "180\n",
            "245\n",
            "261\n",
            "249\n",
            "238\n",
            "222\n",
            "226\n",
            "236\n",
            "230\n",
            "111\n",
            "198\n",
            "261\n",
            "174\n",
            "165\n",
            "188\n",
            "272\n",
            "222\n",
            "104\n",
            "204\n",
            "216\n",
            "236\n",
            "182\n",
            "163\n",
            "168\n",
            "52\n",
            "119\n",
            "341\n",
            "180\n",
            "124\n",
            "179\n",
            "305\n",
            "189\n",
            "121\n",
            "134\n",
            "144\n",
            "231\n",
            "68\n",
            "170\n",
            "314\n",
            "159\n",
            "131\n",
            "221\n",
            "116\n",
            "98\n",
            "284\n",
            "94\n",
            "250\n",
            "154\n",
            "196\n",
            "158\n",
            "171\n",
            "209\n",
            "215\n",
            "157\n",
            "216\n",
            "177\n",
            "125\n",
            "163\n",
            "112\n",
            "145\n",
            "222\n",
            "127\n",
            "187\n",
            "233\n",
            "268\n",
            "100\n",
            "133\n",
            "219\n",
            "97\n",
            "95\n",
            "246\n",
            "86\n",
            "127\n",
            "170\n",
            "381\n",
            "208\n",
            "217\n",
            "138\n",
            "244\n",
            "228\n",
            "271\n",
            "97\n",
            "276\n",
            "244\n",
            "502\n",
            "297\n",
            "289\n",
            "202\n",
            "339\n",
            "135\n",
            "311\n",
            "120\n",
            "247\n",
            "231\n",
            "346\n",
            "244\n",
            "218\n",
            "177\n",
            "45\n",
            "131\n",
            "221\n",
            "258\n",
            "151\n",
            "202\n",
            "263\n",
            "146\n",
            "295\n",
            "114\n",
            "250\n",
            "274\n",
            "288\n",
            "225\n",
            "125\n",
            "145\n",
            "272\n",
            "209\n",
            "169\n",
            "140\n",
            "197\n",
            "176\n",
            "114\n",
            "255\n",
            "105\n",
            "239\n",
            "200\n",
            "275\n",
            "115\n",
            "120\n",
            "113\n",
            "168\n",
            "153\n",
            "265\n",
            "241\n",
            "143\n",
            "339\n",
            "181\n",
            "281\n",
            "233\n",
            "228\n",
            "171\n",
            "220\n",
            "189\n",
            "180\n",
            "229\n",
            "137\n",
            "86\n",
            "194\n",
            "157\n",
            "190\n",
            "136\n",
            "220\n",
            "185\n",
            "168\n",
            "116\n",
            "136\n",
            "133\n",
            "197\n",
            "199\n",
            "228\n",
            "215\n",
            "217\n",
            "247\n",
            "134\n",
            "191\n",
            "214\n",
            "185\n",
            "298\n",
            "251\n",
            "226\n",
            "97\n",
            "152\n",
            "93\n",
            "286\n",
            "277\n",
            "174\n",
            "336\n",
            "159\n",
            "187\n",
            "444\n",
            "228\n",
            "315\n",
            "154\n",
            "294\n",
            "109\n",
            "117\n",
            "270\n",
            "153\n",
            "137\n",
            "135\n",
            "138\n",
            "205\n",
            "187\n",
            "153\n",
            "321\n",
            "208\n",
            "246\n",
            "258\n",
            "110\n",
            "161\n",
            "192\n",
            "295\n",
            "329\n",
            "147\n",
            "145\n",
            "209\n",
            "253\n",
            "248\n",
            "177\n",
            "199\n",
            "327\n",
            "193\n",
            "165\n",
            "382\n",
            "123\n",
            "221\n",
            "287\n",
            "204\n",
            "148\n",
            "132\n",
            "310\n",
            "315\n",
            "193\n",
            "134\n",
            "204\n",
            "216\n",
            "150\n",
            "257\n",
            "203\n",
            "203\n",
            "150\n",
            "85\n",
            "231\n",
            "127\n",
            "280\n",
            "206\n",
            "297\n",
            "444\n",
            "98\n",
            "124\n",
            "143\n",
            "224\n",
            "194\n",
            "145\n",
            "208\n",
            "290\n",
            "113\n",
            "223\n",
            "101\n",
            "269\n",
            "134\n",
            "274\n",
            "196\n",
            "237\n",
            "141\n",
            "143\n",
            "185\n",
            "185\n",
            "133\n",
            "163\n",
            "258\n",
            "201\n",
            "201\n",
            "206\n",
            "260\n",
            "117\n",
            "125\n",
            "240\n",
            "277\n",
            "269\n",
            "179\n",
            "230\n",
            "100\n",
            "192\n",
            "168\n",
            "205\n",
            "164\n",
            "210\n",
            "503\n",
            "109\n",
            "161\n",
            "255\n",
            "163\n",
            "397\n",
            "187\n",
            "155\n",
            "105\n",
            "234\n",
            "253\n",
            "205\n",
            "318\n",
            "207\n",
            "225\n",
            "209\n",
            "191\n",
            "176\n",
            "231\n",
            "299\n",
            "168\n",
            "141\n",
            "199\n",
            "211\n",
            "170\n",
            "232\n",
            "132\n",
            "231\n",
            "341\n",
            "182\n",
            "193\n",
            "179\n",
            "157\n",
            "146\n",
            "192\n",
            "208\n",
            "92\n",
            "176\n",
            "280\n",
            "225\n",
            "224\n",
            "154\n",
            "163\n",
            "97\n",
            "84\n",
            "127\n",
            "298\n",
            "199\n",
            "229\n",
            "245\n",
            "275\n",
            "198\n",
            "400\n",
            "113\n",
            "223\n",
            "345\n",
            "234\n",
            "341\n",
            "150\n",
            "209\n",
            "137\n",
            "64\n",
            "269\n",
            "156\n",
            "147\n",
            "366\n",
            "201\n",
            "289\n",
            "62\n",
            "133\n",
            "305\n",
            "134\n",
            "194\n",
            "158\n",
            "158\n",
            "66\n",
            "163\n",
            "195\n",
            "503\n",
            "503\n",
            "253\n",
            "143\n",
            "163\n",
            "111\n",
            "288\n",
            "111\n",
            "108\n",
            "239\n",
            "116\n",
            "248\n",
            "311\n",
            "272\n",
            "73\n",
            "120\n",
            "358\n",
            "208\n",
            "180\n",
            "239\n",
            "123\n",
            "172\n",
            "333\n",
            "206\n",
            "198\n",
            "179\n",
            "324\n",
            "143\n",
            "190\n",
            "282\n",
            "270\n",
            "228\n",
            "275\n",
            "503\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "503"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_tokenizer = Tokenizer(num_words=tot_cnt-cnt) \n",
        "x_tokenizer.fit_on_texts(list(x_tr))\n",
        "\n",
        "#convert text sequences into integer sequences\n",
        "x_tr_seq    =   x_tokenizer.texts_to_sequences(x_tr) \n",
        "x_val_seq   =   x_tokenizer.texts_to_sequences(x_val)\n",
        "\n",
        "#padding zero upto maximum length\n",
        "x_tr    =   pad_sequences(x_tr_seq,  maxlen=max_text_len, padding='post')\n",
        "x_val   =   pad_sequences(x_val_seq, maxlen=max_text_len, padding='post')\n",
        "\n",
        "#size of vocabulary ( +1 for padding token)\n",
        "x_voc   =  x_tokenizer.num_words + 1"
      ],
      "metadata": {
        "id": "Ql8MzBwtOLxE"
      },
      "id": "Ql8MzBwtOLxE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_voc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zdSoklLjSTyt",
        "outputId": "b8c0a5a4-3316-4cc9-e9da-abcd7e4858db"
      },
      "id": "zdSoklLjSTyt",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7487"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_tokenizer = Tokenizer()   \n",
        "y_tokenizer.fit_on_texts(list(y_tr))"
      ],
      "metadata": {
        "id": "thmG5PxASWjX"
      },
      "id": "thmG5PxASWjX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "thresh=5\n",
        "\n",
        "cnt=0\n",
        "tot_cnt=0\n",
        "freq=0\n",
        "tot_freq=0\n",
        "\n",
        "for key,value in y_tokenizer.word_counts.items():\n",
        "    tot_cnt=tot_cnt+1\n",
        "    tot_freq=tot_freq+value\n",
        "    if(value<thresh):\n",
        "        cnt=cnt+1\n",
        "        freq=freq+value\n",
        "    \n",
        "print(\"% of rare words in vocabulary:\",(cnt/tot_cnt)*100)\n",
        "print(\"Total Coverage of rare words:\",(freq/tot_freq)*100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vl5a89QcSaqQ",
        "outputId": "afb53e9b-8c08-4421-de44-5a50a4da1752"
      },
      "id": "Vl5a89QcSaqQ",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "% of rare words in vocabulary: 74.01287553648068\n",
            "Total Coverage of rare words: 13.610599078341012\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_tokenizer = Tokenizer(num_words=tot_cnt-cnt) \n",
        "y_tokenizer.fit_on_texts(list(y_tr))\n",
        "\n",
        "#convert text sequences into integer sequences\n",
        "y_tr_seq    =   y_tokenizer.texts_to_sequences(y_tr) \n",
        "y_val_seq   =   y_tokenizer.texts_to_sequences(y_val) \n",
        "\n",
        "#padding zero upto maximum length\n",
        "y_tr    =   pad_sequences(y_tr_seq, maxlen=100, padding='post')\n",
        "y_val   =   pad_sequences(y_val_seq, maxlen=100, padding='post')\n",
        "\n",
        "#size of vocabulary\n",
        "y_voc  =   y_tokenizer.num_words +1"
      ],
      "metadata": {
        "id": "iRbJxuZgSfpb"
      },
      "id": "iRbJxuZgSfpb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_voc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Px-6bxweSm1D",
        "outputId": "e106787f-ed7e-4f5f-e01b-d87d96fc93b4"
      },
      "id": "Px-6bxweSm1D",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2423"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.layers import Attention\n",
        "import pandas as pd \n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "from keras.preprocessing.text import Tokenizer \n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from nltk.corpus import stopwords\n",
        "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate, TimeDistributed\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import warnings\n",
        "pd.set_option(\"display.max_colwidth\", 200)\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "B2mZsuNQTSHI"
      },
      "id": "B2mZsuNQTSHI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras import backend as K \n",
        "import gensim\n",
        "from numpy import *\n",
        "import numpy as np\n",
        "import pandas as pd \n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "from keras.preprocessing.text import Tokenizer \n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from nltk.corpus import stopwords\n",
        "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate, TimeDistributed\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import warnings\n",
        "pd.set_option(\"display.max_colwidth\", 200)\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "print(\"Size of vocabulary from the w2v model = {}\".format(x_voc))\n",
        "\n",
        "K.clear_session()\n",
        "\n",
        "latent_dim = 300\n",
        "embedding_dim=200\n",
        "\n",
        "# Encoder\n",
        "encoder_inputs = Input(shape=(max_text_len,))\n",
        "\n",
        "#embedding layer\n",
        "enc_emb =  Embedding(x_voc, embedding_dim,trainable=True)(encoder_inputs)\n",
        "\n",
        "#encoder lstm 1\n",
        "encoder_lstm1 = LSTM(latent_dim,return_sequences=True,return_state=True,dropout=0.4,recurrent_dropout=0.4)\n",
        "encoder_output1, state_h1, state_c1 = encoder_lstm1(enc_emb)\n",
        "\n",
        "#encoder lstm 2\n",
        "encoder_lstm2 = LSTM(latent_dim,return_sequences=True,return_state=True,dropout=0.4,recurrent_dropout=0.4)\n",
        "encoder_output2, state_h2, state_c2 = encoder_lstm2(encoder_output1)\n",
        "\n",
        "#encoder lstm 3\n",
        "encoder_lstm3=LSTM(latent_dim, return_state=True, return_sequences=True,dropout=0.4,recurrent_dropout=0.4)\n",
        "encoder_outputs, state_h, state_c= encoder_lstm3(encoder_output2)\n",
        "\n",
        "# Set up the decoder, using `encoder_states` as initial state.\n",
        "decoder_inputs = Input(shape=(None,))\n",
        "\n",
        "#embedding layer\n",
        "dec_emb_layer = Embedding(y_voc, embedding_dim,trainable=True)\n",
        "dec_emb = dec_emb_layer(decoder_inputs)\n",
        "\n",
        "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True,dropout=0.4,recurrent_dropout=0.2)\n",
        "decoder_outputs,decoder_fwd_state, decoder_back_state = decoder_lstm(dec_emb,initial_state=[state_h, state_c])\n",
        "\n",
        "#dense layer\n",
        "decoder_dense =  TimeDistributed(Dense(y_voc, activation='softmax'))\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "# Define the model \n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dRAaAl_USoIb",
        "outputId": "b4fc6d35-b769-4a58-e5ef-3c69aa7e14eb"
      },
      "id": "dRAaAl_USoIb",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of vocabulary from the w2v model = 7487\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 503)]        0           []                               \n",
            "                                                                                                  \n",
            " embedding (Embedding)          (None, 503, 200)     1497400     ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " lstm (LSTM)                    [(None, 503, 300),   601200      ['embedding[0][0]']              \n",
            "                                 (None, 300),                                                     \n",
            "                                 (None, 300)]                                                     \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " lstm_1 (LSTM)                  [(None, 503, 300),   721200      ['lstm[0][0]']                   \n",
            "                                 (None, 300),                                                     \n",
            "                                 (None, 300)]                                                     \n",
            "                                                                                                  \n",
            " embedding_1 (Embedding)        (None, None, 200)    484600      ['input_2[0][0]']                \n",
            "                                                                                                  \n",
            " lstm_2 (LSTM)                  [(None, 503, 300),   721200      ['lstm_1[0][0]']                 \n",
            "                                 (None, 300),                                                     \n",
            "                                 (None, 300)]                                                     \n",
            "                                                                                                  \n",
            " lstm_3 (LSTM)                  [(None, None, 300),  601200      ['embedding_1[0][0]',            \n",
            "                                 (None, 300),                     'lstm_2[0][1]',                 \n",
            "                                 (None, 300)]                     'lstm_2[0][2]']                 \n",
            "                                                                                                  \n",
            " time_distributed (TimeDistribu  (None, None, 2423)  729323      ['lstm_3[0][0]']                 \n",
            " ted)                                                                                             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 5,356,123\n",
            "Trainable params: 5,356,123\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy')"
      ],
      "metadata": {
        "id": "PxUOU2MlS1pA"
      },
      "id": "PxUOU2MlS1pA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1,patience=2)"
      ],
      "metadata": {
        "id": "It7gILj4X5Aa"
      },
      "id": "It7gILj4X5Aa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history=model.fit([x_tr,y_tr[:,:-1]], y_tr.reshape(y_tr.shape[0],y_tr.shape[1], 1)[:,1:] ,epochs=3,callbacks=[es],batch_size=128, validation_data=([x_val,y_val[:,:-1]], y_val.reshape(y_val.shape[0],y_val.shape[1], 1)[:,1:]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UXLCNtthXdkz",
        "outputId": "2808e82e-b18d-4849-df33-58b21f431d95"
      },
      "id": "UXLCNtthXdkz",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "13/13 [==============================] - 576s 45s/step - loss: 3.0591 - val_loss: 2.8753\n",
            "Epoch 2/3\n",
            "13/13 [==============================] - 589s 45s/step - loss: 2.9430 - val_loss: 2.7215\n",
            "Epoch 3/3\n",
            "13/13 [==============================] - 593s 46s/step - loss: 2.9724 - val_loss: 2.7019\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_model = Model(inputs=encoder_inputs,outputs=[encoder_outputs, state_h, state_c])\n",
        "\n",
        "# Decoder setup\n",
        "# Below tensors will hold the states of the previous time step\n",
        "decoder_state_input_h = Input(shape=(latent_dim,))\n",
        "decoder_state_input_c = Input(shape=(latent_dim,))\n",
        "decoder_hidden_state_input = Input(shape=(max_text_len,latent_dim))\n",
        "\n",
        "# Get the embeddings of the decoder sequence\n",
        "dec_emb2= dec_emb_layer(decoder_inputs) \n",
        "# To predict the next word in the sequence, set the initial states to the states from the previous time step\n",
        "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=[decoder_state_input_h, decoder_state_input_c])\n",
        "\n",
        "# A dense softmax layer to generate prob dist. over the target vocabulary\n",
        "decoder_outputs2 = decoder_dense(decoder_outputs2) \n",
        "\n",
        "# Final decoder model\n",
        "decoder_model = Model(\n",
        "    [decoder_inputs] + [decoder_hidden_state_input,decoder_state_input_h, decoder_state_input_c],\n",
        "    [decoder_outputs2] + [state_h2, state_c2])"
      ],
      "metadata": {
        "id": "P-dqcqASg1bb"
      },
      "id": "P-dqcqASg1bb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def decode_sequence(input_seq):\n",
        "    # Encode the input as state vectors.\n",
        "    e_out, e_h, e_c = encoder_model.predict(input_seq)\n",
        "    \n",
        "    # Generate empty target sequence of length 1.\n",
        "    target_seq = np.zeros((1,1))\n",
        "    \n",
        "    # Populate the first word of target sequence with the start word.\n",
        "    target_seq[0, 0] = target_word_index['sostok']\n",
        "\n",
        "    stop_condition = False\n",
        "    decoded_sentence = ''\n",
        "    while not stop_condition:\n",
        "      \n",
        "        output_tokens, h, c = decoder_model.predict([target_seq] + [e_out, e_h, e_c])\n",
        "\n",
        "        # Sample a token\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_token = reverse_target_word_index[sampled_token_index]\n",
        "        \n",
        "        if(sampled_token!='eostok'):\n",
        "            decoded_sentence += ' '+sampled_token\n",
        "\n",
        "        # Exit condition: either hit max length or find stop word.\n",
        "        if (sampled_token == 'eostok'  or len(decoded_sentence.split()) >= (max_summary_len-1)):\n",
        "            stop_condition = True\n",
        "\n",
        "        # Update the target sequence (of length 1).\n",
        "        target_seq = np.zeros((1,1))\n",
        "        target_seq[0, 0] = sampled_token_index\n",
        "\n",
        "        # Update internal states\n",
        "        e_h, e_c = h, c\n",
        "\n",
        "    return decoded_sentence"
      ],
      "metadata": {
        "id": "UglYHQyzXgh7"
      },
      "id": "UglYHQyzXgh7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reverse_target_word_index=y_tokenizer.index_word\n",
        "reverse_source_word_index=x_tokenizer.index_word\n",
        "target_word_index=y_tokenizer.word_index"
      ],
      "metadata": {
        "id": "wbVKf_1gifDN"
      },
      "id": "wbVKf_1gifDN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def seq2summary(input_seq):\n",
        "    newString=''\n",
        "    for i in input_seq:\n",
        "        if((i!=0 and i!=target_word_index['sostok']) and i!=target_word_index['eostok']):\n",
        "            newString=newString+reverse_target_word_index[i]+' '\n",
        "    return newString\n",
        "\n",
        "def seq2text(input_seq):\n",
        "    newString=''\n",
        "    for i in input_seq:\n",
        "        if(i!=0):\n",
        "            newString=newString+reverse_source_word_index[i]+' '\n",
        "    return newString"
      ],
      "metadata": {
        "id": "CQAmGA0viEXE"
      },
      "id": "CQAmGA0viEXE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for i in range(0,100):\n",
        "    print(\"Review:\",seq2text(x_tr[i]))\n",
        "    print(\"Original summary:\",seq2summary(y_tr[i]))\n",
        "    print(\"Predicted summary:\",decode_sequence(x_tr[i].reshape(1,max_text_len)))\n",
        "    print(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "k7e9a08oiIyM",
        "outputId": "d82231cb-7238-422b-d0f3-5d251110bfa6"
      },
      "id": "k7e9a08oiIyM",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Review: this paper deals with jacobi equation based stabilized optimal control of hybrid dynamical systems this paper presents the fuzzy clustering based event wise multiple linearized modeling approaches for hds to describe the continuous dynamic in each event in the present work a fuzzy clustering validation approach is presented for the selection of number of linearized models which span entire hds the method also describes how to obtain event wise operating point using fuzzy membership function which is used to find the event wise model bank by the first principles model the event wise linearized models are used for the formulation of the optimal control law the equation is formulated using a suitable quadratic term in the objective function by use of the direct method of lyapunov stability the control law is shown to be optimal with respect to objective functional and stabilized the event wise linearized models the global lyapunov function is proposed with discrete variables which stabilized the hds the proposed modeling and control algorithm have been applied on two necessary theoretical and simulation experiments are presented to demonstrate the performance and validation of the proposed algorithm \n",
            "Original summary: equation based optimal control of hybrid systems is presented the fuzzy validity based method is used to find the number of linear models present in the stability proof for the event and generalized solution based optimal control is proposed the proposed modeling and control algorithm have been applied on two \n",
            "Predicted summary:  not address 8 address mesh differential collected sentence comparing disease processor maximum measures various maximum examples microscopic fatigue cirs discrimination designed translation baselines possible factors produce one one simple preserve processed particle klpp weights clinical efficiency quasi pb background assessment manner representations opinion representations impact computational computational computational o evolutionary duration duration failure grating forest consists square square relationships consists square independent independent sensitivity connectivity relevant flow aspects age age landmarks acoustic fixed fixed fixed fixed fixed estimated mixing fixed fixed fixed estimated mixing fixed fixed fixed estimated mixing fixed fixed fixed estimated mixing fixed fixed fixed estimated mixing\n",
            "\n",
            "\n",
            "Review: mechanical stimuli play a significant role in the process of long bone development as by clinical observations and in vivo studies up to now approaches to understand stimuli characteristics have been limited to the first stages of epiphyseal development furthermore growth plate mechanical behavior has not been widely studied in order to better understand mechanical influences on bone growth we used and approximation to analyze growth plate mechanical behavior and explore stress patterns for different morphological stages of the growth plate to the best of our knowledge this work is the first attempt to study stress distribution on growth plate during different possible stages of bone development from to stress distribution analysis on the and growth plate was performed using finite element analysis in a simplified generic epiphyseal geometry using a linear elastic model as the first approximation we took into account different growth plate locations morphologies and widths as well as different epiphyseal developmental stages we found stress distribution during bone development established osteogenic index patterns that seem to influence locally epiphyseal structures growth and coincide with growth plate \n",
            "Original summary: growth plate characteristics affect development environment within growth plate according to state stimuli may affect growth plate and \n",
            "Predicted summary:  not address 8 address mesh differential collected sentence comparing disease processor maximum measures various maximum examples microscopic fatigue cirs discrimination designed translation baselines possible factors produce one one simple preserve processed particle klpp weights clinical efficiency quasi pb background assessment manner representations opinion representations impact computational computational computational o evolutionary duration duration failure grating forest consists square square relationships consists square independent independent sensitivity connectivity relevant flow aspects age age landmarks acoustic fixed fixed fixed fixed fixed estimated mixing fixed fixed fixed estimated mixing fixed fixed fixed estimated mixing fixed fixed fixed estimated mixing fixed fixed fixed estimated mixing\n",
            "\n",
            "\n",
            "Review: the assessment of the state of the acrosome is a priority in artificial centres since it is one of the main causes of function loss in this work present in gray scale images acquired with a phase contrast microscope have been classified as acrosome intact or acrosome damaged after using fluorescent images for creating the ground truth based on shape prior criteria combined with otsu s thresholding regional minima and watershed transform the heads were segmented and registered one of the main of this proposal is that unlike what previous works stated the obtained results show that the contour information of the head is important for improving description and classification other of this work is that it confirms that combining different texture descriptors and contour descriptors yield the best classification rates for this problem up to date the classification was performed with a support vector machine by a least squares training algorithm and a linear kernel using the biggest acrosome intact damaged dataset ever created the early fusion approach followed provides a 0 f score outperforming all previous related works \n",
            "Original summary: a new early fusion approach for classification is proposed specific segmentation based on shape was carried out the dataset created was created and published contour is important for improving description and classification the best result up to combining shape and texture features has been obtained \n",
            "Predicted summary:  not address 8 address mesh differential collected sentence comparing disease processor maximum measures various maximum examples microscopic fatigue cirs discrimination designed translation baselines possible factors produce one one simple preserve processed particle klpp weights clinical efficiency quasi pb background assessment manner representations opinion representations impact computational computational computational o evolutionary duration duration failure grating forest consists square square relationships consists square independent independent sensitivity connectivity relevant flow aspects age age landmarks acoustic fixed fixed fixed fixed fixed estimated mixing fixed fixed fixed estimated mixing fixed fixed fixed estimated mixing fixed fixed fixed estimated mixing fixed fixed fixed estimated mixing\n",
            "\n",
            "\n",
            "Review: visual speech information plays an important role in automatic speech recognition asr especially when audio is corrupted or even inaccessible despite the success of audio based asr the problem of visual speech decoding remains widely open this paper provides a detailed review of recent advances in this research area in comparison with the previous survey 97 which covers the whole asr system that uses visual speech information we focus on the important questions asked by researchers and summarize the recent studies that attempt to answer them in particular there are three questions related to the extraction of visual features concerning speaker dependency pose variation and temporal information respectively another question is about audio visual speech fusion considering the dynamic changes of modality encountered in practice in addition the state of the art on facial landmark localization is briefly introduced in this paper those advanced techniques can be used to improve the region of interest detection but have been largely ignored when building a visual based asr system we also provide details of audio visual speech databases finally we discuss the remaining challenges and offer our insights into the future research on visual speech decoding \n",
            "Original summary: a detailed review of the recent advances in the area of visual speech visual features speaker head and temporal information dynamic audio visual speech information fusion recent techniques of facial landmark localization of audio visual speech databases and performance on them \n",
            "Predicted summary:  not address 8 address mesh differential collected sentence comparing disease processor maximum measures various maximum examples microscopic fatigue cirs discrimination designed translation baselines possible factors produce one one simple preserve processed particle klpp weights clinical efficiency quasi pb background assessment manner representations opinion representations impact computational computational computational o evolutionary duration duration failure grating forest consists square square relationships consists square independent independent sensitivity connectivity relevant flow aspects age age landmarks acoustic fixed fixed fixed fixed fixed estimated mixing fixed fixed fixed estimated mixing fixed fixed fixed estimated mixing fixed fixed fixed estimated mixing fixed fixed fixed estimated mixing\n",
            "\n",
            "\n",
            "Review: in this study a dynamic screening strategy is proposed to discriminate subjects with autistic spectrum disorder from healthy controls the is defined as a disorder that normal patterns of connectivity between the brain regions therefore the potential use of such for screening is investigated the connectivity patterns are estimated from electroencephalogram data collected from 8 brain regions under various mental states the eeg data of 12 healthy controls and 6 autistic children were collected during eyes open and eyes close states as well as when subjects were exposed to affective faces subsequently the subjects were classified as autistic or healthy groups based on their brain connectivity patterns using pattern recognition techniques performance of the proposed system in each mental state is separately evaluated the results present higher recognition rates using functional connectivity features when compared against other existing feature extraction methods \n",
            "Original summary: a dynamic strategy for screening of is proposed based on patterns of information flow between 8 brain regions the eeg data is collected from 12 healthy and 6 children in the age of 7 years old the subjects are then classified as or healthy based on the connectivity features the connectivity features are also compared with other established methods the promising recognition rates of were achieved in this study this study shows that patterns of functional and effective connectivity in subjects are different from healthy subjects \n",
            "Predicted summary:  not address 8 address mesh differential collected sentence comparing disease processor maximum measures various maximum examples microscopic fatigue cirs discrimination designed translation baselines possible factors produce one one simple preserve processed particle klpp weights clinical efficiency quasi pb background assessment manner representations opinion representations impact computational computational computational o evolutionary duration duration failure grating forest consists square square relationships consists square independent independent sensitivity connectivity relevant flow aspects age age landmarks acoustic fixed fixed fixed fixed fixed estimated mixing fixed fixed fixed estimated mixing fixed fixed fixed estimated mixing fixed fixed fixed estimated mixing fixed fixed fixed estimated mixing\n",
            "\n",
            "\n",
            "Review: there has been much surrounding the potential benefits and costs of online interaction the present research that engagement with online discussion can have benefits for users well being and engagement in offline civic action and that identification with other online forum users plays a key role in this regard users of a variety of online discussion participated in this study we hypothesized and found that participants who felt their expectations had been by the forum reported higher levels of forum identification identification in turn predicted their satisfaction with life and involvement in offline civic activities formal analyses confirmed that identification served as a for both of these outcomes importantly whether the forum concerned a topic certain of these relationships findings are discussed in the context of theoretical and applied implications \n",
            "Original summary: online discussion have benefits at individual and level they are positively linked to well being for group online discussion use is linked to in related areas identification with other users the above relationships online discussion are of applied importance than has been realized \n",
            "Predicted summary:  not address 8 address mesh differential collected sentence comparing disease processor maximum measures various maximum examples microscopic fatigue cirs discrimination designed translation baselines possible factors produce one one simple preserve processed particle klpp weights clinical efficiency quasi pb background assessment manner representations opinion representations impact computational computational computational o evolutionary duration duration failure grating forest consists square square relationships consists square independent independent sensitivity connectivity relevant flow aspects age age landmarks acoustic fixed fixed fixed fixed fixed estimated mixing fixed fixed fixed estimated mixing fixed fixed fixed estimated mixing fixed fixed fixed estimated mixing fixed fixed fixed estimated mixing\n",
            "\n",
            "\n",
            "Review: common to much work on land cover classification in imagery is the use of single satellite images for training the classifiers for the different land types unfortunately more often than not decision boundaries derived in this manner do not well from one image to another this happens for several reasons most having to do with the fact that different satellite images correspond to different view angles on the earth s surface different sun angles different and so on in this paper we get around these limitations of the current state of the art by first proposing a new integrated representation for all of the images overlapping and non overlapping that cover a large geographic roi in addition to helping understand the data variability in the images this representation also makes it possible to create the ground truth that can be used for roi based wide area learning of the classifiers we use this integrated representation in a new bayesian framework for data classification that is characterized by learning of the decision boundaries from a sampling of all the satellite data available for an entire geographic roi probabilistic modeling of within class and between class variations as to the more traditional probabilistic modeling of the feature vectors extracted from the measurement data and using variance based ml and map classifiers whose decision boundary calculations incorporate all of the multi view data for a geographic point if that point is selected for learning and testing we show results with the new classification framework for an roi in whose size is roughly 10 000 square this roi is covered by satellite images with varying degrees of overlap we compare the classification performance of the proposed roi based framework with the results obtained by the decision boundaries learned from a single image to the entire roi using a 10 fold cross validation test we demonstrate significant increases in the classification accuracy for five of the six land cover classes in addition we show that our variance based bayesian classifier outperforms a traditional support vector machine based approach to classification for four out of six classes \n",
            "Original summary: a wide area efficient sampling for training classify with \n",
            "Predicted summary:  not address 8 address mesh differential collected sentence comparing disease processor maximum measures various maximum examples microscopic fatigue cirs discrimination designed translation baselines possible factors produce one one simple preserve processed particle klpp weights clinical efficiency quasi pb background assessment manner representations opinion representations impact computational computational computational o evolutionary duration duration failure grating forest consists square square relationships consists square independent independent sensitivity connectivity relevant flow aspects age age landmarks acoustic fixed fixed fixed fixed fixed estimated mixing fixed fixed fixed estimated mixing fixed fixed fixed estimated mixing fixed fixed fixed estimated mixing fixed fixed fixed estimated mixing\n",
            "\n",
            "\n",
            "Review: analytic hierarchy process is increasingly applied to healthcare and medical research and applications however knowledge representation of pairwise reciprocal matrix is still this research discusses the related drawbacks and pairwise opposite matrix as the ideal alternative pairwise opposite matrix is the key foundation of primitive cognitive network process which the ahp approach with practical changes a medical decision treatment evaluation using ahp is by p with a step by step comparisons with ahp have been discussed the proposed method could be a promising decision tool to replace ahp to share information among patients or and doctors and to evaluate therapies medical treatments health care technologies medical resources and healthcare policies \n",
            "Original summary: the rating scale problems of the process ahp and proposing the interval scale addressing the limitations introducing cognitive network process p cnp to medical treatment decision making and showing how to use it from perspective how the current ahp data to medical decision can be to p cnp data which is further processed by the p cnp applications with the ahp data can be by p cnp to explore the more reliable research findings or make more reliable decisions p cnp can be a promising decision making approach to evaluate medical and healthcare decisions \n",
            "Predicted summary:  not address 8 address mesh differential collected sentence comparing disease processor maximum measures various maximum examples microscopic fatigue cirs discrimination designed translation baselines possible factors produce one one simple preserve processed particle klpp weights clinical efficiency quasi pb background assessment manner representations opinion representations impact computational computational computational o evolutionary duration duration failure grating forest consists square square relationships consists square independent independent sensitivity connectivity relevant flow aspects age age landmarks acoustic fixed fixed fixed fixed fixed estimated mixing fixed fixed fixed estimated mixing fixed fixed fixed estimated mixing fixed fixed fixed estimated mixing fixed fixed fixed estimated mixing\n",
            "\n",
            "\n",
            "Review: in this paper we address the tasks of audio source counting and separation for a stereo mixture of audio signals this will be achieved in two stages in the first stage a novel approach is introduced for estimating the number of sources as well as the channel mixing coefficients for this purpose a 2 d spectrum is evaluated against both the phase and amplitude differences of the two channels hence obtaining the peak locations of the spectrum yields the number of the sources and the corresponding channel coefficients in the second stage an extension of a single channel complex matrix factorization method to multichannel is developed to extract the individual source signals we find primary estimates of the sources via binary masking and then apply the complex factorization to the complex spectrogram of each source the obtained factors are then utilized as initial values in the complex multichannel factorization model we also suggest a method for estimating the number of required components for modeling each source the separation performance improvement over the conventional methods is investigated by calculating evaluation metrics the comparison is also carried out in terms of source counting and localization with the recently proposed method \n",
            "Original summary: the tasks of audio source counting and separation for a stereo mixture of audio signals are addressed a novel approach is introduced for estimating the number of sources as well as the channel mixing coefficients based on evaluating a 2 d spectrum an extension of a single channel complex matrix factorization method to is developed to extract the individual source signals a model order selection method is proposed to infer the optimal number of the components required for modeling each source the separation performance improvement over the conventional methods is investigated by evaluation metrics \n",
            "Predicted summary:  not address 8 address mesh differential collected sentence comparing disease processor maximum measures various maximum examples microscopic fatigue cirs discrimination designed translation baselines possible factors produce one one simple preserve processed particle klpp weights clinical efficiency quasi pb background assessment manner representations opinion representations impact computational computational computational o evolutionary duration duration failure grating forest consists square square relationships consists square independent independent sensitivity connectivity relevant flow aspects age age landmarks acoustic fixed fixed fixed fixed fixed estimated mixing fixed fixed fixed estimated mixing fixed fixed fixed estimated mixing fixed fixed fixed estimated mixing fixed fixed fixed estimated mixing\n",
            "\n",
            "\n",
            "Review: cognitive radio technology can solve the problems of spectrum scarcity and low spectrum utilization however random behavior of the primary user appears to be an enormous challenge in this paper we propose a pu behavior aware joint channel selection and allocation scheme in the first step the channels are ranked based on statistics of the pu usage whereas in the second phase a proportional fair oriented channel allocation scheme is employed to allocate channels among we also introduce the concept of a time varying process that minimizes the overall data transmission time simulation results show that the proposed scheme outperforms existing schemes in terms of the transmission time and the number of with the in addition it helps to save a significant amount of transmission power moreover it provides a significantly higher system throughput as compared to the existing schemes \n",
            "Original summary: a fair oriented sharing scheme is proposed to channels a activity model is developed to provide most stable channels to users a time varying process is used to make variable frames at mac layer the proposed scheme reduces the delay and increases the throughput \n",
            "Predicted summary:  not address 8 address mesh differential collected sentence comparing disease processor maximum measures various maximum examples microscopic fatigue cirs discrimination designed translation baselines possible factors produce one one simple preserve processed particle klpp weights clinical efficiency quasi pb background assessment manner representations opinion representations impact computational computational computational o evolutionary duration duration failure grating forest consists square square relationships consists square independent independent sensitivity connectivity relevant flow aspects age age landmarks acoustic fixed fixed fixed fixed fixed estimated mixing fixed fixed fixed estimated mixing fixed fixed fixed estimated mixing fixed fixed fixed estimated mixing fixed fixed fixed estimated mixing\n",
            "\n",
            "\n",
            "Review: this paper addresses the problem of determining the symmetries of a plane or space curve defined by a rational parametrization we provide effective methods to compute the and rotation symmetries for the planar case as for space curves our method finds the in all cases and all the rotation symmetries in the particular case of pythagorean hodograph curves our algorithms solve these problems without converting to implicit form instead we make use of a relationship between two proper parametrizations of the same curve which leads to algorithms that involve only univariate polynomials these algorithms have been implemented and tested in the system \n",
            "Original summary: we provide a deterministic method to detect of rational curves the method detects all the of plane rational curves the method also all the of space rational curves in the space case the method detects also of curves \n",
            "Predicted summary:  not address 8 address mesh differential collected sentence comparing disease processor maximum measures various maximum examples microscopic fatigue cirs discrimination designed translation baselines possible factors produce one one simple preserve processed particle klpp weights clinical efficiency quasi pb background assessment manner representations opinion representations impact computational computational computational o evolutionary duration duration failure grating forest consists square square relationships consists square independent independent sensitivity connectivity relevant flow aspects age age landmarks acoustic fixed fixed fixed fixed fixed estimated mixing fixed fixed fixed estimated mixing fixed fixed fixed estimated mixing fixed fixed fixed estimated mixing fixed fixed fixed estimated mixing\n",
            "\n",
            "\n",
            "Review: an analysis of the relative motion and point feature model configurations leading to solution degeneracy is presented for the case of a simultaneous localization and mapping system using clusters with non overlapping fields of view the slam optimization system seeks to minimize image space reprojection error and is formulated for a cluster containing any number of component cameras observing any number of point features over two the measurement is transformed to expose a reduced dimension representation such that the degeneracy of the system can be determined by the rank of a dense a set of relative motions sufficient for degeneracy are identified for certain cluster configurations independent of target model geometry furthermore it is shown that increasing the number of cameras within the cluster and observing features across different cameras over the two reduces the size of the motion sets significantly \n",
            "Original summary: analysis of solution in general cluster decomposition of system conditions for dense sub matrix geometric structure for analysis identification of motions independent of feature experiments demonstrate that adding features and cameras decreases set \n",
            "Predicted summary:  not address 8 address mesh differential collected sentence comparing disease processor maximum measures various maximum examples microscopic fatigue cirs discrimination designed translation baselines possible factors produce one one simple preserve processed particle klpp weights clinical efficiency quasi pb background assessment manner representations opinion representations impact computational computational computational o evolutionary duration duration failure grating forest consists square square relationships consists square independent independent sensitivity connectivity relevant flow aspects age age landmarks acoustic fixed fixed fixed fixed fixed estimated mixing fixed fixed fixed estimated mixing fixed fixed fixed estimated mixing fixed fixed fixed estimated mixing fixed fixed fixed estimated mixing\n",
            "\n",
            "\n",
            "Review: broadcasting is the simplest form of communication in which nodes disseminate the same information simultaneously to all of their neighbors broadcasting has been widely used in many types of networks including wireless networks wireless sensor networks and mobile ad hoc networks these networks broadcasting is also used in cognitive radio networks to accomplish various tasks such as spectrum sensing spectrum sharing spectrum management and spectrum mobility this article investigates and provides a comprehensive overview of various broadcasting strategies that have been proposed so far for cognitive radio networks moreover it provides a detailed study of broadcast storm problem in crns finally it discusses issues challenges and future research directions for broadcasting strategies in crns \n",
            "Original summary: we give an overview of various broadcasting strategies proposed so far for cognitive radio networks crns we identify required key characteristics of broadcasting strategies in crns we propose a comprehensive and detailed classification of broadcasting strategies in crns we provide a detailed study of broadcast problem in crns we discuss the possible scenarios for the generation of broadcast problem and its related challenges in crns \n",
            "Predicted summary:  not address 8 address mesh differential collected sentence comparing disease processor maximum measures various maximum examples microscopic fatigue cirs discrimination designed translation baselines possible factors produce one one simple preserve processed particle klpp weights clinical efficiency quasi pb background assessment manner representations opinion representations impact computational computational computational o evolutionary duration duration failure grating forest consists square square relationships consists square independent independent sensitivity connectivity relevant flow aspects age age landmarks acoustic fixed fixed fixed fixed fixed estimated mixing fixed fixed fixed estimated mixing fixed fixed fixed estimated mixing fixed fixed fixed estimated mixing fixed fixed fixed estimated mixing\n",
            "\n",
            "\n",
            "Review: graphical abstract using rigid and non rigid registration to correct in geometry and texture two input textured surfaces are captured by rgb d cameras the camera configuration provides initial alignment successive rigid and non rigid steps improve it giving a final surface with high quality textures \n",
            "Original summary: using color and geometric to in both rigid and non rigid cases using color when determining the transformation deformation in surfaces proving our algorithm s effectiveness when textured surfaces \n",
            "Predicted summary:  not address 8 address mesh differential collected sentence comparing disease processor maximum measures various maximum examples microscopic fatigue cirs discrimination designed translation baselines possible factors produce one one simple preserve processed particle klpp weights clinical efficiency quasi pb background assessment manner representations opinion representations impact computational computational computational o evolutionary duration duration failure grating forest consists square square relationships consists square independent independent sensitivity connectivity relevant flow aspects age age landmarks acoustic fixed fixed fixed fixed fixed estimated mixing fixed fixed fixed estimated mixing fixed fixed fixed estimated mixing fixed fixed fixed estimated mixing fixed fixed fixed estimated mixing\n",
            "\n",
            "\n",
            "Review: breast thermography still has inherent limitations that prevent it from being fully accepted as a breast screening modality in medicine the main challenges of breast thermography are to reduce false positive results and to increase the sensitivity of a further it is still difficult to obtain information about tumour parameters such as metabolic heat tumour depth and diameter from a however infrared technology and image processing have advanced significantly and recent clinical studies have shown increased sensitivity of thermography in cancer diagnosis the aim of this paper is to study numerically the possibilities of extracting information about the tumour depth from steady state thermography and transient thermography after cold stress with no need to use any specific inversion technique both methods are based on the numerical solution of equation for a simple three dimensional breast model the effectiveness of two approaches used for depth detection from steady state thermography is assessed the effect of breast density on the steady state thermal contrast has also been studied the use of a cold stress test and the recording of transient during were found to be potentially suitable for tumour depth detection during the process sensitivity to parameters such as cold stress temperature and cooling time is investigated using the numerical model and simulation results reveal two prominent depth related characteristic times which do not strongly depend on the temperature of the cold stress or on the cooling period \n",
            "Original summary: state thermal contrast magnitude depends on the tumour diameter and depth as well as on the breast density the full width at maximum calculated at the surface of the breast as the tumour depth increases but also depends on the tumour diameter thermal contrast present three important characteristics including the response time the peak and its corresponding observation time the observation time is likely a potential time for tumour depth detection as it does not on the tumour diameter or on the temperature and its duration \n",
            "Predicted summary:  not address 8 address mesh differential collected sentence comparing disease processor maximum measures various maximum examples microscopic fatigue cirs discrimination designed translation baselines possible factors produce one one simple preserve processed particle klpp weights clinical efficiency quasi pb background assessment manner representations opinion representations impact computational computational computational o evolutionary duration duration failure grating forest consists square square relationships consists square independent independent sensitivity connectivity relevant flow aspects age age landmarks acoustic fixed fixed fixed fixed fixed estimated mixing fixed fixed fixed estimated mixing fixed fixed fixed estimated mixing fixed fixed fixed estimated mixing fixed fixed fixed estimated mixing\n",
            "\n",
            "\n",
            "Review: when wireless sensor networks are deployed in areas inaccessible by human security becomes extremely important as they are prone to different types of malicious attacks we propose a scheme to build a security mechanism in a query processing paradigm within wsns with clustered architecture this work aims to preserve the basic security features such as confidentiality and integrity as well as to protect from attack in presence of class attacker considering the limitations of such an attacker the probability of attacking cluster head and member nodes is higher than attacking the base station paying attention to this fact in all communication between cluster head and member nodes the key is neither transmitted nor pre deployed performance of the scheme is evaluated and compared through qualitative and quantitative analyses results show the present scheme s dominance over the competing schemes \n",
            "Original summary: low overhead query processing mechanism in wsn environment preserves security features and against attack query is from base station to cluster in form the cluster their nodes by messages the cluster responses to the base station \n",
            "Predicted summary:  not address 8 address mesh differential collected sentence comparing disease processor maximum measures various maximum examples microscopic fatigue cirs discrimination designed translation baselines possible factors produce one one simple preserve processed particle klpp weights clinical efficiency quasi pb background assessment manner representations opinion representations impact computational computational computational o evolutionary duration duration failure grating forest consists square square relationships consists square independent independent sensitivity connectivity relevant flow aspects age age landmarks acoustic fixed fixed fixed fixed fixed estimated mixing fixed fixed fixed estimated mixing fixed fixed fixed estimated mixing fixed fixed fixed estimated mixing fixed fixed fixed estimated mixing\n",
            "\n",
            "\n",
            "Review: this paper investigates the problem of cross domain action recognition specifically we present a cross domain action recognition framework by utilizing some labeled data from other data sets as the auxiliary source domain it is a challenging task as data from different domains may have different feature distribution to map data from different domains into the same abstract space and boost the action recognition performance we propose a method named collective matrix factorization with graph laplacian regularization our approach is built upon the technique of collective matrix factorization which simultaneously learns a common latent space linear projection matrices for obtaining semantic representations and an optimal linear classifier moreover we explore the label consistency across different domain and the local geometric consistency in each domain and obtain a graph laplacian regularization term to enhance the discrimination of learned features experimental results verify that significantly outperforms several state of the art methods \n",
            "Original summary: we apply matrix factorization to cross domain action recognition we build a novel graph laplacian term the framework semantic representations and a linear classifier \n",
            "Predicted summary:  not address 8 address mesh differential collected sentence comparing disease processor maximum measures various maximum examples microscopic fatigue cirs discrimination designed translation baselines possible factors produce one one simple preserve processed particle klpp weights clinical efficiency quasi pb background assessment manner representations opinion representations impact computational computational computational o evolutionary duration duration failure grating forest consists square square relationships consists square independent independent sensitivity connectivity relevant flow aspects age age landmarks acoustic fixed fixed fixed fixed fixed estimated mixing fixed fixed fixed estimated mixing fixed fixed fixed estimated mixing fixed fixed fixed estimated mixing fixed fixed fixed estimated mixing\n",
            "\n",
            "\n",
            "Review: we present a simple and efficient way to improve the performance of the twisted nematic liquid crystal system by doping a small amount of n alcohols the lcs modified by n demonstrates the optimum electro optical properties of lower driving voltage and shorter response time our measurements indicate that the liquid n alcohols can be used to modify lcs for energy relative to lcs without the drawbacks of and aggregation that the nanoparticles could have the method of doping n liquids provides a more stable and reliable choice to apply in the various lc display systems \n",
            "Original summary: we present a simple and efficient way to improve the performance of the liquid system the modified by liquid dopant exhibit the functions of lower voltage and shorter response time we the solid dopant by the liquid dopant for the drawbacks of and the result of time shows the voltage can be after the doping of n liquid \n",
            "Predicted summary:  not address 8 address mesh differential collected sentence comparing disease processor maximum measures various maximum examples microscopic fatigue cirs discrimination designed translation baselines possible factors produce one one simple preserve processed particle klpp weights clinical efficiency quasi pb background assessment manner representations opinion representations impact computational computational computational o evolutionary duration duration failure grating forest consists square square relationships consists square independent independent sensitivity connectivity relevant flow aspects age age landmarks acoustic fixed fixed fixed fixed fixed estimated mixing fixed fixed fixed estimated mixing fixed fixed fixed estimated mixing fixed fixed fixed estimated mixing fixed fixed fixed estimated mixing\n",
            "\n",
            "\n",
            "Review: abnormal values of vital parameters such as or may occur during anesthesia and may be detected by analyzing time series data collected during the procedure by the anesthesia information management system when with other data from the hospital information system abnormal values of vital parameters have been linked with postoperative morbidity and mortality however methods for the automatic detection of these events are poorly documented in the literature and differ between studies making it difficult to results in this paper we propose a methodology for the automatic detection of abnormal values of vital parameters this methodology uses an algorithm allowing the configuration of threshold values for any vital parameters as well as the management of missing data four examples illustrate the application of the algorithm after which it is applied to three vital signs to all 2014 anesthetic records at our institution \n",
            "Original summary: methods for automatic detection of abnormal vital during anesthesia are in the literature existing methods are not between databases we have developed a data model and an algorithm that allow automatic detection of abnormal values of vital parameters during anesthesia and various parameters such as time between measurements and time outside provide adaptability to various clinical situations e g after start of anesthesia the relation between occurrence of abnormal values of vital parameters and and length of may then be studied on a large and automated scale \n",
            "Predicted summary:  not address 8 address mesh differential collected sentence comparing disease processor maximum measures various maximum examples microscopic fatigue cirs discrimination designed translation baselines possible factors produce one one simple preserve processed particle klpp weights clinical efficiency quasi pb background assessment manner representations opinion representations impact computational computational computational o evolutionary duration duration failure grating forest consists square square relationships consists square independent independent sensitivity connectivity relevant flow aspects age age landmarks acoustic fixed fixed fixed fixed fixed estimated mixing fixed fixed fixed estimated mixing fixed fixed fixed estimated mixing fixed fixed fixed estimated mixing fixed fixed fixed estimated mixing\n",
            "\n",
            "\n",
            "Review: one of the leading time of flight imaging technologies for depth sensing is based on photonic devices pmd in pmd sensors each pixel samples the correlation between emitted and received light signals current pmd cameras compute eight correlation samples per pixel in four sequential stages to obtain depth with invariance to signal amplitude and offset variations with motion pmd pixels capture different at each stage as a result correlation samples are not coherent with a single depth producing artifacts we propose to detect and remove motion artifacts from a single frame taken by a pmd camera the algorithm we propose is very fast simple and can be easily included in camera hardware we recover depth of each pixel by exploiting consistency of the correlation samples and local neighbors of the pixel in addition our method obtains the motion flow of contours in the image from a single frame the system has been validated in real scenes using a commercial low cost pmd camera and high speed dynamics in all cases our method produces accurate results and it highly reduces motion artifacts \n",
            "Original summary: motion are a non systematic error in time of tof imaging when motion during the integration time cause tof measurements depth and amplitude we propose a single frame correction of tof measurements with motion flow estimation we present quantitative and qualitative results in challenging scenarios \n",
            "Predicted summary:  not address 8 address mesh differential collected sentence comparing disease processor maximum measures various maximum examples microscopic fatigue cirs discrimination designed translation baselines possible factors produce one one simple preserve processed particle klpp weights clinical efficiency quasi pb background assessment manner representations opinion representations impact computational computational computational o evolutionary duration duration failure grating forest consists square square relationships consists square independent independent sensitivity connectivity relevant flow aspects age age landmarks acoustic fixed fixed fixed fixed fixed estimated mixing fixed fixed fixed estimated mixing fixed fixed fixed estimated mixing fixed fixed fixed estimated mixing fixed fixed fixed estimated mixing\n",
            "\n",
            "\n",
            "Review: hierarchical generating systems that are derived from powell elements can be used to generate quadratic splines on adaptively refined criss cross triangulations we propose two extensions of these hierarchical generating systems firstly decoupling the hierarchical elements and secondly the system by including auxiliary functions these extensions allow us to generate the entire hierarchical spline space which consists of all piecewise quadratic smooth functions on an adaptively refined criss cross triangulation if the triangulation certain technical assumptions special attention is dedicated to the characterization of the linear dependencies that are present in the resulting enriched decoupled hierarchical generating system \n",
            "Original summary: hierarchical powell elements are studied sufficient conditions for algebraic completeness are given construction uses and partial functions characterization of linear is provided \n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-82-0e69bf8de83c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Review:\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mseq2text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_tr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Original summary:\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mseq2summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_tr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Predicted summary:\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdecode_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_tr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_text_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-76-219f1da445c8>\u001b[0m in \u001b[0;36mdecode_sequence\u001b[0;34m(input_seq)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mstop_condition\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0moutput_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtarget_seq\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0me_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me_c\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;31m# Sample a token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1959\u001b[0m           \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1960\u001b[0m           \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1961\u001b[0;31m           steps_per_execution=self._steps_per_execution)\n\u001b[0m\u001b[1;32m   1962\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1963\u001b[0m       \u001b[0;31m# Container that configures and calls `tf.keras.Callback`s.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36mget_data_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1397\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_cluster_coordinator\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1398\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_ClusterCoordinatorDataHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1399\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mDataHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1400\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1401\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute)\u001b[0m\n\u001b[1;32m   1159\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1160\u001b[0m         \u001b[0mdistribution_strategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1161\u001b[0;31m         model=model)\n\u001b[0m\u001b[1;32m   1162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1163\u001b[0m     \u001b[0mstrategy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[1;32m    290\u001b[0m     \u001b[0;31m# trigger the next permutation. On the other hand, too many simultaneous\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m     \u001b[0;31m# shuffles can contend on a hardware level and degrade all performance.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m     \u001b[0mindices_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindices_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpermutation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprefetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mslice_batch_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, map_func, num_parallel_calls, deterministic, name)\u001b[0m\n\u001b[1;32m   2014\u001b[0m         warnings.warn(\"The `deterministic` argument has no effect unless the \"\n\u001b[1;32m   2015\u001b[0m                       \"`num_parallel_calls` argument is specified.\")\n\u001b[0;32m-> 2016\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mMapDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreserve_cardinality\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2017\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2018\u001b[0m       return ParallelMapDataset(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_dataset, map_func, use_inter_op_parallelism, preserve_cardinality, use_legacy_function, name)\u001b[0m\n\u001b[1;32m   5203\u001b[0m         \u001b[0muse_inter_op_parallelism\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_use_inter_op_parallelism\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5204\u001b[0m         \u001b[0mpreserve_cardinality\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_preserve_cardinality\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5205\u001b[0;31m         **self._common_args)\n\u001b[0m\u001b[1;32m   5206\u001b[0m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMapDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariant_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36mmap_dataset\u001b[0;34m(input_dataset, other_arguments, f, output_types, output_shapes, use_inter_op_parallelism, preserve_cardinality, metadata, name)\u001b[0m\n\u001b[1;32m   3368\u001b[0m         \u001b[0;34m\"output_types\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"output_shapes\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3369\u001b[0m         \u001b[0;34m\"use_inter_op_parallelism\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_inter_op_parallelism\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3370\u001b[0;31m         \"preserve_cardinality\", preserve_cardinality, \"metadata\", metadata)\n\u001b[0m\u001b[1;32m   3371\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3372\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Qe-OkxagiQkn"
      },
      "id": "Qe-OkxagiQkn",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    },
    "colab": {
      "provenance": []
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1e7a613c8710465399a901bfc12951cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0df08209f41141c9b1fce29a3f0608be",
              "IPY_MODEL_c0f08f7f41944c7b84276a00cdbc9d18",
              "IPY_MODEL_3e8a49e7a7a94e77a0af514073b6aadd"
            ],
            "layout": "IPY_MODEL_3dcce6d11f554314bb727538ef44964d"
          }
        },
        "0df08209f41141c9b1fce29a3f0608be": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3eeb37d6865f46d2a1434e649dd2d133",
            "placeholder": "​",
            "style": "IPY_MODEL_da2bd2c5298f4d3cbea42379f870336d",
            "value": "Downloading config.json: 100%"
          }
        },
        "c0f08f7f41944c7b84276a00cdbc9d18": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_805c41cb378649fb8bc0a4c04816336e",
            "max": 1802,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_61a20e5f9f684615a2cb43a2fd6ae199",
            "value": 1802
          }
        },
        "3e8a49e7a7a94e77a0af514073b6aadd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f20cdb5c35c54a61a2ad1dce2ee37848",
            "placeholder": "​",
            "style": "IPY_MODEL_a711319a0983451ca7643b58d6524640",
            "value": " 1.76k/1.76k [00:00&lt;00:00, 37.4kB/s]"
          }
        },
        "3dcce6d11f554314bb727538ef44964d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3eeb37d6865f46d2a1434e649dd2d133": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "da2bd2c5298f4d3cbea42379f870336d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "805c41cb378649fb8bc0a4c04816336e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "61a20e5f9f684615a2cb43a2fd6ae199": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f20cdb5c35c54a61a2ad1dce2ee37848": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a711319a0983451ca7643b58d6524640": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2173d6e1edb64a648f7e9549709ded67": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b3baee21f115444fb3de11298b048156",
              "IPY_MODEL_f79865069828487eb1e2e4db8918ffb7",
              "IPY_MODEL_da0b03298bc44c22a28b9a94eeef5e27"
            ],
            "layout": "IPY_MODEL_62aa418cc35a4454af73999602450d96"
          }
        },
        "b3baee21f115444fb3de11298b048156": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8183a5cb2e9b4a30bc20d7beebb0c118",
            "placeholder": "​",
            "style": "IPY_MODEL_9266fd9991284055841a6af2efc42d04",
            "value": "Downloading pytorch_model.bin: 100%"
          }
        },
        "f79865069828487eb1e2e4db8918ffb7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_35cf72af739f4d77951eebe1cff1f4cd",
            "max": 1222317369,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1bce10dae6014704a5e0ae7c91c37985",
            "value": 1222317369
          }
        },
        "da0b03298bc44c22a28b9a94eeef5e27": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_744a6d0533e54da1a4498cee2579b8df",
            "placeholder": "​",
            "style": "IPY_MODEL_c22d81453a034b0b844ec41d7984edae",
            "value": " 1.14G/1.14G [00:24&lt;00:00, 51.1MB/s]"
          }
        },
        "62aa418cc35a4454af73999602450d96": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8183a5cb2e9b4a30bc20d7beebb0c118": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9266fd9991284055841a6af2efc42d04": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "35cf72af739f4d77951eebe1cff1f4cd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1bce10dae6014704a5e0ae7c91c37985": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "744a6d0533e54da1a4498cee2579b8df": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c22d81453a034b0b844ec41d7984edae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bda20703fa3e45069d858fec53a7baac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_888b431d8ee4425087f66aaba84ec547",
              "IPY_MODEL_e10e4a1892ed4b32883e80411e72ad15",
              "IPY_MODEL_1c8e79bd12a44792a49ce956d54f9e7e"
            ],
            "layout": "IPY_MODEL_424220a9927f4ed682b7ff0c93a999c7"
          }
        },
        "888b431d8ee4425087f66aaba84ec547": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0abcafc9de8d4a6ab2e1b4413bfb8ffc",
            "placeholder": "​",
            "style": "IPY_MODEL_5f21add9a63b4fd7b0233451d9862696",
            "value": "Downloading tokenizer_config.json: 100%"
          }
        },
        "e10e4a1892ed4b32883e80411e72ad15": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_69964ea9c23040ccb3fe057f6c1e4053",
            "max": 26,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_dda9a3d52299409d95b80f6d7514b1f3",
            "value": 26
          }
        },
        "1c8e79bd12a44792a49ce956d54f9e7e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2e41862ac45c4206a6276717efe7e334",
            "placeholder": "​",
            "style": "IPY_MODEL_2169b54b8eaf466d9b15a6a8876ba485",
            "value": " 26.0/26.0 [00:00&lt;00:00, 470B/s]"
          }
        },
        "424220a9927f4ed682b7ff0c93a999c7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0abcafc9de8d4a6ab2e1b4413bfb8ffc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5f21add9a63b4fd7b0233451d9862696": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "69964ea9c23040ccb3fe057f6c1e4053": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dda9a3d52299409d95b80f6d7514b1f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2e41862ac45c4206a6276717efe7e334": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2169b54b8eaf466d9b15a6a8876ba485": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d4fe0f2908b041f3bd61dedb8f32806a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_280001d5a33f42f4ad9a174c2dca12f9",
              "IPY_MODEL_32d65e67163b4f7594dd401ebb6c5aad",
              "IPY_MODEL_75feaa5186684d618271988113b4b7c3"
            ],
            "layout": "IPY_MODEL_896ed465d16a4575bff55e94bd70d20f"
          }
        },
        "280001d5a33f42f4ad9a174c2dca12f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b51e3c0822f54d37b0499f38d905a206",
            "placeholder": "​",
            "style": "IPY_MODEL_e532048672ee4c638781a73834df7a6d",
            "value": "Downloading vocab.json: 100%"
          }
        },
        "32d65e67163b4f7594dd401ebb6c5aad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a47b4ed055b2422f9fb00bfbb9f3f505",
            "max": 898822,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7d03c58110c046049ac36507c66898b0",
            "value": 898822
          }
        },
        "75feaa5186684d618271988113b4b7c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5d7111662216473d8540f2a5d7806d46",
            "placeholder": "​",
            "style": "IPY_MODEL_dd4d88dc5092474080a4ca34ae5aa434",
            "value": " 878k/878k [00:00&lt;00:00, 951kB/s]"
          }
        },
        "896ed465d16a4575bff55e94bd70d20f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b51e3c0822f54d37b0499f38d905a206": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e532048672ee4c638781a73834df7a6d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a47b4ed055b2422f9fb00bfbb9f3f505": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7d03c58110c046049ac36507c66898b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5d7111662216473d8540f2a5d7806d46": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dd4d88dc5092474080a4ca34ae5aa434": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "949830752c8846128b4188fe4da1197b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_28ac29efe361406fbee17c1015850264",
              "IPY_MODEL_c9eccfd5a17c4a23b20472a539fac418",
              "IPY_MODEL_6b80a316d862453ca3ee3a423d11ec9c"
            ],
            "layout": "IPY_MODEL_b3fed85f60ee42e490fe39a6e07b1bab"
          }
        },
        "28ac29efe361406fbee17c1015850264": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_827be06e0b064b0ab7dea66653301abd",
            "placeholder": "​",
            "style": "IPY_MODEL_27b6cdcc16334c5daec0ac4a1ee1e52a",
            "value": "Downloading merges.txt: 100%"
          }
        },
        "c9eccfd5a17c4a23b20472a539fac418": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_91bc5ca8582c42a7a53a3fca5734154e",
            "max": 456318,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_77a134ceb23b47e89c74269f9e10341c",
            "value": 456318
          }
        },
        "6b80a316d862453ca3ee3a423d11ec9c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cb45d0695a9c4688b76aaa860d51d408",
            "placeholder": "​",
            "style": "IPY_MODEL_24169045690e417c9465cea23fb04dd6",
            "value": " 446k/446k [00:00&lt;00:00, 878kB/s]"
          }
        },
        "b3fed85f60ee42e490fe39a6e07b1bab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "827be06e0b064b0ab7dea66653301abd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "27b6cdcc16334c5daec0ac4a1ee1e52a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "91bc5ca8582c42a7a53a3fca5734154e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "77a134ceb23b47e89c74269f9e10341c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cb45d0695a9c4688b76aaa860d51d408": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "24169045690e417c9465cea23fb04dd6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9f7172d72a1049b183f3ab67c5eeaf75": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8d920cdc9f294c45a0c51ce8e496b45b",
              "IPY_MODEL_c8dedc59aa994e33a716870c0942d5d7",
              "IPY_MODEL_b1cb33778ae14b18a024d2501d7fe97b"
            ],
            "layout": "IPY_MODEL_3f8d8e3c5b8b4bc6a27d1612681b5def"
          }
        },
        "8d920cdc9f294c45a0c51ce8e496b45b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5624bf8982674c439d940f8c5cef0d4a",
            "placeholder": "​",
            "style": "IPY_MODEL_adecf808224944198a1ee0ea56ca55cb",
            "value": "Downloading config.json: 100%"
          }
        },
        "c8dedc59aa994e33a716870c0942d5d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bc77b655dc134ac19297c01ac2584990",
            "max": 1199,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ced33ad39c8b44a796af95b85811d52b",
            "value": 1199
          }
        },
        "b1cb33778ae14b18a024d2501d7fe97b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d34b435ca05d421d8f253e1196c4c806",
            "placeholder": "​",
            "style": "IPY_MODEL_75350df6ede74320ac8f1a0040ee4306",
            "value": " 1.17k/1.17k [00:00&lt;00:00, 6.03kB/s]"
          }
        },
        "3f8d8e3c5b8b4bc6a27d1612681b5def": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5624bf8982674c439d940f8c5cef0d4a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "adecf808224944198a1ee0ea56ca55cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bc77b655dc134ac19297c01ac2584990": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ced33ad39c8b44a796af95b85811d52b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d34b435ca05d421d8f253e1196c4c806": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "75350df6ede74320ac8f1a0040ee4306": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1b5d8e0af6204ddbb84bc5981bc9a107": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5930fc681e0a4b2185327888e0ac6643",
              "IPY_MODEL_761ea4e618914de385781cf8d534b6ea",
              "IPY_MODEL_3916a333e4d14041bb55ae94e10bced6"
            ],
            "layout": "IPY_MODEL_d20f74f60fe54b45a9db1078cb3f0d95"
          }
        },
        "5930fc681e0a4b2185327888e0ac6643": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_06d450e226974ef18a74fd79181ace6c",
            "placeholder": "​",
            "style": "IPY_MODEL_3ea1b6bf4dd24bc3808ff70c823d9004",
            "value": "Downloading tf_model.h5: 100%"
          }
        },
        "761ea4e618914de385781cf8d534b6ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_25a518e683974715bb56dccbab4c226a",
            "max": 892146080,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4771e33872094547b385f1d32b3ac2ec",
            "value": 892146080
          }
        },
        "3916a333e4d14041bb55ae94e10bced6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3f83c2b3b8d94ff98db20fce1031a425",
            "placeholder": "​",
            "style": "IPY_MODEL_6703952f8edc47e5a83de901c357fbef",
            "value": " 851M/851M [00:20&lt;00:00, 52.3MB/s]"
          }
        },
        "d20f74f60fe54b45a9db1078cb3f0d95": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "06d450e226974ef18a74fd79181ace6c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3ea1b6bf4dd24bc3808ff70c823d9004": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "25a518e683974715bb56dccbab4c226a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4771e33872094547b385f1d32b3ac2ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3f83c2b3b8d94ff98db20fce1031a425": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6703952f8edc47e5a83de901c357fbef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "daa55cc26fcc415ab54ba5a86b1ef848": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2416c0e5591a4ee29ab46e708b003dbd",
              "IPY_MODEL_3924a3c6e80349308510e5ae91a89c60",
              "IPY_MODEL_e13752b7beeb403c9073faf1fdc9f7b2"
            ],
            "layout": "IPY_MODEL_ff0836c31f9f44dba10f23a8ce68b634"
          }
        },
        "2416c0e5591a4ee29ab46e708b003dbd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4afaf9ff404d4482ba55cef5211fe0a3",
            "placeholder": "​",
            "style": "IPY_MODEL_23ebc32d938e47f38284a8882c36590f",
            "value": "Downloading spiece.model: 100%"
          }
        },
        "3924a3c6e80349308510e5ae91a89c60": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ff3da6b3eaed42a1a3700734ebeb6bed",
            "max": 791656,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1155f945fb7f433a82ec074a803fe030",
            "value": 791656
          }
        },
        "e13752b7beeb403c9073faf1fdc9f7b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_19aadf810c6a4018825806f2a8517326",
            "placeholder": "​",
            "style": "IPY_MODEL_d1418033f6e74fb5bd6d4d459f416133",
            "value": " 773k/773k [00:00&lt;00:00, 962kB/s]"
          }
        },
        "ff0836c31f9f44dba10f23a8ce68b634": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4afaf9ff404d4482ba55cef5211fe0a3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "23ebc32d938e47f38284a8882c36590f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ff3da6b3eaed42a1a3700734ebeb6bed": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1155f945fb7f433a82ec074a803fe030": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "19aadf810c6a4018825806f2a8517326": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d1418033f6e74fb5bd6d4d459f416133": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6a5499de06a24397b029f88251d6ba7f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e95a6a03a2e843fd996f22c26dea2323",
              "IPY_MODEL_fefb64b77d6a49cb8db89be9a5fec440",
              "IPY_MODEL_e2bbeb7879924d7bbdfb4d13ec463ebd"
            ],
            "layout": "IPY_MODEL_dbff55e92edb4320bc25185ad990d234"
          }
        },
        "e95a6a03a2e843fd996f22c26dea2323": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3d227249fd174e419db05655cc80a1a6",
            "placeholder": "​",
            "style": "IPY_MODEL_0f88fb466e4a4a3da44f1c5e1ff1714e",
            "value": "Downloading tokenizer.json: 100%"
          }
        },
        "fefb64b77d6a49cb8db89be9a5fec440": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2f53e8a5ca0b4e6c9ccf737b43542f47",
            "max": 1389353,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a3ea20a734f941a2a59a6538e0bc4897",
            "value": 1389353
          }
        },
        "e2bbeb7879924d7bbdfb4d13ec463ebd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aba64c855048455f99f0dec51c334370",
            "placeholder": "​",
            "style": "IPY_MODEL_a8785c2ca66e47d187d034d8ae1bb738",
            "value": " 1.32M/1.32M [00:00&lt;00:00, 3.85MB/s]"
          }
        },
        "dbff55e92edb4320bc25185ad990d234": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3d227249fd174e419db05655cc80a1a6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0f88fb466e4a4a3da44f1c5e1ff1714e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2f53e8a5ca0b4e6c9ccf737b43542f47": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a3ea20a734f941a2a59a6538e0bc4897": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "aba64c855048455f99f0dec51c334370": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a8785c2ca66e47d187d034d8ae1bb738": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}